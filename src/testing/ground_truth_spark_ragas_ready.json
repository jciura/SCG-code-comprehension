{
  "test_suite": "RAG System - Spark Project",
  "version": "2.0",
  "created": "2025-11-12",
  "description": "Test suite with actual code fragments in ground_truth_contexts",
  "questions": [
    {
      "id": "Q001",
      "question": "Describe ContextWaiter class",
      "category": "specific",
      "ground_truth_contexts": [
        "private[streaming] class ContextWaiter { private val lock = new ReentrantLock() private val condition = lock.newCondition() // Guarded by \"lock\" private var error: Throwable = null // Guarded by \"lock\" private var stopped: Boolean = false def notifyError(e: Throwable): Unit = { lock.lock() try { error = e condition.signalAll() } finally { lock.unlock() } } def notifyStop(): Unit = { lock.lock() try { stopped = true condition.signalAll() } finally { lock.unlock() } } /** * Return `true` if it's stopped; or throw the reported error if `notifyError` has been called; or * `false` if the waiting time detectably elapsed before return from the method. */ def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }",
        "*/ class StreamingContext private[streaming] ( _sc: SparkContext, _cp: Checkpoint, _batchDur: Duration ) extends Logging { /** * Create a StreamingContext using an existing SparkContext. * @param sparkContext existing SparkContext * @param batchDuration the time interval at which streaming data will be divided into batches */ def this(sparkContext: SparkContext, batchDuration: Duration) = { this(sparkContext, null, batchDuration) } /** * Create a StreamingContext by providing the configuration necessary for a new SparkContext. * @param conf a org.apache.spark.SparkConf object specifying Spark parameters * @param batchDuration the time interval at which streaming data will be divided into batches */ def this(conf: SparkConf, batchDuration: Duration) = { this(StreamingContext.createNewSparkContext(conf), null, batchDuration) } /** * Create a StreamingContext by providing the details necessary for creating a new SparkContext. * @param master cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName a name for your job, to display on the cluster web UI * @param batchDuration the time interval at which streaming data will be divided into batches */ def this( master: String, appName: String, batchDuration: Duration, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(StreamingContext.createNewSparkContext(master, appName, sparkHome, jars, environment), null, batchDuration) } /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory * @param hadoopConf Optional, configuration object if necessary for reading from * HDFS compatible filesystems */ def this(path: String, hadoopConf: Configuration) = this(null, CheckpointReader.read(path, new SparkConf(), hadoopConf).orNull, null) /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory */ def this(path: String) = this(path, SparkHadoopUtil.get.conf) /** * Recreate a StreamingContext from a checkpoint file using an existing SparkContext. * @param path Path to the directory that was specified as the checkpoint directory * @param sparkContext Existing SparkContext */ def this(path: String, sparkContext: SparkContext) = { this( sparkContext, CheckpointReader.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull, null) } require(_sc != null || _cp != null, \"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\") private[streaming] val isCheckpointPresent: Boolean = _cp != null private[streaming] val sc: SparkContext = { if (_sc != null) { _sc } else if (isCheckpointPresent) { SparkContext.getOrCreate(_cp.createSparkConf()) } else { throw new SparkException(\"Cannot create StreamingContext without a SparkContext\") } } if (sc.conf.get(\"spark.master\") == \"local\" || sc.conf.get(\"spark.master\") == \"local[1]\") { logWarning(\"spark.master should be set as local[n], n > 1 in local mode if you have receivers\" + \" to get data, otherwise Spark jobs will not get resources to process the received data.\") } private[streaming] val conf = sc.conf private[streaming] val env = sc.env private[streaming] val graph: DStreamGraph = { if (isCheckpointPresent) { _cp.graph.setContext(this) _cp.graph.restoreCheckpointData() _cp.graph } else { require(_batchDur != null, \"Batch duration for StreamingContext cannot be null\") val newGraph = new DStreamGraph() newGraph.setBatchDuration(_batchDur) newGraph } } private val nextInputStreamId = new AtomicInteger(0) private[streaming] var checkpointDir: String = { if (isCheckpointPresent) { sc.setCheckpointDir(_cp.checkpointDir) _cp.checkpointDir } else { null } } private[streaming] val checkpointDuration: Duration = { if (isCheckpointPresent) _cp.checkpointDuration else graph.batchDuration } private[streaming] val scheduler = new JobScheduler(this) private[streaming] val waiter = new ContextWaiter private[streaming] val progressListener = new StreamingJobProgressListener(this) private[streaming] val uiTab: Option[StreamingTab] = sparkContext.ui match { case Some(ui) => Some(new StreamingTab(this, ui)) case None => None } /* Initializing a streamingSource to register metrics */ private val streamingSource = new StreamingSource(this) private var state: StreamingContextState = INITIALIZED private val startSite = new AtomicReference[CallSite](null) // Copy of thread-local properties from SparkContext. These properties will be set in all tasks // submitted by this StreamingContext after start. private[streaming] val savedProperties = new AtomicReference[Properties](new Properties) private[streaming] def getStartSite(): CallSite = startSite.get() private var shutdownHookRef: AnyRef = _ conf.getOption(\"spark.streaming.checkpoint.directory\").foreach(checkpoint) /** * Return the associated Spark context */ def sparkContext: SparkContext = sc /** * Set each DStream in this context to remember RDDs it generated in the last given duration. * DStreams remember RDDs only for a limited duration of time and release them for garbage * collection. This method allows the developer to specify how long to remember the RDDs ( * if the developer wishes to query old data outside the DStream computation). * @param duration Minimum duration that each DStream should remember its RDDs */ def remember(duration: Duration): Unit = { graph.remember(duration) } /** * Set the context to periodically checkpoint the DStream operations for driver * fault-tolerance. * @param directory HDFS-compatible directory where the checkpoint data will be reliably stored. * Note that this must be a fault-tolerant file system like HDFS. */ def checkpoint(directory: String): Unit = { if (directory != null) { val path = new Path(directory) val fs = path.getFileSystem(sparkContext.hadoopConfiguration) fs.mkdirs(path) val fullPath = fs.getFileStatus(path).getPath().toString sc.setCheckpointDir(fullPath) checkpointDir = fullPath } else { checkpointDir = null } } private[streaming] def isCheckpointingEnabled: Boolean = { checkpointDir != null } private[streaming] def initialCheckpoint: Checkpoint = { if (isCheckpointPresent) _cp else null } private[streaming] def getNewInputStreamId() = nextInputStreamId.getAndIncrement() /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body. */ private[streaming] def withScope[U](body: => U): U = sparkContext.withScope(body) /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body. */ private[streaming] def withNamedScope[U](name: String)(body: => U): U = { RDDOperationScope.withScope(sc, name, allowNesting = false, ignoreParent = false)(body) } /** * Create an input stream with any arbitrary user implemented receiver. * Find more details at https://spark.apache.org/docs/latest/streaming-custom-receivers.html * @param receiver Custom implementation of Receiver */ def receiverStream[T: ClassTag](receiver: Receiver[T]): ReceiverInputDStream[T] = { withNamedScope(\"receiver stream\") { new PluggableInputDStream[T](this, receiver) } } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes is interpreted as UTF8 encoded `\\n` delimited * lines. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @see [[socketStream]] */ def socketTextStream( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[String] = withNamedScope(\"socket text stream\") { socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel) } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes it interpreted as object using the given * converter. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param converter Function to convert the byte stream to objects * @param storageLevel Storage level to use for storing the received objects * @tparam T Type of the objects received (after converting bytes to objects) */ def socketStream[T: ClassTag]( hostname: String, port: Int, converter: (InputStream) => Iterator[T], storageLevel: StorageLevel ): ReceiverInputDStream[T] = { new SocketInputDStream[T](this, hostname, port, converter, storageLevel) } /** * Create an input stream from network source hostname:port, where data is received * as serialized blocks (serialized using the Spark's serializer) that can be directly * pushed into the block manager without deserializing them. This is the most efficient * way to receive data. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @tparam T Type of the objects in the received blocks */ def rawSocketStream[T: ClassTag]( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[T] = withNamedScope(\"raw socket stream\") { new RawInputDStream[T](this, hostname, port, storageLevel) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file */ def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file */ def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @param conf Hadoop configuration * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file */ def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean, conf: Configuration): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly, Option(conf)) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as text files (using key as LongWritable, value * as Text and input format as TextInputFormat). Files must be written to the * monitored directory by \"moving\" them from another location within the same * file system. File names starting with . are ignored. * The text files must be encoded as UTF-8. * * @param directory HDFS directory to monitor for new file */ def textFileStream(directory: String): DStream[String] = withNamedScope(\"text file stream\") { fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as flat binary files, assuming a fixed length per record, * generating one byte array per record. Files must be written to the monitored directory * by \"moving\" them from another location within the same file system. File names * starting with . are ignored. * * @param directory HDFS directory to monitor for new file * @param recordLength length of each record in bytes * * @note We ensure that the byte array for each record in the * resulting RDDs of the DStream has the provided record length. */ def binaryRecordsStream( directory: String, recordLength: Int): DStream[Array[Byte]] = withNamedScope(\"binary records stream\") { val conf = _sc.hadoopConfiguration conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = fileStream[LongWritable, BytesWritable, FixedLengthBinaryInputFormat]( directory, FileInputDStream.defaultFilter: Path => Boolean, newFilesOnly = true, conf) br.map { case (k, v) => val bytes = v.copyBytes() require(bytes.length == recordLength, \"Byte array does not have correct length. \" + s\"${bytes.length} did not equal recordLength: $recordLength\") bytes } } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing. */ def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean = true ): InputDStream[T] = { queueStream(queue, oneAtATime, sc.makeRDD(Seq.empty[T], 1)) } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @param defaultRDD Default RDD is returned by the DStream when the queue is empty. * Set as null if no RDD should be returned when empty * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing. */ def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean, defaultRDD: RDD[T] ): InputDStream[T] = { new QueueInputDStream(this, queue, oneAtATime, defaultRDD) } /** * Create a unified DStream from multiple DStreams of the same type and same slide duration. */ def union[T: ClassTag](streams: Seq[DStream[T]]): DStream[T] = withScope { new UnionDStream[T](streams.toArray) } /** * Create a new DStream in which each RDD is generated by applying a function on RDDs of * the DStreams. */ def transform[T: ClassTag]( dstreams: Seq[DStream[_]], transformFunc: (Seq[RDD[_]], Time) => RDD[T] ): DStream[T] = withScope { new TransformedDStream[T](dstreams, sparkContext.clean(transformFunc)) } /** * Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for * receiving system events related to streaming. */ def addStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.addListener(streamingListener) } def removeStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.removeListener(streamingListener) } private def validate(): Unit = { assert(graph != null, \"Graph is null\") graph.validate() require( !isCheckpointingEnabled || checkpointDuration != null, \"Checkpoint directory has been set, but the graph checkpointing interval has \" + \"not been set. Please use StreamingContext.checkpoint() to set the interval.\" ) // Verify whether the DStream checkpoint is serializable if (isCheckpointingEnabled) { val checkpoint = new Checkpoint(this, Time(0)) try { Checkpoint.serialize(checkpoint, conf) } catch { case e: NotSerializableException => throw new NotSerializableException( \"DStream checkpointing has been enabled but the DStreams with their functions \" + \"are not serializable\\n\" + SerializationDebugger.improveException(checkpoint, e).getMessage() ) } } if (Utils.isDynamicAllocationEnabled(sc.conf) || ExecutorAllocationManager.isDynamicAllocationEnabled(conf)) { logWarning(\"Dynamic Allocation is enabled for this application. \" + \"Enabling Dynamic allocation for Spark Streaming applications can cause data loss if \" + \"Write Ahead Log is not enabled for non-replayable sources. \" + \"See the programming guide for details on how to enable the Write Ahead Log.\") } } /** * :: DeveloperApi :: * * Return the current state of the context. The context can be in three possible states - * * - StreamingContextState.INITIALIZED - The context has been created, but not started yet. * Input DStreams, transformations and output operations can be created on the context. * - StreamingContextState.ACTIVE - The context has been started, and not stopped. * Input DStreams, transformations and output operations cannot be created on the context. * - StreamingContextState.STOPPED - The context has been stopped and cannot be used any more. */ @DeveloperApi def getState(): StreamingContextState = synchronized { state } /** * Start the execution of the streams. * * @throws IllegalStateException if the StreamingContext is already stopped. */ def start(): Unit = synchronized { state match { case INITIALIZED => startSite.set(DStream.getCreationSite()) StreamingContext.ACTIVATION_LOCK.synchronized { StreamingContext.assertNoOtherContextIsActive() try { validate() registerProgressListener() // Start the streaming scheduler in a new thread, so that thread local properties // like call sites and job groups can be reset without affecting those of the // current thread. ThreadUtils.runInNewThread(\"streaming-start\") { sparkContext.setCallSite(startSite.get) sparkContext.clearJobGroup() sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\") savedProperties.set(Utils.cloneProperties(sparkContext.localProperties.get())) scheduler.start() } state = StreamingContextState.ACTIVE scheduler.listenerBus.post( StreamingListenerStreamingStarted(System.currentTimeMillis())) } catch { case NonFatal(e) => logError(\"Error starting the context, marking it as stopped\", e) scheduler.stop(false) state = StreamingContextState.STOPPED throw e } StreamingContext.setActiveContext(this) } logDebug(\"Adding shutdown hook\") // force eager creation of logger shutdownHookRef = ShutdownHookManager.addShutdownHook( StreamingContext.SHUTDOWN_HOOK_PRIORITY)(() => stopOnShutdown()) // Registering Streaming Metrics at the start of the StreamingContext assert(env.metricsSystem != null) env.metricsSystem.registerSource(streamingSource) uiTab.foreach(_.attach()) logInfo(\"StreamingContext started\") case ACTIVE => logWarning(\"StreamingContext has already been started\") case STOPPED => throw new IllegalStateException(\"StreamingContext has already been stopped\") } } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread. */ def awaitTermination(): Unit = { waiter.waitForStopOrError() } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread. * * @param timeout time to wait in milliseconds * @return `true` if it's stopped; or throw the reported error during the execution; or `false` * if the waiting time elapsed before returning from the method. */ def awaitTerminationOrTimeout(timeout: Long): Boolean = { waiter.waitForStopOrError(timeout) } /** * Stop the execution of the streams immediately (does not wait for all received data * to be processed). By default, if `stopSparkContext` is not specified, the underlying * SparkContext will also be stopped. This implicit behavior can be configured using the * SparkConf configuration spark.streaming.stopSparkContextByDefault. * * @param stopSparkContext If true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started. */ def stop( stopSparkContext: Boolean = conf.getBoolean(\"spark.streaming.stopSparkContextByDefault\", true) ): Unit = synchronized { stop(stopSparkContext, false) } /** * Stop the execution of the streams, with option of ensuring all received data * has been processed. * * @param stopSparkContext if true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started. * @param stopGracefully if true, stops gracefully by waiting for the processing of all * received data to be completed */ def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit = { var shutdownHookRefToRemove: AnyRef = null if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop StreamingContext within listener bus thread.\") } synchronized { // The state should always be Stopped after calling `stop()`, even if we haven't started yet state match { case INITIALIZED => logWarning(\"StreamingContext has not been started yet\") state = STOPPED case STOPPED => logWarning(\"StreamingContext has already been stopped\") state = STOPPED case ACTIVE => // It's important that we don't set state = STOPPED until the very end of this case, // since we need to ensure that we're still able to call `stop()` to recover from // a partially-stopped StreamingContext which resulted from this `stop()` call being // interrupted. See SPARK-12001 for more details. Because the body of this case can be // executed twice in the case of a partial stop, all methods called here need to be // idempotent. Utils.tryLogNonFatalError { scheduler.stop(stopGracefully) } // Removing the streamingSource to de-register the metrics on stop() Utils.tryLogNonFatalError { env.metricsSystem.removeSource(streamingSource) } Utils.tryLogNonFatalError { uiTab.foreach(_.detach()) } Utils.tryLogNonFatalError { unregisterProgressListener() } StreamingContext.setActiveContext(null) Utils.tryLogNonFatalError { waiter.notifyStop() } if (shutdownHookRef != null) { shutdownHookRefToRemove = shutdownHookRef shutdownHookRef = null } logInfo(\"StreamingContext stopped successfully\") state = STOPPED } } if (shutdownHookRefToRemove != null) { ShutdownHookManager.removeShutdownHook(shutdownHookRefToRemove) } // Even if we have already stopped, we still need to attempt to stop the SparkContext because // a user might stop(stopSparkContext = false) and then call stop(stopSparkContext = true). if (stopSparkContext) sc.stop() } private def stopOnShutdown(): Unit = { val stopGracefully = conf.get(STOP_GRACEFULLY_ON_SHUTDOWN) logInfo(s\"Invoking stop(stopGracefully=$stopGracefully) from shutdown hook\") // Do not stop SparkContext, let its own shutdown hook stop it stop(stopSparkContext = false, stopGracefully = stopGracefully) } private def registerProgressListener(): Unit = { addStreamingListener(progressListener) sc.addSparkListener(progressListener) sc.ui.foreach(_.setStreamingJobProgressListener(progressListener)) } private def unregisterProgressListener(): Unit = { removeStreamingListener(progressListener) sc.removeSparkListener(progressListener) sc.ui.foreach(_.clearStreamingJobProgressListener()) } } /** * StreamingContext object contains a number of utility functions related to the * StreamingContext class. */ object StreamingContext extends Logging { /** * Lock that guards activation of a StreamingContext as well as access to the singleton active * StreamingContext in getActiveOrCreate(). */ private val ACTIVATION_LOCK = new Object() private val SHUTDOWN_HOOK_PRIORITY = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY + 1 private val activeContext = new AtomicReference[StreamingContext](null) private def assertNoOtherContextIsActive(): Unit = { ACTIVATION_LOCK.synchronized { if (activeContext.get() != null) { throw new IllegalStateException( \"Only one StreamingContext may be started in this JVM. \" + \"Currently running StreamingContext was started at\" + activeContext.get.getStartSite().longForm) } } } private def setActiveContext(ssc: StreamingContext): Unit = { ACTIVATION_LOCK.synchronized { activeContext.set(ssc) } } /** * Get the currently active context, if there is one. Active means started but not stopped. */ def getActive(): Option[StreamingContext] = { ACTIVATION_LOCK.synchronized { Option(activeContext.get()) } } /** * Either return the \"active\" StreamingContext (that is, started but not stopped), or create a * new StreamingContext that is * @param creatingFunc Function to create a new StreamingContext */ def getActiveOrCreate(creatingFunc: () => StreamingContext): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { creatingFunc() } } } /** * Either get the currently active StreamingContext (that is, started but not stopped), * OR recreate a StreamingContext from checkpoint data in the given path. If checkpoint data * does not exist in the provided, then create a new StreamingContext by calling the provided * `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error. */ def getActiveOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { getOrCreate(checkpointPath, creatingFunc, hadoopConf, createOnError) } } } /** * Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. * If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be * recreated from the checkpoint data. If the data does not exist, then the StreamingContext * will be created by called the provided `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error. */ def getOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), hadoopConf, createOnError) checkpointOption.map(new StreamingContext(null, _, null)).getOrElse(creatingFunc()) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to StreamingContext. */ def jarOfClass(cls: Class[_]): Option[String] = SparkContext.jarOfClass(cls) private[streaming] def createNewSparkContext(conf: SparkConf): SparkContext = { new SparkContext(conf) } private[streaming] def createNewSparkContext( master: String, appName: String, sparkHome: String, jars: Seq[String], environment: Map[String, String] ): SparkContext = { val conf = SparkContext.updatedConf( new SparkConf(), master, appName, sparkHome, jars, environment) new SparkContext(conf) } private[streaming] def rddToFileName[T](prefix: String, suffix: String, time: Time): String = { var result = time.milliseconds.toString if (prefix != null && prefix.length > 0) { result = s\"$prefix-$result\" } if (suffix != null && suffix.length > 0) { result = s\"$result.$suffix\" } result } } private class StreamingContextPythonHelper { /** * This is a private method only for Python to implement `getOrCreate`. */ def tryRecoverFromCheckpoint(checkpointPath: String): Option[StreamingContext] = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), SparkHadoopUtil.get.conf, ignoreReadError = false) checkpointOption.map(new StreamingContext(null, _, null)) } }",
        "*/ def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }"
      ],
      "key_entities": [
        "org/apache/spark/streaming/ContextWaiter#",
        "org/apache/spark/streaming/StreamingContext#",
        "org/apache/spark/streaming/ContextWaiter#waitForStopOrError()."
      ],
      "claude_stats": {
        "answer": "The `ContextWaiter` class is a private streaming utility class that manages synchronization and signaling for stopping or error conditions in a streaming context.\n\nThe class maintains three key fields:\n- `lock` - a `ReentrantLock` for thread synchronization\n- `condition` - a `Condition` object created from the lock for thread coordination\n- `error` - a `Throwable` that stores any reported error (guarded by lock)\n- `stopped` - a `Boolean` flag indicating whether the context has stopped (guarded by lock)\n\nThe class provides three core methods:\n\n**`notifyError(e: Throwable)`** - Acquires the lock, stores the provided error in the `error` field, signals all waiting threads via `condition.signalAll()`, and releases the lock.\n\n**`notifyStop()`** - Acquires the lock, sets the `stopped` flag to true, signals all waiting threads, and releases the lock.\n\n**`waitForStopOrError(timeout: Long = -1)`** - Blocks the calling thread until either the context stops or an error occurs. It accepts an optional timeout in milliseconds (defaults to -1 for no timeout). If no timeout is specified, it waits indefinitely until `stopped` is true or `error` is non-null. With a timeout, it waits for the specified duration. If an error was reported, it throws that error. Returns `true` if stopped, or `false` if the timeout elapsed without stopping or error.\n\nBased on the `StreamingContext` code, I can see that `ContextWaiter` is instantiated as `private[streaming] val waiter = new ContextWaiter` and is used to coordinate the shutdown process and allow waiting for context termination.",
        "prompt_time": 0.253,
        "answer_time": 22,
        "tokens": 402,
        "context_tokens": 8153,
        "used_context": [
          "## CLASS: org/apache/spark/streaming/ContextWaiter#\nprivate[streaming] class ContextWaiter { private val lock = new ReentrantLock() private val condition = lock.newCondition() // Guarded by \"lock\" private var error: Throwable = null // Guarded by \"lock\" private var stopped: Boolean = false def notifyError(e: Throwable): Unit = { lock.lock() try { error = e condition.signalAll() } finally { lock.unlock() } } def notifyStop(): Unit = { lock.lock() try { stopped = true condition.signalAll() } finally { lock.unlock() } } /** * Return `true` if it's stopped; or throw the reported error if `notifyError` has been called; or * `false` if the waiting time detectably elapsed before return from the method.  def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }",
          "## CLASS: org/apache/spark/streaming/StreamingContext#\n class StreamingContext private[streaming] ( _sc: SparkContext, _cp: Checkpoint, _batchDur: Duration ) extends Logging { /** * Create a StreamingContext using an existing SparkContext. * @param sparkContext existing SparkContext * @param batchDuration the time interval at which streaming data will be divided into batches  def this(sparkContext: SparkContext, batchDuration: Duration) = { this(sparkContext, null, batchDuration) } /** * Create a StreamingContext by providing the configuration necessary for a new SparkContext. * @param conf a org.apache.spark.SparkConf object specifying Spark parameters * @param batchDuration the time interval at which streaming data will be divided into batches  def this(conf: SparkConf, batchDuration: Duration) = { this(StreamingContext.createNewSparkContext(conf), null, batchDuration) } /** * Create a StreamingContext by providing the details necessary for creating a new SparkContext. * @param master cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName a name for your job, to display on the cluster web UI * @param batchDuration the time interval at which streaming data will be divided into batches  def this( master: String, appName: String, batchDuration: Duration, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(StreamingContext.createNewSparkContext(master, appName, sparkHome, jars, environment), null, batchDuration) } /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory * @param hadoopConf Optional, configuration object if necessary for reading from * HDFS compatible filesystems  def this(path: String, hadoopConf: Configuration) = this(null, CheckpointReader.read(path, new SparkConf(), hadoopConf).orNull, null) /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory  def this(path: String) = this(path, SparkHadoopUtil.get.conf) /** * Recreate a StreamingContext from a checkpoint file using an existing SparkContext. * @param path Path to the directory that was specified as the checkpoint directory * @param sparkContext Existing SparkContext  def this(path: String, sparkContext: SparkContext) = { this( sparkContext, CheckpointReader.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull, null) } require(_sc != null || _cp != null, \"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\") private[streaming] val isCheckpointPresent: Boolean = _cp != null private[streaming] val sc: SparkContext = { if (_sc != null) { _sc } else if (isCheckpointPresent) { SparkContext.getOrCreate(_cp.createSparkConf()) } else { throw new SparkException(\"Cannot create StreamingContext without a SparkContext\") } } if (sc.conf.get(\"spark.master\") == \"local\" || sc.conf.get(\"spark.master\") == \"local[1]\") { logWarning(\"spark.master should be set as local[n], n > 1 in local mode if you have receivers\" + \" to get data, otherwise Spark jobs will not get resources to process the received data.\") } private[streaming] val conf = sc.conf private[streaming] val env = sc.env private[streaming] val graph: DStreamGraph = { if (isCheckpointPresent) { _cp.graph.setContext(this) _cp.graph.restoreCheckpointData() _cp.graph } else { require(_batchDur != null, \"Batch duration for StreamingContext cannot be null\") val newGraph = new DStreamGraph() newGraph.setBatchDuration(_batchDur) newGraph } } private val nextInputStreamId = new AtomicInteger(0) private[streaming] var checkpointDir: String = { if (isCheckpointPresent) { sc.setCheckpointDir(_cp.checkpointDir) _cp.checkpointDir } else { null } } private[streaming] val checkpointDuration: Duration = { if (isCheckpointPresent) _cp.checkpointDuration else graph.batchDuration } private[streaming] val scheduler = new JobScheduler(this) private[streaming] val waiter = new ContextWaiter private[streaming] val progressListener = new StreamingJobProgressListener(this) private[streaming] val uiTab: Option[StreamingTab] = sparkContext.ui match { case Some(ui) => Some(new StreamingTab(this, ui)) case None => None } /* Initializing a streamingSource to register metrics  private val streamingSource = new StreamingSource(this) private var state: StreamingContextState = INITIALIZED private val startSite = new AtomicReference[CallSite](null) // Copy of thread-local properties from SparkContext. These properties will be set in all tasks // submitted by this StreamingContext after start. private[streaming] val savedProperties = new AtomicReference[Properties](new Properties) private[streaming] def getStartSite(): CallSite = startSite.get() private var shutdownHookRef: AnyRef = _ conf.getOption(\"spark.streaming.checkpoint.directory\").foreach(checkpoint) /** * Return the associated Spark context  def sparkContext: SparkContext = sc /** * Set each DStream in this context to remember RDDs it generated in the last given duration. * DStreams remember RDDs only for a limited duration of time and release them for garbage * collection. This method allows the developer to specify how long to remember the RDDs ( * if the developer wishes to query old data outside the DStream computation). * @param duration Minimum duration that each DStream should remember its RDDs  def remember(duration: Duration): Unit = { graph.remember(duration) } /** * Set the context to periodically checkpoint the DStream operations for driver * fault-tolerance. * @param directory HDFS-compatible directory where the checkpoint data will be reliably stored. * Note that this must be a fault-tolerant file system like HDFS.  def checkpoint(directory: String): Unit = { if (directory != null) { val path = new Path(directory) val fs = path.getFileSystem(sparkContext.hadoopConfiguration) fs.mkdirs(path) val fullPath = fs.getFileStatus(path).getPath().toString sc.setCheckpointDir(fullPath) checkpointDir = fullPath } else { checkpointDir = null } } private[streaming] def isCheckpointingEnabled: Boolean = { checkpointDir != null } private[streaming] def initialCheckpoint: Checkpoint = { if (isCheckpointPresent) _cp else null } private[streaming] def getNewInputStreamId() = nextInputStreamId.getAndIncrement() /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body.  private[streaming] def withScope[U](body: => U): U = sparkContext.withScope(body) /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body.  private[streaming] def withNamedScope[U](name: String)(body: => U): U = { RDDOperationScope.withScope(sc, name, allowNesting = false, ignoreParent = false)(body) } /** * Create an input stream with any arbitrary user implemented receiver. * Find more details at https://spark.apache.org/docs/latest/streaming-custom-receivers.html * @param receiver Custom implementation of Receiver  def receiverStream[T: ClassTag](receiver: Receiver[T]): ReceiverInputDStream[T] = { withNamedScope(\"receiver stream\") { new PluggableInputDStream[T](this, receiver) } } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes is interpreted as UTF8 encoded `\\n` delimited * lines. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @see [[socketStream]]  def socketTextStream( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[String] = withNamedScope(\"socket text stream\") { socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel) } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes it interpreted as object using the given * converter. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param converter Function to convert the byte stream to objects * @param storageLevel Storage level to use for storing the received objects * @tparam T Type of the objects received (after converting bytes to objects)  def socketStream[T: ClassTag]( hostname: String, port: Int, converter: (InputStream) => Iterator[T], storageLevel: StorageLevel ): ReceiverInputDStream[T] = { new SocketInputDStream[T](this, hostname, port, converter, storageLevel) } /** * Create an input stream from network source hostname:port, where data is received * as serialized blocks (serialized using the Spark's serializer) that can be directly * pushed into the block manager without deserializing them. This is the most efficient * way to receive data. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @tparam T Type of the objects in the received blocks  def rawSocketStream[T: ClassTag]( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[T] = withNamedScope(\"raw socket stream\") { new RawInputDStream[T](this, hostname, port, storageLevel) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @param conf Hadoop configuration * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean, conf: Configuration): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly, Option(conf)) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as text files (using key as LongWritable, value * as Text and input format as TextInputFormat). Files must be written to the * monitored directory by \"moving\" them from another location within the same * file system. File names starting with . are ignored. * The text files must be encoded as UTF-8. * * @param directory HDFS directory to monitor for new file  def textFileStream(directory: String): DStream[String] = withNamedScope(\"text file stream\") { fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as flat binary files, assuming a fixed length per record, * generating one byte array per record. Files must be written to the monitored directory * by \"moving\" them from another location within the same file system. File names * starting with . are ignored. * * @param directory HDFS directory to monitor for new file * @param recordLength length of each record in bytes * * @note We ensure that the byte array for each record in the * resulting RDDs of the DStream has the provided record length.  def binaryRecordsStream( directory: String, recordLength: Int): DStream[Array[Byte]] = withNamedScope(\"binary records stream\") { val conf = _sc.hadoopConfiguration conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = fileStream[LongWritable, BytesWritable, FixedLengthBinaryInputFormat]( directory, FileInputDStream.defaultFilter: Path => Boolean, newFilesOnly = true, conf) br.map { case (k, v) => val bytes = v.copyBytes() require(bytes.length == recordLength, \"Byte array does not have correct length. \" + s\"${bytes.length} did not equal recordLength: $recordLength\") bytes } } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing.  def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean = true ): InputDStream[T] = { queueStream(queue, oneAtATime, sc.makeRDD(Seq.empty[T], 1)) } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @param defaultRDD Default RDD is returned by the DStream when the queue is empty. * Set as null if no RDD should be returned when empty * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing.  def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean, defaultRDD: RDD[T] ): InputDStream[T] = { new QueueInputDStream(this, queue, oneAtATime, defaultRDD) } /** * Create a unified DStream from multiple DStreams of the same type and same slide duration.  def union[T: ClassTag](streams: Seq[DStream[T]]): DStream[T] = withScope { new UnionDStream[T](streams.toArray) } /** * Create a new DStream in which each RDD is generated by applying a function on RDDs of * the DStreams.  def transform[T: ClassTag]( dstreams: Seq[DStream[_]], transformFunc: (Seq[RDD[_]], Time) => RDD[T] ): DStream[T] = withScope { new TransformedDStream[T](dstreams, sparkContext.clean(transformFunc)) } /** * Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for * receiving system events related to streaming.  def addStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.addListener(streamingListener) } def removeStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.removeListener(streamingListener) } private def validate(): Unit = { assert(graph != null, \"Graph is null\") graph.validate() require( !isCheckpointingEnabled || checkpointDuration != null, \"Checkpoint directory has been set, but the graph checkpointing interval has \" + \"not been set. Please use StreamingContext.checkpoint() to set the interval.\" ) // Verify whether the DStream checkpoint is serializable if (isCheckpointingEnabled) { val checkpoint = new Checkpoint(this, Time(0)) try { Checkpoint.serialize(checkpoint, conf) } catch { case e: NotSerializableException => throw new NotSerializableException( \"DStream checkpointing has been enabled but the DStreams with their functions \" + \"are not serializable\\n\" + SerializationDebugger.improveException(checkpoint, e).getMessage() ) } } if (Utils.isDynamicAllocationEnabled(sc.conf) || ExecutorAllocationManager.isDynamicAllocationEnabled(conf)) { logWarning(\"Dynamic Allocation is enabled for this application. \" + \"Enabling Dynamic allocation for Spark Streaming applications can cause data loss if \" + \"Write Ahead Log is not enabled for non-replayable sources. \" + \"See the programming guide for details on how to enable the Write Ahead Log.\") } } /** * :: DeveloperApi :: * * Return the current state of the context. The context can be in three possible states - * * - StreamingContextState.INITIALIZED - The context has been created, but not started yet. * Input DStreams, transformations and output operations can be created on the context. * - StreamingContextState.ACTIVE - The context has been started, and not stopped. * Input DStreams, transformations and output operations cannot be created on the context. * - StreamingContextState.STOPPED - The context has been stopped and cannot be used any more.  @DeveloperApi def getState(): StreamingContextState = synchronized { state } /** * Start the execution of the streams. * * @throws IllegalStateException if the StreamingContext is already stopped.  def start(): Unit = synchronized { state match { case INITIALIZED => startSite.set(DStream.getCreationSite()) StreamingContext.ACTIVATION_LOCK.synchronized { StreamingContext.assertNoOtherContextIsActive() try { validate() registerProgressListener() // Start the streaming scheduler in a new thread, so that thread local properties // like call sites and job groups can be reset without affecting those of the // current thread. ThreadUtils.runInNewThread(\"streaming-start\") { sparkContext.setCallSite(startSite.get) sparkContext.clearJobGroup() sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\") savedProperties.set(Utils.cloneProperties(sparkContext.localProperties.get())) scheduler.start() } state = StreamingContextState.ACTIVE scheduler.listenerBus.post( StreamingListenerStreamingStarted(System.currentTimeMillis())) } catch { case NonFatal(e) => logError(\"Error starting the context, marking it as stopped\", e) scheduler.stop(false) state = StreamingContextState.STOPPED throw e } StreamingContext.setActiveContext(this) } logDebug(\"Adding shutdown hook\") // force eager creation of logger shutdownHookRef = ShutdownHookManager.addShutdownHook( StreamingContext.SHUTDOWN_HOOK_PRIORITY)(() => stopOnShutdown()) // Registering Streaming Metrics at the start of the StreamingContext assert(env.metricsSystem != null) env.metricsSystem.registerSource(streamingSource) uiTab.foreach(_.attach()) logInfo(\"StreamingContext started\") case ACTIVE => logWarning(\"StreamingContext has already been started\") case STOPPED => throw new IllegalStateException(\"StreamingContext has already been stopped\") } } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread.  def awaitTermination(): Unit = { waiter.waitForStopOrError() } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread. * * @param timeout time to wait in milliseconds * @return `true` if it's stopped; or throw the reported error during the execution; or `false` * if the waiting time elapsed before returning from the method.  def awaitTerminationOrTimeout(timeout: Long): Boolean = { waiter.waitForStopOrError(timeout) } /** * Stop the execution of the streams immediately (does not wait for all received data * to be processed). By default, if `stopSparkContext` is not specified, the underlying * SparkContext will also be stopped. This implicit behavior can be configured using the * SparkConf configuration spark.streaming.stopSparkContextByDefault. * * @param stopSparkContext If true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started.  def stop( stopSparkContext: Boolean = conf.getBoolean(\"spark.streaming.stopSparkContextByDefault\", true) ): Unit = synchronized { stop(stopSparkContext, false) } /** * Stop the execution of the streams, with option of ensuring all received data * has been processed. * * @param stopSparkContext if true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started. * @param stopGracefully if true, stops gracefully by waiting for the processing of all * received data to be completed  def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit = { var shutdownHookRefToRemove: AnyRef = null if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop StreamingContext within listener bus thread.\") } synchronized { // The state should always be Stopped after calling `stop()`, even if we haven't started yet state match { case INITIALIZED => logWarning(\"StreamingContext has not been started yet\") state = STOPPED case STOPPED => logWarning(\"StreamingContext has already been stopped\") state = STOPPED case ACTIVE => // It's important that we don't set state = STOPPED until the very end of this case, // since we need to ensure that we're still able to call `stop()` to recover from // a partially-stopped StreamingContext which resulted from this `stop()` call being // interrupted. See SPARK-12001 for more details. Because the body of this case can be // executed twice in the case of a partial stop, all methods called here need to be // idempotent. Utils.tryLogNonFatalError { scheduler.stop(stopGracefully) } // Removing the streamingSource to de-register the metrics on stop() Utils.tryLogNonFatalError { env.metricsSystem.removeSource(streamingSource) } Utils.tryLogNonFatalError { uiTab.foreach(_.detach()) } Utils.tryLogNonFatalError { unregisterProgressListener() } StreamingContext.setActiveContext(null) Utils.tryLogNonFatalError { waiter.notifyStop() } if (shutdownHookRef != null) { shutdownHookRefToRemove = shutdownHookRef shutdownHookRef = null } logInfo(\"StreamingContext stopped successfully\") state = STOPPED } } if (shutdownHookRefToRemove != null) { ShutdownHookManager.removeShutdownHook(shutdownHookRefToRemove) } // Even if we have already stopped, we still need to attempt to stop the SparkContext because // a user might stop(stopSparkContext = false) and then call stop(stopSparkContext = true). if (stopSparkContext) sc.stop() } private def stopOnShutdown(): Unit = { val stopGracefully = conf.get(STOP_GRACEFULLY_ON_SHUTDOWN) logInfo(s\"Invoking stop(stopGracefully=$stopGracefully) from shutdown hook\") // Do not stop SparkContext, let its own shutdown hook stop it stop(stopSparkContext = false, stopGracefully = stopGracefully) } private def registerProgressListener(): Unit = { addStreamingListener(progressListener) sc.addSparkListener(progressListener) sc.ui.foreach(_.setStreamingJobProgressListener(progressListener)) } private def unregisterProgressListener(): Unit = { removeStreamingListener(progressListener) sc.removeSparkListener(progressListener) sc.ui.foreach(_.clearStreamingJobProgressListener()) } } /** * StreamingContext object contains a number of utility functions related to the * StreamingContext class.  object StreamingContext extends Logging { /** * Lock that guards activation of a StreamingContext as well as access to the singleton active * StreamingContext in getActiveOrCreate().  private val ACTIVATION_LOCK = new Object() private val SHUTDOWN_HOOK_PRIORITY = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY + 1 private val activeContext = new AtomicReference[StreamingContext](null) private def assertNoOtherContextIsActive(): Unit = { ACTIVATION_LOCK.synchronized { if (activeContext.get() != null) { throw new IllegalStateException( \"Only one StreamingContext may be started in this JVM. \" + \"Currently running StreamingContext was started at\" + activeContext.get.getStartSite().longForm) } } } private def setActiveContext(ssc: StreamingContext): Unit = { ACTIVATION_LOCK.synchronized { activeContext.set(ssc) } } /** * Get the currently active context, if there is one. Active means started but not stopped.  def getActive(): Option[StreamingContext] = { ACTIVATION_LOCK.synchronized { Option(activeContext.get()) } } /** * Either return the \"active\" StreamingContext (that is, started but not stopped), or create a * new StreamingContext that is * @param creatingFunc Function to create a new StreamingContext  def getActiveOrCreate(creatingFunc: () => StreamingContext): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { creatingFunc() } } } /** * Either get the currently active StreamingContext (that is, started but not stopped), * OR recreate a StreamingContext from checkpoint data in the given path. If checkpoint data * does not exist in the provided, then create a new StreamingContext by calling the provided * `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error.  def getActiveOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { getOrCreate(checkpointPath, creatingFunc, hadoopConf, createOnError) } } } /** * Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. * If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be * recreated from the checkpoint data. If the data does not exist, then the StreamingContext * will be created by called the provided `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error.  def getOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), hadoopConf, createOnError) checkpointOption.map(new StreamingContext(null, _, null)).getOrElse(creatingFunc()) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to StreamingContext.  def jarOfClass(cls: Class[_]): Option[String] = SparkContext.jarOfClass(cls) private[streaming] def createNewSparkContext(conf: SparkConf): SparkContext = { new SparkContext(conf) } private[streaming] def createNewSparkContext( master: String, appName: String, sparkHome: String, jars: Seq[String], environment: Map[String, String] ): SparkContext = { val conf = SparkContext.updatedConf( new SparkConf(), master, appName, sparkHome, jars, environment) new SparkContext(conf) } private[streaming] def rddToFileName[T](prefix: String, suffix: String, time: Time): String = { var result = time.milliseconds.toString if (prefix != null && prefix.length > 0) { result = s\"$prefix-$result\" } if (suffix != null && suffix.length > 0) { result = s\"$result.$suffix\" } result } } private class StreamingContextPythonHelper { /** * This is a private method only for Python to implement `getOrCreate`.  def tryRecoverFromCheckpoint(checkpointPath: String): Option[StreamingContext] = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), SparkHadoopUtil.get.conf, ignoreReadError = false) checkpointOption.map(new StreamingContext(null, _, null)) } }",
          "## METHOD: org/apache/spark/streaming/ContextWaiter#waitForStopOrError().\ndef waitForStopOrError(timeout: Long;\n def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }\n"
        ]
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "ContextWaiter is a Scala class coordinating wait/notify for streaming context stop or error. It uses a ReentrantLock and Condition to manage state changes and waiting threads. The class provides methods to notify errors, notify stop, and wait for stop or error with optional timeout.",
          "prompt_time": 0.23,
          "answer_time": 59,
          "context_tokens": 7024,
          "used_context": [
            "## CLASS: org/apache/spark/streaming/ContextWaiter#\nprivate[streaming] class ContextWaiter { private val lock = new ReentrantLock() private val condition = lock.newCondition() // Guarded by \"lock\" private var error: Throwable = null // Guarded by \"lock\" private var stopped: Boolean = false def notifyError(e: Throwable): Unit = { lock.lock() try { error = e condition.signalAll() } finally { lock.unlock() } } def notifyStop(): Unit = { lock.lock() try { stopped = true condition.signalAll() } finally { lock.unlock() } } /** * Return `true` if it's stopped; or throw the reported error if `notifyError` has been called; or * `false` if the waiting time detectably elapsed before return from the method.  def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }",
            "## CLASS: org/apache/spark/streaming/StreamingContext#\n class StreamingContext private[streaming] ( _sc: SparkContext, _cp: Checkpoint, _batchDur: Duration ) extends Logging { /** * Create a StreamingContext using an existing SparkContext. * @param sparkContext existing SparkContext * @param batchDuration the time interval at which streaming data will be divided into batches  def this(sparkContext: SparkContext, batchDuration: Duration) = { this(sparkContext, null, batchDuration) } /** * Create a StreamingContext by providing the configuration necessary for a new SparkContext. * @param conf a org.apache.spark.SparkConf object specifying Spark parameters * @param batchDuration the time interval at which streaming data will be divided into batches  def this(conf: SparkConf, batchDuration: Duration) = { this(StreamingContext.createNewSparkContext(conf), null, batchDuration) } /** * Create a StreamingContext by providing the details necessary for creating a new SparkContext. * @param master cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName a name for your job, to display on the cluster web UI * @param batchDuration the time interval at which streaming data will be divided into batches  def this( master: String, appName: String, batchDuration: Duration, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(StreamingContext.createNewSparkContext(master, appName, sparkHome, jars, environment), null, batchDuration) } /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory * @param hadoopConf Optional, configuration object if necessary for reading from * HDFS compatible filesystems  def this(path: String, hadoopConf: Configuration) = this(null, CheckpointReader.read(path, new SparkConf(), hadoopConf).orNull, null) /** * Recreate a StreamingContext from a checkpoint file. * @param path Path to the directory that was specified as the checkpoint directory  def this(path: String) = this(path, SparkHadoopUtil.get.conf) /** * Recreate a StreamingContext from a checkpoint file using an existing SparkContext. * @param path Path to the directory that was specified as the checkpoint directory * @param sparkContext Existing SparkContext  def this(path: String, sparkContext: SparkContext) = { this( sparkContext, CheckpointReader.read(path, sparkContext.conf, sparkContext.hadoopConfiguration).orNull, null) } require(_sc != null || _cp != null, \"Spark Streaming cannot be initialized with both SparkContext and checkpoint as null\") private[streaming] val isCheckpointPresent: Boolean = _cp != null private[streaming] val sc: SparkContext = { if (_sc != null) { _sc } else if (isCheckpointPresent) { SparkContext.getOrCreate(_cp.createSparkConf()) } else { throw new SparkException(\"Cannot create StreamingContext without a SparkContext\") } } if (sc.conf.get(\"spark.master\") == \"local\" || sc.conf.get(\"spark.master\") == \"local[1]\") { logWarning(\"spark.master should be set as local[n], n > 1 in local mode if you have receivers\" + \" to get data, otherwise Spark jobs will not get resources to process the received data.\") } private[streaming] val conf = sc.conf private[streaming] val env = sc.env private[streaming] val graph: DStreamGraph = { if (isCheckpointPresent) { _cp.graph.setContext(this) _cp.graph.restoreCheckpointData() _cp.graph } else { require(_batchDur != null, \"Batch duration for StreamingContext cannot be null\") val newGraph = new DStreamGraph() newGraph.setBatchDuration(_batchDur) newGraph } } private val nextInputStreamId = new AtomicInteger(0) private[streaming] var checkpointDir: String = { if (isCheckpointPresent) { sc.setCheckpointDir(_cp.checkpointDir) _cp.checkpointDir } else { null } } private[streaming] val checkpointDuration: Duration = { if (isCheckpointPresent) _cp.checkpointDuration else graph.batchDuration } private[streaming] val scheduler = new JobScheduler(this) private[streaming] val waiter = new ContextWaiter private[streaming] val progressListener = new StreamingJobProgressListener(this) private[streaming] val uiTab: Option[StreamingTab] = sparkContext.ui match { case Some(ui) => Some(new StreamingTab(this, ui)) case None => None } /* Initializing a streamingSource to register metrics  private val streamingSource = new StreamingSource(this) private var state: StreamingContextState = INITIALIZED private val startSite = new AtomicReference[CallSite](null) // Copy of thread-local properties from SparkContext. These properties will be set in all tasks // submitted by this StreamingContext after start. private[streaming] val savedProperties = new AtomicReference[Properties](new Properties) private[streaming] def getStartSite(): CallSite = startSite.get() private var shutdownHookRef: AnyRef = _ conf.getOption(\"spark.streaming.checkpoint.directory\").foreach(checkpoint) /** * Return the associated Spark context  def sparkContext: SparkContext = sc /** * Set each DStream in this context to remember RDDs it generated in the last given duration. * DStreams remember RDDs only for a limited duration of time and release them for garbage * collection. This method allows the developer to specify how long to remember the RDDs ( * if the developer wishes to query old data outside the DStream computation). * @param duration Minimum duration that each DStream should remember its RDDs  def remember(duration: Duration): Unit = { graph.remember(duration) } /** * Set the context to periodically checkpoint the DStream operations for driver * fault-tolerance. * @param directory HDFS-compatible directory where the checkpoint data will be reliably stored. * Note that this must be a fault-tolerant file system like HDFS.  def checkpoint(directory: String): Unit = { if (directory != null) { val path = new Path(directory) val fs = path.getFileSystem(sparkContext.hadoopConfiguration) fs.mkdirs(path) val fullPath = fs.getFileStatus(path).getPath().toString sc.setCheckpointDir(fullPath) checkpointDir = fullPath } else { checkpointDir = null } } private[streaming] def isCheckpointingEnabled: Boolean = { checkpointDir != null } private[streaming] def initialCheckpoint: Checkpoint = { if (isCheckpointPresent) _cp else null } private[streaming] def getNewInputStreamId() = nextInputStreamId.getAndIncrement() /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body.  private[streaming] def withScope[U](body: => U): U = sparkContext.withScope(body) /** * Execute a block of code in a scope such that all new DStreams created in this body will * be part of the same scope. For more detail, see the comments in `doCompute`. * * Note: Return statements are NOT allowed in the given body.  private[streaming] def withNamedScope[U](name: String)(body: => U): U = { RDDOperationScope.withScope(sc, name, allowNesting = false, ignoreParent = false)(body) } /** * Create an input stream with any arbitrary user implemented receiver. * Find more details at https://spark.apache.org/docs/latest/streaming-custom-receivers.html * @param receiver Custom implementation of Receiver  def receiverStream[T: ClassTag](receiver: Receiver[T]): ReceiverInputDStream[T] = { withNamedScope(\"receiver stream\") { new PluggableInputDStream[T](this, receiver) } } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes is interpreted as UTF8 encoded `\\n` delimited * lines. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @see [[socketStream]]  def socketTextStream( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[String] = withNamedScope(\"socket text stream\") { socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel) } /** * Creates an input stream from TCP source hostname:port. Data is received using * a TCP socket and the receive bytes it interpreted as object using the given * converter. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param converter Function to convert the byte stream to objects * @param storageLevel Storage level to use for storing the received objects * @tparam T Type of the objects received (after converting bytes to objects)  def socketStream[T: ClassTag]( hostname: String, port: Int, converter: (InputStream) => Iterator[T], storageLevel: StorageLevel ): ReceiverInputDStream[T] = { new SocketInputDStream[T](this, hostname, port, converter, storageLevel) } /** * Create an input stream from network source hostname:port, where data is received * as serialized blocks (serialized using the Spark's serializer) that can be directly * pushed into the block manager without deserializing them. This is the most efficient * way to receive data. * @param hostname Hostname to connect to for receiving data * @param port Port to connect to for receiving data * @param storageLevel Storage level to use for storing the received objects * (default: StorageLevel.MEMORY_AND_DISK_SER_2) * @tparam T Type of the objects in the received blocks  def rawSocketStream[T: ClassTag]( hostname: String, port: Int, storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 ): ReceiverInputDStream[T] = withNamedScope(\"raw socket stream\") { new RawInputDStream[T](this, hostname, port, storageLevel) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them using the given key-value types and input format. * Files must be written to the monitored directory by \"moving\" them from another * location within the same file system. File names starting with . are ignored. * @param directory HDFS directory to monitor for new file * @param filter Function to filter paths to process * @param newFilesOnly Should process only new files and ignore existing files in the directory * @param conf Hadoop configuration * @tparam K Key type for reading HDFS file * @tparam V Value type for reading HDFS file * @tparam F Input format for reading HDFS file  def fileStream[ K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag ] (directory: String, filter: Path => Boolean, newFilesOnly: Boolean, conf: Configuration): InputDStream[(K, V)] = { new FileInputDStream[K, V, F](this, directory, filter, newFilesOnly, Option(conf)) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as text files (using key as LongWritable, value * as Text and input format as TextInputFormat). Files must be written to the * monitored directory by \"moving\" them from another location within the same * file system. File names starting with . are ignored. * The text files must be encoded as UTF-8. * * @param directory HDFS directory to monitor for new file  def textFileStream(directory: String): DStream[String] = withNamedScope(\"text file stream\") { fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString) } /** * Create an input stream that monitors a Hadoop-compatible filesystem * for new files and reads them as flat binary files, assuming a fixed length per record, * generating one byte array per record. Files must be written to the monitored directory * by \"moving\" them from another location within the same file system. File names * starting with . are ignored. * * @param directory HDFS directory to monitor for new file * @param recordLength length of each record in bytes * * @note We ensure that the byte array for each record in the * resulting RDDs of the DStream has the provided record length.  def binaryRecordsStream( directory: String, recordLength: Int): DStream[Array[Byte]] = withNamedScope(\"binary records stream\") { val conf = _sc.hadoopConfiguration conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = fileStream[LongWritable, BytesWritable, FixedLengthBinaryInputFormat]( directory, FileInputDStream.defaultFilter: Path => Boolean, newFilesOnly = true, conf) br.map { case (k, v) => val bytes = v.copyBytes() require(bytes.length == recordLength, \"Byte array does not have correct length. \" + s\"${bytes.length} did not equal recordLength: $recordLength\") bytes } } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing.  def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean = true ): InputDStream[T] = { queueStream(queue, oneAtATime, sc.makeRDD(Seq.empty[T], 1)) } /** * Create an input stream from a queue of RDDs. In each batch, * it will process either one or all of the RDDs returned by the queue. * * @param queue Queue of RDDs. Modifications to this data structure must be synchronized. * @param oneAtATime Whether only one RDD should be consumed from the queue in every interval * @param defaultRDD Default RDD is returned by the DStream when the queue is empty. * Set as null if no RDD should be returned when empty * @tparam T Type of objects in the RDD * * @note Arbitrary RDDs can be added to `queueStream`, there is no way to recover data of * those RDDs, so `queueStream` doesn't support checkpointing.  def queueStream[T: ClassTag]( queue: Queue[RDD[T]], oneAtATime: Boolean, defaultRDD: RDD[T] ): InputDStream[T] = { new QueueInputDStream(this, queue, oneAtATime, defaultRDD) } /** * Create a unified DStream from multiple DStreams of the same type and same slide duration.  def union[T: ClassTag](streams: Seq[DStream[T]]): DStream[T] = withScope { new UnionDStream[T](streams.toArray) } /** * Create a new DStream in which each RDD is generated by applying a function on RDDs of * the DStreams.  def transform[T: ClassTag]( dstreams: Seq[DStream[_]], transformFunc: (Seq[RDD[_]], Time) => RDD[T] ): DStream[T] = withScope { new TransformedDStream[T](dstreams, sparkContext.clean(transformFunc)) } /** * Add a [[org.apache.spark.streaming.scheduler.StreamingListener]] object for * receiving system events related to streaming.  def addStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.addListener(streamingListener) } def removeStreamingListener(streamingListener: StreamingListener): Unit = { scheduler.listenerBus.removeListener(streamingListener) } private def validate(): Unit = { assert(graph != null, \"Graph is null\") graph.validate() require( !isCheckpointingEnabled || checkpointDuration != null, \"Checkpoint directory has been set, but the graph checkpointing interval has \" + \"not been set. Please use StreamingContext.checkpoint() to set the interval.\" ) // Verify whether the DStream checkpoint is serializable if (isCheckpointingEnabled) { val checkpoint = new Checkpoint(this, Time(0)) try { Checkpoint.serialize(checkpoint, conf) } catch { case e: NotSerializableException => throw new NotSerializableException( \"DStream checkpointing has been enabled but the DStreams with their functions \" + \"are not serializable\\n\" + SerializationDebugger.improveException(checkpoint, e).getMessage() ) } } if (Utils.isDynamicAllocationEnabled(sc.conf) || ExecutorAllocationManager.isDynamicAllocationEnabled(conf)) { logWarning(\"Dynamic Allocation is enabled for this application. \" + \"Enabling Dynamic allocation for Spark Streaming applications can cause data loss if \" + \"Write Ahead Log is not enabled for non-replayable sources. \" + \"See the programming guide for details on how to enable the Write Ahead Log.\") } } /** * :: DeveloperApi :: * * Return the current state of the context. The context can be in three possible states - * * - StreamingContextState.INITIALIZED - The context has been created, but not started yet. * Input DStreams, transformations and output operations can be created on the context. * - StreamingContextState.ACTIVE - The context has been started, and not stopped. * Input DStreams, transformations and output operations cannot be created on the context. * - StreamingContextState.STOPPED - The context has been stopped and cannot be used any more.  @DeveloperApi def getState(): StreamingContextState = synchronized { state } /** * Start the execution of the streams. * * @throws IllegalStateException if the StreamingContext is already stopped.  def start(): Unit = synchronized { state match { case INITIALIZED => startSite.set(DStream.getCreationSite()) StreamingContext.ACTIVATION_LOCK.synchronized { StreamingContext.assertNoOtherContextIsActive() try { validate() registerProgressListener() // Start the streaming scheduler in a new thread, so that thread local properties // like call sites and job groups can be reset without affecting those of the // current thread. ThreadUtils.runInNewThread(\"streaming-start\") { sparkContext.setCallSite(startSite.get) sparkContext.clearJobGroup() sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\") savedProperties.set(Utils.cloneProperties(sparkContext.localProperties.get())) scheduler.start() } state = StreamingContextState.ACTIVE scheduler.listenerBus.post( StreamingListenerStreamingStarted(System.currentTimeMillis())) } catch { case NonFatal(e) => logError(\"Error starting the context, marking it as stopped\", e) scheduler.stop(false) state = StreamingContextState.STOPPED throw e } StreamingContext.setActiveContext(this) } logDebug(\"Adding shutdown hook\") // force eager creation of logger shutdownHookRef = ShutdownHookManager.addShutdownHook( StreamingContext.SHUTDOWN_HOOK_PRIORITY)(() => stopOnShutdown()) // Registering Streaming Metrics at the start of the StreamingContext assert(env.metricsSystem != null) env.metricsSystem.registerSource(streamingSource) uiTab.foreach(_.attach()) logInfo(\"StreamingContext started\") case ACTIVE => logWarning(\"StreamingContext has already been started\") case STOPPED => throw new IllegalStateException(\"StreamingContext has already been stopped\") } } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread.  def awaitTermination(): Unit = { waiter.waitForStopOrError() } /** * Wait for the execution to stop. Any exceptions that occurs during the execution * will be thrown in this thread. * * @param timeout time to wait in milliseconds * @return `true` if it's stopped; or throw the reported error during the execution; or `false` * if the waiting time elapsed before returning from the method.  def awaitTerminationOrTimeout(timeout: Long): Boolean = { waiter.waitForStopOrError(timeout) } /** * Stop the execution of the streams immediately (does not wait for all received data * to be processed). By default, if `stopSparkContext` is not specified, the underlying * SparkContext will also be stopped. This implicit behavior can be configured using the * SparkConf configuration spark.streaming.stopSparkContextByDefault. * * @param stopSparkContext If true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started.  def stop( stopSparkContext: Boolean = conf.getBoolean(\"spark.streaming.stopSparkContextByDefault\", true) ): Unit = synchronized { stop(stopSparkContext, false) } /** * Stop the execution of the streams, with option of ensuring all received data * has been processed. * * @param stopSparkContext if true, stops the associated SparkContext. The underlying SparkContext * will be stopped regardless of whether this StreamingContext has been * started. * @param stopGracefully if true, stops gracefully by waiting for the processing of all * received data to be completed  def stop(stopSparkContext: Boolean, stopGracefully: Boolean): Unit = { var shutdownHookRefToRemove: AnyRef = null if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop StreamingContext within listener bus thread.\") } synchronized { // The state should always be Stopped after calling `stop()`, even if we haven't started yet state match { case INITIALIZED => logWarning(\"StreamingContext has not been started yet\") state = STOPPED case STOPPED => logWarning(\"StreamingContext has already been stopped\") state = STOPPED case ACTIVE => // It's important that we don't set state = STOPPED until the very end of this case, // since we need to ensure that we're still able to call `stop()` to recover from // a partially-stopped StreamingContext which resulted from this `stop()` call being // interrupted. See SPARK-12001 for more details. Because the body of this case can be // executed twice in the case of a partial stop, all methods called here need to be // idempotent. Utils.tryLogNonFatalError { scheduler.stop(stopGracefully) } // Removing the streamingSource to de-register the metrics on stop() Utils.tryLogNonFatalError { env.metricsSystem.removeSource(streamingSource) } Utils.tryLogNonFatalError { uiTab.foreach(_.detach()) } Utils.tryLogNonFatalError { unregisterProgressListener() } StreamingContext.setActiveContext(null) Utils.tryLogNonFatalError { waiter.notifyStop() } if (shutdownHookRef != null) { shutdownHookRefToRemove = shutdownHookRef shutdownHookRef = null } logInfo(\"StreamingContext stopped successfully\") state = STOPPED } } if (shutdownHookRefToRemove != null) { ShutdownHookManager.removeShutdownHook(shutdownHookRefToRemove) } // Even if we have already stopped, we still need to attempt to stop the SparkContext because // a user might stop(stopSparkContext = false) and then call stop(stopSparkContext = true). if (stopSparkContext) sc.stop() } private def stopOnShutdown(): Unit = { val stopGracefully = conf.get(STOP_GRACEFULLY_ON_SHUTDOWN) logInfo(s\"Invoking stop(stopGracefully=$stopGracefully) from shutdown hook\") // Do not stop SparkContext, let its own shutdown hook stop it stop(stopSparkContext = false, stopGracefully = stopGracefully) } private def registerProgressListener(): Unit = { addStreamingListener(progressListener) sc.addSparkListener(progressListener) sc.ui.foreach(_.setStreamingJobProgressListener(progressListener)) } private def unregisterProgressListener(): Unit = { removeStreamingListener(progressListener) sc.removeSparkListener(progressListener) sc.ui.foreach(_.clearStreamingJobProgressListener()) } } /** * StreamingContext object contains a number of utility functions related to the * StreamingContext class.  object StreamingContext extends Logging { /** * Lock that guards activation of a StreamingContext as well as access to the singleton active * StreamingContext in getActiveOrCreate().  private val ACTIVATION_LOCK = new Object() private val SHUTDOWN_HOOK_PRIORITY = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY + 1 private val activeContext = new AtomicReference[StreamingContext](null) private def assertNoOtherContextIsActive(): Unit = { ACTIVATION_LOCK.synchronized { if (activeContext.get() != null) { throw new IllegalStateException( \"Only one StreamingContext may be started in this JVM. \" + \"Currently running StreamingContext was started at\" + activeContext.get.getStartSite().longForm) } } } private def setActiveContext(ssc: StreamingContext): Unit = { ACTIVATION_LOCK.synchronized { activeContext.set(ssc) } } /** * Get the currently active context, if there is one. Active means started but not stopped.  def getActive(): Option[StreamingContext] = { ACTIVATION_LOCK.synchronized { Option(activeContext.get()) } } /** * Either return the \"active\" StreamingContext (that is, started but not stopped), or create a * new StreamingContext that is * @param creatingFunc Function to create a new StreamingContext  def getActiveOrCreate(creatingFunc: () => StreamingContext): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { creatingFunc() } } } /** * Either get the currently active StreamingContext (that is, started but not stopped), * OR recreate a StreamingContext from checkpoint data in the given path. If checkpoint data * does not exist in the provided, then create a new StreamingContext by calling the provided * `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error.  def getActiveOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { ACTIVATION_LOCK.synchronized { getActive().getOrElse { getOrCreate(checkpointPath, creatingFunc, hadoopConf, createOnError) } } } /** * Either recreate a StreamingContext from checkpoint data or create a new StreamingContext. * If checkpoint data exists in the provided `checkpointPath`, then StreamingContext will be * recreated from the checkpoint data. If the data does not exist, then the StreamingContext * will be created by called the provided `creatingFunc`. * * @param checkpointPath Checkpoint directory used in an earlier StreamingContext program * @param creatingFunc Function to create a new StreamingContext * @param hadoopConf Optional Hadoop configuration if necessary for reading from the * file system * @param createOnError Optional, whether to create a new StreamingContext if there is an * error in reading checkpoint data. By default, an exception will be * thrown on error.  def getOrCreate( checkpointPath: String, creatingFunc: () => StreamingContext, hadoopConf: Configuration = SparkHadoopUtil.get.conf, createOnError: Boolean = false ): StreamingContext = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), hadoopConf, createOnError) checkpointOption.map(new StreamingContext(null, _, null)).getOrElse(creatingFunc()) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to StreamingContext.  def jarOfClass(cls: Class[_]): Option[String] = SparkContext.jarOfClass(cls) private[streaming] def createNewSparkContext(conf: SparkConf): SparkContext = { new SparkContext(conf) } private[streaming] def createNewSparkContext( master: String, appName: String, sparkHome: String, jars: Seq[String], environment: Map[String, String] ): SparkContext = { val conf = SparkContext.updatedConf( new SparkConf(), master, appName, sparkHome, jars, environment) new SparkContext(conf) } private[streaming] def rddToFileName[T](prefix: String, suffix: String, time: Time): String = { var result = time.milliseconds.toString if (prefix != null && prefix.length > 0) { result = s\"$prefix-$result\" } if (suffix != null && suffix.length > 0) { result = s\"$result.$suffix\" } result } } private class StreamingContextPythonHelper { /** * This is a private method only for Python to implement `getOrCreate`.  def tryRecoverFromCheckpoint(checkpointPath: String): Option[StreamingContext] = { val checkpointOption = CheckpointReader.read( checkpointPath, new SparkConf(), SparkHadoopUtil.get.conf, ignoreReadError = false) checkpointOption.map(new StreamingContext(null, _, null)) } }",
            "## METHOD: org/apache/spark/streaming/ContextWaiter#waitForStopOrError().\ndef waitForStopOrError(timeout: Long;\n def waitForStopOrError(timeout: Long = -1): Boolean = { lock.lock() try { if (timeout < 0) { while (!stopped && error == null) { condition.await() } } else { var nanos = TimeUnit.MILLISECONDS.toNanos(timeout) while (!stopped && error == null && nanos > 0) { nanos = condition.awaitNanos(nanos) } } // If already had error, then throw it if (error != null) throw error // already stopped or timeout stopped } finally { lock.unlock() } } }\n"
          ],
          "tokens": 55,
          "more_info": "Used same query 2 times, I didn't ask for full solution in .md file so answer is very basic"
        },
        "without_mcp": {
          "answer": "# ContextWaiter (Spark Streaming)\n\nThis document describes the internal class `org.apache.spark.streaming.ContextWaiter` found in the Spark Streaming module.\n\nAlthough `ContextWaiter` is marked as `private[streaming]` (internal API), understanding it can help contributors and advanced users reason about how `StreamingContext` coordinates shutdown and failure signaling across threads.\n\n\n## Purpose\n\n`ContextWaiter` is a small synchronization utility used by `StreamingContext` to:\n\n- Block a thread until the streaming context either stops successfully, or an error is reported from another thread.\n- Optionally bound the waiting time with a timeout.\n- Propagate the first terminal error that occurs in the streaming context to waiting threads by throwing it.\n\nIn other words, it centralizes the wait/notify logic for the StreamingContext lifecycle, so that threads can efficiently and safely wait for either a normal stop or a failure.\n\n\n## Location\n\nFile: `streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala`\n\nPackage: `org.apache.spark.streaming`\n\n\n## Design and Threading Model\n\n`ContextWaiter` uses:\n\n- `java.util.concurrent.locks.ReentrantLock` to guard its internal state.\n- A `Condition` (created from the same lock) to coordinate wait/notify among threads.\n\nIt maintains two guarded fields:\n\n- `error: Throwable`  the terminal error reported via `notifyError`. Null when no error has been reported.\n- `stopped: Boolean`  true after `notifyStop` is called; false initially.\n\nAll reads/writes to these fields happen under the same lock, ensuring visibility and safety across threads.\n\n\n## API Overview\n\n- `def notifyError(e: Throwable): Unit`\n  - Records a terminal error and signals all waiters.\n\n- `def notifyStop(): Unit`\n  - Marks the context as stopped and signals all waiters.\n\n- `def waitForStopOrError(timeout: Long = -1): Boolean`\n  - Waits until one of the following happens:\n    1) `notifyStop()` is called (normal termination), or\n    2) `notifyError(e)` is called (failure), or\n    3) The specified timeout elapses (when `timeout >= 0`).\n  - Behavior:\n    - If an error has been reported, the method throws that `Throwable`.\n    - If the context has stopped, returns `true`.\n    - If the timeout elapses without stop or error, returns `false`.\n  - Timeout units: milliseconds. A negative timeout (default `-1`) means wait indefinitely.\n\n\n## Detailed Behavior\n\n- Waiting indefinitely\n  - When `timeout < 0`, the method repeatedly calls `condition.await()` while `!stopped && error == null`.\n\n- Waiting with timeout\n  - Converts the provided millisecond `timeout` to nanoseconds and loops using `condition.awaitNanos(nanos)` while `!stopped && error == null && nanos > 0`.\n  - If the loop exits due to `nanos <= 0`, the method considers this a timeout and returns `stopped` (which will be `false` if neither stop nor error occurred).\n\n- Error precedence\n  - If `error` is non-null after waiting, the method throws the stored `Throwable` instead of returning a boolean. This ensures failures are not silently ignored by waiters.\n\n- Stop vs. timeout return value\n  - The final return value is `stopped`. After a timeout without stop or error, `stopped` remains `false` and the method returns `false`.\n\n- Signaling\n  - Both `notifyError` and `notifyStop` signal all waiting threads (`condition.signalAll()`), ensuring no waiter remains blocked once a terminal state is reached or a failure occurs.\n\n\n## Typical Usage in StreamingContext\n\n`StreamingContext` creates and owns a `ContextWaiter` instance:\n\n```scala\nprivate[streaming] val waiter = new ContextWaiter\n```\n\n- When the streaming job network stops cleanly, `StreamingContext` (or one of its coordinator threads) calls `waiter.notifyStop()`.\n- If a background thread detects a fatal error (e.g., receiver failure not recoverable, scheduler crash), it calls `waiter.notifyError(e)`.\n- External callers (e.g., `StreamingContext.awaitTermination()` variants) block by calling `waiter.waitForStopOrError(...)` and respond accordingly:\n  - Return normally when `true` is returned (stopped),\n  - Re-throw the failure if an exception is raised by `waitForStopOrError`.\n\n\n## Edge Cases and Guarantees\n\n- Idempotency\n  - Multiple calls to `notifyStop()` are safe; `stopped` simply remains `true` and waiters continue to be signaled.\n  - Multiple calls to `notifyError()` will overwrite the previous `error` reference with the latest `Throwable`. In practice, the first call typically determines the failure observed by waiters, because they wake and throw immediately.\n\n- Memory visibility\n  - Because all accesses to `stopped` and `error` are guarded by the same `ReentrantLock`, updates are visible to other threads after they acquire the lock.\n\n- Spurious wakeups\n  - The `while`-loop form around `await`/`awaitNanos` safely handles spurious wakeups.\n\n- Timeout precision\n  - Uses `awaitNanos` to reduce drift; however, like all timers, actual wake-up may be later than requested due to scheduling.\n\n\n## Minimal Example\n\nThe following pseudo-usage shows how a component might use `ContextWaiter`:\n\n```scala\nval waiter = new ContextWaiter\n\nval t = new Thread(new Runnable {\n  override def run(): Unit = {\n    try {\n      // do streaming work ...\n      // on success:\n      waiter.notifyStop()\n    } catch {\n      case t: Throwable => waiter.notifyError(t)\n    }\n  }\n})\n\nt.start()\n\ntry {\n  val stopped = waiter.waitForStopOrError(30000) // 30s\n  if (!stopped) {\n    // handle timeout\n  }\n} catch {\n  case e: Throwable =>\n    // handle failure\n}\n```\n\n\n## Source Code (Spark 3.3.0)\n\n```scala\nprivate[streaming] class ContextWaiter {\n  private val lock = new ReentrantLock()\n  private val condition = lock.newCondition()\n\n  // Guarded by \"lock\"\n  private var error: Throwable = null\n\n  // Guarded by \"lock\"\n  private var stopped: Boolean = false\n\n  def notifyError(e: Throwable): Unit = {\n    lock.lock()\n    try {\n      error = e\n      condition.signalAll()\n    } finally {\n      lock.unlock()\n    }\n  }\n\n  def notifyStop(): Unit = {\n    lock.lock()\n    try {\n      stopped = true\n      condition.signalAll()\n    } finally {\n      lock.unlock()\n    }\n  }\n\n  /**\n   * Return `true` if it's stopped; or throw the reported error if `notifyError` has been called; or\n   * `false` if the waiting time detectably elapsed before return from the method.\n   */\n  def waitForStopOrError(timeout: Long = -1): Boolean = {\n    lock.lock()\n    try {\n      if (timeout < 0) {\n        while (!stopped && error == null) {\n          condition.await()\n        }\n      } else {\n        var nanos = TimeUnit.MILLISECONDS.toNanos(timeout)\n        while (!stopped && error == null && nanos > 0) {\n          nanos = condition.awaitNanos(nanos)\n        }\n      }\n      if (error != null) throw error\n      stopped\n    } finally {\n      lock.unlock()\n    }\n  }\n}\n```\n\n\n## When to Read or Modify\n\n- Read this class when working on:\n  - `StreamingContext.awaitTermination*` methods,\n  - error propagation or shutdown sequences in Spark Streaming,\n  - thread coordination between receivers/schedulers and the driver.\n\n- Modify with care:\n  - This class is intentionally small and battle-tested. Any change should maintain the same semantics (error-first throw, proper signaling, and lock-guarded state).\n",
          "answer_time": 58,
          "tokens": 1693,
          "more_info": "Added `return full answer in .md file` at the end of question"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 3,
          "reasoning": "Answer A provides a comprehensive, well-structured explanation with detailed field descriptions, complete method signatures, parameter explanations, return value descriptions, and contextual usage information. Answer B is accurate but overly brief, lacking the depth and detail needed for a thorough understanding of the class's functionality and implementation."
        },
        "mcp_vs_no_mcp": {
          "winner": "without_mcp",
          "score_a": 4,
          "score_b": 5,
          "reasoning": "Both answers accurately describe the ContextWaiter class with correct technical details. Answer A is well-structured and covers all key aspects concisely. However, Answer B excels with superior organization using clear headings, more comprehensive coverage including file location and typical usage patterns, better formatting with code examples, and more detailed explanations of behavior like error precedence and timeout handling. Answer B provides a more complete and professionally formatted documentation-style response."
        }
      }
    },
    {
      "id": "Q002",
      "question": "Describe PythonRunner Object and all methods that use that object",
      "category": "specific",
      "ground_truth_contexts": [
        "} } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark. */ private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
        "mem.map(_ / cores) } def compute( inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): Iterator[OUT] = { val startTime = System.currentTimeMillis val env = SparkEnv.get // Get the executor cores and pyspark memory, they are passed via the local properties when // the user specified them in a ResourceProfile. val execCoresProp = Option(context.getLocalProperty(EXECUTOR_CORES_LOCAL_PROPERTY)) val memoryMb = Option(context.getLocalProperty(PYSPARK_MEMORY_LOCAL_PROPERTY)).map(_.toLong) val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\") // if OMP_NUM_THREADS is not explicitly set, override it with the number of cores if (conf.getOption(\"spark.executorEnv.OMP_NUM_THREADS\").isEmpty) { // SPARK-28843: limit the OpenMP thread pool to the number of cores assigned to this executor // this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool // see https://github.com/numpy/numpy/issues/10455 execCoresProp.foreach(envVars.put(\"OMP_NUM_THREADS\", _)) } envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread if (reuseWorker) { envVars.put(\"SPARK_REUSE_WORKER\", \"1\") } if (simplifiedTraceback) { envVars.put(\"SPARK_SIMPLIFIED_TRACEBACK\", \"1\") } // SPARK-30299 this could be wrong with standalone mode when executor // cores might not be correct because it defaults to all cores on the box. val execCores = execCoresProp.map(_.toInt).getOrElse(conf.get(EXECUTOR_CORES)) val workerMemoryMb = getWorkerMemoryMb(memoryMb, execCores) if (workerMemoryMb.isDefined) { envVars.put(\"PYSPARK_EXECUTOR_MEMORY_MB\", workerMemoryMb.get.toString) } envVars.put(\"SPARK_AUTH_SOCKET_TIMEOUT\", authSocketTimeout.toString) envVars.put(\"SPARK_BUFFER_SIZE\", bufferSize.toString) if (faultHandlerEnabled) { envVars.put(\"PYTHON_FAULTHANDLER_DIR\", BasePythonRunner.faultHandlerLogDir.toString) } val (worker: Socket, pid: Option[Int]) = env.createPythonWorker( pythonExec, envVars.asScala.toMap) // Whether is the worker released into idle pool or closed. When any codes try to release or // close a worker, they should use `releasedOrClosed.compareAndSet` to flip the state to make // sure there is only one winner that is going to release or close the worker. val releasedOrClosed = new AtomicBoolean(false) // Start a thread to feed the process input from our parent's iterator val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context) context.addTaskCompletionListener[Unit] { _ => writerThread.shutdownOnTaskCompletion() if (!reuseWorker || releasedOrClosed.compareAndSet(false, true)) { try { worker.close() } catch { case e: Exception => logWarning(\"Failed to close worker socket\", e) } } } writerThread.start() new WriterMonitorThread(SparkEnv.get, worker, writerThread, context).start() if (reuseWorker) { val key = (worker, context.taskAttemptId) // SPARK-35009: avoid creating multiple monitor threads for the same python worker // and task context if (PythonRunner.runningMonitorThreads.add(key)) { new MonitorThread(SparkEnv.get, worker, context).start() } } else { new MonitorThread(SparkEnv.get, worker, context).start() } // Return an iterator that read lines from the process's stdout val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize)) val stdoutIterator = newReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) new InterruptibleIterator(context, stdoutIterator) } protected def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): WriterThread protected def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[OUT] /** * The thread responsible for writing the data from the PythonRDD's parent iterator to the * Python process. */ abstract class WriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext) extends Thread(s\"stdout writer for $pythonExec\") { @volatile private var _exception: Throwable = null private val pythonIncludes = funcs.flatMap(_.funcs.flatMap(_.pythonIncludes.asScala)).toSet private val broadcastVars = funcs.flatMap(_.funcs.flatMap(_.broadcastVars.asScala)) setDaemon(true) /** Contains the throwable thrown while writing the parent iterator to the Python process. */ def exception: Option[Throwable] = Option(_exception) /** * Terminates the writer thread and waits for it to exit, ignoring any exceptions that may occur * due to cleanup. */ def shutdownOnTaskCompletion(): Unit = { assert(context.isCompleted) this.interrupt() // Task completion listeners that run after this method returns may invalidate // `inputIterator`. For example, when `inputIterator` was generated by the off-heap vectorized // reader, a task completion listener will free the underlying off-heap buffers. If the writer // thread is still running when `inputIterator` is invalidated, it can cause a use-after-free // bug that crashes the executor (SPARK-33277). Therefore this method must wait for the writer // thread to exit before returning. this.join() } /** * Writes a command section to the stream connected to the Python worker. */ protected def writeCommand(dataOut: DataOutputStream): Unit /** * Writes input data to the stream connected to the Python worker. */ protected def writeIteratorToStream(dataOut: DataOutputStream): Unit override def run(): Unit = Utils.logUncaughtExceptions { try { TaskContext.setTaskContext(context) val stream = new BufferedOutputStream(worker.getOutputStream, bufferSize) val dataOut = new DataOutputStream(stream) // Partition index dataOut.writeInt(partitionIndex) // Python version of driver PythonRDD.writeUTF(pythonVer, dataOut) // Init a ServerSocket to accept method calls from Python side. val isBarrier = context.isInstanceOf[BarrierTaskContext] if (isBarrier) { serverSocket = Some(new ServerSocket(/* port */ 0, /* backlog */ 1, InetAddress.getByName(\"localhost\"))) // A call to accept() for ServerSocket shall block infinitely. serverSocket.foreach(_.setSoTimeout(0)) new Thread(\"accept-connections\") { setDaemon(true) override def run(): Unit = { while (!serverSocket.get.isClosed()) { var sock: Socket = null try { sock = serverSocket.get.accept() // Wait for function call from python side. sock.setSoTimeout(10000) authHelper.authClient(sock) val input = new DataInputStream(sock.getInputStream()) val requestMethod = input.readInt() // The BarrierTaskContext function may wait infinitely, socket shall not timeout // before the function finishes. sock.setSoTimeout(0) requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => barrierAndServe(requestMethod, sock) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => val length = input.readInt() val message = new Array[Byte](length) input.readFully(message) barrierAndServe(requestMethod, sock, new String(message, UTF_8)) case _ => val out = new DataOutputStream(new BufferedOutputStream( sock.getOutputStream)) writeUTF(BarrierTaskContextMessageProtocol.ERROR_UNRECOGNIZED_FUNCTION, out) } } catch { case e: SocketException if e.getMessage.contains(\"Socket closed\") => // It is possible that the ServerSocket is not closed, but the native socket // has already been closed, we shall catch and silently ignore this case. } finally { if (sock != null) { sock.close() } } } } }.start() } val secret = if (isBarrier) { authHelper.secret } else { \"\" } // Close ServerSocket on task completion. serverSocket.foreach { server => context.addTaskCompletionListener[Unit](_ => server.close()) } val boundPort: Int = serverSocket.map(_.getLocalPort).getOrElse(0) if (boundPort == -1) { val message = \"ServerSocket failed to bind to Java side.\" logError(message) throw new SparkException(message) } else if (isBarrier) { logDebug(s\"Started ServerSocket on port $boundPort.\") } // Write out the TaskContextInfo dataOut.writeBoolean(isBarrier) dataOut.writeInt(boundPort) val secretBytes = secret.getBytes(UTF_8) dataOut.writeInt(secretBytes.length) dataOut.write(secretBytes, 0, secretBytes.length) dataOut.writeInt(context.stageId()) dataOut.writeInt(context.partitionId()) dataOut.writeInt(context.attemptNumber()) dataOut.writeLong(context.taskAttemptId()) dataOut.writeInt(context.cpus()) val resources = context.resources() dataOut.writeInt(resources.size) resources.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v.name, dataOut) dataOut.writeInt(v.addresses.size) v.addresses.foreach { case addr => PythonRDD.writeUTF(addr, dataOut) } } val localProps = context.getLocalProperties.asScala dataOut.writeInt(localProps.size) localProps.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v, dataOut) } // sparkFilesDir PythonRDD.writeUTF(SparkFiles.getRootDirectory(), dataOut) // Python includes (*.zip and *.egg files) dataOut.writeInt(pythonIncludes.size) for (include <- pythonIncludes) { PythonRDD.writeUTF(include, dataOut) } // Broadcast variables val oldBids = PythonRDD.getWorkerBroadcasts(worker) val newBids = broadcastVars.map(_.id).toSet // number of different broadcasts val toRemove = oldBids.diff(newBids) val addedBids = newBids.diff(oldBids) val cnt = toRemove.size + addedBids.size val needsDecryptionServer = env.serializerManager.encryptionEnabled && addedBids.nonEmpty dataOut.writeBoolean(needsDecryptionServer) dataOut.writeInt(cnt) def sendBidsToRemove(): Unit = { for (bid <- toRemove) { // remove the broadcast from worker dataOut.writeLong(-bid - 1) // bid >= 0 oldBids.remove(bid) } } if (needsDecryptionServer) { // if there is encryption, we setup a server which reads the encrypted files, and sends // the decrypted data to python val idsAndFiles = broadcastVars.flatMap { broadcast => if (!oldBids.contains(broadcast.id)) { Some((broadcast.id, broadcast.value.path)) } else { None } } val server = new EncryptedPythonBroadcastServer(env, idsAndFiles) dataOut.writeInt(server.port) logTrace(s\"broadcast decryption server setup on ${server.port}\") PythonRDD.writeUTF(server.secret, dataOut) sendBidsToRemove() idsAndFiles.foreach { case (id, _) => // send new broadcast dataOut.writeLong(id) oldBids.add(id) } dataOut.flush() logTrace(\"waiting for python to read decrypted broadcast data from server\") server.waitTillBroadcastDataSent() logTrace(\"done sending decrypted data to python\") } else { sendBidsToRemove() for (broadcast <- broadcastVars) { if (!oldBids.contains(broadcast.id)) { // send new broadcast dataOut.writeLong(broadcast.id) PythonRDD.writeUTF(broadcast.value.path, dataOut) oldBids.add(broadcast.id) } } } dataOut.flush() dataOut.writeInt(evalType) writeCommand(dataOut) writeIteratorToStream(dataOut) dataOut.writeInt(SpecialLengths.END_OF_STREAM) dataOut.flush() } catch { case t: Throwable if (NonFatal(t) || t.isInstanceOf[Exception]) => if (context.isCompleted || context.isInterrupted) { logDebug(\"Exception/NonFatal Error thrown after task completion (likely due to \" + \"cleanup)\", t) if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } else { // We must avoid throwing exceptions/NonFatals here, because the thread uncaught // exception handler will kill the whole executor (see // org.apache.spark.executor.Executor). _exception = t if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } } } /** * Gateway to call BarrierTaskContext methods. */ def barrierAndServe(requestMethod: Int, sock: Socket, message: String = \"\"): Unit = { require( serverSocket.isDefined, \"No available ServerSocket to redirect the BarrierTaskContext method call.\" ) val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream)) try { val messages = requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => context.asInstanceOf[BarrierTaskContext].barrier() Array(BarrierTaskContextMessageProtocol.BARRIER_RESULT_SUCCESS) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => context.asInstanceOf[BarrierTaskContext].allGather(message) } out.writeInt(messages.length) messages.foreach(writeUTF(_, out)) } catch { case e: SparkException => writeUTF(e.getMessage, out) } finally { out.close() } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } } abstract class ReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext) extends Iterator[OUT] { private var nextObj: OUT = _ private var eos = false override def hasNext: Boolean = nextObj != null || { if (!eos) { nextObj = read() hasNext } else { false } } override def next(): OUT = { if (hasNext) { val obj = nextObj nextObj = null.asInstanceOf[OUT] obj } else { Iterator.empty.next() } } /** * Reads next object from the stream. * When the stream reaches end of data, needs to process the following sections, * and then returns null. */ protected def read(): OUT protected def handleTimingData(): Unit = { // Timing data from worker val bootTime = stream.readLong() val initTime = stream.readLong() val finishTime = stream.readLong() val boot = bootTime - startTime val init = initTime - bootTime val finish = finishTime - initTime val total = finishTime - startTime logInfo(\"Times: total = %s, boot = %s, init = %s, finish = %s\".format(total, boot, init, finish)) val memoryBytesSpilled = stream.readLong() val diskBytesSpilled = stream.readLong() context.taskMetrics.incMemoryBytesSpilled(memoryBytesSpilled) context.taskMetrics.incDiskBytesSpilled(diskBytesSpilled) } protected def handlePythonException(): PythonException = { // Signals that an exception has been thrown in python val exLength = stream.readInt() val obj = new Array[Byte](exLength) stream.readFully(obj) new PythonException(new String(obj, StandardCharsets.UTF_8), writerThread.exception.getOrElse(null)) } protected def handleEndOfDataSection(): Unit = { // We've finished the data section of the output, but we can still // read some accumulator updates: val numAccumulatorUpdates = stream.readInt() (1 to numAccumulatorUpdates).foreach { _ => val updateLen = stream.readInt() val update = new Array[Byte](updateLen) stream.readFully(update) maybeAccumulator.foreach(_.add(update)) } // Check whether the worker is ready to be re-used. if (stream.readInt() == SpecialLengths.END_OF_STREAM) { if (reuseWorker && releasedOrClosed.compareAndSet(false, true)) { env.releasePythonWorker(pythonExec, envVars.asScala.toMap, worker) } } eos = true } protected val handleException: PartialFunction[Throwable, OUT] = { case e: Exception if context.isInterrupted => logDebug(\"Exception thrown after task interruption\", e) throw new TaskKilledException(context.getKillReason().getOrElse(\"unknown reason\")) case e: Exception if writerThread.exception.isDefined => logError(\"Python worker exited unexpectedly (crashed)\", e) logError(\"This may have been caused by a prior exception:\", writerThread.exception.get) throw writerThread.exception.get case eof: EOFException if faultHandlerEnabled && pid.isDefined && JavaFiles.exists(BasePythonRunner.faultHandlerLogPath(pid.get)) => val path = BasePythonRunner.faultHandlerLogPath(pid.get) val error = String.join(\"\\n\", JavaFiles.readAllLines(path)) + \"\\n\" JavaFiles.deleteIfExists(path) throw new SparkException(s\"Python worker exited unexpectedly (crashed): $error\", eof) case eof: EOFException => throw new SparkException(\"Python worker exited unexpectedly (crashed)\", eof) } } /** * It is necessary to have a monitor thread for python workers if the user cancels with * interrupts disabled. In that case we will need to explicitly kill the worker, otherwise the * threads can block indefinitely. */ class MonitorThread(env: SparkEnv, worker: Socket, context: TaskContext) extends Thread(s\"Worker Monitor for $pythonExec\") { /** How long to wait before killing the python worker if a task cannot be interrupted. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) private def monitorWorker(): Unit = { // Kill the worker if it is interrupted, checking until task completion. // TODO: This has a race condition if interruption occurs, as completed may still become true. while (!context.isInterrupted && !context.isCompleted) { Thread.sleep(2000) } if (!context.isCompleted) { Thread.sleep(taskKillTimeout) if (!context.isCompleted) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning(s\"Incomplete task $taskName interrupted: Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark. */ private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
        "} } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark. */ private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
        "override def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]] = { val runner = PythonRunner(func) runner.compute(firstParent.iterator(split, context), split.index, context) } @transient protected lazy override val isBarrier_ : Boolean = isFromBarrier || dependencies.exists(_.rdd.isBarrier()) } /** * A wrapper for a Python function, contains all necessary context to run the function in Python * runner. */ private[spark] case class PythonFunction( command: Seq[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) { def this( command: Array[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) = { this(command.toSeq, envVars, pythonIncludes, pythonExec, pythonVer, broadcastVars, accumulator) } } /** * A wrapper for chained Python functions (from bottom to top). * @param funcs */ private[spark] case class ChainedPythonFunctions(funcs: Seq[PythonFunction]) /** Thrown for exceptions in user Python code. */ private[spark] class PythonException(msg: String, cause: Throwable) extends RuntimeException(msg, cause) /** * Form an RDD[(Array[Byte], Array[Byte])] from key-value pairs returned from Python. * This is used by PySpark's shuffle operations. */ private class PairwiseRDD(prev: RDD[Array[Byte]]) extends RDD[(Long, Array[Byte])](prev) { override def getPartitions: Array[Partition] = prev.partitions override val partitioner: Option[Partitioner] = prev.partitioner override def compute(split: Partition, context: TaskContext): Iterator[(Long, Array[Byte])] = prev.iterator(split, context).grouped(2).map { case Seq(a, b) => (Utils.deserializeLongValue(a), b) case x => throw new SparkException(\"PairwiseRDD: unexpected value: \" + x) } val asJavaPairRDD : JavaPairRDD[Long, Array[Byte]] = JavaPairRDD.fromRDD(this) } private[spark] object PythonRDD extends Logging { // remember the broadcasts sent to each worker private val workerBroadcasts = new mutable.WeakHashMap[Socket, mutable.Set[Long]]() // Authentication helper used when serving iterator data. private lazy val authHelper = { val conf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf()) new SocketAuthHelper(conf) } def getWorkerBroadcasts(worker: Socket): mutable.Set[Long] = { synchronized { workerBroadcasts.getOrElseUpdate(worker, new mutable.HashSet[Long]()) } } /** * Return an RDD of values from an RDD of (Long, Array[Byte]), with preservePartitions=true * * This is useful for PySpark to have the partitioner after partitionBy() */ def valueOfPair(pair: JavaPairRDD[Long, Array[Byte]]): JavaRDD[Array[Byte]] = { pair.rdd.mapPartitions(it => it.map(_._2), true) } /** * Adapter for calling SparkContext#runJob from Python. * * This method will serve an iterator of an array that contains all elements in the RDD * (effectively a collect()), but allows you to run on a certain subset of partitions, * or to enable local execution. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ def runJob( sc: SparkContext, rdd: JavaRDD[Array[Byte]], partitions: JArrayList[Int]): Array[Any] = { type ByteArray = Array[Byte] type UnrolledPartition = Array[ByteArray] val allPartitions: Array[UnrolledPartition] = sc.runJob(rdd, (x: Iterator[ByteArray]) => x.toArray, partitions.asScala.toSeq) val flattenedPartition: UnrolledPartition = Array.concat(allPartitions: _*) serveIterator(flattenedPartition.iterator, s\"serve RDD ${rdd.id} with partitions ${partitions.asScala.mkString(\",\")}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ def collectAndServe[T](rdd: RDD[T]): Array[Any] = { serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * This method is similar with `PythonRDD.collectAndServe`, but user can specify job group id, * job description, and interruptOnCancel option. */ def collectAndServeWithJobGroup[T]( rdd: RDD[T], groupId: String, description: String, interruptOnCancel: Boolean): Array[Any] = { val sc = rdd.sparkContext sc.setJobGroup(groupId, description, interruptOnCancel) serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to create a local RDD iterator and serve it via socket. Partitions are * are collected as separate jobs, by order of index. Partition data is first requested by a * non-zero integer to start a collection job. The response is prefaced by an integer with 1 * meaning partition data will be served, 0 meaning the local iterator has been consumed, * and -1 meaning an error occurred during collection. This function is used by * pyspark.rdd._local_iterator_from_socket(). * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ def toLocalIteratorAndServe[T](rdd: RDD[T], prefetchPartitions: Boolean = false): Array[Any] = { val handleFunc = (sock: Socket) => { val out = new DataOutputStream(sock.getOutputStream) val in = new DataInputStream(sock.getInputStream) Utils.tryWithSafeFinallyAndFailureCallbacks(block = { // Collects a partition on each iteration val collectPartitionIter = rdd.partitions.indices.iterator.map { i => var result: Array[Any] = null rdd.sparkContext.submitJob( rdd, (iter: Iterator[Any]) => iter.toArray, Seq(i), // The partition we are evaluating (_, res: Array[Any]) => result = res, result) } val prefetchIter = collectPartitionIter.buffered // Write data until iteration is complete, client stops iteration, or error occurs var complete = false while (!complete) { // Read request for data, value of zero will stop iteration or non-zero to continue if (in.readInt() == 0) { complete = true } else if (prefetchIter.hasNext) { // Client requested more data, attempt to collect the next partition val partitionFuture = prefetchIter.next() // Cause the next job to be submitted if prefetchPartitions is enabled. if (prefetchPartitions) { prefetchIter.headOption } val partitionArray = ThreadUtils.awaitResult(partitionFuture, Duration.Inf) // Send response there is a partition to read out.writeInt(1) // Write the next object and signal end of data for this iteration writeIteratorToStream(partitionArray.iterator, out) out.writeInt(SpecialLengths.END_OF_DATA_SECTION) out.flush() } else { // Send response there are no more partitions to read and close out.writeInt(0) complete = true } } })(catchBlock = { // Send response that an error occurred, original exception is re-thrown out.writeInt(-1) }, finallyBlock = { out.close() in.close() }) } val server = new SocketFuncServer(authHelper, \"serve toLocalIterator\", handleFunc) Array(server.port, server.secret, server) } def readRDDFromFile( sc: JavaSparkContext, filename: String, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromFile(sc, filename, parallelism) } def readRDDFromInputStream( sc: SparkContext, in: InputStream, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromInputStream(sc, in, parallelism) } def setupBroadcast(path: String): PythonBroadcast = { new PythonBroadcast(path) } def writeIteratorToStream[T](iter: Iterator[T], dataOut: DataOutputStream): Unit = { def write(obj: Any): Unit = obj match { case null => dataOut.writeInt(SpecialLengths.NULL) case arr: Array[Byte] => dataOut.writeInt(arr.length) dataOut.write(arr) case str: String => writeUTF(str, dataOut) case stream: PortableDataStream => write(stream.toArray()) case (key, value) => write(key) write(value) case other => throw new SparkException(\"Unexpected element type \" + other.getClass) } iter.foreach(write) } /** * Create an RDD from a path using [[org.apache.hadoop.mapred.SequenceFileInputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def sequenceFile[K, V]( sc: JavaSparkContext, path: String, keyClassMaybeNull: String, valueClassMaybeNull: String, keyConverterClass: String, valueConverterClass: String, minSplits: Int, batchSize: Int): JavaRDD[Array[Byte]] = { val keyClass = Option(keyClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val valueClass = Option(valueClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val rdd = sc.sc.sequenceFile[K, V](path, kc, vc, minSplits) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration())) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map that is * passed in from Python, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def newAPIHadoopRDDFromClassNames[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration): RDD[(K, V)] = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.newAPIHadoopFile[K, V, F](path.get, fc, kc, vc, conf) } else { sc.sc.newAPIHadoopRDD[K, V, F](conf, fc, kc, vc) } } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def hadoopFile[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map * that is passed in from Python, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]]) */ def hadoopRDD[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def hadoopRDDFromClassNames[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration) = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.hadoopFile(path.get, fc, kc, vc) } else { sc.sc.hadoopRDD(new JobConf(conf), fc, kc, vc) } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(StandardCharsets.UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } /** * Create a socket server and a background thread to serve the data in `items`, * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it tries to serialize all the data in `items` * and send them into this connection. * * The thread will terminate after all the data are sent or any exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ def serveIterator(items: Iterator[_], threadName: String): Array[Any] = { serveToStream(threadName) { out => writeIteratorToStream(items, new DataOutputStream(out)) } } /** * Create a socket server and background thread to execute the writeFunc * with the given OutputStream. * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it will execute the block of code and pass in * the socket output stream. * * The thread will terminate after the block of code is executed or any * exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python. */ private[spark] def serveToStream( threadName: String)(writeFunc: OutputStream => Unit): Array[Any] = { SocketAuthServer.serveToStream(threadName, authHelper)(writeFunc) } private def getMergedConf(confAsMap: java.util.HashMap[String, String], baseConf: Configuration): Configuration = { val conf = PythonHadoopUtil.mapToConf(confAsMap) PythonHadoopUtil.mergeConfs(baseConf, conf) } private def inferKeyValueTypes[K, V, KK, VV](rdd: RDD[(K, V)], keyConverterClass: String = null, valueConverterClass: String = null): (Class[_ <: KK], Class[_ <: VV]) = { // Peek at an element to figure out key/value types. Since Writables are not serializable, // we cannot call first() on the converted RDD. Instead, we call first() on the original RDD // and then convert locally. val (key, value) = rdd.first() val (kc, vc) = getKeyValueConverters[K, V, KK, VV]( keyConverterClass, valueConverterClass, new JavaToWritableConverter) (kc.convert(key).getClass, vc.convert(value).getClass) } private def getKeyValueTypes[K, V](keyClass: String, valueClass: String): Option[(Class[K], Class[V])] = { for { k <- Option(keyClass) v <- Option(valueClass) } yield (Utils.classForName(k), Utils.classForName(v)) } private def getKeyValueConverters[K, V, KK, VV]( keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[_, _]): (Converter[K, KK], Converter[V, VV]) = { val keyConverter = Converter.getInstance(Option(keyConverterClass), defaultConverter.asInstanceOf[Converter[K, KK]]) val valueConverter = Converter.getInstance(Option(valueConverterClass), defaultConverter.asInstanceOf[Converter[V, VV]]) (keyConverter, valueConverter) } /** * Convert an RDD of key-value pairs from internal types to serializable types suitable for * output, or vice versa. */ private def convertRDD[K, V](rdd: RDD[(K, V)], keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[Any, Any]): RDD[(Any, Any)] = { val (kc, vc) = getKeyValueConverters[K, V, Any, Any](keyConverterClass, valueConverterClass, defaultConverter) PythonHadoopUtil.convertRDD(rdd, kc, vc) } /** * Output a Python RDD of key-value pairs as a Hadoop SequenceFile using the Writable types * we convert from the RDD's key and value types. Note that keys and values can't be * [[org.apache.hadoop.io.Writable]] types already, since Writables are not Java * `Serializable` and we can't peek at them. The `path` can be on any Hadoop file system. */ def saveAsSequenceFile[C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, compressionCodecClass: String): Unit = { saveAsHadoopFile( pyRDD, batchSerialized, path, \"org.apache.hadoop.mapred.SequenceFileOutputFormat\", null, null, null, null, new java.util.HashMap(), compressionCodecClass) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using old Hadoop * `OutputFormat` in mapred package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD. */ def saveAsHadoopFile[F <: OutputFormat[_, _], C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], compressionCodecClass: String): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val codec = Option(compressionCodecClass).map(Utils.classForName(_).asInstanceOf[Class[C]]) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName[F](outputFormatClass) converted.saveAsHadoopFile(path, kc, vc, fc, new JobConf(mergedConf), codec = codec) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using new Hadoop * `OutputFormat` in mapreduce package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD. */ def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String]): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName(outputFormatClass).asInstanceOf[Class[F]] converted.saveAsNewAPIHadoopFile(path, kc, vc, fc, mergedConf) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using a Hadoop conf * converted from the passed-in `confAsMap`. The conf should set relevant output params ( * e.g., output path, output format, etc), in the same way as it would be configured for * a Hadoop MapReduce job. Both old and new Hadoop OutputFormat APIs are supported * (mapred vs. mapreduce). Keys/values are converted for output using either user specified * converters or, by default, [[org.apache.spark.api.python.JavaToWritableConverter]]. */ def saveAsHadoopDataset( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, confAsMap: java.util.HashMap[String, String], keyConverterClass: String, valueConverterClass: String, useNewAPI: Boolean): Unit = { val conf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized), keyConverterClass, valueConverterClass, new JavaToWritableConverter) if (useNewAPI) { converted.saveAsNewAPIHadoopDataset(conf) } else { converted.saveAsHadoopDataset(new JobConf(conf)) } } } private class BytesToString extends org.apache.spark.api.java.function.Function[Array[Byte], String] { override def call(arr: Array[Byte]) : String = new String(arr, StandardCharsets.UTF_8) } /** * Internal class that acts as an `AccumulatorV2` for Python accumulators. Inside, it * collects a list of pickled strings that we pass to Python through a socket. */ private[spark] class PythonAccumulatorV2( @transient private val serverHost: String, private val serverPort: Int, private val secretToken: String) extends CollectionAccumulator[Array[Byte]] with Logging{ Utils.checkHost(serverHost) val bufferSize = SparkEnv.get.conf.get(BUFFER_SIZE) /** * We try to reuse a single Socket to transfer accumulator updates, as they are all added * by the DAGScheduler's single-threaded RpcEndpoint anyway. */ @transient private var socket: Socket = _ private def openSocket(): Socket = synchronized { if (socket == null || socket.isClosed) { socket = new Socket(serverHost, serverPort) logInfo(s\"Connected to AccumulatorServer at host: $serverHost port: $serverPort\") // send the secret just for the initial authentication when opening a new connection socket.getOutputStream.write(secretToken.getBytes(StandardCharsets.UTF_8)) } socket } // Need to override so the types match with PythonFunction override def copyAndReset(): PythonAccumulatorV2 = { new PythonAccumulatorV2(serverHost, serverPort, secretToken) } override def merge(other: AccumulatorV2[Array[Byte], JList[Array[Byte]]]): Unit = synchronized { val otherPythonAccumulator = other.asInstanceOf[PythonAccumulatorV2] // This conditional isn't strictly speaking needed - merging only currently happens on the // driver program - but that isn't guaranteed so incase this changes. if (serverHost == null) { // We are on the worker super.merge(otherPythonAccumulator) } else { // This happens on the master, where we pass the updates to Python through a socket val socket = openSocket() val in = socket.getInputStream val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream, bufferSize)) val values = other.value out.writeInt(values.size) for (array <- values.asScala) { out.writeInt(array.length) out.write(array) } out.flush() // Wait for a byte from the Python side as an acknowledgement val byteRead = in.read() if (byteRead == -1) { throw new SparkException(\"EOF reached before Python server acknowledged\") } } } } private[spark] class PythonBroadcast(@transient var path: String) extends Serializable with Logging { // id of the Broadcast variable which wrapped this PythonBroadcast private var broadcastId: Long = _ private var encryptionServer: SocketAuthServer[Unit] = null private var decryptionServer: SocketAuthServer[Unit] = null /** * Read data from disks, then copy it to `out` */ private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException { out.writeLong(broadcastId) val in = new FileInputStream(new File(path)) try { Utils.copyStream(in, out) } finally { in.close() } } /** * Write data into disk and map it to a broadcast block. */ private def readObject(in: ObjectInputStream): Unit = { broadcastId = in.readLong() val blockId = BroadcastBlockId(broadcastId, \"python\") val blockManager = SparkEnv.get.blockManager val diskBlockManager = blockManager.diskBlockManager if (!diskBlockManager.containsBlock(blockId)) { Utils.tryOrIOException { val dir = new File(Utils.getLocalDir(SparkEnv.get.conf)) val file = File.createTempFile(\"broadcast\", \"\", dir) val out = new FileOutputStream(file) Utils.tryWithSafeFinally { val size = Utils.copyStream(in, out) val ct = implicitly[ClassTag[Object]] // SPARK-28486: map broadcast file to a broadcast block, so that it could be // cleared by unpersist/destroy rather than gc(previously). val blockStoreUpdater = blockManager. TempFileBasedBlockStoreUpdater(blockId, StorageLevel.DISK_ONLY, ct, file, size) blockStoreUpdater.save() } { out.close() } } } path = diskBlockManager.getFile(blockId).getAbsolutePath } def setBroadcastId(bid: Long): Unit = { this.broadcastId = bid } def setupEncryptionServer(): Array[Any] = { encryptionServer = new SocketAuthServer[Unit](\"broadcast-encrypt-server\") { override def handleConnection(sock: Socket): Unit = { val env = SparkEnv.get val in = sock.getInputStream() val abspath = new File(path).getAbsolutePath val out = env.serializerManager.wrapForEncryption(new FileOutputStream(abspath)) DechunkedInputStream.dechunkAndCopyToOutput(in, out) } } Array(encryptionServer.port, encryptionServer.secret) } def setupDecryptionServer(): Array[Any] = { decryptionServer = new SocketAuthServer[Unit](\"broadcast-decrypt-server-for-driver\") { override def handleConnection(sock: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream())) Utils.tryWithSafeFinally { val in = SparkEnv.get.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } out.flush() } { JavaUtils.closeQuietly(out) } } } Array(decryptionServer.port, decryptionServer.secret) } def waitTillBroadcastDataSent(): Unit = decryptionServer.getResult() def waitTillDataReceived(): Unit = encryptionServer.getResult() } /** * The inverse of pyspark's ChunkedStream for sending data of unknown size. * * We might be serializing a really large object from python -- we don't want * python to buffer the whole thing in memory, nor can it write to a file, * so we don't know the length in advance. So python writes it in chunks, each chunk * preceded by a length, till we get a \"length\" of -1 which serves as EOF. * * Tested from python tests. */ private[spark] class DechunkedInputStream(wrapped: InputStream) extends InputStream with Logging { private val din = new DataInputStream(wrapped) private var remainingInChunk = din.readInt() override def read(): Int = { val into = new Array[Byte](1) val n = read(into, 0, 1) if (n == -1) { -1 } else { // if you just cast a byte to an int, then anything > 127 is negative, which is interpreted // as an EOF val b = into(0) if (b < 0) { 256 + b } else { b } } } override def read(dest: Array[Byte], off: Int, len: Int): Int = { if (remainingInChunk == -1) { return -1 } var destSpace = len var destPos = off while (destSpace > 0 && remainingInChunk != -1) { val toCopy = math.min(remainingInChunk, destSpace) val read = din.read(dest, destPos, toCopy) destPos += read destSpace -= read remainingInChunk -= read if (remainingInChunk == 0) { remainingInChunk = din.readInt() } } assert(destSpace == 0 || remainingInChunk == -1) return destPos - off } override def close(): Unit = wrapped.close() } private[spark] object DechunkedInputStream { /** * Dechunks the input, copies to output, and closes both input and the output safely. */ def dechunkAndCopyToOutput(chunked: InputStream, out: OutputStream): Unit = { val dechunked = new DechunkedInputStream(chunked) Utils.tryWithSafeFinally { Utils.copyStream(dechunked, out) } { JavaUtils.closeQuietly(out) JavaUtils.closeQuietly(dechunked) } } } /** * Sends decrypted broadcast data to python worker. See [[PythonRunner]] for entire protocol. */ private[spark] class EncryptedPythonBroadcastServer( val env: SparkEnv, val idsAndFiles: Seq[(Long, String)]) extends SocketAuthServer[Unit](\"broadcast-decrypt-server\") with Logging { override def handleConnection(socket: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream())) var socketIn: InputStream = null // send the broadcast id, then the decrypted data. We don't need to send the length, the // the python pickle module just needs a stream. Utils.tryWithSafeFinally { (idsAndFiles).foreach { case (id, path) => out.writeLong(id) val in = env.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } } logTrace(\"waiting for python to accept broadcast data over socket\") out.flush() socketIn = socket.getInputStream() socketIn.read() logTrace(\"done serving broadcast data\") } { JavaUtils.closeQuietly(socketIn) JavaUtils.closeQuietly(out) } } def waitTillBroadcastDataSent(): Unit = { getResult() } } /** * Helper for making RDD[Array[Byte]] from some python data, by reading the data from python * over a socket. This is used in preference to writing data to a file when encryption is enabled. */ private[spark] abstract class PythonRDDServer extends SocketAuthServer[JavaRDD[Array[Byte]]](\"pyspark-parallelize-server\") { def handleConnection(sock: Socket): JavaRDD[Array[Byte]] = { val in = sock.getInputStream() val dechunkedInput: InputStream = new DechunkedInputStream(in) streamToRDD(dechunkedInput) } protected def streamToRDD(input: InputStream): RDD[Array[Byte]] } private[spark] class PythonParallelizeServer(sc: SparkContext, parallelism: Int) extends PythonRDDServer { override protected def streamToRDD(input: InputStream): RDD[Array[Byte]] = { PythonRDD.readRDDFromInputStream(sc, input, parallelism) } }"
      ],
      "key_entities": [
        "org/apache/spark/api/python/PythonRunner.",
        "org/apache/spark/api/python/BasePythonRunner#compute().",
        "org/apache/spark/api/python/BasePythonRunner#MonitorThread#run().",
        "org/apache/spark/api/python/PythonRDD#compute()."
      ],
      "claude_stats": {
        "answer": "Based on the code provided, I can describe the PythonRunner Object and the methods that use it:\n\n## PythonRunner Object\n\nThe `PythonRunner` object is a companion object (singleton) located in the package `org.apache.spark.api.python`. It serves as a factory and tracking utility for Python worker execution in Apache Spark.\n\n**Key Components:**\n\n1. **Static State:**\n   - Contains a `ConcurrentHashMap` named `runningMonitorThreads` that tracks already running worker monitor threads. It stores pairs of `(Socket, Long)` representing worker sockets and task attempt IDs.\n\n2. **Factory Method:**\n   - `apply(func: PythonFunction): PythonRunner` - This is the main factory method that creates a new `PythonRunner` instance. It wraps the provided `PythonFunction` inside a `ChainedPythonFunctions` sequence and returns a new `PythonRunner`.\n\n## Methods That Use PythonRunner Object\n\nBased on the visible code, here are the methods that use the PythonRunner Object:\n\n### 1. **BasePythonRunner#compute()**\nThis method uses `PythonRunner.runningMonitorThreads` to:\n- Check if a monitor thread already exists for a worker-task pair using `runningMonitorThreads.add(key)`\n- Prevent creating multiple monitor threads for the same Python worker and task context\n- This occurs when `reuseWorker` is enabled\n\n### 2. **BasePythonRunner.MonitorThread#run()**\nThis method uses `PythonRunner.runningMonitorThreads` to:\n- Remove the worker-task pair from the running monitor threads set using `runningMonitorThreads.remove(key)`\n- This cleanup happens in the `finally` block after monitoring is complete, but only when `reuseWorker` is enabled\n\n### 3. **PythonForeachWriter Class**\nThis class uses `PythonRunner` to:\n- Create a Python runner instance via the `apply` factory method: `PythonRunner(func)`\n- Execute Python code for foreach operations on streaming data\n- The runner is initialized lazily and used to process input data iterators\n\n### 4. **PythonRDD#compute()**\nThis method uses `PythonRunner` to:\n- Create a runner instance via `PythonRunner(func)`\n- Execute the runner's `compute` method to process partition data\n- Bridge Scala RDD operations with Python worker processes\n\nThe PythonRunner Object essentially acts as a centralized coordination point for managing Python worker lifecycle and preventing resource conflicts when workers are reused across tasks.",
        "prompt_time": 0.187,
        "answer_time": 33,
        "tokens": 616,
        "context_tokens": 20211,
        "used_context": [
          "## OBJECT: org/apache/spark/api/python/PythonRunner.\n} } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner ",
          "## METHOD: org/apache/spark/api/python/BasePythonRunner#compute().\nmem.map(_ / cores) } def compute( inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): Iterator[OUT] = { val startTime = System.currentTimeMillis val env = SparkEnv.get // Get the executor cores and pyspark memory, they are passed via the local properties when // the user specified them in a ResourceProfile. val execCoresProp = Option(context.getLocalProperty(EXECUTOR_CORES_LOCAL_PROPERTY)) val memoryMb = Option(context.getLocalProperty(PYSPARK_MEMORY_LOCAL_PROPERTY)).map(_.toLong) val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\") // if OMP_NUM_THREADS is not explicitly set, override it with the number of cores if (conf.getOption(\"spark.executorEnv.OMP_NUM_THREADS\").isEmpty) { // SPARK-28843: limit the OpenMP thread pool to the number of cores assigned to this executor // this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool // see https://github.com/numpy/numpy/issues/10455 execCoresProp.foreach(envVars.put(\"OMP_NUM_THREADS\", _)) } envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread if (reuseWorker) { envVars.put(\"SPARK_REUSE_WORKER\", \"1\") } if (simplifiedTraceback) { envVars.put(\"SPARK_SIMPLIFIED_TRACEBACK\", \"1\") } // SPARK-30299 this could be wrong with standalone mode when executor // cores might not be correct because it defaults to all cores on the box. val execCores = execCoresProp.map(_.toInt).getOrElse(conf.get(EXECUTOR_CORES)) val workerMemoryMb = getWorkerMemoryMb(memoryMb, execCores) if (workerMemoryMb.isDefined) { envVars.put(\"PYSPARK_EXECUTOR_MEMORY_MB\", workerMemoryMb.get.toString) } envVars.put(\"SPARK_AUTH_SOCKET_TIMEOUT\", authSocketTimeout.toString) envVars.put(\"SPARK_BUFFER_SIZE\", bufferSize.toString) if (faultHandlerEnabled) { envVars.put(\"PYTHON_FAULTHANDLER_DIR\", BasePythonRunner.faultHandlerLogDir.toString) } val (worker: Socket, pid: Option[Int]) = env.createPythonWorker( pythonExec, envVars.asScala.toMap) // Whether is the worker released into idle pool or closed. When any codes try to release or // close a worker, they should use `releasedOrClosed.compareAndSet` to flip the state to make // sure there is only one winner that is going to release or close the worker. val releasedOrClosed = new AtomicBoolean(false) // Start a thread to feed the process input from our parent's iterator val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context) context.addTaskCompletionListener[Unit] { _ => writerThread.shutdownOnTaskCompletion() if (!reuseWorker || releasedOrClosed.compareAndSet(false, true)) { try { worker.close() } catch { case e: Exception => logWarning(\"Failed to close worker socket\", e) } } } writerThread.start() new WriterMonitorThread(SparkEnv.get, worker, writerThread, context).start() if (reuseWorker) { val key = (worker, context.taskAttemptId) // SPARK-35009: avoid creating multiple monitor threads for the same python worker // and task context if (PythonRunner.runningMonitorThreads.add(key)) { new MonitorThread(SparkEnv.get, worker, context).start() } } else { new MonitorThread(SparkEnv.get, worker, context).start() } // Return an iterator that read lines from the process's stdout val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize)) val stdoutIterator = newReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) new InterruptibleIterator(context, stdoutIterator) } protected def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): WriterThread protected def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[OUT] /** * The thread responsible for writing the data from the PythonRDD's parent iterator to the * Python process.  abstract class WriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext) extends Thread(s\"stdout writer for $pythonExec\") { @volatile private var _exception: Throwable = null private val pythonIncludes = funcs.flatMap(_.funcs.flatMap(_.pythonIncludes.asScala)).toSet private val broadcastVars = funcs.flatMap(_.funcs.flatMap(_.broadcastVars.asScala)) setDaemon(true) /** Contains the throwable thrown while writing the parent iterator to the Python process.  def exception: Option[Throwable] = Option(_exception) /** * Terminates the writer thread and waits for it to exit, ignoring any exceptions that may occur * due to cleanup.  def shutdownOnTaskCompletion(): Unit = { assert(context.isCompleted) this.interrupt() // Task completion listeners that run after this method returns may invalidate // `inputIterator`. For example, when `inputIterator` was generated by the off-heap vectorized // reader, a task completion listener will free the underlying off-heap buffers. If the writer // thread is still running when `inputIterator` is invalidated, it can cause a use-after-free // bug that crashes the executor (SPARK-33277). Therefore this method must wait for the writer // thread to exit before returning. this.join() } /** * Writes a command section to the stream connected to the Python worker.  protected def writeCommand(dataOut: DataOutputStream): Unit /** * Writes input data to the stream connected to the Python worker.  protected def writeIteratorToStream(dataOut: DataOutputStream): Unit override def run(): Unit = Utils.logUncaughtExceptions { try { TaskContext.setTaskContext(context) val stream = new BufferedOutputStream(worker.getOutputStream, bufferSize) val dataOut = new DataOutputStream(stream) // Partition index dataOut.writeInt(partitionIndex) // Python version of driver PythonRDD.writeUTF(pythonVer, dataOut) // Init a ServerSocket to accept method calls from Python side. val isBarrier = context.isInstanceOf[BarrierTaskContext] if (isBarrier) { serverSocket = Some(new ServerSocket(/* port  0, /* backlog  1, InetAddress.getByName(\"localhost\"))) // A call to accept() for ServerSocket shall block infinitely. serverSocket.foreach(_.setSoTimeout(0)) new Thread(\"accept-connections\") { setDaemon(true) override def run(): Unit = { while (!serverSocket.get.isClosed()) { var sock: Socket = null try { sock = serverSocket.get.accept() // Wait for function call from python side. sock.setSoTimeout(10000) authHelper.authClient(sock) val input = new DataInputStream(sock.getInputStream()) val requestMethod = input.readInt() // The BarrierTaskContext function may wait infinitely, socket shall not timeout // before the function finishes. sock.setSoTimeout(0) requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => barrierAndServe(requestMethod, sock) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => val length = input.readInt() val message = new Array[Byte](length) input.readFully(message) barrierAndServe(requestMethod, sock, new String(message, UTF_8)) case _ => val out = new DataOutputStream(new BufferedOutputStream( sock.getOutputStream)) writeUTF(BarrierTaskContextMessageProtocol.ERROR_UNRECOGNIZED_FUNCTION, out) } } catch { case e: SocketException if e.getMessage.contains(\"Socket closed\") => // It is possible that the ServerSocket is not closed, but the native socket // has already been closed, we shall catch and silently ignore this case. } finally { if (sock != null) { sock.close() } } } } }.start() } val secret = if (isBarrier) { authHelper.secret } else { \"\" } // Close ServerSocket on task completion. serverSocket.foreach { server => context.addTaskCompletionListener[Unit](_ => server.close()) } val boundPort: Int = serverSocket.map(_.getLocalPort).getOrElse(0) if (boundPort == -1) { val message = \"ServerSocket failed to bind to Java side.\" logError(message) throw new SparkException(message) } else if (isBarrier) { logDebug(s\"Started ServerSocket on port $boundPort.\") } // Write out the TaskContextInfo dataOut.writeBoolean(isBarrier) dataOut.writeInt(boundPort) val secretBytes = secret.getBytes(UTF_8) dataOut.writeInt(secretBytes.length) dataOut.write(secretBytes, 0, secretBytes.length) dataOut.writeInt(context.stageId()) dataOut.writeInt(context.partitionId()) dataOut.writeInt(context.attemptNumber()) dataOut.writeLong(context.taskAttemptId()) dataOut.writeInt(context.cpus()) val resources = context.resources() dataOut.writeInt(resources.size) resources.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v.name, dataOut) dataOut.writeInt(v.addresses.size) v.addresses.foreach { case addr => PythonRDD.writeUTF(addr, dataOut) } } val localProps = context.getLocalProperties.asScala dataOut.writeInt(localProps.size) localProps.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v, dataOut) } // sparkFilesDir PythonRDD.writeUTF(SparkFiles.getRootDirectory(), dataOut) // Python includes (*.zip and *.egg files) dataOut.writeInt(pythonIncludes.size) for (include <- pythonIncludes) { PythonRDD.writeUTF(include, dataOut) } // Broadcast variables val oldBids = PythonRDD.getWorkerBroadcasts(worker) val newBids = broadcastVars.map(_.id).toSet // number of different broadcasts val toRemove = oldBids.diff(newBids) val addedBids = newBids.diff(oldBids) val cnt = toRemove.size + addedBids.size val needsDecryptionServer = env.serializerManager.encryptionEnabled && addedBids.nonEmpty dataOut.writeBoolean(needsDecryptionServer) dataOut.writeInt(cnt) def sendBidsToRemove(): Unit = { for (bid <- toRemove) { // remove the broadcast from worker dataOut.writeLong(-bid - 1) // bid >= 0 oldBids.remove(bid) } } if (needsDecryptionServer) { // if there is encryption, we setup a server which reads the encrypted files, and sends // the decrypted data to python val idsAndFiles = broadcastVars.flatMap { broadcast => if (!oldBids.contains(broadcast.id)) { Some((broadcast.id, broadcast.value.path)) } else { None } } val server = new EncryptedPythonBroadcastServer(env, idsAndFiles) dataOut.writeInt(server.port) logTrace(s\"broadcast decryption server setup on ${server.port}\") PythonRDD.writeUTF(server.secret, dataOut) sendBidsToRemove() idsAndFiles.foreach { case (id, _) => // send new broadcast dataOut.writeLong(id) oldBids.add(id) } dataOut.flush() logTrace(\"waiting for python to read decrypted broadcast data from server\") server.waitTillBroadcastDataSent() logTrace(\"done sending decrypted data to python\") } else { sendBidsToRemove() for (broadcast <- broadcastVars) { if (!oldBids.contains(broadcast.id)) { // send new broadcast dataOut.writeLong(broadcast.id) PythonRDD.writeUTF(broadcast.value.path, dataOut) oldBids.add(broadcast.id) } } } dataOut.flush() dataOut.writeInt(evalType) writeCommand(dataOut) writeIteratorToStream(dataOut) dataOut.writeInt(SpecialLengths.END_OF_STREAM) dataOut.flush() } catch { case t: Throwable if (NonFatal(t) || t.isInstanceOf[Exception]) => if (context.isCompleted || context.isInterrupted) { logDebug(\"Exception/NonFatal Error thrown after task completion (likely due to \" + \"cleanup)\", t) if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } else { // We must avoid throwing exceptions/NonFatals here, because the thread uncaught // exception handler will kill the whole executor (see // org.apache.spark.executor.Executor). _exception = t if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } } } /** * Gateway to call BarrierTaskContext methods.  def barrierAndServe(requestMethod: Int, sock: Socket, message: String = \"\"): Unit = { require( serverSocket.isDefined, \"No available ServerSocket to redirect the BarrierTaskContext method call.\" ) val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream)) try { val messages = requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => context.asInstanceOf[BarrierTaskContext].barrier() Array(BarrierTaskContextMessageProtocol.BARRIER_RESULT_SUCCESS) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => context.asInstanceOf[BarrierTaskContext].allGather(message) } out.writeInt(messages.length) messages.foreach(writeUTF(_, out)) } catch { case e: SparkException => writeUTF(e.getMessage, out) } finally { out.close() } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } } abstract class ReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext) extends Iterator[OUT] { private var nextObj: OUT = _ private var eos = false override def hasNext: Boolean = nextObj != null || { if (!eos) { nextObj = read() hasNext } else { false } } override def next(): OUT = { if (hasNext) { val obj = nextObj nextObj = null.asInstanceOf[OUT] obj } else { Iterator.empty.next() } } /** * Reads next object from the stream. * When the stream reaches end of data, needs to process the following sections, * and then returns null.  protected def read(): OUT protected def handleTimingData(): Unit = { // Timing data from worker val bootTime = stream.readLong() val initTime = stream.readLong() val finishTime = stream.readLong() val boot = bootTime - startTime val init = initTime - bootTime val finish = finishTime - initTime val total = finishTime - startTime logInfo(\"Times: total = %s, boot = %s, init = %s, finish = %s\".format(total, boot, init, finish)) val memoryBytesSpilled = stream.readLong() val diskBytesSpilled = stream.readLong() context.taskMetrics.incMemoryBytesSpilled(memoryBytesSpilled) context.taskMetrics.incDiskBytesSpilled(diskBytesSpilled) } protected def handlePythonException(): PythonException = { // Signals that an exception has been thrown in python val exLength = stream.readInt() val obj = new Array[Byte](exLength) stream.readFully(obj) new PythonException(new String(obj, StandardCharsets.UTF_8), writerThread.exception.getOrElse(null)) } protected def handleEndOfDataSection(): Unit = { // We've finished the data section of the output, but we can still // read some accumulator updates: val numAccumulatorUpdates = stream.readInt() (1 to numAccumulatorUpdates).foreach { _ => val updateLen = stream.readInt() val update = new Array[Byte](updateLen) stream.readFully(update) maybeAccumulator.foreach(_.add(update)) } // Check whether the worker is ready to be re-used. if (stream.readInt() == SpecialLengths.END_OF_STREAM) { if (reuseWorker && releasedOrClosed.compareAndSet(false, true)) { env.releasePythonWorker(pythonExec, envVars.asScala.toMap, worker) } } eos = true } protected val handleException: PartialFunction[Throwable, OUT] = { case e: Exception if context.isInterrupted => logDebug(\"Exception thrown after task interruption\", e) throw new TaskKilledException(context.getKillReason().getOrElse(\"unknown reason\")) case e: Exception if writerThread.exception.isDefined => logError(\"Python worker exited unexpectedly (crashed)\", e) logError(\"This may have been caused by a prior exception:\", writerThread.exception.get) throw writerThread.exception.get case eof: EOFException if faultHandlerEnabled && pid.isDefined && JavaFiles.exists(BasePythonRunner.faultHandlerLogPath(pid.get)) => val path = BasePythonRunner.faultHandlerLogPath(pid.get) val error = String.join(\"\\n\", JavaFiles.readAllLines(path)) + \"\\n\" JavaFiles.deleteIfExists(path) throw new SparkException(s\"Python worker exited unexpectedly (crashed): $error\", eof) case eof: EOFException => throw new SparkException(\"Python worker exited unexpectedly (crashed)\", eof) } } /** * It is necessary to have a monitor thread for python workers if the user cancels with * interrupts disabled. In that case we will need to explicitly kill the worker, otherwise the * threads can block indefinitely.  class MonitorThread(env: SparkEnv, worker: Socket, context: TaskContext) extends Thread(s\"Worker Monitor for $pythonExec\") { /** How long to wait before killing the python worker if a task cannot be interrupted.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) private def monitorWorker(): Unit = { // Kill the worker if it is interrupted, checking until task completion. // TODO: This has a race condition if interruption occurs, as completed may still become true. while (!context.isInterrupted && !context.isCompleted) { Thread.sleep(2000) } if (!context.isCompleted) { Thread.sleep(taskKillTimeout) if (!context.isCompleted) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning(s\"Incomplete task $taskName interrupted: Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }\n",
          "## CLASS: org/apache/spark/sql/execution/python/PythonForeachWriter#\nclass PythonForeachWriter(func: PythonFunction, schema: StructType) extends ForeachWriter[UnsafeRow] { private lazy val context = TaskContext.get() private lazy val buffer = new PythonForeachWriter.UnsafeRowBuffer( context.taskMemoryManager, new File(Utils.getLocalDir(SparkEnv.get.conf)), schema.fields.length) private lazy val inputRowIterator = buffer.iterator private lazy val inputByteIterator = { EvaluatePython.registerPicklers() val objIterator = inputRowIterator.map { row => EvaluatePython.toJava(row, schema) } new SerDeUtil.AutoBatchedPickler(objIterator) } private lazy val pythonRunner = { PythonRunner(func) } private lazy val outputIterator = pythonRunner.compute(inputByteIterator, context.partitionId(), context) override def open(partitionId: Long, version: Long): Boolean = { outputIterator // initialize everything TaskContext.get.addTaskCompletionListener[Unit] { _ => buffer.close() } true } override def process(value: UnsafeRow): Unit = { buffer.add(value) } override def close(errorOrNull: Throwable): Unit = { buffer.allRowsAdded() if (outputIterator.hasNext) outputIterator.next() // to throw python exception if there was one } } object PythonForeachWriter { /** * A buffer that is designed for the sole purpose of buffering UnsafeRows in PythonForeachWriter. * It is designed to be used with only 1 writer thread (i.e. JVM task thread) and only 1 reader * thread (i.e. PythonRunner writing thread that reads from the buffer and writes to the Python * worker stdin). Adds to the buffer are non-blocking, and reads through the buffer's iterator * are blocking, that is, it blocks until new data is available or all data has been added. * * Internally, it uses a [[HybridRowQueue]] to buffer the rows in a practically unlimited queue * across memory and local disk. However, HybridRowQueue is designed to be used only with * EvalPythonExec where the reader is always behind the writer, that is, the reader does not * try to read n+1 rows if the writer has only written n rows at any point of time. This * assumption is not true for PythonForeachWriter where rows may be added at a different rate as * they are consumed by the python worker. Hence, to maintain the invariant of the reader being * behind the writer while using HybridRowQueue, the buffer does the following * - Keeps a count of the rows in the HybridRowQueue * - Blocks the buffer's consuming iterator when the count is 0 so that the reader does not * try to read more rows than what has been written. * * The implementation of the blocking iterator (ReentrantLock, Condition, etc.) has been borrowed * from that of ArrayBlockingQueue.  class UnsafeRowBuffer(taskMemoryManager: TaskMemoryManager, tempDir: File, numFields: Int) extends Logging { private val queue = HybridRowQueue(taskMemoryManager, tempDir, numFields) private val lock = new ReentrantLock() private val unblockRemove = lock.newCondition() // All of these are guarded by `lock` private var count = 0L private var allAdded = false private var exception: Throwable = null val iterator = new NextIterator[UnsafeRow] { override protected def getNext(): UnsafeRow = { val row = remove() if (row == null) finished = true row } override protected def close(): Unit = { } } def add(row: UnsafeRow): Unit = withLock { assert(queue.add(row), s\"Failed to add row to HybridRowQueue while sending data to Python\" + s\"[count = $count, allAdded = $allAdded, exception = $exception]\") count += 1 unblockRemove.signal() logTrace(s\"Added $row, $count left\") } private def remove(): UnsafeRow = withLock { while (count == 0 && !allAdded && exception == null) { unblockRemove.await(100, TimeUnit.MILLISECONDS) } // If there was any error in the adding thread, then rethrow it in the removing thread if (exception != null) throw exception if (count > 0) { val row = queue.remove() assert(row != null, \"HybridRowQueue.remove() returned null \" + s\"[count = $count, allAdded = $allAdded, exception = $exception]\") count -= 1 logTrace(s\"Removed $row, $count left\") row } else { null } } def allRowsAdded(): Unit = withLock { allAdded = true unblockRemove.signal() } def close(): Unit = { queue.close() } private def withLock[T](f: => T): T = { lock.lockInterruptibly() try { f } catch { case e: Throwable => if (exception == null) exception = e throw e } finally { lock.unlock() } } } }",
          "## METHOD: org/apache/spark/api/python/PythonRunner.apply().\n* * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
          "## METHOD: org/apache/spark/api/python/BasePythonRunner#MonitorThread#run().\n} } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
          "## METHOD: org/apache/spark/api/python/PythonRDD#compute().\noverride def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]] = { val runner = PythonRunner(func) runner.compute(firstParent.iterator(split, context), split.index, context) } @transient protected lazy override val isBarrier_ : Boolean = isFromBarrier || dependencies.exists(_.rdd.isBarrier()) } /** * A wrapper for a Python function, contains all necessary context to run the function in Python * runner.  private[spark] case class PythonFunction( command: Seq[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) { def this( command: Array[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) = { this(command.toSeq, envVars, pythonIncludes, pythonExec, pythonVer, broadcastVars, accumulator) } } /** * A wrapper for chained Python functions (from bottom to top). * @param funcs  private[spark] case class ChainedPythonFunctions(funcs: Seq[PythonFunction]) /** Thrown for exceptions in user Python code.  private[spark] class PythonException(msg: String, cause: Throwable) extends RuntimeException(msg, cause) /** * Form an RDD[(Array[Byte], Array[Byte])] from key-value pairs returned from Python. * This is used by PySpark's shuffle operations.  private class PairwiseRDD(prev: RDD[Array[Byte]]) extends RDD[(Long, Array[Byte])](prev) { override def getPartitions: Array[Partition] = prev.partitions override val partitioner: Option[Partitioner] = prev.partitioner override def compute(split: Partition, context: TaskContext): Iterator[(Long, Array[Byte])] = prev.iterator(split, context).grouped(2).map { case Seq(a, b) => (Utils.deserializeLongValue(a), b) case x => throw new SparkException(\"PairwiseRDD: unexpected value: \" + x) } val asJavaPairRDD : JavaPairRDD[Long, Array[Byte]] = JavaPairRDD.fromRDD(this) } private[spark] object PythonRDD extends Logging { // remember the broadcasts sent to each worker private val workerBroadcasts = new mutable.WeakHashMap[Socket, mutable.Set[Long]]() // Authentication helper used when serving iterator data. private lazy val authHelper = { val conf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf()) new SocketAuthHelper(conf) } def getWorkerBroadcasts(worker: Socket): mutable.Set[Long] = { synchronized { workerBroadcasts.getOrElseUpdate(worker, new mutable.HashSet[Long]()) } } /** * Return an RDD of values from an RDD of (Long, Array[Byte]), with preservePartitions=true * * This is useful for PySpark to have the partitioner after partitionBy()  def valueOfPair(pair: JavaPairRDD[Long, Array[Byte]]): JavaRDD[Array[Byte]] = { pair.rdd.mapPartitions(it => it.map(_._2), true) } /** * Adapter for calling SparkContext#runJob from Python. * * This method will serve an iterator of an array that contains all elements in the RDD * (effectively a collect()), but allows you to run on a certain subset of partitions, * or to enable local execution. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def runJob( sc: SparkContext, rdd: JavaRDD[Array[Byte]], partitions: JArrayList[Int]): Array[Any] = { type ByteArray = Array[Byte] type UnrolledPartition = Array[ByteArray] val allPartitions: Array[UnrolledPartition] = sc.runJob(rdd, (x: Iterator[ByteArray]) => x.toArray, partitions.asScala.toSeq) val flattenedPartition: UnrolledPartition = Array.concat(allPartitions: _*) serveIterator(flattenedPartition.iterator, s\"serve RDD ${rdd.id} with partitions ${partitions.asScala.mkString(\",\")}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def collectAndServe[T](rdd: RDD[T]): Array[Any] = { serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * This method is similar with `PythonRDD.collectAndServe`, but user can specify job group id, * job description, and interruptOnCancel option.  def collectAndServeWithJobGroup[T]( rdd: RDD[T], groupId: String, description: String, interruptOnCancel: Boolean): Array[Any] = { val sc = rdd.sparkContext sc.setJobGroup(groupId, description, interruptOnCancel) serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to create a local RDD iterator and serve it via socket. Partitions are * are collected as separate jobs, by order of index. Partition data is first requested by a * non-zero integer to start a collection job. The response is prefaced by an integer with 1 * meaning partition data will be served, 0 meaning the local iterator has been consumed, * and -1 meaning an error occurred during collection. This function is used by * pyspark.rdd._local_iterator_from_socket(). * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def toLocalIteratorAndServe[T](rdd: RDD[T], prefetchPartitions: Boolean = false): Array[Any] = { val handleFunc = (sock: Socket) => { val out = new DataOutputStream(sock.getOutputStream) val in = new DataInputStream(sock.getInputStream) Utils.tryWithSafeFinallyAndFailureCallbacks(block = { // Collects a partition on each iteration val collectPartitionIter = rdd.partitions.indices.iterator.map { i => var result: Array[Any] = null rdd.sparkContext.submitJob( rdd, (iter: Iterator[Any]) => iter.toArray, Seq(i), // The partition we are evaluating (_, res: Array[Any]) => result = res, result) } val prefetchIter = collectPartitionIter.buffered // Write data until iteration is complete, client stops iteration, or error occurs var complete = false while (!complete) { // Read request for data, value of zero will stop iteration or non-zero to continue if (in.readInt() == 0) { complete = true } else if (prefetchIter.hasNext) { // Client requested more data, attempt to collect the next partition val partitionFuture = prefetchIter.next() // Cause the next job to be submitted if prefetchPartitions is enabled. if (prefetchPartitions) { prefetchIter.headOption } val partitionArray = ThreadUtils.awaitResult(partitionFuture, Duration.Inf) // Send response there is a partition to read out.writeInt(1) // Write the next object and signal end of data for this iteration writeIteratorToStream(partitionArray.iterator, out) out.writeInt(SpecialLengths.END_OF_DATA_SECTION) out.flush() } else { // Send response there are no more partitions to read and close out.writeInt(0) complete = true } } })(catchBlock = { // Send response that an error occurred, original exception is re-thrown out.writeInt(-1) }, finallyBlock = { out.close() in.close() }) } val server = new SocketFuncServer(authHelper, \"serve toLocalIterator\", handleFunc) Array(server.port, server.secret, server) } def readRDDFromFile( sc: JavaSparkContext, filename: String, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromFile(sc, filename, parallelism) } def readRDDFromInputStream( sc: SparkContext, in: InputStream, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromInputStream(sc, in, parallelism) } def setupBroadcast(path: String): PythonBroadcast = { new PythonBroadcast(path) } def writeIteratorToStream[T](iter: Iterator[T], dataOut: DataOutputStream): Unit = { def write(obj: Any): Unit = obj match { case null => dataOut.writeInt(SpecialLengths.NULL) case arr: Array[Byte] => dataOut.writeInt(arr.length) dataOut.write(arr) case str: String => writeUTF(str, dataOut) case stream: PortableDataStream => write(stream.toArray()) case (key, value) => write(key) write(value) case other => throw new SparkException(\"Unexpected element type \" + other.getClass) } iter.foreach(write) } /** * Create an RDD from a path using [[org.apache.hadoop.mapred.SequenceFileInputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def sequenceFile[K, V]( sc: JavaSparkContext, path: String, keyClassMaybeNull: String, valueClassMaybeNull: String, keyConverterClass: String, valueConverterClass: String, minSplits: Int, batchSize: Int): JavaRDD[Array[Byte]] = { val keyClass = Option(keyClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val valueClass = Option(valueClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val rdd = sc.sc.sequenceFile[K, V](path, kc, vc, minSplits) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration())) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map that is * passed in from Python, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def newAPIHadoopRDDFromClassNames[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration): RDD[(K, V)] = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.newAPIHadoopFile[K, V, F](path.get, fc, kc, vc, conf) } else { sc.sc.newAPIHadoopRDD[K, V, F](conf, fc, kc, vc) } } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def hadoopFile[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map * that is passed in from Python, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def hadoopRDD[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def hadoopRDDFromClassNames[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration) = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.hadoopFile(path.get, fc, kc, vc) } else { sc.sc.hadoopRDD(new JobConf(conf), fc, kc, vc) } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(StandardCharsets.UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } /** * Create a socket server and a background thread to serve the data in `items`, * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it tries to serialize all the data in `items` * and send them into this connection. * * The thread will terminate after all the data are sent or any exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def serveIterator(items: Iterator[_], threadName: String): Array[Any] = { serveToStream(threadName) { out => writeIteratorToStream(items, new DataOutputStream(out)) } } /** * Create a socket server and background thread to execute the writeFunc * with the given OutputStream. * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it will execute the block of code and pass in * the socket output stream. * * The thread will terminate after the block of code is executed or any * exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  private[spark] def serveToStream( threadName: String)(writeFunc: OutputStream => Unit): Array[Any] = { SocketAuthServer.serveToStream(threadName, authHelper)(writeFunc) } private def getMergedConf(confAsMap: java.util.HashMap[String, String], baseConf: Configuration): Configuration = { val conf = PythonHadoopUtil.mapToConf(confAsMap) PythonHadoopUtil.mergeConfs(baseConf, conf) } private def inferKeyValueTypes[K, V, KK, VV](rdd: RDD[(K, V)], keyConverterClass: String = null, valueConverterClass: String = null): (Class[_ <: KK], Class[_ <: VV]) = { // Peek at an element to figure out key/value types. Since Writables are not serializable, // we cannot call first() on the converted RDD. Instead, we call first() on the original RDD // and then convert locally. val (key, value) = rdd.first() val (kc, vc) = getKeyValueConverters[K, V, KK, VV]( keyConverterClass, valueConverterClass, new JavaToWritableConverter) (kc.convert(key).getClass, vc.convert(value).getClass) } private def getKeyValueTypes[K, V](keyClass: String, valueClass: String): Option[(Class[K], Class[V])] = { for { k <- Option(keyClass) v <- Option(valueClass) } yield (Utils.classForName(k), Utils.classForName(v)) } private def getKeyValueConverters[K, V, KK, VV]( keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[_, _]): (Converter[K, KK], Converter[V, VV]) = { val keyConverter = Converter.getInstance(Option(keyConverterClass), defaultConverter.asInstanceOf[Converter[K, KK]]) val valueConverter = Converter.getInstance(Option(valueConverterClass), defaultConverter.asInstanceOf[Converter[V, VV]]) (keyConverter, valueConverter) } /** * Convert an RDD of key-value pairs from internal types to serializable types suitable for * output, or vice versa.  private def convertRDD[K, V](rdd: RDD[(K, V)], keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[Any, Any]): RDD[(Any, Any)] = { val (kc, vc) = getKeyValueConverters[K, V, Any, Any](keyConverterClass, valueConverterClass, defaultConverter) PythonHadoopUtil.convertRDD(rdd, kc, vc) } /** * Output a Python RDD of key-value pairs as a Hadoop SequenceFile using the Writable types * we convert from the RDD's key and value types. Note that keys and values can't be * [[org.apache.hadoop.io.Writable]] types already, since Writables are not Java * `Serializable` and we can't peek at them. The `path` can be on any Hadoop file system.  def saveAsSequenceFile[C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, compressionCodecClass: String): Unit = { saveAsHadoopFile( pyRDD, batchSerialized, path, \"org.apache.hadoop.mapred.SequenceFileOutputFormat\", null, null, null, null, new java.util.HashMap(), compressionCodecClass) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using old Hadoop * `OutputFormat` in mapred package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD.  def saveAsHadoopFile[F <: OutputFormat[_, _], C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], compressionCodecClass: String): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val codec = Option(compressionCodecClass).map(Utils.classForName(_).asInstanceOf[Class[C]]) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName[F](outputFormatClass) converted.saveAsHadoopFile(path, kc, vc, fc, new JobConf(mergedConf), codec = codec) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using new Hadoop * `OutputFormat` in mapreduce package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD.  def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String]): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName(outputFormatClass).asInstanceOf[Class[F]] converted.saveAsNewAPIHadoopFile(path, kc, vc, fc, mergedConf) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using a Hadoop conf * converted from the passed-in `confAsMap`. The conf should set relevant output params ( * e.g., output path, output format, etc), in the same way as it would be configured for * a Hadoop MapReduce job. Both old and new Hadoop OutputFormat APIs are supported * (mapred vs. mapreduce). Keys/values are converted for output using either user specified * converters or, by default, [[org.apache.spark.api.python.JavaToWritableConverter]].  def saveAsHadoopDataset( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, confAsMap: java.util.HashMap[String, String], keyConverterClass: String, valueConverterClass: String, useNewAPI: Boolean): Unit = { val conf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized), keyConverterClass, valueConverterClass, new JavaToWritableConverter) if (useNewAPI) { converted.saveAsNewAPIHadoopDataset(conf) } else { converted.saveAsHadoopDataset(new JobConf(conf)) } } } private class BytesToString extends org.apache.spark.api.java.function.Function[Array[Byte], String] { override def call(arr: Array[Byte]) : String = new String(arr, StandardCharsets.UTF_8) } /** * Internal class that acts as an `AccumulatorV2` for Python accumulators. Inside, it * collects a list of pickled strings that we pass to Python through a socket.  private[spark] class PythonAccumulatorV2( @transient private val serverHost: String, private val serverPort: Int, private val secretToken: String) extends CollectionAccumulator[Array[Byte]] with Logging{ Utils.checkHost(serverHost) val bufferSize = SparkEnv.get.conf.get(BUFFER_SIZE) /** * We try to reuse a single Socket to transfer accumulator updates, as they are all added * by the DAGScheduler's single-threaded RpcEndpoint anyway.  @transient private var socket: Socket = _ private def openSocket(): Socket = synchronized { if (socket == null || socket.isClosed) { socket = new Socket(serverHost, serverPort) logInfo(s\"Connected to AccumulatorServer at host: $serverHost port: $serverPort\") // send the secret just for the initial authentication when opening a new connection socket.getOutputStream.write(secretToken.getBytes(StandardCharsets.UTF_8)) } socket } // Need to override so the types match with PythonFunction override def copyAndReset(): PythonAccumulatorV2 = { new PythonAccumulatorV2(serverHost, serverPort, secretToken) } override def merge(other: AccumulatorV2[Array[Byte], JList[Array[Byte]]]): Unit = synchronized { val otherPythonAccumulator = other.asInstanceOf[PythonAccumulatorV2] // This conditional isn't strictly speaking needed - merging only currently happens on the // driver program - but that isn't guaranteed so incase this changes. if (serverHost == null) { // We are on the worker super.merge(otherPythonAccumulator) } else { // This happens on the master, where we pass the updates to Python through a socket val socket = openSocket() val in = socket.getInputStream val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream, bufferSize)) val values = other.value out.writeInt(values.size) for (array <- values.asScala) { out.writeInt(array.length) out.write(array) } out.flush() // Wait for a byte from the Python side as an acknowledgement val byteRead = in.read() if (byteRead == -1) { throw new SparkException(\"EOF reached before Python server acknowledged\") } } } } private[spark] class PythonBroadcast(@transient var path: String) extends Serializable with Logging { // id of the Broadcast variable which wrapped this PythonBroadcast private var broadcastId: Long = _ private var encryptionServer: SocketAuthServer[Unit] = null private var decryptionServer: SocketAuthServer[Unit] = null /** * Read data from disks, then copy it to `out`  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException { out.writeLong(broadcastId) val in = new FileInputStream(new File(path)) try { Utils.copyStream(in, out) } finally { in.close() } } /** * Write data into disk and map it to a broadcast block.  private def readObject(in: ObjectInputStream): Unit = { broadcastId = in.readLong() val blockId = BroadcastBlockId(broadcastId, \"python\") val blockManager = SparkEnv.get.blockManager val diskBlockManager = blockManager.diskBlockManager if (!diskBlockManager.containsBlock(blockId)) { Utils.tryOrIOException { val dir = new File(Utils.getLocalDir(SparkEnv.get.conf)) val file = File.createTempFile(\"broadcast\", \"\", dir) val out = new FileOutputStream(file) Utils.tryWithSafeFinally { val size = Utils.copyStream(in, out) val ct = implicitly[ClassTag[Object]] // SPARK-28486: map broadcast file to a broadcast block, so that it could be // cleared by unpersist/destroy rather than gc(previously). val blockStoreUpdater = blockManager. TempFileBasedBlockStoreUpdater(blockId, StorageLevel.DISK_ONLY, ct, file, size) blockStoreUpdater.save() } { out.close() } } } path = diskBlockManager.getFile(blockId).getAbsolutePath } def setBroadcastId(bid: Long): Unit = { this.broadcastId = bid } def setupEncryptionServer(): Array[Any] = { encryptionServer = new SocketAuthServer[Unit](\"broadcast-encrypt-server\") { override def handleConnection(sock: Socket): Unit = { val env = SparkEnv.get val in = sock.getInputStream() val abspath = new File(path).getAbsolutePath val out = env.serializerManager.wrapForEncryption(new FileOutputStream(abspath)) DechunkedInputStream.dechunkAndCopyToOutput(in, out) } } Array(encryptionServer.port, encryptionServer.secret) } def setupDecryptionServer(): Array[Any] = { decryptionServer = new SocketAuthServer[Unit](\"broadcast-decrypt-server-for-driver\") { override def handleConnection(sock: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream())) Utils.tryWithSafeFinally { val in = SparkEnv.get.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } out.flush() } { JavaUtils.closeQuietly(out) } } } Array(decryptionServer.port, decryptionServer.secret) } def waitTillBroadcastDataSent(): Unit = decryptionServer.getResult() def waitTillDataReceived(): Unit = encryptionServer.getResult() } /** * The inverse of pyspark's ChunkedStream for sending data of unknown size. * * We might be serializing a really large object from python -- we don't want * python to buffer the whole thing in memory, nor can it write to a file, * so we don't know the length in advance. So python writes it in chunks, each chunk * preceded by a length, till we get a \"length\" of -1 which serves as EOF. * * Tested from python tests.  private[spark] class DechunkedInputStream(wrapped: InputStream) extends InputStream with Logging { private val din = new DataInputStream(wrapped) private var remainingInChunk = din.readInt() override def read(): Int = { val into = new Array[Byte](1) val n = read(into, 0, 1) if (n == -1) { -1 } else { // if you just cast a byte to an int, then anything > 127 is negative, which is interpreted // as an EOF val b = into(0) if (b < 0) { 256 + b } else { b } } } override def read(dest: Array[Byte], off: Int, len: Int): Int = { if (remainingInChunk == -1) { return -1 } var destSpace = len var destPos = off while (destSpace > 0 && remainingInChunk != -1) { val toCopy = math.min(remainingInChunk, destSpace) val read = din.read(dest, destPos, toCopy) destPos += read destSpace -= read remainingInChunk -= read if (remainingInChunk == 0) { remainingInChunk = din.readInt() } } assert(destSpace == 0 || remainingInChunk == -1) return destPos - off } override def close(): Unit = wrapped.close() } private[spark] object DechunkedInputStream { /** * Dechunks the input, copies to output, and closes both input and the output safely.  def dechunkAndCopyToOutput(chunked: InputStream, out: OutputStream): Unit = { val dechunked = new DechunkedInputStream(chunked) Utils.tryWithSafeFinally { Utils.copyStream(dechunked, out) } { JavaUtils.closeQuietly(out) JavaUtils.closeQuietly(dechunked) } } } /** * Sends decrypted broadcast data to python worker. See [[PythonRunner]] for entire protocol.  private[spark] class EncryptedPythonBroadcastServer( val env: SparkEnv, val idsAndFiles: Seq[(Long, String)]) extends SocketAuthServer[Unit](\"broadcast-decrypt-server\") with Logging { override def handleConnection(socket: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream())) var socketIn: InputStream = null // send the broadcast id, then the decrypted data. We don't need to send the length, the // the python pickle module just needs a stream. Utils.tryWithSafeFinally { (idsAndFiles).foreach { case (id, path) => out.writeLong(id) val in = env.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } } logTrace(\"waiting for python to accept broadcast data over socket\") out.flush() socketIn = socket.getInputStream() socketIn.read() logTrace(\"done serving broadcast data\") } { JavaUtils.closeQuietly(socketIn) JavaUtils.closeQuietly(out) } } def waitTillBroadcastDataSent(): Unit = { getResult() } } /** * Helper for making RDD[Array[Byte]] from some python data, by reading the data from python * over a socket. This is used in preference to writing data to a file when encryption is enabled.  private[spark] abstract class PythonRDDServer extends SocketAuthServer[JavaRDD[Array[Byte]]](\"pyspark-parallelize-server\") { def handleConnection(sock: Socket): JavaRDD[Array[Byte]] = { val in = sock.getInputStream() val dechunkedInput: InputStream = new DechunkedInputStream(in) streamToRDD(dechunkedInput) } protected def streamToRDD(input: InputStream): RDD[Array[Byte]] } private[spark] class PythonParallelizeServer(sc: SparkContext, parallelism: Int) extends PythonRDDServer { override protected def streamToRDD(input: InputStream): RDD[Array[Byte]] = { PythonRDD.readRDDFromInputStream(sc, input, parallelism) } }"
        ],
        "more_info": "Used ANY in relation_types instead of call_by and added CLASS in neighbor_types"
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### PythonRunner object and methods that use it\n\nScope limited strictly to the code present in this repository. All paths and signatures are taken verbatim from the sources.\n\n1) Definition: PythonRunner companion object\n- File: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala\n- Fully-qualified name: org.apache.spark.api.python.PythonRunner (object)\n- Visibility: private[spark]\n- Members:\n  - val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]()\n    - Type: java.util.Set[(Socket, Long)] created via ConcurrentHashMap.newKeySet\n    - Purpose in code: tracks already running worker monitor threads for (worker socket, taskAttemptId) pairs.\n  - def apply(func: PythonFunction): PythonRunner\n    - Body: new PythonRunner(Seq(ChainedPythonFunctions(Seq(func))))\n    - Role: convenience constructor that wraps a single PythonFunction into a ChainedPythonFunctions sequence and instantiates a PythonRunner (class).\n\n2) Methods that reference the PythonRunner object\n\nA. org.apache.spark.api.python.PythonRDD#compute\n- File: core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala\n- Signature:\n  override def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]]\n- Usage of PythonRunner object:\n  - val runner = PythonRunner(func)\n  - Calls the companion object's apply to create a PythonRunner, then invokes runner.compute(...) to execute.\n- Exact snippet (abridged, preserving the relevant lines):\n  val runner = PythonRunner(func)\n  runner.compute(firstParent.iterator(split, context), split.index, context)\n\nB. org.apache.spark.sql.execution.python.PythonForeachWriter (class)  lazy vals that invoke PythonRunner.apply\n- File: sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonForeachWriter.scala\n- Declarations:\n  - private lazy val pythonRunner = { PythonRunner(func) }\n  - private lazy val outputIterator =\n      pythonRunner.compute(inputByteIterator, context.partitionId(), context)\n- How it is used by methods:\n  - open(partitionId: Long, version: Long): Boolean triggers initialization of lazy vals by referencing outputIterator.\n  - close(errorOrNull: Throwable): accesses outputIterator (hasNext/next()).\n- Methods in this class that indirectly use the PythonRunner object through these lazy vals:\n  - override def open(partitionId: Long, version: Long): Boolean\n  - override def close(errorOrNull: Throwable): Unit\n\nC. org.apache.spark.api.python.BasePythonRunner#compute  references PythonRunner.runningMonitorThreads\n- File: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala (inside BasePythonRunner)\n- Signature:\n  def compute(\n      inputIterator: Iterator[IN],\n      partitionIndex: Int,\n      context: TaskContext): Iterator[OUT]\n- Usage of PythonRunner object:\n  - When reuseWorker is true, it constructs a key of type (Socket, Long) and calls\n    if (PythonRunner.runningMonitorThreads.add(key)) { new MonitorThread(...).start() }\n- This avoids creating multiple monitor threads for the same python worker and task context.\n\nD. org.apache.spark.api.python.BasePythonRunner#MonitorThread#run  references PythonRunner.runningMonitorThreads\n- File: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala (inside BasePythonRunner)\n- Class: MonitorThread(env: SparkEnv, worker: Socket, context: TaskContext) extends Thread(...)\n- Method signature:\n  override def run(): Unit\n- Usage of PythonRunner object:\n  - In the finally block, when reuseWorker is true, it removes the key from the set:\n    val key = (worker, context.taskAttemptId)\n    PythonRunner.runningMonitorThreads.remove(key)\n\nNotes\n- The term \"PythonRunner\" refers to two distinct entities in the same file:\n  1) The companion object described in section 1.\n  2) The class private[spark] class PythonRunner(...) which extends BasePythonRunner[...] and implements the concrete writer/reader behavior. This document focuses on the object and the methods that reference the object per the request.\n- All behaviors and relationships above are stated exactly as visible in code without external assumptions.",
           "prompt_time": 0.191,
          "answer_time": 197,
          "context_tokens": 16294,
          "used_context": [
            "## OBJECT: org/apache/spark/api/python/PythonRunner.\n} } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket. */ class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends. */ private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner ",
            "## METHOD: org/apache/spark/api/python/BasePythonRunner#compute().\nmem.map(_ / cores) } def compute( inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): Iterator[OUT] = { val startTime = System.currentTimeMillis val env = SparkEnv.get // Get the executor cores and pyspark memory, they are passed via the local properties when // the user specified them in a ResourceProfile. val execCoresProp = Option(context.getLocalProperty(EXECUTOR_CORES_LOCAL_PROPERTY)) val memoryMb = Option(context.getLocalProperty(PYSPARK_MEMORY_LOCAL_PROPERTY)).map(_.toLong) val localdir = env.blockManager.diskBlockManager.localDirs.map(f => f.getPath()).mkString(\",\") // if OMP_NUM_THREADS is not explicitly set, override it with the number of cores if (conf.getOption(\"spark.executorEnv.OMP_NUM_THREADS\").isEmpty) { // SPARK-28843: limit the OpenMP thread pool to the number of cores assigned to this executor // this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool // see https://github.com/numpy/numpy/issues/10455 execCoresProp.foreach(envVars.put(\"OMP_NUM_THREADS\", _)) } envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread if (reuseWorker) { envVars.put(\"SPARK_REUSE_WORKER\", \"1\") } if (simplifiedTraceback) { envVars.put(\"SPARK_SIMPLIFIED_TRACEBACK\", \"1\") } // SPARK-30299 this could be wrong with standalone mode when executor // cores might not be correct because it defaults to all cores on the box. val execCores = execCoresProp.map(_.toInt).getOrElse(conf.get(EXECUTOR_CORES)) val workerMemoryMb = getWorkerMemoryMb(memoryMb, execCores) if (workerMemoryMb.isDefined) { envVars.put(\"PYSPARK_EXECUTOR_MEMORY_MB\", workerMemoryMb.get.toString) } envVars.put(\"SPARK_AUTH_SOCKET_TIMEOUT\", authSocketTimeout.toString) envVars.put(\"SPARK_BUFFER_SIZE\", bufferSize.toString) if (faultHandlerEnabled) { envVars.put(\"PYTHON_FAULTHANDLER_DIR\", BasePythonRunner.faultHandlerLogDir.toString) } val (worker: Socket, pid: Option[Int]) = env.createPythonWorker( pythonExec, envVars.asScala.toMap) // Whether is the worker released into idle pool or closed. When any codes try to release or // close a worker, they should use `releasedOrClosed.compareAndSet` to flip the state to make // sure there is only one winner that is going to release or close the worker. val releasedOrClosed = new AtomicBoolean(false) // Start a thread to feed the process input from our parent's iterator val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context) context.addTaskCompletionListener[Unit] { _ => writerThread.shutdownOnTaskCompletion() if (!reuseWorker || releasedOrClosed.compareAndSet(false, true)) { try { worker.close() } catch { case e: Exception => logWarning(\"Failed to close worker socket\", e) } } } writerThread.start() new WriterMonitorThread(SparkEnv.get, worker, writerThread, context).start() if (reuseWorker) { val key = (worker, context.taskAttemptId) // SPARK-35009: avoid creating multiple monitor threads for the same python worker // and task context if (PythonRunner.runningMonitorThreads.add(key)) { new MonitorThread(SparkEnv.get, worker, context).start() } } else { new MonitorThread(SparkEnv.get, worker, context).start() } // Return an iterator that read lines from the process's stdout val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize)) val stdoutIterator = newReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) new InterruptibleIterator(context, stdoutIterator) } protected def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext): WriterThread protected def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[OUT] /** * The thread responsible for writing the data from the PythonRDD's parent iterator to the * Python process.  abstract class WriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[IN], partitionIndex: Int, context: TaskContext) extends Thread(s\"stdout writer for $pythonExec\") { @volatile private var _exception: Throwable = null private val pythonIncludes = funcs.flatMap(_.funcs.flatMap(_.pythonIncludes.asScala)).toSet private val broadcastVars = funcs.flatMap(_.funcs.flatMap(_.broadcastVars.asScala)) setDaemon(true) /** Contains the throwable thrown while writing the parent iterator to the Python process.  def exception: Option[Throwable] = Option(_exception) /** * Terminates the writer thread and waits for it to exit, ignoring any exceptions that may occur * due to cleanup.  def shutdownOnTaskCompletion(): Unit = { assert(context.isCompleted) this.interrupt() // Task completion listeners that run after this method returns may invalidate // `inputIterator`. For example, when `inputIterator` was generated by the off-heap vectorized // reader, a task completion listener will free the underlying off-heap buffers. If the writer // thread is still running when `inputIterator` is invalidated, it can cause a use-after-free // bug that crashes the executor (SPARK-33277). Therefore this method must wait for the writer // thread to exit before returning. this.join() } /** * Writes a command section to the stream connected to the Python worker.  protected def writeCommand(dataOut: DataOutputStream): Unit /** * Writes input data to the stream connected to the Python worker.  protected def writeIteratorToStream(dataOut: DataOutputStream): Unit override def run(): Unit = Utils.logUncaughtExceptions { try { TaskContext.setTaskContext(context) val stream = new BufferedOutputStream(worker.getOutputStream, bufferSize) val dataOut = new DataOutputStream(stream) // Partition index dataOut.writeInt(partitionIndex) // Python version of driver PythonRDD.writeUTF(pythonVer, dataOut) // Init a ServerSocket to accept method calls from Python side. val isBarrier = context.isInstanceOf[BarrierTaskContext] if (isBarrier) { serverSocket = Some(new ServerSocket(/* port  0, /* backlog  1, InetAddress.getByName(\"localhost\"))) // A call to accept() for ServerSocket shall block infinitely. serverSocket.foreach(_.setSoTimeout(0)) new Thread(\"accept-connections\") { setDaemon(true) override def run(): Unit = { while (!serverSocket.get.isClosed()) { var sock: Socket = null try { sock = serverSocket.get.accept() // Wait for function call from python side. sock.setSoTimeout(10000) authHelper.authClient(sock) val input = new DataInputStream(sock.getInputStream()) val requestMethod = input.readInt() // The BarrierTaskContext function may wait infinitely, socket shall not timeout // before the function finishes. sock.setSoTimeout(0) requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => barrierAndServe(requestMethod, sock) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => val length = input.readInt() val message = new Array[Byte](length) input.readFully(message) barrierAndServe(requestMethod, sock, new String(message, UTF_8)) case _ => val out = new DataOutputStream(new BufferedOutputStream( sock.getOutputStream)) writeUTF(BarrierTaskContextMessageProtocol.ERROR_UNRECOGNIZED_FUNCTION, out) } } catch { case e: SocketException if e.getMessage.contains(\"Socket closed\") => // It is possible that the ServerSocket is not closed, but the native socket // has already been closed, we shall catch and silently ignore this case. } finally { if (sock != null) { sock.close() } } } } }.start() } val secret = if (isBarrier) { authHelper.secret } else { \"\" } // Close ServerSocket on task completion. serverSocket.foreach { server => context.addTaskCompletionListener[Unit](_ => server.close()) } val boundPort: Int = serverSocket.map(_.getLocalPort).getOrElse(0) if (boundPort == -1) { val message = \"ServerSocket failed to bind to Java side.\" logError(message) throw new SparkException(message) } else if (isBarrier) { logDebug(s\"Started ServerSocket on port $boundPort.\") } // Write out the TaskContextInfo dataOut.writeBoolean(isBarrier) dataOut.writeInt(boundPort) val secretBytes = secret.getBytes(UTF_8) dataOut.writeInt(secretBytes.length) dataOut.write(secretBytes, 0, secretBytes.length) dataOut.writeInt(context.stageId()) dataOut.writeInt(context.partitionId()) dataOut.writeInt(context.attemptNumber()) dataOut.writeLong(context.taskAttemptId()) dataOut.writeInt(context.cpus()) val resources = context.resources() dataOut.writeInt(resources.size) resources.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v.name, dataOut) dataOut.writeInt(v.addresses.size) v.addresses.foreach { case addr => PythonRDD.writeUTF(addr, dataOut) } } val localProps = context.getLocalProperties.asScala dataOut.writeInt(localProps.size) localProps.foreach { case (k, v) => PythonRDD.writeUTF(k, dataOut) PythonRDD.writeUTF(v, dataOut) } // sparkFilesDir PythonRDD.writeUTF(SparkFiles.getRootDirectory(), dataOut) // Python includes (*.zip and *.egg files) dataOut.writeInt(pythonIncludes.size) for (include <- pythonIncludes) { PythonRDD.writeUTF(include, dataOut) } // Broadcast variables val oldBids = PythonRDD.getWorkerBroadcasts(worker) val newBids = broadcastVars.map(_.id).toSet // number of different broadcasts val toRemove = oldBids.diff(newBids) val addedBids = newBids.diff(oldBids) val cnt = toRemove.size + addedBids.size val needsDecryptionServer = env.serializerManager.encryptionEnabled && addedBids.nonEmpty dataOut.writeBoolean(needsDecryptionServer) dataOut.writeInt(cnt) def sendBidsToRemove(): Unit = { for (bid <- toRemove) { // remove the broadcast from worker dataOut.writeLong(-bid - 1) // bid >= 0 oldBids.remove(bid) } } if (needsDecryptionServer) { // if there is encryption, we setup a server which reads the encrypted files, and sends // the decrypted data to python val idsAndFiles = broadcastVars.flatMap { broadcast => if (!oldBids.contains(broadcast.id)) { Some((broadcast.id, broadcast.value.path)) } else { None } } val server = new EncryptedPythonBroadcastServer(env, idsAndFiles) dataOut.writeInt(server.port) logTrace(s\"broadcast decryption server setup on ${server.port}\") PythonRDD.writeUTF(server.secret, dataOut) sendBidsToRemove() idsAndFiles.foreach { case (id, _) => // send new broadcast dataOut.writeLong(id) oldBids.add(id) } dataOut.flush() logTrace(\"waiting for python to read decrypted broadcast data from server\") server.waitTillBroadcastDataSent() logTrace(\"done sending decrypted data to python\") } else { sendBidsToRemove() for (broadcast <- broadcastVars) { if (!oldBids.contains(broadcast.id)) { // send new broadcast dataOut.writeLong(broadcast.id) PythonRDD.writeUTF(broadcast.value.path, dataOut) oldBids.add(broadcast.id) } } } dataOut.flush() dataOut.writeInt(evalType) writeCommand(dataOut) writeIteratorToStream(dataOut) dataOut.writeInt(SpecialLengths.END_OF_STREAM) dataOut.flush() } catch { case t: Throwable if (NonFatal(t) || t.isInstanceOf[Exception]) => if (context.isCompleted || context.isInterrupted) { logDebug(\"Exception/NonFatal Error thrown after task completion (likely due to \" + \"cleanup)\", t) if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } else { // We must avoid throwing exceptions/NonFatals here, because the thread uncaught // exception handler will kill the whole executor (see // org.apache.spark.executor.Executor). _exception = t if (!worker.isClosed) { Utils.tryLog(worker.shutdownOutput()) } } } } /** * Gateway to call BarrierTaskContext methods.  def barrierAndServe(requestMethod: Int, sock: Socket, message: String = \"\"): Unit = { require( serverSocket.isDefined, \"No available ServerSocket to redirect the BarrierTaskContext method call.\" ) val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream)) try { val messages = requestMethod match { case BarrierTaskContextMessageProtocol.BARRIER_FUNCTION => context.asInstanceOf[BarrierTaskContext].barrier() Array(BarrierTaskContextMessageProtocol.BARRIER_RESULT_SUCCESS) case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION => context.asInstanceOf[BarrierTaskContext].allGather(message) } out.writeInt(messages.length) messages.foreach(writeUTF(_, out)) } catch { case e: SparkException => writeUTF(e.getMessage, out) } finally { out.close() } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } } abstract class ReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext) extends Iterator[OUT] { private var nextObj: OUT = _ private var eos = false override def hasNext: Boolean = nextObj != null || { if (!eos) { nextObj = read() hasNext } else { false } } override def next(): OUT = { if (hasNext) { val obj = nextObj nextObj = null.asInstanceOf[OUT] obj } else { Iterator.empty.next() } } /** * Reads next object from the stream. * When the stream reaches end of data, needs to process the following sections, * and then returns null.  protected def read(): OUT protected def handleTimingData(): Unit = { // Timing data from worker val bootTime = stream.readLong() val initTime = stream.readLong() val finishTime = stream.readLong() val boot = bootTime - startTime val init = initTime - bootTime val finish = finishTime - initTime val total = finishTime - startTime logInfo(\"Times: total = %s, boot = %s, init = %s, finish = %s\".format(total, boot, init, finish)) val memoryBytesSpilled = stream.readLong() val diskBytesSpilled = stream.readLong() context.taskMetrics.incMemoryBytesSpilled(memoryBytesSpilled) context.taskMetrics.incDiskBytesSpilled(diskBytesSpilled) } protected def handlePythonException(): PythonException = { // Signals that an exception has been thrown in python val exLength = stream.readInt() val obj = new Array[Byte](exLength) stream.readFully(obj) new PythonException(new String(obj, StandardCharsets.UTF_8), writerThread.exception.getOrElse(null)) } protected def handleEndOfDataSection(): Unit = { // We've finished the data section of the output, but we can still // read some accumulator updates: val numAccumulatorUpdates = stream.readInt() (1 to numAccumulatorUpdates).foreach { _ => val updateLen = stream.readInt() val update = new Array[Byte](updateLen) stream.readFully(update) maybeAccumulator.foreach(_.add(update)) } // Check whether the worker is ready to be re-used. if (stream.readInt() == SpecialLengths.END_OF_STREAM) { if (reuseWorker && releasedOrClosed.compareAndSet(false, true)) { env.releasePythonWorker(pythonExec, envVars.asScala.toMap, worker) } } eos = true } protected val handleException: PartialFunction[Throwable, OUT] = { case e: Exception if context.isInterrupted => logDebug(\"Exception thrown after task interruption\", e) throw new TaskKilledException(context.getKillReason().getOrElse(\"unknown reason\")) case e: Exception if writerThread.exception.isDefined => logError(\"Python worker exited unexpectedly (crashed)\", e) logError(\"This may have been caused by a prior exception:\", writerThread.exception.get) throw writerThread.exception.get case eof: EOFException if faultHandlerEnabled && pid.isDefined && JavaFiles.exists(BasePythonRunner.faultHandlerLogPath(pid.get)) => val path = BasePythonRunner.faultHandlerLogPath(pid.get) val error = String.join(\"\\n\", JavaFiles.readAllLines(path)) + \"\\n\" JavaFiles.deleteIfExists(path) throw new SparkException(s\"Python worker exited unexpectedly (crashed): $error\", eof) case eof: EOFException => throw new SparkException(\"Python worker exited unexpectedly (crashed)\", eof) } } /** * It is necessary to have a monitor thread for python workers if the user cancels with * interrupts disabled. In that case we will need to explicitly kill the worker, otherwise the * threads can block indefinitely.  class MonitorThread(env: SparkEnv, worker: Socket, context: TaskContext) extends Thread(s\"Worker Monitor for $pythonExec\") { /** How long to wait before killing the python worker if a task cannot be interrupted.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) private def monitorWorker(): Unit = { // Kill the worker if it is interrupted, checking until task completion. // TODO: This has a race condition if interruption occurs, as completed may still become true. while (!context.isInterrupted && !context.isCompleted) { Thread.sleep(2000) } if (!context.isCompleted) { Thread.sleep(taskKillTimeout) if (!context.isCompleted) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning(s\"Incomplete task $taskName interrupted: Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
            "## CLASS: org/apache/spark/sql/execution/python/PythonForeachWriter#\nclass PythonForeachWriter(func: PythonFunction, schema: StructType) extends ForeachWriter[UnsafeRow] { private lazy val context = TaskContext.get() private lazy val buffer = new PythonForeachWriter.UnsafeRowBuffer( context.taskMemoryManager, new File(Utils.getLocalDir(SparkEnv.get.conf)), schema.fields.length) private lazy val inputRowIterator = buffer.iterator private lazy val inputByteIterator = { EvaluatePython.registerPicklers() val objIterator = inputRowIterator.map { row => EvaluatePython.toJava(row, schema) } new SerDeUtil.AutoBatchedPickler(objIterator) } private lazy val pythonRunner = { PythonRunner(func) } private lazy val outputIterator = pythonRunner.compute(inputByteIterator, context.partitionId(), context) override def open(partitionId: Long, version: Long): Boolean = { outputIterator // initialize everything TaskContext.get.addTaskCompletionListener[Unit] { _ => buffer.close() } true } override def process(value: UnsafeRow): Unit = { buffer.add(value) } override def close(errorOrNull: Throwable): Unit = { buffer.allRowsAdded() if (outputIterator.hasNext) outputIterator.next() // to throw python exception if there was one } } object PythonForeachWriter { /** * A buffer that is designed for the sole purpose of buffering UnsafeRows in PythonForeachWriter. * It is designed to be used with only 1 writer thread (i.e. JVM task thread) and only 1 reader * thread (i.e. PythonRunner writing thread that reads from the buffer and writes to the Python * worker stdin). Adds to the buffer are non-blocking, and reads through the buffer's iterator * are blocking, that is, it blocks until new data is available or all data has been added. * * Internally, it uses a [[HybridRowQueue]] to buffer the rows in a practically unlimited queue * across memory and local disk. However, HybridRowQueue is designed to be used only with * EvalPythonExec where the reader is always behind the writer, that is, the reader does not * try to read n+1 rows if the writer has only written n rows at any point of time. This * assumption is not true for PythonForeachWriter where rows may be added at a different rate as * they are consumed by the python worker. Hence, to maintain the invariant of the reader being * behind the writer while using HybridRowQueue, the buffer does the following * - Keeps a count of the rows in the HybridRowQueue * - Blocks the buffer's consuming iterator when the count is 0 so that the reader does not * try to read more rows than what has been written. * * The implementation of the blocking iterator (ReentrantLock, Condition, etc.) has been borrowed * from that of ArrayBlockingQueue.  class UnsafeRowBuffer(taskMemoryManager: TaskMemoryManager, tempDir: File, numFields: Int) extends Logging { private val queue = HybridRowQueue(taskMemoryManager, tempDir, numFields) private val lock = new ReentrantLock() private val unblockRemove = lock.newCondition() // All of these are guarded by `lock` private var count = 0L private var allAdded = false private var exception: Throwable = null val iterator = new NextIterator[UnsafeRow] { override protected def getNext(): UnsafeRow = { val row = remove() if (row == null) finished = true row } override protected def close(): Unit = { } } def add(row: UnsafeRow): Unit = withLock { assert(queue.add(row), s\"Failed to add row to HybridRowQueue while sending data to Python\" + s\"[count = $count, allAdded = $allAdded, exception = $exception]\") count += 1 unblockRemove.signal() logTrace(s\"Added $row, $count left\") } private def remove(): UnsafeRow = withLock { while (count == 0 && !allAdded && exception == null) { unblockRemove.await(100, TimeUnit.MILLISECONDS) } // If there was any error in the adding thread, then rethrow it in the removing thread if (exception != null) throw exception if (count > 0) { val row = queue.remove() assert(row != null, \"HybridRowQueue.remove() returned null \" + s\"[count = $count, allAdded = $allAdded, exception = $exception]\") count -= 1 logTrace(s\"Removed $row, $count left\") row } else { null } } def allRowsAdded(): Unit = withLock { allAdded = true unblockRemove.signal() } def close(): Unit = { queue.close() } private def withLock[T](f: => T): T = { lock.lockInterruptibly() try { f } catch { case e: Throwable => if (exception == null) exception = e throw e } finally { lock.unlock() } } } }",
            "## METHOD: org/apache/spark/api/python/PythonRunner.apply().\n* * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }\n",
            "## METHOD: org/apache/spark/api/python/BasePythonRunner#MonitorThread#run().\n} } } override def run(): Unit = { try { monitorWorker() } finally { if (reuseWorker) { val key = (worker, context.taskAttemptId) PythonRunner.runningMonitorThreads.remove(key) } } } } /** * This thread monitors the WriterThread and kills it in case of deadlock. * * A deadlock can arise if the task completes while the writer thread is sending input to the * Python process (e.g. due to the use of `take()`), and the Python process is still producing * output. When the inputs are sufficiently large, this can result in a deadlock due to the use of * blocking I/O (SPARK-38677). To resolve the deadlock, we need to close the socket.  class WriterMonitorThread( env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext) extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") { /** * How long to wait before closing the socket if the writer thread has not exited after the task * ends.  private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT) setDaemon(true) override def run(): Unit = { // Wait until the task is completed (or the writer thread exits, in which case this thread has // nothing to do). while (!context.isCompleted && writerThread.isAlive) { Thread.sleep(2000) } if (writerThread.isAlive) { Thread.sleep(taskKillTimeout) // If the writer thread continues running, this indicates a deadlock. Kill the worker to // resolve the deadlock. if (writerThread.isAlive) { try { // Mimic the task name used in `Executor` to help the user find out the task to blame. val taskName = s\"${context.partitionId}.${context.attemptNumber} \" + s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\" logWarning( s\"Detected deadlock while completing task $taskName: \" + \"Attempting to kill Python Worker\") env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker) } catch { case e: Exception => logError(\"Exception when trying to kill worker\", e) } } } } } } private[spark] object PythonRunner { // already running worker monitor threads for worker and task attempts ID pairs val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]() def apply(func: PythonFunction): PythonRunner = { new PythonRunner(Seq(ChainedPythonFunctions(Seq(func)))) } } /** * A helper class to run Python mapPartition in Spark.  private[spark] class PythonRunner(funcs: Seq[ChainedPythonFunctions]) extends BasePythonRunner[Array[Byte], Array[Byte]]( funcs, PythonEvalType.NON_UDF, Array(Array(0))) { protected override def newWriterThread( env: SparkEnv, worker: Socket, inputIterator: Iterator[Array[Byte]], partitionIndex: Int, context: TaskContext): WriterThread = { new WriterThread(env, worker, inputIterator, partitionIndex, context) { protected override def writeCommand(dataOut: DataOutputStream): Unit = { val command = funcs.head.funcs.head.command dataOut.writeInt(command.length) dataOut.write(command.toArray) } protected override def writeIteratorToStream(dataOut: DataOutputStream): Unit = { PythonRDD.writeIteratorToStream(inputIterator, dataOut) dataOut.writeInt(SpecialLengths.END_OF_DATA_SECTION) } } } protected override def newReaderIterator( stream: DataInputStream, writerThread: WriterThread, startTime: Long, env: SparkEnv, worker: Socket, pid: Option[Int], releasedOrClosed: AtomicBoolean, context: TaskContext): Iterator[Array[Byte]] = { new ReaderIterator( stream, writerThread, startTime, env, worker, pid, releasedOrClosed, context) { protected override def read(): Array[Byte] = { if (writerThread.exception.isDefined) { throw writerThread.exception.get } try { stream.readInt() match { case length if length > 0 => val obj = new Array[Byte](length) stream.readFully(obj) obj case 0 => Array.emptyByteArray case SpecialLengths.TIMING_DATA => handleTimingData() read() case SpecialLengths.PYTHON_EXCEPTION_THROWN => throw handlePythonException() case SpecialLengths.END_OF_DATA_SECTION => handleEndOfDataSection() null } } catch handleException } } } } private[spark] object SpecialLengths { val END_OF_DATA_SECTION = -1 val PYTHON_EXCEPTION_THROWN = -2 val TIMING_DATA = -3 val END_OF_STREAM = -4 val NULL = -5 val START_ARROW_STREAM = -6 } private[spark] object BarrierTaskContextMessageProtocol { val BARRIER_FUNCTION = 1 val ALL_GATHER_FUNCTION = 2 val BARRIER_RESULT_SUCCESS = \"success\" val ERROR_UNRECOGNIZED_FUNCTION = \"Not recognized function call from python side.\" }",
            "## METHOD: org/apache/spark/api/python/PythonRDD#compute().\noverride def compute(split: Partition, context: TaskContext): Iterator[Array[Byte]] = { val runner = PythonRunner(func) runner.compute(firstParent.iterator(split, context), split.index, context) } @transient protected lazy override val isBarrier_ : Boolean = isFromBarrier || dependencies.exists(_.rdd.isBarrier()) } /** * A wrapper for a Python function, contains all necessary context to run the function in Python * runner.  private[spark] case class PythonFunction( command: Seq[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) { def this( command: Array[Byte], envVars: JMap[String, String], pythonIncludes: JList[String], pythonExec: String, pythonVer: String, broadcastVars: JList[Broadcast[PythonBroadcast]], accumulator: PythonAccumulatorV2) = { this(command.toSeq, envVars, pythonIncludes, pythonExec, pythonVer, broadcastVars, accumulator) } } /** * A wrapper for chained Python functions (from bottom to top). * @param funcs  private[spark] case class ChainedPythonFunctions(funcs: Seq[PythonFunction]) /** Thrown for exceptions in user Python code.  private[spark] class PythonException(msg: String, cause: Throwable) extends RuntimeException(msg, cause) /** * Form an RDD[(Array[Byte], Array[Byte])] from key-value pairs returned from Python. * This is used by PySpark's shuffle operations.  private class PairwiseRDD(prev: RDD[Array[Byte]]) extends RDD[(Long, Array[Byte])](prev) { override def getPartitions: Array[Partition] = prev.partitions override val partitioner: Option[Partitioner] = prev.partitioner override def compute(split: Partition, context: TaskContext): Iterator[(Long, Array[Byte])] = prev.iterator(split, context).grouped(2).map { case Seq(a, b) => (Utils.deserializeLongValue(a), b) case x => throw new SparkException(\"PairwiseRDD: unexpected value: \" + x) } val asJavaPairRDD : JavaPairRDD[Long, Array[Byte]] = JavaPairRDD.fromRDD(this) } private[spark] object PythonRDD extends Logging { // remember the broadcasts sent to each worker private val workerBroadcasts = new mutable.WeakHashMap[Socket, mutable.Set[Long]]() // Authentication helper used when serving iterator data. private lazy val authHelper = { val conf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf()) new SocketAuthHelper(conf) } def getWorkerBroadcasts(worker: Socket): mutable.Set[Long] = { synchronized { workerBroadcasts.getOrElseUpdate(worker, new mutable.HashSet[Long]()) } } /** * Return an RDD of values from an RDD of (Long, Array[Byte]), with preservePartitions=true * * This is useful for PySpark to have the partitioner after partitionBy()  def valueOfPair(pair: JavaPairRDD[Long, Array[Byte]]): JavaRDD[Array[Byte]] = { pair.rdd.mapPartitions(it => it.map(_._2), true) } /** * Adapter for calling SparkContext#runJob from Python. * * This method will serve an iterator of an array that contains all elements in the RDD * (effectively a collect()), but allows you to run on a certain subset of partitions, * or to enable local execution. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def runJob( sc: SparkContext, rdd: JavaRDD[Array[Byte]], partitions: JArrayList[Int]): Array[Any] = { type ByteArray = Array[Byte] type UnrolledPartition = Array[ByteArray] val allPartitions: Array[UnrolledPartition] = sc.runJob(rdd, (x: Iterator[ByteArray]) => x.toArray, partitions.asScala.toSeq) val flattenedPartition: UnrolledPartition = Array.concat(allPartitions: _*) serveIterator(flattenedPartition.iterator, s\"serve RDD ${rdd.id} with partitions ${partitions.asScala.mkString(\",\")}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def collectAndServe[T](rdd: RDD[T]): Array[Any] = { serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to collect an RDD as an iterator, then serve it via socket. * This method is similar with `PythonRDD.collectAndServe`, but user can specify job group id, * job description, and interruptOnCancel option.  def collectAndServeWithJobGroup[T]( rdd: RDD[T], groupId: String, description: String, interruptOnCancel: Boolean): Array[Any] = { val sc = rdd.sparkContext sc.setJobGroup(groupId, description, interruptOnCancel) serveIterator(rdd.collect().iterator, s\"serve RDD ${rdd.id}\") } /** * A helper function to create a local RDD iterator and serve it via socket. Partitions are * are collected as separate jobs, by order of index. Partition data is first requested by a * non-zero integer to start a collection job. The response is prefaced by an integer with 1 * meaning partition data will be served, 0 meaning the local iterator has been consumed, * and -1 meaning an error occurred during collection. This function is used by * pyspark.rdd._local_iterator_from_socket(). * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def toLocalIteratorAndServe[T](rdd: RDD[T], prefetchPartitions: Boolean = false): Array[Any] = { val handleFunc = (sock: Socket) => { val out = new DataOutputStream(sock.getOutputStream) val in = new DataInputStream(sock.getInputStream) Utils.tryWithSafeFinallyAndFailureCallbacks(block = { // Collects a partition on each iteration val collectPartitionIter = rdd.partitions.indices.iterator.map { i => var result: Array[Any] = null rdd.sparkContext.submitJob( rdd, (iter: Iterator[Any]) => iter.toArray, Seq(i), // The partition we are evaluating (_, res: Array[Any]) => result = res, result) } val prefetchIter = collectPartitionIter.buffered // Write data until iteration is complete, client stops iteration, or error occurs var complete = false while (!complete) { // Read request for data, value of zero will stop iteration or non-zero to continue if (in.readInt() == 0) { complete = true } else if (prefetchIter.hasNext) { // Client requested more data, attempt to collect the next partition val partitionFuture = prefetchIter.next() // Cause the next job to be submitted if prefetchPartitions is enabled. if (prefetchPartitions) { prefetchIter.headOption } val partitionArray = ThreadUtils.awaitResult(partitionFuture, Duration.Inf) // Send response there is a partition to read out.writeInt(1) // Write the next object and signal end of data for this iteration writeIteratorToStream(partitionArray.iterator, out) out.writeInt(SpecialLengths.END_OF_DATA_SECTION) out.flush() } else { // Send response there are no more partitions to read and close out.writeInt(0) complete = true } } })(catchBlock = { // Send response that an error occurred, original exception is re-thrown out.writeInt(-1) }, finallyBlock = { out.close() in.close() }) } val server = new SocketFuncServer(authHelper, \"serve toLocalIterator\", handleFunc) Array(server.port, server.secret, server) } def readRDDFromFile( sc: JavaSparkContext, filename: String, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromFile(sc, filename, parallelism) } def readRDDFromInputStream( sc: SparkContext, in: InputStream, parallelism: Int): JavaRDD[Array[Byte]] = { JavaRDD.readRDDFromInputStream(sc, in, parallelism) } def setupBroadcast(path: String): PythonBroadcast = { new PythonBroadcast(path) } def writeIteratorToStream[T](iter: Iterator[T], dataOut: DataOutputStream): Unit = { def write(obj: Any): Unit = obj match { case null => dataOut.writeInt(SpecialLengths.NULL) case arr: Array[Byte] => dataOut.writeInt(arr.length) dataOut.write(arr) case str: String => writeUTF(str, dataOut) case stream: PortableDataStream => write(stream.toArray()) case (key, value) => write(key) write(value) case other => throw new SparkException(\"Unexpected element type \" + other.getClass) } iter.foreach(write) } /** * Create an RDD from a path using [[org.apache.hadoop.mapred.SequenceFileInputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def sequenceFile[K, V]( sc: JavaSparkContext, path: String, keyClassMaybeNull: String, valueClassMaybeNull: String, keyConverterClass: String, valueConverterClass: String, minSplits: Int, batchSize: Int): JavaRDD[Array[Byte]] = { val keyClass = Option(keyClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val valueClass = Option(valueClassMaybeNull).getOrElse(\"org.apache.hadoop.io.Text\") val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val rdd = sc.sc.sequenceFile[K, V](path, kc, vc, minSplits) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(sc.hadoopConfiguration())) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map that is * passed in from Python, using an arbitrary [[org.apache.hadoop.mapreduce.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = newAPIHadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def newAPIHadoopRDDFromClassNames[K, V, F <: NewInputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration): RDD[(K, V)] = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.newAPIHadoopFile[K, V, F](path.get, fc, kc, vc, conf) } else { sc.sc.newAPIHadoopRDD[K, V, F](conf, fc, kc, vc) } } /** * Create an RDD from a file path, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class. * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def hadoopFile[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: String, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val mergedConf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, Some(path), inputFormatClass, keyClass, valueClass, mergedConf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(mergedConf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } /** * Create an RDD from a [[org.apache.hadoop.conf.Configuration]] converted from a map * that is passed in from Python, using an arbitrary [[org.apache.hadoop.mapred.InputFormat]], * key and value class * A key and/or value converter class can optionally be passed in * (see [[org.apache.spark.api.python.Converter]])  def hadoopRDD[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, inputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], batchSize: Int): JavaRDD[Array[Byte]] = { val conf = getMergedConf(confAsMap, sc.hadoopConfiguration()) val rdd = hadoopRDDFromClassNames[K, V, F](sc, None, inputFormatClass, keyClass, valueClass, conf) val confBroadcasted = sc.sc.broadcast(new SerializableConfiguration(conf)) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new WritableToJavaConverter(confBroadcasted)) JavaRDD.fromRDD(SerDeUtil.pairRDDToPython(converted, batchSize)) } private def hadoopRDDFromClassNames[K, V, F <: InputFormat[K, V]]( sc: JavaSparkContext, path: Option[String] = None, inputFormatClass: String, keyClass: String, valueClass: String, conf: Configuration) = { val kc = Utils.classForName[K](keyClass) val vc = Utils.classForName[V](valueClass) val fc = Utils.classForName[F](inputFormatClass) if (path.isDefined) { sc.sc.hadoopFile(path.get, fc, kc, vc) } else { sc.sc.hadoopRDD(new JobConf(conf), fc, kc, vc) } } def writeUTF(str: String, dataOut: DataOutputStream): Unit = { val bytes = str.getBytes(StandardCharsets.UTF_8) dataOut.writeInt(bytes.length) dataOut.write(bytes) } /** * Create a socket server and a background thread to serve the data in `items`, * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it tries to serialize all the data in `items` * and send them into this connection. * * The thread will terminate after all the data are sent or any exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  def serveIterator(items: Iterator[_], threadName: String): Array[Any] = { serveToStream(threadName) { out => writeIteratorToStream(items, new DataOutputStream(out)) } } /** * Create a socket server and background thread to execute the writeFunc * with the given OutputStream. * * The socket server can only accept one connection, or close if no connection * in 15 seconds. * * Once a connection comes in, it will execute the block of code and pass in * the socket output stream. * * The thread will terminate after the block of code is executed or any * exceptions happen. * * @return 3-tuple (as a Java array) with the port number of a local socket which serves the * data collected from this job, the secret for authentication, and a socket auth * server object that can be used to join the JVM serving thread in Python.  private[spark] def serveToStream( threadName: String)(writeFunc: OutputStream => Unit): Array[Any] = { SocketAuthServer.serveToStream(threadName, authHelper)(writeFunc) } private def getMergedConf(confAsMap: java.util.HashMap[String, String], baseConf: Configuration): Configuration = { val conf = PythonHadoopUtil.mapToConf(confAsMap) PythonHadoopUtil.mergeConfs(baseConf, conf) } private def inferKeyValueTypes[K, V, KK, VV](rdd: RDD[(K, V)], keyConverterClass: String = null, valueConverterClass: String = null): (Class[_ <: KK], Class[_ <: VV]) = { // Peek at an element to figure out key/value types. Since Writables are not serializable, // we cannot call first() on the converted RDD. Instead, we call first() on the original RDD // and then convert locally. val (key, value) = rdd.first() val (kc, vc) = getKeyValueConverters[K, V, KK, VV]( keyConverterClass, valueConverterClass, new JavaToWritableConverter) (kc.convert(key).getClass, vc.convert(value).getClass) } private def getKeyValueTypes[K, V](keyClass: String, valueClass: String): Option[(Class[K], Class[V])] = { for { k <- Option(keyClass) v <- Option(valueClass) } yield (Utils.classForName(k), Utils.classForName(v)) } private def getKeyValueConverters[K, V, KK, VV]( keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[_, _]): (Converter[K, KK], Converter[V, VV]) = { val keyConverter = Converter.getInstance(Option(keyConverterClass), defaultConverter.asInstanceOf[Converter[K, KK]]) val valueConverter = Converter.getInstance(Option(valueConverterClass), defaultConverter.asInstanceOf[Converter[V, VV]]) (keyConverter, valueConverter) } /** * Convert an RDD of key-value pairs from internal types to serializable types suitable for * output, or vice versa.  private def convertRDD[K, V](rdd: RDD[(K, V)], keyConverterClass: String, valueConverterClass: String, defaultConverter: Converter[Any, Any]): RDD[(Any, Any)] = { val (kc, vc) = getKeyValueConverters[K, V, Any, Any](keyConverterClass, valueConverterClass, defaultConverter) PythonHadoopUtil.convertRDD(rdd, kc, vc) } /** * Output a Python RDD of key-value pairs as a Hadoop SequenceFile using the Writable types * we convert from the RDD's key and value types. Note that keys and values can't be * [[org.apache.hadoop.io.Writable]] types already, since Writables are not Java * `Serializable` and we can't peek at them. The `path` can be on any Hadoop file system.  def saveAsSequenceFile[C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, compressionCodecClass: String): Unit = { saveAsHadoopFile( pyRDD, batchSerialized, path, \"org.apache.hadoop.mapred.SequenceFileOutputFormat\", null, null, null, null, new java.util.HashMap(), compressionCodecClass) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using old Hadoop * `OutputFormat` in mapred package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD.  def saveAsHadoopFile[F <: OutputFormat[_, _], C <: CompressionCodec]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String], compressionCodecClass: String): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val codec = Option(compressionCodecClass).map(Utils.classForName(_).asInstanceOf[Class[C]]) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName[F](outputFormatClass) converted.saveAsHadoopFile(path, kc, vc, fc, new JobConf(mergedConf), codec = codec) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using new Hadoop * `OutputFormat` in mapreduce package. Keys and values are converted to suitable output * types using either user specified converters or, if not specified, * [[org.apache.spark.api.python.JavaToWritableConverter]]. Post-conversion types * `keyClass` and `valueClass` are automatically inferred if not specified. The passed-in * `confAsMap` is merged with the default Hadoop conf associated with the SparkContext of * this RDD.  def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, path: String, outputFormatClass: String, keyClass: String, valueClass: String, keyConverterClass: String, valueConverterClass: String, confAsMap: java.util.HashMap[String, String]): Unit = { val rdd = SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized) val (kc, vc) = getKeyValueTypes(keyClass, valueClass).getOrElse( inferKeyValueTypes(rdd, keyConverterClass, valueConverterClass)) val mergedConf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(rdd, keyConverterClass, valueConverterClass, new JavaToWritableConverter) val fc = Utils.classForName(outputFormatClass).asInstanceOf[Class[F]] converted.saveAsNewAPIHadoopFile(path, kc, vc, fc, mergedConf) } /** * Output a Python RDD of key-value pairs to any Hadoop file system, using a Hadoop conf * converted from the passed-in `confAsMap`. The conf should set relevant output params ( * e.g., output path, output format, etc), in the same way as it would be configured for * a Hadoop MapReduce job. Both old and new Hadoop OutputFormat APIs are supported * (mapred vs. mapreduce). Keys/values are converted for output using either user specified * converters or, by default, [[org.apache.spark.api.python.JavaToWritableConverter]].  def saveAsHadoopDataset( pyRDD: JavaRDD[Array[Byte]], batchSerialized: Boolean, confAsMap: java.util.HashMap[String, String], keyConverterClass: String, valueConverterClass: String, useNewAPI: Boolean): Unit = { val conf = getMergedConf(confAsMap, pyRDD.context.hadoopConfiguration) val converted = convertRDD(SerDeUtil.pythonToPairRDD(pyRDD, batchSerialized), keyConverterClass, valueConverterClass, new JavaToWritableConverter) if (useNewAPI) { converted.saveAsNewAPIHadoopDataset(conf) } else { converted.saveAsHadoopDataset(new JobConf(conf)) } } } private class BytesToString extends org.apache.spark.api.java.function.Function[Array[Byte], String] { override def call(arr: Array[Byte]) : String = new String(arr, StandardCharsets.UTF_8) } /** * Internal class that acts as an `AccumulatorV2` for Python accumulators. Inside, it * collects a list of pickled strings that we pass to Python through a socket.  private[spark] class PythonAccumulatorV2( @transient private val serverHost: String, private val serverPort: Int, private val secretToken: String) extends CollectionAccumulator[Array[Byte]] with Logging{ Utils.checkHost(serverHost) val bufferSize = SparkEnv.get.conf.get(BUFFER_SIZE) /** * We try to reuse a single Socket to transfer accumulator updates, as they are all added * by the DAGScheduler's single-threaded RpcEndpoint anyway.  @transient private var socket: Socket = _ private def openSocket(): Socket = synchronized { if (socket == null || socket.isClosed) { socket = new Socket(serverHost, serverPort) logInfo(s\"Connected to AccumulatorServer at host: $serverHost port: $serverPort\") // send the secret just for the initial authentication when opening a new connection socket.getOutputStream.write(secretToken.getBytes(StandardCharsets.UTF_8)) } socket } // Need to override so the types match with PythonFunction override def copyAndReset(): PythonAccumulatorV2 = { new PythonAccumulatorV2(serverHost, serverPort, secretToken) } override def merge(other: AccumulatorV2[Array[Byte], JList[Array[Byte]]]): Unit = synchronized { val otherPythonAccumulator = other.asInstanceOf[PythonAccumulatorV2] // This conditional isn't strictly speaking needed - merging only currently happens on the // driver program - but that isn't guaranteed so incase this changes. if (serverHost == null) { // We are on the worker super.merge(otherPythonAccumulator) } else { // This happens on the master, where we pass the updates to Python through a socket val socket = openSocket() val in = socket.getInputStream val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream, bufferSize)) val values = other.value out.writeInt(values.size) for (array <- values.asScala) { out.writeInt(array.length) out.write(array) } out.flush() // Wait for a byte from the Python side as an acknowledgement val byteRead = in.read() if (byteRead == -1) { throw new SparkException(\"EOF reached before Python server acknowledged\") } } } } private[spark] class PythonBroadcast(@transient var path: String) extends Serializable with Logging { // id of the Broadcast variable which wrapped this PythonBroadcast private var broadcastId: Long = _ private var encryptionServer: SocketAuthServer[Unit] = null private var decryptionServer: SocketAuthServer[Unit] = null /** * Read data from disks, then copy it to `out`  private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException { out.writeLong(broadcastId) val in = new FileInputStream(new File(path)) try { Utils.copyStream(in, out) } finally { in.close() } } /** * Write data into disk and map it to a broadcast block.  private def readObject(in: ObjectInputStream): Unit = { broadcastId = in.readLong() val blockId = BroadcastBlockId(broadcastId, \"python\") val blockManager = SparkEnv.get.blockManager val diskBlockManager = blockManager.diskBlockManager if (!diskBlockManager.containsBlock(blockId)) { Utils.tryOrIOException { val dir = new File(Utils.getLocalDir(SparkEnv.get.conf)) val file = File.createTempFile(\"broadcast\", \"\", dir) val out = new FileOutputStream(file) Utils.tryWithSafeFinally { val size = Utils.copyStream(in, out) val ct = implicitly[ClassTag[Object]] // SPARK-28486: map broadcast file to a broadcast block, so that it could be // cleared by unpersist/destroy rather than gc(previously). val blockStoreUpdater = blockManager. TempFileBasedBlockStoreUpdater(blockId, StorageLevel.DISK_ONLY, ct, file, size) blockStoreUpdater.save() } { out.close() } } } path = diskBlockManager.getFile(blockId).getAbsolutePath } def setBroadcastId(bid: Long): Unit = { this.broadcastId = bid } def setupEncryptionServer(): Array[Any] = { encryptionServer = new SocketAuthServer[Unit](\"broadcast-encrypt-server\") { override def handleConnection(sock: Socket): Unit = { val env = SparkEnv.get val in = sock.getInputStream() val abspath = new File(path).getAbsolutePath val out = env.serializerManager.wrapForEncryption(new FileOutputStream(abspath)) DechunkedInputStream.dechunkAndCopyToOutput(in, out) } } Array(encryptionServer.port, encryptionServer.secret) } def setupDecryptionServer(): Array[Any] = { decryptionServer = new SocketAuthServer[Unit](\"broadcast-decrypt-server-for-driver\") { override def handleConnection(sock: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream())) Utils.tryWithSafeFinally { val in = SparkEnv.get.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } out.flush() } { JavaUtils.closeQuietly(out) } } } Array(decryptionServer.port, decryptionServer.secret) } def waitTillBroadcastDataSent(): Unit = decryptionServer.getResult() def waitTillDataReceived(): Unit = encryptionServer.getResult() } /** * The inverse of pyspark's ChunkedStream for sending data of unknown size. * * We might be serializing a really large object from python -- we don't want * python to buffer the whole thing in memory, nor can it write to a file, * so we don't know the length in advance. So python writes it in chunks, each chunk * preceded by a length, till we get a \"length\" of -1 which serves as EOF. * * Tested from python tests.  private[spark] class DechunkedInputStream(wrapped: InputStream) extends InputStream with Logging { private val din = new DataInputStream(wrapped) private var remainingInChunk = din.readInt() override def read(): Int = { val into = new Array[Byte](1) val n = read(into, 0, 1) if (n == -1) { -1 } else { // if you just cast a byte to an int, then anything > 127 is negative, which is interpreted // as an EOF val b = into(0) if (b < 0) { 256 + b } else { b } } } override def read(dest: Array[Byte], off: Int, len: Int): Int = { if (remainingInChunk == -1) { return -1 } var destSpace = len var destPos = off while (destSpace > 0 && remainingInChunk != -1) { val toCopy = math.min(remainingInChunk, destSpace) val read = din.read(dest, destPos, toCopy) destPos += read destSpace -= read remainingInChunk -= read if (remainingInChunk == 0) { remainingInChunk = din.readInt() } } assert(destSpace == 0 || remainingInChunk == -1) return destPos - off } override def close(): Unit = wrapped.close() } private[spark] object DechunkedInputStream { /** * Dechunks the input, copies to output, and closes both input and the output safely.  def dechunkAndCopyToOutput(chunked: InputStream, out: OutputStream): Unit = { val dechunked = new DechunkedInputStream(chunked) Utils.tryWithSafeFinally { Utils.copyStream(dechunked, out) } { JavaUtils.closeQuietly(out) JavaUtils.closeQuietly(dechunked) } } } /** * Sends decrypted broadcast data to python worker. See [[PythonRunner]] for entire protocol.  private[spark] class EncryptedPythonBroadcastServer( val env: SparkEnv, val idsAndFiles: Seq[(Long, String)]) extends SocketAuthServer[Unit](\"broadcast-decrypt-server\") with Logging { override def handleConnection(socket: Socket): Unit = { val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream())) var socketIn: InputStream = null // send the broadcast id, then the decrypted data. We don't need to send the length, the // the python pickle module just needs a stream. Utils.tryWithSafeFinally { (idsAndFiles).foreach { case (id, path) => out.writeLong(id) val in = env.serializerManager.wrapForEncryption(new FileInputStream(path)) Utils.tryWithSafeFinally { Utils.copyStream(in, out, false) } { in.close() } } logTrace(\"waiting for python to accept broadcast data over socket\") out.flush() socketIn = socket.getInputStream() socketIn.read() logTrace(\"done serving broadcast data\") } { JavaUtils.closeQuietly(socketIn) JavaUtils.closeQuietly(out) } } def waitTillBroadcastDataSent(): Unit = { getResult() } } /** * Helper for making RDD[Array[Byte]] from some python data, by reading the data from python * over a socket. This is used in preference to writing data to a file when encryption is enabled.  private[spark] abstract class PythonRDDServer extends SocketAuthServer[JavaRDD[Array[Byte]]](\"pyspark-parallelize-server\") { def handleConnection(sock: Socket): JavaRDD[Array[Byte]] = { val in = sock.getInputStream() val dechunkedInput: InputStream = new DechunkedInputStream(in) streamToRDD(dechunkedInput) } protected def streamToRDD(input: InputStream): RDD[Array[Byte]] } private[spark] class PythonParallelizeServer(sc: SparkContext, parallelism: Int) extends PythonRDDServer { override protected def streamToRDD(input: InputStream): RDD[Array[Byte]] = { PythonRDD.readRDDFromInputStream(sc, input, parallelism) } }"
          ],
          "tokens": 896,
          "more_info": "After getting context started checking files on it's own and relation_types is ANY, returned answer in .md file"
        },
        "without_mcp": {
          "answer": "# Spark PythonRunner: Objects, Classes, and Usages\n\nThis document describes the two distinct PythonRunner entities present in this Spark 3.3.0 source tree and all methods/usages that interact with them.\n\n- org.apache.spark.deploy.PythonRunner (a standalone launcher for Python applications used by spark-submit)\n- org.apache.spark.api.python.PythonRunner (an object with factory apply and a helper class to run Python functions/UDFs inside executors)\n\nIt also enumerates where and how these are used across the codebase.\n\n\n## 1) org.apache.spark.deploy.PythonRunner\n\nFile: core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala\n\nRole: Entry point used by SparkSubmit to launch Python applications. It starts a Py4J gateway, constructs PYTHONPATH (including pyFiles), and executes the target Python script as a subprocess with the proper environment variables.\n\n### Public API (object methods)\n\n- def main(args: Array[String]): Unit\n  - Args: \n    - args(0): pythonFile (path/URI to the Python script to run)\n    - args(1): pyFiles (comma-delimited list of additional Python files/archives)\n    - args(2..): any additional user arguments to pass to the Python script\n  - Behavior:\n    - Determines pythonExec using SparkConf and environment: spark.pyspark.driver.python, spark.pyspark.python, PYSPARK_DRIVER_PYTHON, PYSPARK_PYTHON; default \"python3\".\n    - Normalizes pythonFile and pyFiles via formatPath/formatPaths; resolves .py files into a temp directory via resolvePyFiles so that PYTHONPATH receives directories, not single .py files.\n    - Starts a Py4JServer to expose a JVM gateway for the Python process; publishes PYSPARK_GATEWAY_PORT and PYSPARK_GATEWAY_SECRET.\n    - Builds PYTHONPATH with formattedPyFiles + Sparks python path + existing PYTHONPATH.\n    - Exports PYTHONUNBUFFERED=YES, propagates PYSPARK_PYTHON if set, and PYTHONHASHSEED if present.\n    - Optionally sets OMP_NUM_THREADS to spark.driver.cores when not explicitly set in yarn/mesos/k8s driver env, to avoid oversized OpenMP thread pools (SPARK-28843).\n    - Launches the Python process (redirecting stderr into stdout). Streams output to the JVM console via RedirectThread. Exits with SparkUserAppException(exitCode) on non-zero exit; always shuts down the gateway server.\n\n- def formatPath(path: String, testWindows: Boolean = false): String\n  - Purpose: Normalize a path/URI for inclusion in PYTHONPATH. Spark only supports local python files here.\n  - Behavior:\n    - Throws IllegalArgumentException if the path is non-local (Utils.nonLocalPaths nonEmpty) or malformed.\n    - Accepts schemes: file, local, or no scheme. Extracts uri.getPath where applicable.\n    - On Windows, strips leading \"/\" off drive-letter paths like \"/C:/...\".\n\n- def formatPaths(paths: String, testWindows: Boolean = false): Array[String]\n  - Purpose: Normalize a comma-delimited list of paths using formatPath.\n  - Behavior: Splits on comma, drops empties, maps formatPath over each entry.\n\n### Private helpers\n\n- private def resolvePyFiles(pyFiles: Array[String]): Array[String]\n  - Purpose: Ensure .py files are not added directly to PYTHONPATH (which expects directories). Copies readable .py files to a temp dir and returns that directory, deduplicated with other path entries.\n\n### Configuration and environment interactions\n\n- Reads: spark.pyspark.driver.python, spark.pyspark.python, spark.driver.cores, various driverEnv.OMP_NUM_THREADS keys.\n- Sets environment variables: PYTHONPATH, PYTHONUNBUFFERED, PYSPARK_GATEWAY_PORT, PYSPARK_GATEWAY_SECRET, optionally PYSPARK_PYTHON, PYTHONHASHSEED, OMP_NUM_THREADS.\n\n### Known usages across the codebase\n\n- org.apache.spark.deploy.SparkSubmit.scala\n  - args.mainClass = \"org.apache.spark.deploy.PythonRunner\" (line ~503)\n  - Adds \"--class org.apache.spark.deploy.PythonRunner\" in certain codepaths and formats pyFiles:\n    - PythonRunner.formatPaths(Utils.resolveURIs(pyFiles)).mkString(\",\")\n- org.apache.spark.deploy.RRunner.scala\n  - val rFile = PythonRunner.formatPath(args(0))  shares path normalization logic with R runner.\n- org.apache.spark.scheduler.DAGScheduler.scala\n  - Comment reference explaining pyspark resource propagation via PythonRunner.\n- resource-managers/yarn/.../ApplicationMaster.scala\n  - Comment reference for PySpark app run using PythonRunner.\n- Tests\n  - core/src/test/scala/.../PythonRunnerSuite.scala validates formatPath and formatPaths behavior including Windows paths.\n  - core/src/test/scala/.../SparkSubmitSuite.scala uses PythonRunner.formatPaths in expectations.\n\n\n## 2) org.apache.spark.api.python.PythonRunner\n\nFile: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala\n\nRole: Infrastructure for executing Python code in Spark tasks/executors. Consists of:\n- BasePythonRunner[IN, OUT]: abstract class implementing the Python worker protocol and I/O plumbing.\n- object PythonRunner: factory to wrap a single PythonFunction into the helper class.\n- class PythonRunner: concrete subclass of BasePythonRunner for NON_UDF mapPartitions-style functions, using raw byte streams.\n\nAlthough the BasePythonRunner is the core, this document focuses on the PythonRunner object and the PythonRunner class and how other parts of Spark use them.\n\n### object PythonRunner\n\n- Visibility: private[spark]\n- Members:\n  - val runningMonitorThreads = ConcurrentHashMap.newKeySet[(Socket, Long)]()\n    - Tracks running worker monitor threads by (workerSocket, taskAttemptId).\n  - def apply(func: PythonFunction): PythonRunner\n    - Usage: Convenience constructor to create a PythonRunner with a single chained function.\n    - Implementation: new PythonRunner(Seq(ChainedPythonFunctions(Seq(func))))\n\n### class PythonRunner(funcs: Seq[ChainedPythonFunctions])\n\n- Visibility: private[spark]\n- Extends: BasePythonRunner[Array[Byte], Array[Byte]](funcs, PythonEvalType.NON_UDF, Array(Array(0)))\n- Purpose: Executes normal (non-UDF) Python functions over partition data via the PySpark worker protocol.\n- Key overrides:\n  - protected override def newWriterThread(...): WriterThread\n    - Writes the command bytes for the function and then the input iterator:\n      - writeCommand: obtains the command from funcs.head.funcs.head.command, writes its length and bytes.\n      - writeIteratorToStream: uses PythonRDD.writeIteratorToStream to serialize input rows to the worker, then writes SpecialLengths.END_OF_DATA_SECTION.\n  - protected override def newReaderIterator(...): Iterator[Array[Byte]]\n    - Reads responses from the Python worker using a DataInputStream protocol:\n      - Positive length N: read N bytes as one result record.\n      - 0: empty byte array.\n      - SpecialLengths.TIMING_DATA: consume timing payload and continue.\n      - SpecialLengths.PYTHON_EXCEPTION_THROWN: decode and throw a Python-side exception in the JVM.\n      - SpecialLengths.END_OF_DATA_SECTION: handle section end and return null to signal iterator end.\n    - Propagates exceptions observed in the writer thread.\n\n- Relevant protocol constants (SpecialLengths): END_OF_DATA_SECTION, PYTHON_EXCEPTION_THROWN, TIMING_DATA, END_OF_STREAM, NULL, START_ARROW_STREAM.\n\n- Interactions inherited from BasePythonRunner (high level):\n  - Manages Python worker process acquisition/reuse, environment variables, auth sockets, buffer sizes.\n  - Supports barrier/allGather messaging for barrier execution mode.\n  - Can enable faulthandler logging and simplified tracebacks.\n  - Monitors writer thread to detect potential deadlocks; after a timeout, destroys the Python worker.\n\n### Configuration knobs affecting PythonRunner behavior\n\nMost are defined/consumed in BasePythonRunner but impact PythonRunner subclasses:\n- spark.buffer.size (BUFFER_SIZE): I/O buffer size for streams to workers.\n- spark.python.worker.reuse (PYTHON_WORKER_REUSE): reuse workers across tasks.\n- spark.python.auth.socket.timeout (PYTHON_AUTH_SOCKET_TIMEOUT): auth handshake timeout.\n- spark.python.worker.faulthandler.enabled (PYTHON_WORKER_FAULTHANLDER_ENABLED): whether to dump Python faulthandler logs.\n- Resource profile related local properties: executor cores and pyspark memory hints.\n\n\n## 3) Methods and places in Spark that use org.apache.spark.api.python.PythonRunner\n\nDirect constructions or usages of the object/class:\n\n- core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala\n  - val runner = PythonRunner(func)\n    - Constructs a PythonRunner via object.apply, wrapping a PythonFunction to run over an RDD partition.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonForeachWriter.scala\n  - private lazy val pythonRunner = PythonRunner(func)\n  - pythonRunner.compute(inputByteIterator, context.partitionId(), context)\n    - Uses PythonRunner to compute side-effecting foreachBatch/foreach-like operations with serialized bytes.\n\nNote: Other SQL execution runners (ArrowPythonRunner, CoGroupedArrowPythonRunner, PythonUDFRunner) extend BasePythonRunner directly for Arrow or UDF-specific protocols and do not construct org.apache.spark.api.python.PythonRunner.\n\n\n## 4) Methods and places in Spark that use org.apache.spark.deploy.PythonRunner\n\n- core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala\n  - Sets main class to org.apache.spark.deploy.PythonRunner for Python apps\n  - Formats/normalizes pyFiles via PythonRunner.formatPaths when constructing child args and options.\n\n- core/src/main/scala/org/apache/spark/deploy/RRunner.scala\n  - Uses PythonRunner.formatPath to normalize the primary R file path.\n\n- Tests referencing behavior\n  - core/src/test/scala/org/apache/spark/deploy/PythonRunnerSuite.scala covers formatPath and formatPaths edge cases, including Windows drive letters and malformed paths.\n  - core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala validates formatting and argument propagation using PythonRunner.formatPaths.\n\n\n## 5) Summary of the execution flow\n\n- Submission time (driver): SparkSubmit launches org.apache.spark.deploy.PythonRunner main(), which:\n  1. Determines Python executable.\n  2. Normalizes python script path and pyFiles; ensures .py files are placed in a temp dir for PYTHONPATH.\n  3. Starts Py4J gateway and exports connection info as environment variables.\n  4. Builds PYTHONPATH and launches the Python process with combined environment.\n  5. Redirects and mirrors Python stdout/stderr, propagating non-zero exit code as SparkUserAppException.\n\n- Executor time (task): RDD/SQL components create a org.apache.spark.api.python.PythonRunner (or Arrow/UDR-specific BasePythonRunner subclass), which:\n  1. Creates a writer thread to send command + input iterator to the worker process.\n  2. Iterates a reader that decodes results and exceptions via a binary protocol.\n  3. Manages worker lifecycle and monitors for deadlocks, timing data, and error propagation.\n\n\n## 6) Quick reference of key signatures\n\n- deploy.PythonRunner\n  - def main(args: Array[String]): Unit\n  - def formatPath(path: String, testWindows: Boolean = false): String\n  - def formatPaths(paths: String, testWindows: Boolean = false): Array[String]\n\n- api.python.PythonRunner object\n  - def apply(func: PythonFunction): PythonRunner\n\n- api.python.PythonRunner class\n  - new PythonRunner(funcs: Seq[ChainedPythonFunctions])\n  - Overrides\n    - def newWriterThread(...): WriterThread\n    - def newReaderIterator(...): Iterator[Array[Byte]]\n\n\n## 7) Code references index (by file)\n\n- core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala  implementation of deploy.PythonRunner\n- core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala  implementation of BasePythonRunner, api.python.PythonRunner object and class\n- core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala  constructs PythonRunner(func)\n- sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonForeachWriter.scala  constructs and uses PythonRunner(func)\n- core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala  refers to deploy.PythonRunner and calls formatPaths\n- core/src/main/scala/org/apache/spark/deploy/RRunner.scala  calls deploy.PythonRunner.formatPath\n- Tests: PythonRunnerSuite.scala, SparkSubmitSuite.scala (under core/src/test/scala/...)\n\n\n## 8) Edge cases and caveats\n\n- Path normalization rejects non-local schemes for deploy.PythonRunner; only file/local paths are supported at submission time.\n- On Windows, ensure drive-letter paths are not prefixed with a slash for Python to accept them.\n- .py files should not be placed directly on PYTHONPATH; the parent directory must be used. Spark handles this by copying .py files to a temp dir when building PYTHONPATH.\n- Worker reuse and faulthandler settings can materially affect stability and diagnostics; ensure configs are set appropriately for your environment.\n- The api.python.PythonRunner class uses raw byte records; schema-aware/Arrow-based processors use specialized runners extending BasePythonRunner rather than this class.\n",
          "answer_time": 90,
          "tokens": 0,
          "more_info": "Added `return full answer in .md file` at the end of question"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "junie_mcp",
          "score_a": 3,
          "score_b": 5,
          "reasoning": "Answer B provides significantly more detailed and precise information with exact file paths, signatures, code snippets, and fully-qualified names. It systematically covers all methods that use the PythonRunner object with specific line references and usage patterns. Answer A gives a good general overview but lacks the technical precision and completeness that Answer B delivers. Answer B's structure is also more methodical, clearly separating the object definition from the methods that use it, and includes important notes about distinctions between the companion object and class."
        },
        "mcp_vs_no_mcp": {
          "winner": "junie_mcp",
          "score_a": 4,
          "score_b": 2,
          "reasoning": "Answer A directly addresses the question by focusing on the PythonRunner object and comprehensively listing all methods that use it, with precise code references and signatures. Answer B provides extensive detail about PythonRunner classes but covers two different entities and gets cut off mid-sentence, failing to complete the analysis of methods that use the PythonRunner object. A is more focused, complete, and better structured for the specific question asked."
        }
      }
    },
    {
      "id": "Q003",
      "question": "What could be improved in class KafkaWrite?",
      "category": "specific",
      "ground_truth_contexts": [
        "case class KafkaWrite( topic: Option[String], producerParams: ju.Map[String, Object], schema: StructType) extends Write { override def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
        "override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
        "override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
        "override def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }"
      ],
      "key_entities": [
        "org/apache/spark/sql/kafka010/KafkaWrite#",
        "org/apache/spark/sql/kafka010/KafkaWrite#toBatch().",
        "org/apache/spark/sql/kafka010/KafkaWrite#toStreaming().",
        "org/apache/spark/sql/kafka010/KafkaWrite#description()."
      ],
      "claude_stats": {
        "answer": "Based on the code analysis of the `KafkaWrite` class, here are several potential improvements:\n\n**1. Schema Validation Timing**\nThe `schema` is validated using `assert` statements in both `toBatch` and `toStreaming` methods. This is problematic because assertions can be disabled at runtime with the `-da` flag. It would be better to validate the schema in the constructor or use `require()` which always throws an exception.\n\n```scala\ncase class KafkaWrite(\n  topic: Option[String],\n  producerParams: ju.Map[String, Object],\n  schema: StructType\n) extends Write {\n  require(schema != null, \"Schema cannot be null\")\n  // ...\n}\n```\n\n**2. Immutability of Producer Parameters**\nThe `producerParams` is a mutable Java Map (`ju.Map[String, Object]`). This could lead to unexpected behavior if the map is modified externally after the `KafkaWrite` instance is created. Consider creating a defensive copy or converting it to an immutable Scala collection.\n\n**3. Description Method**\nThe `description()` method returns a generic \"Kafka\" string. This could be more informative by including details like the topic name (if specified) or other configuration details to aid in debugging and monitoring.\n\n**4. Code Duplication**\nBoth `toBatch` and `toStreaming` methods have identical validation logic and pass the same parameters to their respective write implementations. This duplication could be reduced.\n\n**5. Type Safety**\nThe `producerParams` uses `Object` as the value type, which sacrifices type safety. While this might be necessary for compatibility with Kafka's producer configuration, it could benefit from additional validation.\n\n",
        "prompt_time": 1.466,
        "answer_time": 20,
        "tokens": 428,
        "context_tokens": 416,
        "used_context": [
          "## CLASS: org/apache/spark/sql/kafka010/KafkaWrite#\ncase class KafkaWrite( topic: Option[String], producerParams: ju.Map[String, Object], schema: StructType) extends Write { override def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
          "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#toBatch().\noverride def toBatch: BatchWrite;\noverride def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
          "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#toStreaming().\noverride def toStreaming: StreamingWrite;\noverride def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
          "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#description().\noverride def description(): String;\noverride def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "Improvements to KafkaWrite include better validation, immutability, and code clarity. A minimal refactor example was provided. External API and behavior remain unchanged.",
          "prompt_time": 0.131,
          "answer_time": 136,
          "context_tokens": 236,
          "used_context": [
            "## CLASS: org/apache/spark/sql/kafka010/KafkaWrite#\ncase class KafkaWrite( topic: Option[String], producerParams: ju.Map[String, Object], schema: StructType) extends Write { override def description(): String = \"Kafka\" override def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
            "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#toBatch().\noverride def toBatch: BatchWrite;\noverride def toBatch: BatchWrite = { assert(schema != null) new KafkaBatchWrite(topic, producerParams, schema) } override def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }",
            "## METHOD: org/apache/spark/sql/kafka010/KafkaWrite#toStreaming().\noverride def toStreaming: StreamingWrite;\noverride def toStreaming: StreamingWrite = { assert(schema != null) new KafkaStreamingWrite(topic, producerParams, schema) } }"
          ],
          "tokens": 32,
          "more_info": "Used same query 2 times and very poor answer, edited KafkaWrite.scala on it's own after being asked for full solution"
        },
        "without_mcp": {
          "answer": "# KafkaWrite class  Review and Improvement Suggestions\n\nThis document reviews the current implementation of `org.apache.spark.sql.kafka010.KafkaWrite` and proposes pragmatic, lowrisk improvements. The intent is to enhance correctness, safety, observability, and maintainability without disrupting Sparks DataSource V2 Kafka connector design.\n\nFile under review:\n- external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaWrite.scala\n\nCurrent (abridged) implementation characteristics:\n- Case class with three fields: `topic: Option[String]`, `producerParams: ju.Map[String, Object]`, `schema: StructType` implementing `Write`.\n- Eager parameter validation via `require(...)`.\n- Defensive copy and unmodifiable view around `producerParams`.\n- `description()` returns `Kafka(topic=...)` if topic is specified, else `Kafka`.\n- `toBatch` and `toStreaming` delegate to `KafkaBatchWrite` and `KafkaStreamingWrite` respectively.\n\nOverall, the class is lean and already applies good practices (null checks and defensive copy). The following suggestions are incremental and focus on edge cases and longterm maintainability.\n\n---\n\n## 1) Validation completeness and error messaging\n\n- Topic validation\n  - Today: `require(topic.forall(_.nonEmpty))` prevents empty string but allows whitespace and otherwise invalid topic names.\n  - Improve by trimming and rejecting blank strings: `_.trim.nonEmpty`.\n  - Optionally align with Kafkas topic name constraints (length 1249, allowed characters `[a-zA-Z0-9._-]`, no `..` or `.` only, etc.), with clear error messages. Keep it optin if strict enforcement risks compatibility.\n\n- Producer parameters validation\n  - Validate presence of essential config such as `bootstrap.servers` early; fail fast with actionable message.\n  - Optionally warn or reject unsupported keys or deprecated aliases (if there is a central validator in `KafkaSourceProvider`/`KafkaWriter`). Centralize validation so the same rules apply across batch and streaming.\n\n- Schema validation messaging\n  - The current `require(schema != null)` is correct; consider tailoring the message if downstream expects specific fields (e.g., value/key/topic/partition) to reduce roundtrip debugging. Actual field validation likely happens later, but a short hint improves UX.\n\n- Consistency of exceptions\n  - Use `IllegalArgumentException` (produced by `require`) consistently for precondition violations. Ensure messages include the offending input value when safe.\n\n## 2) Immutability and exposure of state\n\n- Defensive copy is in place; this is great.\n- Consider masking sensitive producer parameters in any logging or description (e.g., `sasl.jaas.config`, `ssl.key.password`, `sasl.password`). This is more relevant where parameters are logged, but adding a small helper here (or in a shared util) avoids accidental leakage when building descriptions or debugging.\n\n## 3) Description/observability improvements\n\n- `description()` currently hides whether batch or streaming is intended; thats fine because `Write` is a neutral interface. However, adding topic trimming in description and guarding against long topic values can help readability.\n- Consider adding a compact, redacted snapshot of relevant, nonsensitive producer settings in debug logs of the builders (`KafkaBatchWrite`/`KafkaStreamingWrite`). Keep `KafkaWrite` description concise.\n\n## 4) API surface and type choices\n\n- Java map in Scala API\n  - `producerParams: ju.Map[String, Object]` mirrors Kafka client expectations, which is reasonable, but using a Scala `Map[String, AnyRef]` at the boundary and converting to Java internally would:\n    - Improve type safety on the Scala side.\n    - Reduce accidental mutation downstream.\n  - This change has compatibility implications (binary/source). If changing the constructor is not feasible, consider adding an auxiliary constructor or a companion `apply`/builder that accepts a Scala map.\n\n- Final/sealed concerns\n  - Case classes in Scala 2 are not `final` by default; given this is a simple value holder, extension is not expected. Marking the class `final` could prevent accidental subclassing, but doing so can break binary compatibility. Weigh this against Sparks compatibility policy before changing.\n\n## 5) Serialization and stability considerations\n\n- While `Write` is typically used on the driver to produce permode write plans, some objects can be serialized across JVM boundaries. Ensure everything remains Javaserializable or Kryofriendly. Case classes are serializable, `StructType` is serializable, and `java.util.Map` is serializable if its values are; current usage is likely safe.\n- Avoid capturing large or nonserializable state in future extensions.\n\n## 6) Documentation improvements\n\n- Add Scaladoc explaining:\n  - What `KafkaWrite` represents in the DataSource V2 lifecycle.\n  - The meaning and expected contents of `producerParams` (with link to Kafka Producer configs), and treatment of sensitive configs.\n  - How `topic` interacts with perrow topic expressions (i.e., if both are present, the row value overrides or is combineddocument actual behavior to reduce confusion).\n\n## 7) Testing enhancements\n\n- Unit tests around construction edge cases:\n  - `topic = Some(\"\")` (reject).\n  - `topic = Some(\"  \")` (if trim check is added, reject).\n  - `topic = None` with and without topic in rows (integration tested in writer).\n  - Missing `bootstrap.servers` (if validation is added).\n  - Producer params immutability: external mutations do not affect internal `params`.\n\n- Property test for redaction (when logging helpers are added): ensure sensitive keys are masked.\n\n## 8) Defensive programming against accidental mutation\n\n- Although `params` is an unmodifiable map wrapper, note that values in the map can still be mutable (e.g., `List`, arrays). If Spark or Kafka code never mutates values, this is fine. Otherwise, consider deep copying for known mutable value types when they appear (rarely necessary for Kafka configs).\n\n## 9) Backward/forward compatibility strategy\n\n- Any stricter validation (topic/required keys) should be guarded by compatibility considerations:\n  - Start with warnings, promote to errors in a major version, or gate behind a configuration flag (e.g., `spark.sql.kafka.enforceStrictTopicValidation`).\n  - Document deprecations and migration paths.\n\n## 10) Small code quality nits\n\n- Use `Objects.requireNonNull` equivalent? Current `require` is idiomatic Scala and fine.\n- Minor microallocation: `description()` creates a `String`; its trivial and not performance sensitive.\n- Keep the constructor lightweight to avoid overhead when planning; heavy validation of configs can be deferred to the first use point if needed for performance.\n\n---\n\n## Suggested minimal, lowrisk patch (illustrative)\n\n- Trim topic and improve message:\n\n```scala\nrequire(topic != null, \"topic Option must not be null; use None when absent\")\nrequire(topic.forall(t => t != null && t.trim.nonEmpty),\n  s\"topic, if provided, must be non-blank\")\n```\n\n- Validate presence of `bootstrap.servers` early (optional):\n\n```scala\nprivate def hasBootstrap(params: ju.Map[String, Object]): Boolean =\n  params.containsKey(\"bootstrap.servers\") &&\n    Option(params.get(\"bootstrap.servers\")).exists(_.toString.trim.nonEmpty)\n\nrequire(hasBootstrap(producerParams),\n  \"Kafka producer parameter 'bootstrap.servers' must be defined and non-empty\")\n```\n\n- Add Scaladoc to the class explaining role and parameters.\n\nThese can be adopted incrementally; full stricter Kafka topic checks and redaction utilities can be added in shared Kafka utils to keep `KafkaWrite` minimal.\n\n---\n\n## Conclusion\n\n`KafkaWrite` is already concise and robust for its role. The improvements above mainly enhance validation clarity, reduce footguns around whitespace topics and missing essentials, improve documentation and test coverage, and guard against accidental leakage of sensitive configs in logs. Changes should be weighed against Sparks compatibility guarantees; starting with documentation and tests plus nonbreaking validations is recommended.\n",
          "answer_time": 58,
          "tokens": 0
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 1,
          "reasoning": "Answer A provides a comprehensive, well-structured analysis with 5 specific improvement areas, detailed explanations, code examples, and actionable recommendations. Answer B is extremely brief, vague, mentions a 'minimal refactor example' that doesn't exist, and fails to provide any concrete details or useful guidance to address the question."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 4,
          "score_b": 3,
          "reasoning": "Answer A provides clear, actionable improvements with concrete code examples and better addresses the user's direct question. While Answer B is more comprehensive and detailed, it's overly verbose for a code review context and harder to extract actionable insights from. Answer A's concise format with specific code suggestions and clear problem identification makes it more practical for developers."
        }
      }
    },
    {
      "id": "Q004",
      "question": "What are 5 classes with most lines of code?",
      "category": "top-n",
      "ground_truth_contexts": [
        "astbuilder - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala - Metric value: 4408.00 dataset - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala - Metric value: 3683.00 analyzer - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala - Metric value: 3483.00 sparkcontext - CLASS - core/src/main/scala/org/apache/spark/SparkContext.scala - Metric value: 2534.00 dagscheduler - CLASS - core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala - Metric value: 2448.00"
      ],
      "key_entities": [
        "functions",
        "astbuilder",
        "dataset",
        "sqlconf",
        "analyzer"
      ],
      "claude_stats": {
        "answer": "Here are the 5 classes with the most lines of code:\n\n1. **astbuilder** - 4,408 lines\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala`\n\n2. **dataset** - 3,683 lines\n   - Location: `sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala`\n\n3. **analyzer** - 3,483 lines\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala`\n\n4. **sparkcontext** - 2,534 lines\n   - Location: `core/src/main/scala/org/apache/spark/SparkContext.scala`\n\n5. **dagscheduler** - 2,448 lines\n   - Location: `core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala`\n\nThese classes are among the largest in the codebase, with `astbuilder` being the largest at over 4,400 lines of code. This suggests they handle complex functionality - `astbuilder` likely deals with parsing and building abstract syntax trees, `dataset` provides the main Dataset API, and `sparkcontext` serves as the entry point for Spark functionality.\n\n",
        "prompt_time": 0.649,
        "answer_time": 10,
        "tokens": 340,
        "context_tokens": 203,
        "used_context": [
          "astbuilder - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala - Metric value: 4408.00 dataset - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala - Metric value: 3683.00 analyzer - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala - Metric value: 3483.00 sparkcontext - CLASS - core/src/main/scala/org/apache/spark/SparkContext.scala - Metric value: 2534.00 dagscheduler - CLASS - core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala - Metric value: 2448.00"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "# Top 5 Classes by Lines of Code (LOC)\n\nThis document records the top 5 classes in Apache Spark 3.3.0 with the most lines of code, as identified during analysis on 2025-12-27 10:50 (local time).\n\nSource project root: projects/spark-3.3.0\n\n## Results\n\n1. astbuilder  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala (LOC: 4408)\n2. dataset  sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala (LOC: 3683)\n3. analyzer  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala (LOC: 3483)\n4. sparkcontext  core/src/main/scala/org/apache/spark/SparkContext.scala (LOC: 2534)\n5. dagscheduler  core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala (LOC: 2448)\n\n## Notes\n\n- These classes rank highest by sheer lines of code, indicating central components:\n  - Catalyst SQL parsing (AstBuilder) and analysis (Analyzer)\n  - Dataset API surface (Dataset)\n  - Core scheduling and context management (DAGScheduler, SparkContext)\n- If deeper analysis is needed, consider starting with AstBuilder to summarize responsibilities, complexity hotspots, and potential refactoring candidates.\n\n.",
          "prompt_time": 0.577,
          "answer_time": 100,
          "context_tokens": 152,
          "used_context": [
            "astbuilder - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala - Metric value: 4408.00 dataset - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala - Metric value: 3683.00 analyzer - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala - Metric value: 3483.00 sparkcontext - CLASS - core/src/main/scala/org/apache/spark/SparkContext.scala - Metric value: 2534.00 dagscheduler - CLASS - core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala - Metric value: 2448.00"
          ],
          "tokens": 308,
          "more_info": "After asking for full answer returned better list in .md file with all the information returned by Server MCP"
        },
        "without_mcp": {
          "answer": "No answer",
          "answer_time": 700,
          "tokens": 2,
          "more_info": "Timeout on powershell script, no answer"
        }
      },
        "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 4,
          "score_b": 3,
          "reasoning": "Answer A provides better user experience with clear formatting, helpful context about class functionality, and an engaging offer for follow-up analysis. Answer B is overly formal with unnecessary documentation-style metadata (timestamps, project paths) that doesn't add value for the user. Both have identical core information, but A presents it more accessibly and conversationally."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 1,
          "reasoning": "Answer A directly addresses the question with a complete, well-structured list of 5 classes with their line counts and file locations. It provides excellent context about what these classes do and offers additional help. Answer B provides no response at all, making it completely unhelpful."
        }
      }
    },
    {
      "id": "Q005",
      "question": "Describe 3 most important classes in project.",
      "category": "top-n",
      "ground_truth_contexts": [
        "*/ abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent */ def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition. */ @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }` */ protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it. */ protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences. */ protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned. */ @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD. */ def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext). */ val id: Int = sc.newRddId() /** A friendly name for this RDD */ @transient var name: String = _ /** Assign a name to this RDD */ def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one */ private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception. */ def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD. */ def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set. */ def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver). */ private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed */ private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not. */ final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing. */ final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not. */ final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD. */ @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed. */ final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD. */ final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned. */ private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing. */ private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached. */ private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body. */ private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD. */ def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */ def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate. */ def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle. */ def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable. */ def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition. */ val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]]. */ def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array */ def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement. */ private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function. */ def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. */ def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD */ def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD */ def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array. */ def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. */ def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process. */ def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process. */ def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD */ def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive. */ private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning. */ private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive. */ private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other). */ def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition. */ def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD. */ def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD. */ def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first. */ def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`. */ def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us. */ def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator. */ def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]] */ def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions */ def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions */ def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2) */ def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor */ def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD. */ def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds */ def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map. */ def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds */ def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped. */ def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. */ def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file. */ def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file. */ def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`. */ def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD. */ def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD * */ def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD * */ def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition. */ def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements. */ def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements. */ def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects. */ def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`. */ def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition */ private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation. */ def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used. */ def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. */ def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing. */ private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing. */ private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized. */ private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally. */ def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned */ @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents. */ def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a> */ @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD. */ @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified */ @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`). */ @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations. */ @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD */ protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T] */ protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on. */ def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility. */ private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility. */ private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs. */ private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions. */ private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example. */ protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging. */ def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier. */ private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level. */ // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`. */ object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE. */ private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
        "@Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { @transient lazy val sparkSession: SparkSession = { if (queryExecution == null || queryExecution.sparkSession == null) { throw QueryExecutionErrors.transformationsAndActionsNotInvokedByDriverError() } queryExecution.sparkSession } // A globally unique id of this Dataset. private val id = Dataset.curId.getAndIncrement() queryExecution.assertAnalyzed() // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) } def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sqlContext.sparkSession, logicalPlan, encoder) } @transient private[sql] val logicalPlan: LogicalPlan = { val plan = queryExecution.commandExecuted if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) { val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long]) dsIds.add(id) plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds) } plan } /** * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use * it when constructing new Dataset objects that have the same object type (that will be * possibly resolved to a different schema). */ private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder) // The resolved `ExpressionEncoder` which can be used to turn rows to objects of type T, after // collecting rows to the driver side. private lazy val resolvedEnc = { exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer) } private implicit def classTag = exprEnc.clsTag // sqlContext must be val because a stable identifier is expected when you import implicits @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext private[sql] def resolve(colName: String): NamedExpression = { val resolver = sparkSession.sessionState.analyzer.resolver queryExecution.analyzed.resolveQuoted(colName, resolver) .getOrElse(throw resolveException(colName, schema.fieldNames)) } private def resolveException(colName: String, fields: Array[String]): AnalysisException = { val extraMsg = if (fields.exists(sparkSession.sessionState.analyzer.resolver(_, colName))) { s\"; did you mean to quote the `$colName` column?\" } else \"\" val fieldsStr = fields.mkString(\", \") QueryCompilationErrors.cannotResolveColumnNameAmongFieldsError(colName, fieldsStr, extraMsg) } private[sql] def numericColumns: Seq[Expression] = { schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n => queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get } } /** * Get rows represented in Sequence by specific truncate and vertical requirement. * * @param numRows Number of rows to return * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. */ private[sql] def getRows( numRows: Int, truncate: Int): Seq[Seq[String]] = { val newDf = toDF() val castCols = newDf.logicalPlan.output.map { col => // Since binary types in top-level schema fields have a specific format to print, // so we do not cast them to strings here. if (col.dataType == BinaryType) { Column(col) } else { Column(col).cast(StringType) } } val data = newDf.select(castCols: _*).take(numRows + 1) // For array values, replace Seq and Array with square brackets // For cells that are beyond `truncate` characters, replace it with the // first `truncate-3` and \"...\" schema.fieldNames.map(SchemaUtils.escapeMetaCharacters).toSeq +: data.map { row => row.toSeq.map { cell => val str = cell match { case null => \"null\" case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\") case _ => // Escapes meta-characters not to break the `showString` format SchemaUtils.escapeMetaCharacters(cell.toString) } if (truncate > 0 && str.length > truncate) { // do not show ellipses for strings shorter than 4 characters. if (truncate < 4) str.substring(0, truncate) else str.substring(0, truncate - 3) + \"...\" } else { str } }: Seq[String] } } /** * Compose the string representing rows for output * * @param _numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). */ private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = { val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate) val hasMoreData = tmpRows.length - 1 > numRows val rows = tmpRows.take(numRows + 1) val sb = new StringBuilder val numCols = schema.fieldNames.length // We set a minimum column width at '3' val minimumColWidth = 3 if (!vertical) { // Initialise the width of each column to a minimum value val colWidths = Array.fill(numCols)(minimumColWidth) // Compute the width of each column for (row <- rows) { for ((cell, i) <- row.zipWithIndex) { colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell)) } } val paddedRows = rows.map { row => row.zipWithIndex.map { case (cell, i) => if (truncate > 0) { StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } else { StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } } } // Create SeparateLine val sep: String = colWidths.map(\"-\" * _).addString(sb, \"+\", \"+\", \"+\\n\").toString() // column names paddedRows.head.addString(sb, \"|\", \"|\", \"|\\n\") sb.append(sep) // data paddedRows.tail.foreach(_.addString(sb, \"|\", \"|\", \"|\\n\")) sb.append(sep) } else { // Extended display mode enabled val fieldNames = rows.head val dataRows = rows.tail // Compute the width of field name and data columns val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) => math.max(curMax, Utils.stringHalfWidth(fieldName)) } val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) => math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max) } dataRows.zipWithIndex.foreach { case (row, i) => // \"+ 5\" in size means a character length except for padded names and data val rowHeader = StringUtils.rightPad( s\"-RECORD $i\", fieldNameColWidth + dataColWidth + 5, \"-\") sb.append(rowHeader).append(\"\\n\") row.zipWithIndex.map { case (cell, j) => val fieldName = StringUtils.rightPad(fieldNames(j), fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length) val data = StringUtils.rightPad(cell, dataColWidth - Utils.stringHalfWidth(cell) + cell.length) s\" $fieldName | $data \" }.addString(sb, \"\", \"\\n\", \"\\n\") } } // Print a footer if (vertical && rows.tail.isEmpty) { // In a vertical mode, print an empty row set explicitly sb.append(\"(0 rows)\\n\") } else if (hasMoreData) { // For Data that has more than \"numRows\" records val rowsString = if (numRows == 1) \"row\" else \"rows\" sb.append(s\"only showing top $numRows $rowsString\\n\") } sb.toString() } override def toString: String = { try { val builder = new StringBuilder val fields = schema.take(2).map { case f => s\"${f.name}: ${f.dataType.simpleString(2)}\" } builder.append(\"[\") builder.append(fields.mkString(\", \")) if (schema.length > 2) { if (schema.length - fields.size == 1) { builder.append(\" ... 1 more field\") } else { builder.append(\" ... \" + (schema.length - 2) + \" more fields\") } } builder.append(\"]\").toString() } catch { case NonFatal(e) => s\"Invalid tree; ${e.getMessage}:\\n$queryExecution\" } } /** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * * @group basic * @since 1.6.0 */ // This is declared with parentheses to prevent the Scala compiler from treating // `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame. def toDF(): DataFrame = new Dataset[Row](queryExecution, RowEncoder(schema)) /** * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * <ul> * <li>When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`).</li> * <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will * be assigned to `_1`).</li> * <li>When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used.</li> * </ul> * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * Note that `as[]` only changes the view of the data that is passed into typed operations, * such as `map()`, and does not eagerly project away any columns that are not present in * the specified class. * * @group basic * @since 1.6.0 */ def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * {{{ * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` * rdd.toDF(\"id\", \"name\") // this creates a DataFrame with column name \"id\" and \"name\" * }}} * * @group basic * @since 2.0.0 */ @scala.annotation.varargs def toDF(colNames: String*): DataFrame = { require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names (${colNames.size}): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) => Column(oldAttribute).as(newName) } select(newCols : _*) } /** * Returns the schema of this Dataset. * * @group basic * @since 1.6.0 */ def schema: StructType = sparkSession.withActive { queryExecution.analyzed.schema } /** * Prints the schema to the console in a nice tree format. * * @group basic * @since 1.6.0 */ def printSchema(): Unit = printSchema(Int.MaxValue) // scalastyle:off println /** * Prints the schema up to the given level to the console in a nice tree format. * * @group basic * @since 3.0.0 */ def printSchema(level: Int): Unit = println(schema.treeString(level)) // scalastyle:on println /** * Prints the plans (logical and physical) with a format specified by a given explain mode. * * @param mode specifies the expected output format of plans. * <ul> * <li>`simple` Print only a physical plan.</li> * <li>`extended`: Print both logical and physical plans.</li> * <li>`codegen`: Print a physical plan and generated codes if they are * available.</li> * <li>`cost`: Print a logical plan and statistics if they are available.</li> * <li>`formatted`: Split explain output into two sections: a physical plan outline * and node details.</li> * </ul> * @group basic * @since 3.0.0 */ def explain(mode: String): Unit = sparkSession.withActive { // Because temporary views are resolved during analysis when we create a Dataset, and // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using // `ExplainCommand` here will probably output different query plans, compared to the results // of evaluation of the Dataset. So just output QueryExecution's query plans here. // scalastyle:off println println(queryExecution.explainString(ExplainMode.fromString(mode))) // scalastyle:on println } /** * Prints the plans (logical and physical) to the console for debugging purposes. * * @param extended default `false`. If `false`, prints only the physical plan. * * @group basic * @since 1.6.0 */ def explain(extended: Boolean): Unit = if (extended) { explain(ExtendedMode.name) } else { explain(SimpleMode.name) } /** * Prints the physical plan to the console for debugging purposes. * * @group basic * @since 1.6.0 */ def explain(): Unit = explain(SimpleMode.name) /** * Returns all column names and their data types as an array. * * @group basic * @since 1.6.0 */ def dtypes: Array[(String, String)] = schema.fields.map { field => (field.name, field.dataType.toString) } /** * Returns all column names as an array. * * @group basic * @since 1.6.0 */ def columns: Array[String] = schema.fields.map(_.name) /** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * * @group basic * @since 1.6.0 */ def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] || logicalPlan.isInstanceOf[CommandResult] /** * Returns true if the `Dataset` is empty. * * @group basic * @since 2.4.0 */ def isEmpty: Boolean = withAction(\"isEmpty\", select().queryExecution) { plan => plan.executeTake(1).isEmpty } /** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * @group streaming * @since 2.0.0 */ def isStreaming: Boolean = logicalPlan.isStreaming /** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0 */ def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0 */ def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true) /** * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be * used to truncate the logical plan of this Dataset, which is especially useful in iterative * algorithms where the plan may grow exponentially. Local checkpoints are written to executor * storage and despite potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0 */ def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false) /** * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. Local checkpoints are written to executor storage and despite * potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0 */ def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint( eager = eager, reliableCheckpoint = false ) /** * Returns a checkpointed version of this Dataset. * * @param eager Whether to checkpoint this dataframe immediately * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the * checkpoint directory. If false creates a local checkpoint using * the caching subsystem */ private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) \"checkpoint\" else \"localCheckpoint\" withAction(actionName, queryExecution) { physicalPlan => val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { internalRdd.checkpoint() } else { internalRdd.localCheckpoint() } if (eager) { internalRdd.doCheckpoint() } // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. @scala.annotation.tailrec def firstLeafPartitioning(partitioning: Partitioning): Partitioning = { partitioning match { case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head) case p => p } } val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering, isStreaming )(sparkSession)).as[T] } } /** * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * Spark will use this watermark for several purposes: * <ul> * <li>To know when a given time window aggregation can be finalized and thus can be emitted * when using output modes that do not allow updates.</li> * <li>To minimize the amount of state that we need to keep for on-going aggregations, * `mapGroupsWithState` and `dropDuplicates` operators.</li> * </ul> * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * @param eventTime the name of the column that contains the event time of the row. * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). NOTE: This should not be negative. * * @group streaming * @since 2.1.0 */ // We only accept an existing column name, not a derived column here as a watermark that is // defined on a derived column cannot referenced elsewhere in the plan. def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan { val parsedDelay = IntervalUtils.fromIntervalString(delayThreshold) require(!IntervalUtils.isNegative(parsedDelay), s\"delay threshold ($delayThreshold) should not be negative.\") EliminateEventTimeWatermark( EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)) } /** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * * @group action * @since 1.6.0 */ def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * * @group action * @since 1.6.0 */ def show(): Unit = show(20) /** * Displays the top 20 rows of Dataset in a tabular form. * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0 */ def show(truncate: Boolean): Unit = show(20, truncate) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * @param numRows Number of rows to show * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0 */ // scalastyle:off println def show(numRows: Int, truncate: Boolean): Unit = if (truncate) { println(showString(numRows, truncate = 20)) } else { println(showString(numRows, truncate = 0)) } /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @group action * @since 1.6.0 */ def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * If `vertical` enabled, this command prints output rows vertically (one line per column value)? * * {{{ * -RECORD 0------------------- * year | 1980 * month | 12 * AVG('Adj Close) | 0.503218 * AVG('Adj Close) | 0.595103 * -RECORD 1------------------- * year | 1981 * month | 01 * AVG('Adj Close) | 0.523289 * AVG('Adj Close) | 0.570307 * -RECORD 2------------------- * year | 1982 * month | 02 * AVG('Adj Close) | 0.436504 * AVG('Adj Close) | 0.475256 * -RECORD 3------------------- * year | 1983 * month | 03 * AVG('Adj Close) | 0.410516 * AVG('Adj Close) | 0.442194 * -RECORD 4------------------- * year | 1984 * month | 04 * AVG('Adj Close) | 0.450090 * AVG('Adj Close) | 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). * @group action * @since 2.3.0 */ // scalastyle:off println def show(numRows: Int, truncate: Int, vertical: Boolean): Unit = println(showString(numRows, truncate, vertical)) // scalastyle:on println /** * Returns a [[DataFrameNaFunctions]] for working with missing data. * {{{ * // Dropping rows containing any null values. * ds.na.drop() * }}} * * @group untypedrel * @since 1.6.0 */ def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) /** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * {{{ * // Finding frequent items in column with name 'a'. * ds.stat.freqItems(Seq(\"a\")) * }}} * * @group untypedrel * @since 1.6.0 */ def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) /** * Join with another `DataFrame`. * * Behaves as an INNER JOIN and requires a subsequent join predicate. * * @param right Right side of the join operation. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE) } /** * Inner equi-join with another `DataFrame` using the given column. * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the column \"user_id\" * df1.join(df2, \"user_id\") * }}} * * @param right Right side of the join operation. * @param usingColumn Name of the column to join on. This column must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumn: String): DataFrame = { join(right, Seq(usingColumn)) } /** * Inner equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * }}} * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = { join(right, usingColumns, \"inner\") } /** * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate * is specified as an inner join. If you would explicitly like to perform a cross join use the * `crossJoin` method. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = { // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None, JoinHint.NONE)) .analyzed.asInstanceOf[Join] withPlan { Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None, JoinHint.NONE) } } /** * Inner join with another `DataFrame`, using the given join expression. * * {{{ * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * }}} * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * find the trivially true predicates and automatically resolves them to both sides. */ private def resolveSelfJoinCondition(plan: Join): Join = { val resolver = sparkSession.sessionState.analyzer.resolver val cond = plan.condition.map { _.transform { case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualTo( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualNullSafe( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) }} plan.copy(condition = cond) } /** * find the trivially true predicates and automatically resolves them to both sides. */ private def resolveSelfJoinCondition( right: Dataset[_], joinExprs: Option[Column], joinType: String): Join = { // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), joinExprs.map(_.expr), JoinHint.NONE)) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) { return plan } // If left/right have no output set intersection, return the plan. val lanalyzed = this.queryExecution.analyzed val ranalyzed = right.queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) { return plan } // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. resolveSelfJoinCondition(plan) } /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * {{{ * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * }}} * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = { withPlan { resolveSelfJoinCondition(right, Some(joinExprs), joinType) } } /** * Explicit cartesian join with another `DataFrame`. * * @param right Right side of the join operation. * * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * * @group untypedrel * @since 2.1.0 */ def crossJoin(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Cross, None, JoinHint.NONE) } /** * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * * @param other Right side of the join. * @param condition Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`,`full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`. * * @group typedrel * @since 1.6.0 */ def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = { // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // etc. var joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr), JoinHint.NONE)).analyzed.asInstanceOf[Join] if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) { throw QueryCompilationErrors.invalidJoinTypeInJoinWithError(joined.joinType) } // If auto self join alias is enable if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) { joined = resolveSelfJoinCondition(joined) } implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) val leftResultExpr = { if (!this.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() } else { Alias(CreateStruct(joined.left.output), \"_1\")() } } val rightResultExpr = { if (!other.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() } else { Alias(CreateStruct(joined.right.output), \"_2\")() } } if (joined.joinType.isInstanceOf[InnerLike]) { // For inner joins, we can directly perform the join and then can project the join // results into structs. This ensures that data remains flat during shuffles / // exchanges (unlike the outer join path, which nests the data before shuffling). withTypedPlan(Project(Seq(leftResultExpr, rightResultExpr), joined)) } else { // outer joins // For both join sides, combine all outputs into a single column and alias it with \"_1 // or \"_2\", to match the schema for the encoder of the join result. // Note that we do this before joining them, to enable the join operator to return null // for one side, in cases like outer-join. val left = Project(leftResultExpr :: Nil, joined.left) val right = Project(rightResultExpr :: Nil, joined.right) // Rewrites the join condition to make the attribute point to correct column/field, // after we combine the outputs of each join side. val conditionExpr = joined.condition.get transformUp { case a: Attribute if joined.left.outputSet.contains(a) => if (!this.exprEnc.isSerializedAsStructForTopLevel) { left.output.head } else { val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) } case a: Attribute if joined.right.outputSet.contains(a) => if (!other.exprEnc.isSerializedAsStructForTopLevel) { right.output.head } else { val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) } } withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr), JoinHint.NONE)) } } /** * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * @param other Right side of the join. * @param condition Join expression. * * @group typedrel * @since 1.6.0 */ def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = { joinWith(other, condition, \"inner\") } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, usingColumns: Seq[String], joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joinExprs = usingColumns.map { column => EqualTo(resolve(column), other.resolve(column)) }.reduceOption(And).map(Column.apply).orNull joinAsOf(other, leftAsOf, rightAsOf, joinExprs, joinType, tolerance, allowExactMatches, direction) } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, joinExprs: Column, joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joined = resolveSelfJoinCondition(other, Option(joinExprs), joinType) val leftAsOfExpr = leftAsOf.expr.transformUp { case a: AttributeReference if logicalPlan.outputSet.contains(a) => val index = logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.left.output(index) } val rightAsOfExpr = rightAsOf.expr.transformUp { case a: AttributeReference if other.logicalPlan.outputSet.contains(a) => val index = other.logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.right.output(index) } withPlan { AsOfJoin( joined.left, joined.right, leftAsOfExpr, rightAsOfExpr, joined.condition, joined.joinType, Option(tolerance).map(_.expr), allowExactMatches, AsOfJoinDirection(direction) ) } } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = { sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sortWithinPartitions(sortExprs: Column*): Dataset[T] = { sortInternal(global = false, sortExprs) } /** * Returns a new Dataset sorted by the specified column, all in ascending order. * {{{ * // The following 3 are equivalent * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * }}} * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = { sort((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset sorted by the given expressions. For example: * {{{ * ds.sort($\"col1\", $\"col2\".desc) * }}} * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = { sortInternal(global = true, sortExprs) } /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*) /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0 */ def apply(colName: String): Column = col(colName) /** * Specifies some hint on the current Dataset. As an example, the following code specifies * that one of the plan can be broadcasted: * * {{{ * df1.join(df2.hint(\"broadcast\")) * }}} * * @group basic * @since 2.2.0 */ @scala.annotation.varargs def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan { UnresolvedHint(name, parameters, logicalPlan) } /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0 */ def col(colName: String): Column = colName match { case \"*\" => Column(ResolvedStar(queryExecution.analyzed.output)) case _ => if (sqlContext.conf.supportQuotedRegexColumnName) { colRegex(colName) } else { Column(addDataFrameIdToCol(resolve(colName))) } } // Attach the dataset id and column position to the column reference, so that we can detect // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`. // This must be called before we return a `Column` that contains `AttributeReference`. // Note that, the metadata added here are only available in the analyzer, as the analyzer rule // `DetectAmbiguousSelfJoin` will remove it. private def addDataFrameIdToCol(expr: NamedExpression): NamedExpression = { val newExpr = expr transform { case a: AttributeReference if sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) => val metadata = new MetadataBuilder() .withMetadata(a.metadata) .putLong(Dataset.DATASET_ID_KEY, id) .putLong(Dataset.COL_POS_KEY, logicalPlan.output.indexWhere(a.semanticEquals)) .build() a.withMetadata(metadata) } newExpr.asInstanceOf[NamedExpression] } /** * Selects column based on the column name specified as a regex and returns it as [[Column]]. * @group untypedrel * @since 2.3.0 */ def colRegex(colName: String): Column = { val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis colName match { case ParserUtils.escapedIdentifier(columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, None, caseSensitive)) case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive)) case _ => Column(addDataFrameIdToCol(resolve(colName))) } } /** * Returns a new Dataset with an alias set. * * @group typedrel * @since 1.6.0 */ def as(alias: String): Dataset[T] = withTypedPlan { SubqueryAlias(alias, logicalPlan) } /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0 */ def as(alias: Symbol): Dataset[T] = as(alias.name) /** * Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: Symbol): Dataset[T] = as(alias) /** * Selects a set of column based expressions. * {{{ * ds.select($\"colA\", $\"colB\" + 1) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def select(cols: Column*): DataFrame = withPlan { val untypedCols = cols.map { case typedCol: TypedColumn[_, _] => // Checks if a `TypedColumn` has been inserted with // specific input type and schema by `withInputType`. val needInputType = typedCol.expr.exists { case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty => true case _ => false } if (!needInputType) { typedCol } else { throw QueryCompilationErrors.cannotPassTypedColumnInUntypedSelectError(typedCol.toString) } case other => other } Project(untypedCols.map(_.named), logicalPlan) } /** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * {{{ * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*) /** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * {{{ * // The following are equivalent: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def selectExpr(exprs: String*): DataFrame = { select(exprs.map { expr => Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) }: _*) } /** * Returns a new Dataset by computing the given [[Column]] expression for each element. * * {{{ * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * }}} * * @group typedrel * @since 1.6.0 */ def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = { implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (!encoder.isSerializedAsStructForTopLevel) { new Dataset[U1](sparkSession, project, encoder) } else { // Flattens inner fields of U1 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) } } /** * Internal helper function for building typed selects that return tuples. For simplicity and * code reuse, we do this without the help of the type system and then use helper functions * that cast appropriately for the user facing interface. */ protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = { val encoders = columns.map(_.encoder) val namedColumns = columns.map(_.withInputType(exprEnc, logicalPlan.output).named) val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan)) new Dataset(execution, ExpressionEncoder.tuple(encoders)) } /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] /** * Filters rows using the given condition. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0 */ def filter(condition: Column): Dataset[T] = withTypedPlan { Filter(condition.expr, logicalPlan) } /** * Filters rows using the given SQL expression. * {{{ * peopleDs.filter(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0 */ def filter(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Filters rows using the given condition. This is an alias for `filter`. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0 */ def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * {{{ * peopleDs.where(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0 */ def where(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def groupBy(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType) } /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) } /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def groupBy(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType) } /** * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0 */ def reduce(func: (T, T) => T): T = withNewRDDExecutionId { rdd.reduce(func) } /** * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0 */ def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) /** * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0 */ def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = { val withGroupingKey = AppendColumns(func, logicalPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, logicalPlan.output, withGroupingKey.newColumns) } /** * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0 */ def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * }}} * * @group untypedrel * @since 2.0.0 */ def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = { groupBy().agg(aggExpr, aggExprs : _*) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0 */ def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0 */ def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*) /** * Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset * that returns the same result as the input, with the following guarantees: * <ul> * <li>It will compute the defined aggregates (metrics) on all the data that is flowing through * the Dataset at that point.</li> * <li>It will report the value of the defined aggregate columns as soon as we reach a completion * point. A completion point is either the end of a query (batch mode) or the end of a streaming * epoch. The value of the aggregates only reflects the data processed since the previous * completion point.</li> * </ul> * Please note that continuous execution is currently not supported. * * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that * contain references to the input Dataset's columns must always be wrapped in an aggregate * function. * * A user can observe these metrics by either adding * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session. * * {{{ * // Monitor the metrics using a listener. * spark.streams.addListener(new StreamingQueryListener() { * override def onQueryStarted(event: QueryStartedEvent): Unit = {} * override def onQueryProgress(event: QueryProgressEvent): Unit = { * event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row => * // Trigger if the number of errors exceeds 5 percent * val num_rows = row.getAs[Long](\"rc\") * val num_error_rows = row.getAs[Long](\"erc\") * val ratio = num_error_rows.toDouble / num_rows * if (ratio > 0.05) { * // Trigger alert * } * } * } * override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {} * }) * // Observe row count (rc) and error row count (erc) in the streaming Dataset * val observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\")) * observed_ds.writeStream.format(\"...\").start() * }}} * * @group typedrel * @since 3.0.0 */ @varargs def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan { CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan) } /** * Observe (named) metrics through an `org.apache.spark.sql.Observation` instance. * This is equivalent to calling `observe(String, Column, Column*)` but does not require * adding `org.apache.spark.sql.util.QueryExecutionListener` to the spark session. * This method does not support streaming datasets. * * A user can retrieve the metrics by accessing `org.apache.spark.sql.Observation.get`. * * {{{ * // Observe row count (rows) and highest id (maxid) in the Dataset while writing it * val observation = Observation(\"my_metrics\") * val observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\")) * observed_ds.write.parquet(\"ds.parquet\") * val metrics = observation.get * }}} * * @throws IllegalArgumentException If this is a streaming Dataset (this.isStreaming == true) * * @group typedrel * @since 3.3.0 */ @varargs def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = { observation.on(this, expr, exprs: _*) } /** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * @group typedrel * @since 2.0.0 */ def limit(n: Int): Dataset[T] = withTypedPlan { Limit(Literal(n), logicalPlan) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.union(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 4| 5| 6| * // +----+----+----+ * }}} * * Notice that the column positions in the schema aren't necessarily matched with the * fields in the strongly typed objects in a Dataset. This function resolves columns * by their positions in the schema, not the fields in the strongly typed objects. Use * [[unionByName]] to resolve columns by field name in the typed objects. * * @group typedrel * @since 2.0.0 */ def union(other: Dataset[T]): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan, other.logicalPlan)) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is an alias for `union`. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.0.0 */ def unionAll(other: Dataset[T]): Dataset[T] = union(other) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set * union (that does deduplication of elements), use this function followed by a [[distinct]]. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.unionByName(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 6| 4| 5| * // +----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. Nested columns in map types * are not currently supported. * * @group typedrel * @since 2.3.0 */ def unionByName(other: Dataset[T]): Dataset[T] = unionByName(other, false) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position). * * When the parameter `allowMissingColumns` is `true`, the set of column names * in this and other `Dataset` can differ; missing columns will be filled with null. * Further, the missing columns of this `Dataset` will be added at the end * in the schema of the union result: * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col0\", \"col3\") * df1.unionByName(df2, true).show * * // output: \"col3\" is missing at left df1 and added at the end of schema. * // +----+----+----+----+ * // |col0|col1|col2|col3| * // +----+----+----+----+ * // | 1| 2| 3|null| * // | 5| 4|null| 6| * // +----+----+----+----+ * * df2.unionByName(df1, true).show * * // output: \"col2\" is missing at left df2 and added at the end of schema. * // +----+----+----+----+ * // |col1|col0|col3|col2| * // +----+----+----+----+ * // | 4| 5| 6|null| * // | 2| 1|null| 3| * // +----+----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. With `allowMissingColumns`, * missing nested columns of struct columns with the same name will also be filled with null * values and added to the end of struct. Nested columns in map types are not currently * supported. * * @group typedrel * @since 3.1.0 */ def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns)) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 1.6.0 */ def intersect(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset while * preserving the duplicates. * This is equivalent to `INTERSECT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard * in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0 */ def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT DISTINCT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0 */ def except(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset while * preserving the duplicates. * This is equivalent to `EXCEPT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in * SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0 */ def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a user-supplied seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0 */ def sample(fraction: Double, seed: Long): Dataset[T] = { sample(withReplacement = false, fraction = fraction, seed = seed) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a random seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0 */ def sample(fraction: Double): Dataset[T] = { sample(withReplacement = false, fraction = fraction) } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = { withTypedPlan { Sample(0.0, fraction, withReplacement, seed, logicalPlan) } } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = { sample(withReplacement, fraction, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * For Java API, use [[randomSplitAsList]]. * * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out // from the sort order. val sortOrder = logicalPlan.output .filter(attr => RowOrdering.isOrderable(attr.dataType)) .map(SortOrder(_, Ascending)) val plan = if (sortOrder.nonEmpty) { Sort(sortOrder, global = false, logicalPlan) } else { // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism cache() logicalPlan } val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder) }.toArray } /** * Returns a Java list that contains randomly split Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * @group typedrel * @since 2.0.0 */ def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = { val values = randomSplit(weights, seed) java.util.Arrays.asList(values : _*) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double]): Array[Dataset[T]] = { randomSplit(weights, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. */ private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = { randomSplit(weights.toArray, seed) } /** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * {{{ * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select($\"title\", explode(split($\"words\", \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(count_distinct(\"title\")) * }}} * * Using `flatMap()` this can similarly be exploded as: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0 */ @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = { val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * {{{ * ds.select(explode(split($\"words\", \" \")).as(\"word\")) * }}} * * or `flatMap()`: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0 */ @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B]) : DataFrame = { val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = { val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o))) } val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * * `column`'s expression must only refer to attributes supplied by this Dataset. It is an * error to add a column that refers to some other Dataset. * * @note this method introduces a projection internally. Therefore, calling it multiple times, * for instance, via loops in order to add multiple columns can generate big plans which * can cause performance issues and even `StackOverflowException`. To avoid this, * use `select` with the multiple columns at once. * * @group untypedrel * @since 2.0.0 */ def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col)) /** * (Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attributes * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0 */ def withColumns(colsMap: Map[String, Column]): DataFrame = { val (colNames, newCols) = colsMap.toSeq.unzip withColumns(colNames, newCols) } /** * (Java-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attribute * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0 */ def withColumns(colsMap: java.util.Map[String, Column]): DataFrame = withColumns( colsMap.asScala.toMap ) /** * Returns a new Dataset by adding columns or replacing the existing columns that has * the same names. */ private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = { require(colNames.size == cols.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of columns: ${cols.size}\") SchemaUtils.checkColumnNameDuplication( colNames, \"in given column names\", sparkSession.sessionState.conf.caseSensitiveAnalysis) val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val columnSeq = colNames.zip(cols) val replacedAndExistingColumns = output.map { field => columnSeq.find { case (colName, _) => resolver(field.name, colName) } match { case Some((colName: String, col: Column)) => col.as(colName) case _ => Column(field) } } val newColumns = columnSeq.filter { case (colName, col) => !output.exists(f => resolver(f.name, colName)) }.map { case (colName, col) => col.as(colName) } select(replacedAndExistingColumns ++ newColumns : _*) } /** * Returns a new Dataset by adding columns with metadata. */ private[spark] def withColumns( colNames: Seq[String], cols: Seq[Column], metadata: Seq[Metadata]): DataFrame = { require(colNames.size == metadata.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of metadata elements: ${metadata.size}\") val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) => col.as(colName, metadata) } withColumns(colNames, newCols) } /** * Returns a new Dataset by adding a column with metadata. */ private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = withColumns(Seq(colName), Seq(col), Seq(metadata)) /** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * * @group untypedrel * @since 2.0.0 */ def withColumnRenamed(existingName: String, newName: String): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f => resolver(f.name, existingName)) if (shouldRename) { val columns = output.map { col => if (resolver(col.name, existingName)) { Column(col).as(newName) } else { Column(col) } } select(columns : _*) } else { toDF() } } /** * Returns a new Dataset by updating an existing column with metadata. * * @group untypedrel * @since 3.3.0 */ def withMetadata(columnName: String, metadata: Metadata): DataFrame = { withColumn(columnName, col(columnName), metadata) } /** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * @group untypedrel * @since 2.0.0 */ def drop(colName: String): DataFrame = { drop(Seq(colName) : _*) } /** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def drop(colNames: String*): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter { attribute => colNames.forall(n => !resolver(attribute.name, n)) }.map(attribute => Column(attribute)) if (remainingCols.size == allColumns.size) { toDF() } else { this.select(remainingCols: _*) } } /** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * @group untypedrel * @since 2.0.0 */ def drop(col: Column): DataFrame = { val expression = col match { case Column(u: UnresolvedAttribute) => queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) => expr } val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter { attr => !attr.semanticEquals(expression) }.map(attr => Column(attr)) select(colsAfterDrop : _*) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns) /** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue // (the Streaming's state store depends on the `groupCols` order). val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) => // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col => resolver(col.name, colName)) if (cols.isEmpty) { throw QueryCompilationErrors.cannotResolveColumnNameAmongAttributesError( colName, schema.fieldNames.mkString(\", \")) } cols } Deduplicate(groupCols, logicalPlan) } /** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq) /** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def dropDuplicates(col1: String, cols: String*): Dataset[T] = { val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames) } /** * Computes basic statistics for numeric and string columns, including count, mean, stddev, min, * and max. If no columns are given, this function computes statistics for all numerical or * string columns. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * }}} * * Use [[summary]] for expanded statistics and control over which statistics to compute. * * @param cols Columns to compute statistics on. * * @group action * @since 1.6.0 */ @scala.annotation.varargs def describe(cols: String*): DataFrame = { val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*) selected.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\") } /** * Computes specified statistics for numeric and string columns. Available statistics are: * <ul> * <li>count</li> * <li>mean</li> * <li>stddev</li> * <li>min</li> * <li>max</li> * <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li> * <li>count_distinct</li> * <li>approx_count_distinct</li> * </ul> * * If no statistics are given, this function computes count, mean, stddev, min, * approximate quartiles (percentiles at 25%, 50%, and 75%), and max. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.summary().show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 50% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * {{{ * ds.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show() * * // output: * // summary age height * // count 10.0 10.0 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * To do a summary for specific columns first select them: * * {{{ * ds.select(\"age\", \"height\").summary().show() * }}} * * Specify statistics to output custom summaries: * * {{{ * ds.summary(\"count\", \"count_distinct\").show() * }}} * * The distinct count isn't included by default. * * You can also run approximate distinct counts which are faster: * * {{{ * ds.summary(\"count\", \"approx_count_distinct\").show() * }}} * * See also [[describe]] for basic statistics. * * @param statistics Statistics from above list to be computed. * * @group action * @since 2.3.0 */ @scala.annotation.varargs def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq) /** * Returns the first `n` rows. * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @group action * @since 1.6.0 */ def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) /** * Returns the first row. * @group action * @since 1.6.0 */ def head(): T = head(1).head /** * Returns the first row. Alias for head(). * @group action * @since 1.6.0 */ def first(): T = head() /** * Concise syntax for chaining custom transformations. * {{{ * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * }}} * * @group typedrel * @since 1.6.0 */ def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this) /** * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0 */ def filter(func: T => Boolean): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0 */ def filter(func: FilterFunction[T]): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0 */ def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan { MapElements[T, U](func, logicalPlan) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0 */ def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * @group typedrel * @since 1.6.0 */ def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = { new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]]) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * @group typedrel * @since 1.6.0 */ def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala mapPartitions(func)(encoder) } /** * Returns a new `DataFrame` that contains the result of applying a serialized R function * `func` to each partition. */ private[sql] def mapPartitionsInR( func: Array[Byte], packageNames: Array[Byte], broadcastVars: Array[Broadcast[Object]], schema: StructType): DataFrame = { val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]] Dataset.ofRows( sparkSession, MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan)) } /** * Applies a Scalar iterator Pandas UDF to each partition. The user-defined function * defines a transformation: `iter(pandas.DataFrame)` -> `iter(pandas.DataFrame)`. * Each partition is each iterator consisting of DataFrames as batches. * * This function uses Apache Arrow as serialization format between Java executors and Python * workers. */ private[sql] def mapInPandas(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, MapInPandas( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * Applies a function to each partition in Arrow format. The user-defined function * defines a transformation: `iter(pyarrow.RecordBatch)` -> `iter(pyarrow.RecordBatch)`. * Each partition is each iterator consisting of `pyarrow.RecordBatch`s as batches. */ private[sql] def pythonMapInArrow(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, PythonMapInArrow( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0 */ def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func)) /** * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0 */ def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (T) => Iterator[U] = x => f.call(x).asScala flatMap(func)(encoder) } /** * Applies a function `f` to all rows. * * @group action * @since 1.6.0 */ def foreach(f: T => Unit): Unit = withNewRDDExecutionId { rdd.foreach(f) } /** * (Java-specific) * Runs `func` on each element of this Dataset. * * @group action * @since 1.6.0 */ def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) /** * Applies a function `f` to each partition of this Dataset. * * @group action * @since 1.6.0 */ def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId { rdd.foreachPartition(f) } /** * (Java-specific) * Runs `func` on each partition of this Dataset. * * @group action * @since 1.6.0 */ def foreachPartition(func: ForeachPartitionFunction[T]): Unit = { foreachPartition((it: Iterator[T]) => func.call(it.asJava)) } /** * Returns the first `n` rows in the Dataset. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def take(n: Int): Array[T] = head(n) /** * Returns the last `n` rows in the Dataset. * * Running tail requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 3.0.0 */ def tail(n: Int): Array[T] = withAction( \"tail\", withTypedPlan(Tail(Literal(n), logicalPlan)).queryExecution)(collectFromPlan) /** * Returns the first `n` rows in the Dataset as a list. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*) /** * Returns an array that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0 */ def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) /** * Returns a Java list that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan => val values = collectFromPlan(plan) java.util.Arrays.asList(values : _*) } /** * Returns an iterator that contains all rows in this Dataset. * * The iterator will consume as much memory as the largest partition in this Dataset. * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * * @group action * @since 2.0.0 */ def toLocalIterator(): java.util.Iterator[T] = { withAction(\"toLocalIterator\", queryExecution) { plan => val fromRow = resolvedEnc.createDeserializer() plan.executeToIterator().map(fromRow).asJava } } /** * Returns the number of rows in the Dataset. * @group action * @since 1.6.0 */ def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan => plan.executeCollect().head.getLong(0) } /** * Returns a new Dataset that has exactly `numPartitions` partitions. * * @group typedrel * @since 1.6.0 */ def repartition(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = true, logicalPlan) } private def repartitionByExpression( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments. // However, we don't want to complicate the semantics of this API method. // Instead, let's give users a friendly error message, pointing them to the new method. val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder]) if (sortOrders.nonEmpty) throw new IllegalArgumentException( s\"\"\"Invalid partitionExprs specified: $sortOrders |For range partitioning use repartitionByRange(...) instead. \"\"\".stripMargin) withTypedPlan { RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByExpression(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = { repartitionByExpression(None, partitionExprs) } private def repartitionByRange( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { require(partitionExprs.nonEmpty, \"At least one partition-by expression must be specified.\") val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) }) withTypedPlan { RepartitionByExpression(sortOrder, logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0 */ @scala.annotation.varargs def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByRange(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0 */ @scala.annotation.varargs def repartitionByRange(partitionExprs: Column*): Dataset[T] = { repartitionByRange(None, partitionExprs) } /** * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions * are requested. If a larger number of partitions is requested, it will stay at the current * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can call repartition. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @group typedrel * @since 1.6.0 */ def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = false, logicalPlan) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * Note that for a streaming [[Dataset]], this method returns distinct rows only once * regardless of the output mode, which the behavior may not be same with `DISTINCT` in SQL * against streaming [[Dataset]]. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0 */ def distinct(): Dataset[T] = dropDuplicates() /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */ def persist(): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this) this } /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */ def cache(): this.type = persist() /** * Persist this Dataset with the given storage level. * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, * `MEMORY_AND_DISK_2`, etc. * * @group basic * @since 1.6.0 */ def persist(newLevel: StorageLevel): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this } /** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * @group basic * @since 2.1.0 */ def storageLevel: StorageLevel = { sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData => cachedData.cachedRepresentation.cacheBuilder.storageLevel }.getOrElse(StorageLevel.NONE) } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @param blocking Whether to block until all blocks are deleted. * * @group basic * @since 1.6.0 */ def unpersist(blocking: Boolean): this.type = { sparkSession.sharedState.cacheManager.uncacheQuery( sparkSession, logicalPlan, cascade = false, blocking) this } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @group basic * @since 1.6.0 */ def unpersist(): this.type = unpersist(blocking = false) // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`. @transient private lazy val rddQueryExecution: QueryExecution = { val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized) } /** * Represents the content of the Dataset as an `RDD` of `T`. * * @group basic * @since 1.6.0 */ lazy val rdd: RDD[T] = { val objectType = exprEnc.deserializer.dataType rddQueryExecution.toRdd.mapPartitions { rows => rows.map(_.get(0, objectType).asInstanceOf[T]) } } /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0 */ def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0 */ def javaRDD: JavaRDD[T] = toJavaRDD /** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 1.6.0 */ @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\") def registerTempTable(tableName: String): Unit = { createOrReplaceTempView(tableName) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.0.0 */ @throws[AnalysisException] def createTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = false) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 2.0.0 */ def createOrReplaceTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = false) } /** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.1.0 */ @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) } /** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0 */ def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) } private def createTempViewCommand( viewName: String, replace: Boolean, global: Boolean): CreateViewCommand = { val viewType = if (global) GlobalTempView else LocalTempView val tableIdentifier = try { sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName) } catch { case _: ParseException => throw QueryCompilationErrors.invalidViewNameError(viewName) } CreateViewCommand( name = tableIdentifier, userSpecifiedColumns = Nil, comment = None, properties = Map.empty, originalText = None, plan = logicalPlan, allowExisting = false, replace = replace, viewType = viewType, isAnalyzed = true) } /** * Interface for saving the content of the non-streaming Dataset out into external storage. * * @group basic * @since 1.6.0 */ def write: DataFrameWriter[T] = { if (isStreaming) { logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriter[T](this) } /** * Create a write configuration builder for v2 sources. * * This builder is used to configure and execute write operations. For example, to append to an * existing table, run: * * {{{ * df.writeTo(\"catalog.db.table\").append() * }}} * * This can also be used to create or replace existing tables: * * {{{ * df.writeTo(\"catalog.db.table\").partitionedBy($\"col\").createOrReplace() * }}} * * @group basic * @since 3.0.0 */ def writeTo(table: String): DataFrameWriterV2[T] = { // TODO: streaming could be adapted to use this interface if (isStreaming) { logicalPlan.failAnalysis( \"'writeTo' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriterV2[T](table, this) } /** * Interface for saving the content of the streaming Dataset out into external storage. * * @group basic * @since 2.0.0 */ def writeStream: DataStreamWriter[T] = { if (!isStreaming) { logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") } new DataStreamWriter[T](this) } /** * Returns the content of the Dataset as a Dataset of JSON strings. * @since 2.0.0 */ def toJSON: Dataset[String] = { val rowSchema = this.schema val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone mapPartitions { iter => val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer, new JSONOptions(Map.empty[String, String], sessionLocalTimeZone)) new Iterator[String] { private val toRow = exprEnc.createSerializer() override def hasNext: Boolean = iter.hasNext override def next(): String = { gen.write(toRow(iter.next())) gen.flush() val json = writer.toString if (hasNext) { writer.reset() } else { gen.close() } json } } } (Encoders.STRING) } /** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * @group basic * @since 2.0.0 */ def inputFiles: Array[String] = { val files: Seq[String] = queryExecution.optimizedPlan.collect { case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) => fsBasedRelation.inputFiles case fr: FileRelation => fr.inputFiles case r: HiveTableRelation => r.tableMeta.storage.locationUri.map(_.toString).toArray case DataSourceV2ScanRelation(DataSourceV2Relation(table: FileTable, _, _, _, _), _, _, _) => table.fileIndex.inputFiles }.flatten files.toSet.toArray } /** * Returns `true` when the logical query plans inside both [[Dataset]]s are equal and * therefore return same results. * * @note The equality comparison here is simplified by tolerating the cosmetic differences * such as attribute names. * @note This API can compare both [[Dataset]]s very fast but can still return `false` on * the [[Dataset]] that return the same results, for instance, from different plans. Such * false negative semantic can be useful when caching as an example. * @since 3.1.0 */ @DeveloperApi def sameSemantics(other: Dataset[T]): Boolean = { queryExecution.analyzed.sameResult(other.queryExecution.analyzed) } /** * Returns a `hashCode` of the logical query plan against this [[Dataset]]. * * @note Unlike the standard `hashCode`, the hash is calculated against the query plan * simplified by tolerating the cosmetic differences such as attribute names. * @since 3.1.0 */ @DeveloperApi def semanticHash(): Int = { queryExecution.analyzed.semanticHash() } //////////////////////////////////////////////////////////////////////////// // For Python API //////////////////////////////////////////////////////////////////////////// /** * It adds a new long column with the name `name` that increases one by one. * This is for 'distributed-sequence' default index in pandas API on Spark. */ private[sql] def withSequenceColumn(name: String) = { Dataset.ofRows( sparkSession, AttachDistributedSequence( AttributeReference(name, LongType, nullable = false)(), logicalPlan)) } /** * Converts a JavaRDD to a PythonRDD. */ private[sql] def javaToPython: JavaRDD[Array[Byte]] = { val structType = schema // capture it for closure val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType)) EvaluatePython.javaToPython(rdd) } private[sql] def collectToPython(): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"collectToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeCollect().iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def tailToPython(n: Int): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"tailToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeTail(n).iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def getRowsToPython( _numRows: Int, truncate: Int): Array[Any] = { EvaluatePython.registerPicklers() val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) val rows = getRows(numRows, truncate).map(_.toArray).toArray val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType))) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( rows.iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-GetRows\") } /** * Collect a Dataset as Arrow batches and serve stream to SparkR. It sends * arrow batches in an ordered manner with buffering. This is inevitable * due to missing R API that reads batches from socket directly. See ARROW-4512. * Eventually, this code should be deduplicated by `collectAsArrowToPython`. */ private[sql] def collectAsArrowToR(): Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone RRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToR\", queryExecution) { plan => val buffer = new ByteArrayOutputStream() val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, buffer, timeZoneId) val arrowBatchRdd = toArrowBatchRdd(plan) val numPartitions = arrowBatchRdd.partitions.length // Store collection results for worst case of 1 to N-1 partitions val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1)) var lastIndex = -1 // index of last partition written // Handler to eagerly write partitions to Python in order def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = { // If result is from next partition in order if (index - 1 == lastIndex) { batchWriter.writeBatches(arrowBatches.iterator) lastIndex += 1 // Write stored partitions that come next in order while (lastIndex < results.length && results(lastIndex) != null) { batchWriter.writeBatches(results(lastIndex).iterator) results(lastIndex) = null lastIndex += 1 } // After last batch, end the stream if (lastIndex == results.length) { batchWriter.end() val batches = buffer.toByteArray out.writeInt(batches.length) out.write(batches) } } else { // Store partitions received out of order results(index - 1) = arrowBatches } } sparkSession.sparkContext.runJob( arrowBatchRdd, (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray, 0 until numPartitions, handlePartitionBatches) } } } /** * Collect a Dataset as Arrow batches and serve stream to PySpark. It sends * arrow batches in an un-ordered manner without buffering, and then batch order * information at the end. The batches should be reordered at Python side. */ private[sql] def collectAsArrowToPython: Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone PythonRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToPython\", queryExecution) { plan => val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId) // Batches ordered by (index of partition, batch index in that partition) tuple val batchOrder = ArrayBuffer.empty[(Int, Int)] // Handler to eagerly write batches to Python as they arrive, un-ordered val handlePartitionBatches = (index: Int, arrowBatches: Array[Array[Byte]]) => if (arrowBatches.nonEmpty) { // Write all batches (can be more than 1) in the partition, store the batch order tuple batchWriter.writeBatches(arrowBatches.iterator) arrowBatches.indices.foreach { partitionBatchIndex => batchOrder.append((index, partitionBatchIndex)) } } Utils.tryWithSafeFinally { val arrowBatchRdd = toArrowBatchRdd(plan) sparkSession.sparkContext.runJob( arrowBatchRdd, (it: Iterator[Array[Byte]]) => it.toArray, handlePartitionBatches) } { // After processing all partitions, end the batch stream batchWriter.end() // Write batch order indices out.writeInt(batchOrder.length) // Sort by (index of partition, batch index in that partition) tuple to get the // overall_batch_index from 0 to N-1 batches, which can be used to put the // transferred batches in the correct order batchOrder.zipWithIndex.sortBy(_._1).foreach { case (_, overallBatchIndex) => out.writeInt(overallBatchIndex) } } } } } private[sql] def toPythonIterator(prefetchPartitions: Boolean = false): Array[Any] = { withNewExecutionId { PythonRDD.toLocalIteratorAndServe(javaToPython.rdd, prefetchPartitions) } } //////////////////////////////////////////////////////////////////////////// // Private Helpers //////////////////////////////////////////////////////////////////////////// /** * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with * an execution. */ private def withNewExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(queryExecution)(body) } /** * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect * them with an execution. Before performing the action, the metrics of the executed plan will be * reset. */ private def withNewRDDExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(rddQueryExecution) { rddQueryExecution.executedPlan.resetMetrics() body } } /** * Wrap a Dataset action to track the QueryExecution and time cost, then report to the * user-registered callback functions, and also to convert asserts/NPE to * the internal error exception. */ private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } /** * Collect all elements from a spark plan. */ private def collectFromPlan(plan: SparkPlan): Array[T] = { val fromRow = resolvedEnc.createDeserializer() plan.executeCollect().map(fromRow) } private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = { val sortOrder: Seq[SortOrder] = sortExprs.map { col => col.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) } } withTypedPlan { Sort(sortOrder, global = global, logicalPlan) } } /** A convenient function to wrap a logical plan and produce a DataFrame. */ @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = { Dataset.ofRows(sparkSession, logicalPlan) } /** A convenient function to wrap a logical plan and produce a Dataset. */ @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { Dataset(sparkSession, logicalPlan) } /** A convenient function to wrap a set based logical plan and produce a Dataset. */ @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) { // Set operators widen types (change the schema), so we cannot reuse the row encoder. Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]] } else { Dataset(sparkSession, logicalPlan) } } /** Convert to an RDD of serialized ArrowRecordBatches. */ private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = { val schemaCaptured = this.schema val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone plan.execute().mapPartitionsInternal { iter => val context = TaskContext.get() ArrowConverters.toBatchIterator( iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context) } } // This is only used in tests, for now. private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = { toArrowBatchRdd(queryExecution.executedPlan) } }",
        "* this config overrides the default configs as well as system properties. */ class SparkContext(config: SparkConf) extends Logging { // The call site where this SparkContext was constructed. private val creationSite: CallSite = Utils.getCallSite() if (!config.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) { // In order to prevent SparkContext from being created in executors. SparkContext.assertOnDriver() } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having started construction. // NOTE: this must be placed at the beginning of the SparkContext constructor. SparkContext.markPartiallyConstructed(this) val startTime = System.currentTimeMillis() private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false) private[spark] def assertNotStopped(): Unit = { if (stopped.get()) { val activeContext = SparkContext.activeContext.get() val activeCreationSite = if (activeContext == null) { \"(No active SparkContext.)\" } else { activeContext.creationSite.longForm } throw new IllegalStateException( s\"\"\"Cannot call methods on a stopped SparkContext. |This stopped SparkContext was created at: | |${creationSite.longForm} | |The currently active SparkContext was created at: | |$activeCreationSite \"\"\".stripMargin) } } /** * Create a SparkContext that loads settings from system properties (for instance, when * launching with ./bin/spark-submit). */ def this() = this(new SparkConf()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters */ def this(master: String, appName: String, conf: SparkConf) = this(SparkContext.updatedConf(conf, master, appName)) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs. * @param environment Environment variables to set on worker nodes. */ def this( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment)) } // The following constructors are required when Java code accesses SparkContext directly. // Please see SI-4278 /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. */ private[spark] def this(master: String, appName: String) = this(master, appName, null, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. */ private[spark] def this(master: String, appName: String, sparkHome: String) = this(master, appName, sparkHome, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs. */ private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) = this(master, appName, sparkHome, jars, Map()) // log out Spark Version in Spark driver log logInfo(s\"Running Spark version $SPARK_VERSION\") /* ------------------------------------------------------------------------------------- * | Private variables. These variables keep the internal state of the context, and are | | not accessible by the outside world. They're mutable since we want to initialize all | | of them to some neutral value ahead of time, so that calling \"stop()\" while the | | constructor is still running is safe. | * ------------------------------------------------------------------------------------- */ private var _conf: SparkConf = _ private var _eventLogDir: Option[URI] = None private var _eventLogCodec: Option[String] = None private var _listenerBus: LiveListenerBus = _ private var _env: SparkEnv = _ private var _statusTracker: SparkStatusTracker = _ private var _progressBar: Option[ConsoleProgressBar] = None private var _ui: Option[SparkUI] = None private var _hadoopConfiguration: Configuration = _ private var _executorMemory: Int = _ private var _schedulerBackend: SchedulerBackend = _ private var _taskScheduler: TaskScheduler = _ private var _heartbeatReceiver: RpcEndpointRef = _ @volatile private var _dagScheduler: DAGScheduler = _ private var _applicationId: String = _ private var _applicationAttemptId: Option[String] = None private var _eventLogger: Option[EventLoggingListener] = None private var _driverLogger: Option[DriverLogger] = None private var _executorAllocationManager: Option[ExecutorAllocationManager] = None private var _cleaner: Option[ContextCleaner] = None private var _listenerBusStarted: Boolean = false private var _jars: Seq[String] = _ private var _files: Seq[String] = _ private var _archives: Seq[String] = _ private var _shutdownHookRef: AnyRef = _ private var _statusStore: AppStatusStore = _ private var _heartbeater: Heartbeater = _ private var _resources: immutable.Map[String, ResourceInformation] = _ private var _shuffleDriverComponents: ShuffleDriverComponents = _ private var _plugins: Option[PluginContainer] = None private var _resourceProfileManager: ResourceProfileManager = _ /* ------------------------------------------------------------------------------------- * | Accessors and public fields. These provide access to the internal state of the | | context. | * ------------------------------------------------------------------------------------- */ private[spark] def conf: SparkConf = _conf /** * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be * changed at runtime. */ def getConf: SparkConf = conf.clone() def resources: Map[String, ResourceInformation] = _resources def jars: Seq[String] = _jars def files: Seq[String] = _files def archives: Seq[String] = _archives def master: String = _conf.get(\"spark.master\") def deployMode: String = _conf.get(SUBMIT_DEPLOY_MODE) def appName: String = _conf.get(\"spark.app.name\") private[spark] def isEventLogEnabled: Boolean = _conf.get(EVENT_LOG_ENABLED) private[spark] def eventLogDir: Option[URI] = _eventLogDir private[spark] def eventLogCodec: Option[String] = _eventLogCodec def isLocal: Boolean = Utils.isLocalMaster(_conf) /** * @return true if context is stopped or in the midst of stopping. */ def isStopped: Boolean = stopped.get() private[spark] def statusStore: AppStatusStore = _statusStore // An asynchronous listener bus for Spark events private[spark] def listenerBus: LiveListenerBus = _listenerBus // This function allows components created by SparkEnv to be mocked in unit tests: private[spark] def createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv = { SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf)) } private[spark] def env: SparkEnv = _env // Used to store a URL for each static file/jar together with the file's local timestamp private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedArchives = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala // Keeps track of all persisted RDDs private[spark] val persistentRdds = { val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]() map.asScala } def statusTracker: SparkStatusTracker = _statusTracker private[spark] def progressBar: Option[ConsoleProgressBar] = _progressBar private[spark] def ui: Option[SparkUI] = _ui def uiWebUrl: Option[String] = _ui.map(_.webUrl) /** * A default Hadoop Configuration for the Hadoop code (e.g. file systems) that we reuse. * * @note As it will be reused in all Hadoop RDDs, it's better not to modify it unless you * plan to set some global configurations for all Hadoop RDDs. */ def hadoopConfiguration: Configuration = _hadoopConfiguration private[spark] def executorMemory: Int = _executorMemory // Environment variables to pass to our executors. private[spark] val executorEnvs = HashMap[String, String]() // Set SPARK_USER for user who is running SparkContext. val sparkUser = Utils.getCurrentUserName() private[spark] def schedulerBackend: SchedulerBackend = _schedulerBackend private[spark] def taskScheduler: TaskScheduler = _taskScheduler private[spark] def taskScheduler_=(ts: TaskScheduler): Unit = { _taskScheduler = ts } private[spark] def dagScheduler: DAGScheduler = _dagScheduler private[spark] def dagScheduler_=(ds: DAGScheduler): Unit = { _dagScheduler = ds } private[spark] def shuffleDriverComponents: ShuffleDriverComponents = _shuffleDriverComponents /** * A unique identifier for the Spark application. * Its format depends on the scheduler implementation. * (i.e. * in case of local spark app something like 'local-1433865536131' * in case of YARN something like 'application_1433865536131_34483' * in case of MESOS something like 'driver-20170926223339-0001' * ) */ def applicationId: String = _applicationId def applicationAttemptId: Option[String] = _applicationAttemptId private[spark] def eventLogger: Option[EventLoggingListener] = _eventLogger private[spark] def executorAllocationManager: Option[ExecutorAllocationManager] = _executorAllocationManager private[spark] def resourceProfileManager: ResourceProfileManager = _resourceProfileManager private[spark] def cleaner: Option[ContextCleaner] = _cleaner private[spark] var checkpointDir: Option[String] = None // Thread Local variable that can be used by users to pass information down the stack protected[spark] val localProperties = new InheritableThreadLocal[Properties] { override def childValue(parent: Properties): Properties = { // Note: make a clone such that changes in the parent properties aren't reflected in // the those of the children threads, which has confusing semantics (SPARK-10563). Utils.cloneProperties(parent) } override protected def initialValue(): Properties = new Properties() } /* ------------------------------------------------------------------------------------- * | Initialization. This code initializes the context in a manner that is exception-safe. | | All internal fields holding state are initialized here, and any error prompts the | | stop() method to be called. | * ------------------------------------------------------------------------------------- */ private def warnSparkMem(value: String): String = { logWarning(\"Using SPARK_MEM to set amount of memory to use per executor process is \" + \"deprecated, please use spark.executor.memory instead.\") value } /** Control our logLevel. This overrides any user-defined log settings. * @param logLevel The desired log level as a string. * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN */ def setLogLevel(logLevel: String): Unit = { // let's allow lowercase or mixed case too val upperCased = logLevel.toUpperCase(Locale.ROOT) require(SparkContext.VALID_LOG_LEVELS.contains(upperCased), s\"Supplied level $logLevel did not match one of:\" + s\" ${SparkContext.VALID_LOG_LEVELS.mkString(\",\")}\") Utils.setLogLevel(Level.toLevel(upperCased)) } try { _conf = config.clone() _conf.validateSettings() _conf.set(\"spark.app.startTime\", startTime.toString) if (!_conf.contains(\"spark.master\")) { throw new SparkException(\"A master URL must be set in your configuration\") } if (!_conf.contains(\"spark.app.name\")) { throw new SparkException(\"An application name must be set in your configuration\") } // This should be set as early as possible. SparkContext.fillMissingMagicCommitterConfsIfNeeded(_conf) SparkContext.supplementJavaModuleOptions(_conf) _driverLogger = DriverLogger(_conf) val resourcesFileOpt = conf.get(DRIVER_RESOURCES_FILE) _resources = getOrDiscoverAllResources(_conf, SPARK_DRIVER_PREFIX, resourcesFileOpt) logResourceInfo(SPARK_DRIVER_PREFIX, _resources) // log out spark.app.name in the Spark driver logs logInfo(s\"Submitted application: $appName\") // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster if (master == \"yarn\" && deployMode == \"cluster\" && !_conf.contains(\"spark.yarn.app.id\")) { throw new SparkException(\"Detected yarn cluster mode, but isn't running on a cluster. \" + \"Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.\") } if (_conf.getBoolean(\"spark.logConf\", false)) { logInfo(\"Spark configuration:\\n\" + _conf.toDebugString) } // Set Spark driver host and port system properties. This explicitly sets the configuration // instead of relying on the default value of the config constant. _conf.set(DRIVER_HOST_ADDRESS, _conf.get(DRIVER_HOST_ADDRESS)) _conf.setIfMissing(DRIVER_PORT, 0) _conf.set(EXECUTOR_ID, SparkContext.DRIVER_IDENTIFIER) _jars = Utils.getUserJars(_conf) _files = _conf.getOption(FILES.key).map(_.split(\",\")).map(_.filter(_.nonEmpty)) .toSeq.flatten _archives = _conf.getOption(ARCHIVES.key).map(Utils.stringToSeq).toSeq.flatten _eventLogDir = if (isEventLogEnabled) { val unresolvedDir = conf.get(EVENT_LOG_DIR).stripSuffix(\"/\") Some(Utils.resolveURI(unresolvedDir)) } else { None } _eventLogCodec = { val compress = _conf.get(EVENT_LOG_COMPRESS) if (compress && isEventLogEnabled) { Some(_conf.get(EVENT_LOG_COMPRESSION_CODEC)).map(CompressionCodec.getShortName) } else { None } } _listenerBus = new LiveListenerBus(_conf) _resourceProfileManager = new ResourceProfileManager(_conf, _listenerBus) // Initialize the app status store and listener before SparkEnv is created so that it gets // all events. val appStatusSource = AppStatusSource.createSource(conf) _statusStore = AppStatusStore.createLiveStore(conf, appStatusSource) listenerBus.addToStatusQueue(_statusStore.listener.get) // Create the Spark execution environment (cache, map output tracker, etc) _env = createSparkEnv(_conf, isLocal, listenerBus) SparkEnv.set(_env) // If running the REPL, register the repl's output dir with the file server. _conf.getOption(\"spark.repl.class.outputDir\").foreach { path => val replUri = _env.rpcEnv.fileServer.addDirectory(\"/classes\", new File(path)) _conf.set(\"spark.repl.class.uri\", replUri) } _statusTracker = new SparkStatusTracker(this, _statusStore) _progressBar = if (_conf.get(UI_SHOW_CONSOLE_PROGRESS)) { Some(new ConsoleProgressBar(this)) } else { None } _ui = if (conf.get(UI_ENABLED)) { Some(SparkUI.create(Some(this), _statusStore, _conf, _env.securityManager, appName, \"\", startTime)) } else { // For tests, do not enable the UI None } // Bind the UI before starting the task scheduler to communicate // the bound port to the cluster manager properly _ui.foreach(_.bind()) _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf) // Performance optimization: this dummy call to .size() triggers eager evaluation of // Configuration's internal `properties` field, guaranteeing that it will be computed and // cached before SessionState.newHadoopConf() uses `sc.hadoopConfiguration` to create // a new per-session Configuration. If `properties` has not been computed by that time // then each newly-created Configuration will perform its own expensive IO and XML // parsing to load configuration defaults and populate its own properties. By ensuring // that we've pre-computed the parent's properties, the child Configuration will simply // clone the parent's properties. _hadoopConfiguration.size() // Add each JAR given through the constructor if (jars != null) { jars.foreach(jar => addJar(jar, true)) if (addedJars.nonEmpty) { _conf.set(\"spark.app.initial.jar.urls\", addedJars.keys.toSeq.mkString(\",\")) } } if (files != null) { files.foreach(file => addFile(file, false, true)) if (addedFiles.nonEmpty) { _conf.set(\"spark.app.initial.file.urls\", addedFiles.keys.toSeq.mkString(\",\")) } } if (archives != null) { archives.foreach(file => addFile(file, false, true, isArchive = true)) if (addedArchives.nonEmpty) { _conf.set(\"spark.app.initial.archive.urls\", addedArchives.keys.toSeq.mkString(\",\")) } } _executorMemory = _conf.getOption(EXECUTOR_MEMORY.key) .orElse(Option(System.getenv(\"SPARK_EXECUTOR_MEMORY\"))) .orElse(Option(System.getenv(\"SPARK_MEM\")) .map(warnSparkMem)) .map(Utils.memoryStringToMb) .getOrElse(1024) // Convert java options to env vars as a work around // since we can't set env vars directly in sbt. for { (envKey, propKey) <- Seq((\"SPARK_TESTING\", IS_TESTING.key)) value <- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))} { executorEnvs(envKey) = value } Option(System.getenv(\"SPARK_PREPEND_CLASSES\")).foreach { v => executorEnvs(\"SPARK_PREPEND_CLASSES\") = v } // The Mesos scheduler backend relies on this environment variable to set executor memory. // TODO: Set this only in the Mesos scheduler. executorEnvs(\"SPARK_EXECUTOR_MEMORY\") = executorMemory + \"m\" executorEnvs ++= _conf.getExecutorEnv executorEnvs(\"SPARK_USER\") = sparkUser _shuffleDriverComponents = ShuffleDataIOUtils.loadShuffleDataIO(config).driver() _shuffleDriverComponents.initializeApplication().asScala.foreach { case (k, v) => _conf.set(ShuffleDataIOUtils.SHUFFLE_SPARK_CONF_PREFIX + k, v) } // We need to register \"HeartbeatReceiver\" before \"createTaskScheduler\" because Executor will // retrieve \"HeartbeatReceiver\" in the constructor. (SPARK-6640) _heartbeatReceiver = env.rpcEnv.setupEndpoint( HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this)) // Initialize any plugins before the task scheduler is initialized. _plugins = PluginContainer(this, _resources.asJava) // Create and start the scheduler val (sched, ts) = SparkContext.createTaskScheduler(this, master) _schedulerBackend = sched _taskScheduler = ts _dagScheduler = new DAGScheduler(this) _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet) val _executorMetricsSource = if (_conf.get(METRICS_EXECUTORMETRICS_SOURCE_ENABLED)) { Some(new ExecutorMetricsSource) } else { None } // create and start the heartbeater for collecting memory metrics _heartbeater = new Heartbeater( () => SparkContext.this.reportHeartBeat(_executorMetricsSource), \"driver-heartbeater\", conf.get(EXECUTOR_HEARTBEAT_INTERVAL)) _heartbeater.start() // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's // constructor _taskScheduler.start() _applicationId = _taskScheduler.applicationId() _applicationAttemptId = _taskScheduler.applicationAttemptId() _conf.set(\"spark.app.id\", _applicationId) _applicationAttemptId.foreach { attemptId => _conf.set(APP_ATTEMPT_ID, attemptId) _env.blockManager.blockStoreClient.setAppAttemptId(attemptId) } if (_conf.get(UI_REVERSE_PROXY)) { val proxyUrl = _conf.get(UI_REVERSE_PROXY_URL.key, \"\").stripSuffix(\"/\") + \"/proxy/\" + _applicationId System.setProperty(\"spark.ui.proxyBase\", proxyUrl) } _ui.foreach(_.setAppId(_applicationId)) _env.blockManager.initialize(_applicationId) FallbackStorage.registerBlockManagerIfNeeded(_env.blockManager.master, _conf) // The metrics system for Driver need to be set spark.app.id to app ID. // So it should start after we get app ID from the task scheduler and set spark.app.id. _env.metricsSystem.start(_conf.get(METRICS_STATIC_SOURCES_ENABLED)) _eventLogger = if (isEventLogEnabled) { val logger = new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get, _conf, _hadoopConfiguration) logger.start() listenerBus.addToEventLogQueue(logger) Some(logger) } else { None } _cleaner = if (_conf.get(CLEANER_REFERENCE_TRACKING)) { Some(new ContextCleaner(this, _shuffleDriverComponents)) } else { None } _cleaner.foreach(_.start()) val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) _executorAllocationManager = if (dynamicAllocationEnabled) { schedulerBackend match { case b: ExecutorAllocationClient => Some(new ExecutorAllocationManager( schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf, cleaner = cleaner, resourceProfileManager = resourceProfileManager)) case _ => None } } else { None } _executorAllocationManager.foreach(_.start()) setupAndStartListenerBus() postEnvironmentUpdate() postApplicationStart() // After application started, attach handlers to started server and start handler. _ui.foreach(_.attachAllHandler()) // Attach the driver metrics servlet handler to the web ui after the metrics system is started. _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler))) // Make sure the context is stopped if the user forgets about it. This avoids leaving // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM // is killed, though. logDebug(\"Adding shutdown hook\") // force eager creation of logger _shutdownHookRef = ShutdownHookManager.addShutdownHook( ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () => logInfo(\"Invoking stop() from shutdown hook\") try { stop() } catch { case e: Throwable => logWarning(\"Ignoring Exception while stopping SparkContext from shutdown hook\", e) } } // Post init _taskScheduler.postStartHook() if (isLocal) { _env.metricsSystem.registerSource(Executor.executorSourceLocalModeOnly) } _env.metricsSystem.registerSource(_dagScheduler.metricsSource) _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager)) _env.metricsSystem.registerSource(new JVMCPUSource()) _executorMetricsSource.foreach(_.register(_env.metricsSystem)) _executorAllocationManager.foreach { e => _env.metricsSystem.registerSource(e.executorAllocationManagerSource) } appStatusSource.foreach(_env.metricsSystem.registerSource(_)) _plugins.foreach(_.registerMetrics(applicationId)) } catch { case NonFatal(e) => logError(\"Error initializing SparkContext.\", e) try { stop() } catch { case NonFatal(inner) => logError(\"Error stopping SparkContext after init error.\", inner) } finally { throw e } } /** * Called by the web UI to obtain executor thread dumps. This method may be expensive. * Logs an error and returns None if we failed to obtain a thread dump, which could occur due * to an executor being dead or unresponsive or due to network issues while sending the thread * dump message back to the driver. */ private[spark] def getExecutorThreadDump(executorId: String): Option[Array[ThreadStackTrace]] = { try { if (executorId == SparkContext.DRIVER_IDENTIFIER) { Some(Utils.getThreadDump()) } else { env.blockManager.master.getExecutorEndpointRef(executorId) match { case Some(endpointRef) => Some(endpointRef.askSync[Array[ThreadStackTrace]](TriggerThreadDump)) case None => logWarning(s\"Executor $executorId might already have stopped and \" + \"can not request thread dump from it.\") None } } } catch { case e: Exception => logError(s\"Exception getting thread dump from executor $executorId\", e) None } } private[spark] def getLocalProperties: Properties = localProperties.get() private[spark] def setLocalProperties(props: Properties): Unit = { localProperties.set(props) } /** * Set a local property that affects jobs submitted from this thread, such as the Spark fair * scheduler pool. User-defined properties may also be set here. These properties are propagated * through to worker tasks and can be accessed there via * [[org.apache.spark.TaskContext#getLocalProperty]]. * * These properties are inherited by child threads spawned from this thread. This * may have unexpected consequences when working with thread pools. The standard java * implementation of thread pools have worker threads spawn other worker threads. * As a result, local properties may propagate unpredictably. */ def setLocalProperty(key: String, value: String): Unit = { if (value == null) { localProperties.get.remove(key) } else { localProperties.get.setProperty(key, value) } } /** * Get a local property set in this thread, or null if it is missing. See * `org.apache.spark.SparkContext.setLocalProperty`. */ def getLocalProperty(key: String): String = Option(localProperties.get).map(_.getProperty(key)).orNull /** Set a human readable description of the current job. */ def setJobDescription(value: String): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, value) } /** * Assigns a group ID to all the jobs started by this thread until the group ID is set to a * different value or cleared. * * Often, a unit of execution in an application consists of multiple Spark actions or jobs. * Application programmers can use this method to group all those jobs together and give a * group description. Once set, the Spark web UI will associate such jobs with this group. * * The application can also use `org.apache.spark.SparkContext.cancelJobGroup` to cancel all * running jobs in this group. For example, * {{{ * // In the main thread: * sc.setJobGroup(\"some_job_to_cancel\", \"some job description\") * sc.parallelize(1 to 10000, 2).map { i => Thread.sleep(10); i }.count() * * // In a separate thread: * sc.cancelJobGroup(\"some_job_to_cancel\") * }}} * * @param interruptOnCancel If true, then job cancellation will result in `Thread.interrupt()` * being called on the job's executor threads. This is useful to help ensure that the tasks * are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS * may respond to Thread.interrupt() by marking nodes as dead. */ def setJobGroup(groupId: String, description: String, interruptOnCancel: Boolean = false): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId) // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup // APIs to also take advantage of this property (e.g., internal job failures or canceling from // JobProgressTab UI) on a per-job basis. setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString) } /** Clear the current thread's job group ID and its description. */ def clearJobGroup(): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, null) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, null) setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, null) } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * @note Return statements are NOT allowed in the given body. */ private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](this)(body) // Methods for creating RDDs /** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection. Pass a copy of the argument to avoid this. * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection */ def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } /** * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by * `step` every element. * * @note if we need to cache this RDD, we should make sure each partition does not exceed limit. * * @param start the start value. * @param end the end value. * @param step the incremental step * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed range */ def range( start: Long, end: Long, step: Long = 1, numSlices: Int = defaultParallelism): RDD[Long] = withScope { assertNotStopped() // when step is 0, range will run infinitely require(step != 0, \"step cannot be 0\") val numElements: BigInt = { val safeStart = BigInt(start) val safeEnd = BigInt(end) if ((safeEnd - safeStart) % step == 0 || (safeEnd > safeStart) != (step > 0)) { (safeEnd - safeStart) / step } else { // the remainder has the same sign with range, could add 1 more (safeEnd - safeStart) / step + 1 } } parallelize(0 until numSlices, numSlices).mapPartitionsWithIndex { (i, _) => val partitionStart = (i * numElements) / numSlices * step + start val partitionEnd = (((i + 1) * numElements) / numSlices) * step + start def getSafeMargin(bi: BigInt): Long = if (bi.isValidLong) { bi.toLong } else if (bi > 0) { Long.MaxValue } else { Long.MinValue } val safePartitionStart = getSafeMargin(partitionStart) val safePartitionEnd = getSafeMargin(partitionEnd) new Iterator[Long] { private[this] var number: Long = safePartitionStart private[this] var overflow: Boolean = false override def hasNext = if (!overflow) { if (step > 0) { number < safePartitionEnd } else { number > safePartitionEnd } } else false override def next() = { val ret = number number += step if (number < ret ^ step < 0) { // we have Long.MaxValue + Long.MaxValue < Long.MaxValue // and Long.MinValue + Long.MinValue > Long.MinValue, so iff the step causes a step // back, we are pretty sure that we have an overflow. overflow = true } ret } } } } /** Distribute a local Scala collection to form an RDD. * * This method is identical to `parallelize`. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection */ def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices) } /** * Distribute a local Scala collection to form an RDD, with one or more * location preferences (hostnames of Spark nodes) for each object. * Create a new partition for each collection item. * @param seq list of tuples of data and location preferences (hostnames of Spark nodes) * @return RDD representing data partitioned according to location preferences */ def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope { assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs) } /** * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * The text files must be encoded as UTF-8. * * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file */ def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope { assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair => pair._2.toString).setName(path) } /** * Read a directory of text files from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI. Each file is read as a single record and returned in a * key-value pair, where the key is the path of each file, the value is the content of each file. * The text files must be encoded as UTF-8. * * <p> For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do `val rdd = sparkContext.wholeTextFile(\"hdfs://a-hdfs-path\")`, * * <p> then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred, large file is also allowable, but may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and the corresponding file content */ def wholeTextFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, String)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that wholeTextFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new WholeTextFileRDD( this, classOf[WholeTextFileInputFormat], classOf[Text], classOf[Text], updateConf, minPartitions).map(record => (record._1.toString, record._2.toString)).setName(path) } /** * Get an RDD for a Hadoop-readable dataset as PortableDataStream for each file * (useful for binary data) * * For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do * `val rdd = sparkContext.binaryFiles(\"hdfs://a-hdfs-path\")`, * * then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred; very large files may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and corresponding file content */ def binaryFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, PortableDataStream)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that binaryFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new BinaryFileRDD( this, classOf[StreamInputFormat], classOf[String], classOf[PortableDataStream], updateConf, minPartitions).setName(path) } /** * Load data from a flat binary file, assuming the length of each record is constant. * * @note We ensure that the byte array for each record in the resulting RDD * has the provided record length. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param recordLength The length at which to split the records * @param conf Configuration for setting up the dataset. * * @return An RDD of data with values, represented as byte arrays */ def binaryRecords( path: String, recordLength: Int, conf: Configuration = hadoopConfiguration): RDD[Array[Byte]] = withScope { assertNotStopped() conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = newAPIHadoopFile[LongWritable, BytesWritable, FixedLengthBinaryInputFormat](path, classOf[FixedLengthBinaryInputFormat], classOf[LongWritable], classOf[BytesWritable], conf = conf) br.map { case (k, v) => val bytes = v.copyBytes() assert(bytes.length == recordLength, \"Byte array does not have correct length\") bytes } } /** * Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf given its InputFormat and other * necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable), * using the older MapReduce API (`org.apache.hadoop.mapred`). * * @param conf JobConf for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions Minimum number of Hadoop Splits to generate. * @return RDD of tuples of key and corresponding value * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. */ def hadoopRDD[K, V]( conf: JobConf, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf before broadcasting it. SparkHadoopUtil.get.addCredentials(conf) new HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minPartitions) } /** Get an RDD for a Hadoop file with an arbitrary InputFormat * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value */ def hadoopFile[K, V]( path: String, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // A Hadoop configuration can be about 10 KiB, which is pretty big, so broadcast it. val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration)) val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path) new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minPartitions) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value */ def hadoopFile[K, V, F <: InputFormat[K, V]] (path: String, minPartitions: Int) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile(path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]], minPartitions) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths as * a list of inputs * @return RDD of tuples of key and corresponding value */ def hadoopFile[K, V, F <: InputFormat[K, V]](path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile[K, V, F](path, defaultMinPartitions) } /** * Smarter version of `newApiHadoopFile` that uses class tags to figure out the classes of keys, * values and the `org.apache.hadoop.mapreduce.InputFormat` (new MapReduce API) so that user * don't need to pass them directly. Instead, callers can just write, for example: * ``` * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * ``` * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @return RDD of tuples of key and corresponding value */ def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]] (path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { newAPIHadoopFile( path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]]) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * @param conf Hadoop configuration * @return RDD of tuples of key and corresponding value */ def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( path: String, fClass: Class[F], kClass: Class[K], vClass: Class[V], conf: Configuration = hadoopConfiguration): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // The call to NewHadoopJob automatically adds security credentials to conf, // so we don't need to explicitly add them ourselves val job = NewHadoopJob.getInstance(conf) // Use setInputPaths so that newAPIHadoopFile aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updatedConf = job.getConfiguration new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @param conf Configuration for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. */ def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( conf: Configuration = hadoopConfiguration, fClass: Class[F], kClass: Class[K], vClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf. Required to access secure HDFS. val jconf = new JobConf(conf) SparkHadoopUtil.get.addCredentials(jconf) new NewHadoopRDD(this, fClass, kClass, vClass, jconf) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value */ def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V], minPartitions: Int ): RDD[(K, V)] = withScope { assertNotStopped() val inputFormatClass = classOf[SequenceFileInputFormat[K, V]] hadoopFile(path, inputFormatClass, keyClass, valueClass, minPartitions) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @return RDD of tuples of key and corresponding value */ def sequenceFile[K, V]( path: String, keyClass: Class[K], valueClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() sequenceFile(path, keyClass, valueClass, defaultMinPartitions) } /** * Version of sequenceFile() for types implicitly convertible to Writables through a * WritableConverter. For example, to access a SequenceFile where the keys are Text and the * values are IntWritable, you could simply write * {{{ * sparkContext.sequenceFile[String, Int](path, ...) * }}} * * WritableConverters are provided in a somewhat strange way (by an implicit function) to support * both subclasses of Writable and types for which we define a converter (e.g. Int to * IntWritable). The most natural thing would've been to have implicit objects for the * converters, but then we couldn't have an object for every subclass of Writable (you can't * have a parameterized singleton object). We use functions instead to create a new converter * for the appropriate type. In addition, we pass the converter a ClassTag of its type to * allow it to figure out the Writable class to use in the subclass case. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value */ def sequenceFile[K, V] (path: String, minPartitions: Int = defaultMinPartitions) (implicit km: ClassTag[K], vm: ClassTag[V], kcf: () => WritableConverter[K], vcf: () => WritableConverter[V]): RDD[(K, V)] = { withScope { assertNotStopped() val kc = clean(kcf)() val vc = clean(vcf)() val format = classOf[SequenceFileInputFormat[Writable, Writable]] val writables = hadoopFile(path, format, kc.writableClass(km).asInstanceOf[Class[Writable]], vc.writableClass(vm).asInstanceOf[Class[Writable]], minPartitions) writables.map { case (k, v) => (kc.convert(k), vc.convert(v)) } } } /** * Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and * BytesWritable values that contain a serialized partition. This is still an experimental * storage format and may not be supported exactly as is in future Spark releases. It will also * be pretty slow if you use the default serializer (Java serialization), * though the nice thing about it is that there's very little effort required to save arbitrary * objects. * * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD representing deserialized data from the file(s) */ def objectFile[T: ClassTag]( path: String, minPartitions: Int = defaultMinPartitions): RDD[T] = withScope { assertNotStopped() sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions) .flatMap(x => Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader)) } protected[spark] def checkpointFile[T: ClassTag](path: String): RDD[T] = withScope { new ReliableCheckpointRDD[T](this, path) } /** Build the union of a list of RDDs. */ def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope { val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty) val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet if (nonEmptyRdds.forall(_.partitioner.isDefined) && partitioners.size == 1) { new PartitionerAwareUnionRDD(this, nonEmptyRdds) } else { new UnionRDD(this, nonEmptyRdds) } } /** Build the union of a list of RDDs passed as variable-length arguments. */ def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope { union(Seq(first) ++ rest) } /** Get an RDD that has no partitions or elements. */ def emptyRDD[T: ClassTag]: RDD[T] = new EmptyRDD[T](this) // Methods for creating shared variables /** * Register the given accumulator. * * @note Accumulators must be registered before use, or it will throw exception. */ def register(acc: AccumulatorV2[_, _]): Unit = { acc.register(this) } /** * Register the given accumulator with given name. * * @note Accumulators must be registered before use, or it will throw exception. */ def register(acc: AccumulatorV2[_, _], name: String): Unit = { acc.register(this, name = Option(name)) } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`. */ def longAccumulator: LongAccumulator = { val acc = new LongAccumulator register(acc) acc } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`. */ def longAccumulator(name: String): LongAccumulator = { val acc = new LongAccumulator register(acc, name) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`. */ def doubleAccumulator: DoubleAccumulator = { val acc = new DoubleAccumulator register(acc) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`. */ def doubleAccumulator(name: String): DoubleAccumulator = { val acc = new DoubleAccumulator register(acc, name) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list. */ def collectionAccumulator[T]: CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list. */ def collectionAccumulator[T](name: String): CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc, name) acc } /** * Broadcast a read-only variable to the cluster, returning a * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions. * The variable will be sent to each cluster only once. * * @param value value to broadcast to the Spark nodes * @return `Broadcast` object, a read-only variable cached on each machine */ def broadcast[T: ClassTag](value: T): Broadcast[T] = { assertNotStopped() require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass), \"Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\") val bc = env.broadcastManager.newBroadcast[T](value, isLocal) val callSite = getCallSite logInfo(\"Created broadcast \" + bc.id + \" from \" + callSite.shortForm) cleaner.foreach(_.registerBroadcastForCleanup(bc)) bc } /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * * @note A path can be added only once. Subsequent additions of the same path are ignored. */ def addFile(path: String): Unit = { addFile(path, false, false) } /** * Returns a list of file paths that are added to resources. */ def listFiles(): Seq[String] = addedFiles.keySet.toSeq /** * :: Experimental :: * Add an archive to be downloaded and unpacked with this Spark job on every node. * * If an archive is added during execution, it will not be available until the next TaskSet * starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(paths-to-files)` to find its download/unpacked location. * The given path should be one of .zip, .tar, .tar.gz, .tgz and .jar. * * @note A path can be added only once. Subsequent additions of the same path are ignored. * * @since 3.1.0 */ @Experimental def addArchive(path: String): Unit = { addFile(path, false, false, isArchive = true) } /** * :: Experimental :: * Returns a list of archive paths that are added to resources. * * @since 3.1.0 */ @Experimental def listArchives(): Seq[String] = addedArchives.keySet.toSeq /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * @param recursive if true, a directory can be given in `path`. Currently directories are * only supported for Hadoop-supported filesystems. * * @note A path can be added only once. Subsequent additions of the same path are ignored. */ def addFile(path: String, recursive: Boolean): Unit = { addFile(path, recursive, false) } private def addFile( path: String, recursive: Boolean, addedOnSubmit: Boolean, isArchive: Boolean = false ): Unit = { val uri = Utils.resolveURI(path) val schemeCorrectedURI = uri.getScheme match { case null => new File(path).getCanonicalFile.toURI case \"local\" => logWarning(s\"File with 'local' scheme $path is not supported to add to file server, \" + s\"since it is already available on every node.\") return case _ => uri } val hadoopPath = new Path(schemeCorrectedURI) val scheme = schemeCorrectedURI.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) { val fs = hadoopPath.getFileSystem(hadoopConfiguration) val isDir = fs.getFileStatus(hadoopPath).isDirectory if (!isLocal && scheme == \"file\" && isDir) { throw new SparkException(s\"addFile does not support local directories when not running \" + \"local mode.\") } if (!recursive && isDir) { throw new SparkException(s\"Added file $hadoopPath is a directory and recursive is not \" + \"turned on.\") } } else { // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) } val key = if (!isLocal && scheme == \"file\") { env.rpcEnv.fileServer.addFile(new File(uri.getPath)) } else if (uri.getScheme == null) { schemeCorrectedURI.toString } else { uri.toString } val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis if (!isArchive && addedFiles.putIfAbsent(key, timestamp).isEmpty) { logInfo(s\"Added file $path at $key with timestamp $timestamp\") // Fetch the file locally so that closures which are run on the driver can still use the // SparkFiles API to access files. Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf, hadoopConfiguration, timestamp, useCache = false) postEnvironmentUpdate() } else if ( isArchive && addedArchives.putIfAbsent( UriBuilder.fromUri(new URI(key)).fragment(uri.getFragment).build().toString, timestamp).isEmpty) { logInfo(s\"Added archive $path at $key with timestamp $timestamp\") // If the scheme is file, use URI to simply copy instead of downloading. val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key) val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build() val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false) val dest = new File( SparkFiles.getRootDirectory(), if (uri.getFragment != null) uri.getFragment else source.getName) logInfo( s\"Unpacking an archive $path from ${source.getAbsolutePath} to ${dest.getAbsolutePath}\") Utils.deleteRecursively(dest) Utils.unpack(source, dest) postEnvironmentUpdate() } else { logWarning(s\"The path $path has been added already. Overwriting of added paths \" + \"is not supported in the current version.\") } } /** * :: DeveloperApi :: * Register a listener to receive up-calls from events that happen during execution. */ @DeveloperApi def addSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.addToSharedQueue(listener) } /** * :: DeveloperApi :: * Deregister the listener from Spark's listener bus. */ @DeveloperApi def removeSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.removeListener(listener) } private[spark] def getExecutorIds(): Seq[String] = { schedulerBackend match { case b: ExecutorAllocationClient => b.getExecutorIds() case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") Nil } } /** * Get the max number of tasks that can be concurrent launched based on the ResourceProfile * could be used, even if some of them are being used at the moment. * Note that please don't cache the value returned by this method, because the number can change * due to add/remove executors. * * @param rp ResourceProfile which to use to calculate max concurrent tasks. * @return The max number of tasks that can be concurrent launched currently. */ private[spark] def maxNumConcurrentTasks(rp: ResourceProfile): Int = { schedulerBackend.maxNumConcurrentTasks(rp) } /** * Update the cluster manager on our scheduling needs. Three bits of information are included * to help it make decisions. This applies to the default ResourceProfile. * @param numExecutors The total number of executors we'd like to have. The cluster manager * shouldn't kill any running executor to reach this number, but, * if all existing executors were to die, this is the number of executors * we'd want to be allocated. * @param localityAwareTasks The number of tasks in all active stages that have a locality * preferences. This includes running, pending, and completed tasks. * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages * that would like to like to run on that host. * This includes running, pending, and completed tasks. * @return whether the request is acknowledged by the cluster manager. */ @DeveloperApi def requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: immutable.Map[String, Int] ): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => // this is being applied to the default resource profile, would need to add api to support // others val defaultProfId = resourceProfileManager.defaultResourceProfile.id b.requestTotalExecutors(immutable.Map(defaultProfId-> numExecutors), immutable.Map(localityAwareTasks -> defaultProfId), immutable.Map(defaultProfId -> hostToLocalTaskCount)) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request an additional number of executors from the cluster manager. * @return whether the request is received. */ @DeveloperApi def requestExecutors(numAdditionalExecutors: Int): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.requestExecutors(numAdditionalExecutors) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executors. * * This is not supported when dynamic allocation is turned on. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executors it kills * through this method with new ones, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received. */ @DeveloperApi def killExecutors(executorIds: Seq[String]): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => require(executorAllocationManager.isEmpty, \"killExecutors() unsupported with Dynamic Allocation turned on\") b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executor. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executor it kills * through this method with a new one, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received. */ @DeveloperApi def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId)) /** * Request that the cluster manager kill the specified executor without adjusting the * application resource requirements. * * The effect is that a new executor will be launched in place of the one killed by * this request. This assumes the cluster manager will automatically and eventually * fulfill all missing application resource requests. * * @note The replace is by no means guaranteed; another application on the same cluster * can steal the window of opportunity and acquire this application's resources in the * mean time. * * @return whether the request is received. */ private[spark] def killAndReplaceExecutor(executorId: String): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** The version of Spark on which this application is running. */ def version: String = SPARK_VERSION /** * Return a map from the block manager to the max memory available for caching and the remaining * memory available for caching. */ def getExecutorMemoryStatus: Map[String, (Long, Long)] = { assertNotStopped() env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) => (blockManagerId.host + \":\" + blockManagerId.port, mem) } } /** * :: DeveloperApi :: * Return information about what RDDs are cached, if they are in mem or on disk, how much space * they take, etc. */ @DeveloperApi def getRDDStorageInfo: Array[RDDInfo] = { getRDDStorageInfo(_ => true) } private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = { assertNotStopped() val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray rddInfos.foreach { rddInfo => val rddId = rddInfo.id val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId)) rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0) rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L) rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L) } rddInfos.filter(_.isCached) } /** * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call. * * @note This does not necessarily mean the caching or computation was successful. */ def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap /** * :: DeveloperApi :: * Return pools for fair scheduler */ @DeveloperApi def getAllPools: Seq[Schedulable] = { assertNotStopped() // TODO(xiajunluan): We should take nested pools into account taskScheduler.rootPool.schedulableQueue.asScala.toSeq } /** * :: DeveloperApi :: * Return the pool associated with the given name, if one exists */ @DeveloperApi def getPoolForName(pool: String): Option[Schedulable] = { assertNotStopped() Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool)) } /** * Return current scheduling mode */ def getSchedulingMode: SchedulingMode.SchedulingMode = { assertNotStopped() taskScheduler.schedulingMode } /** * Gets the locality information associated with the partition in a particular rdd * @param rdd of interest * @param partition to be looked up for locality * @return list of preferred locations for the partition */ private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { dagScheduler.getPreferredLocs(rdd, partition) } /** * Register an RDD to be persisted in memory and/or disk storage */ private[spark] def persistRDD(rdd: RDD[_]): Unit = { persistentRdds(rdd.id) = rdd } /** * Unpersist an RDD from memory and/or disk storage */ private[spark] def unpersistRDD(rddId: Int, blocking: Boolean): Unit = { env.blockManager.master.removeRdd(rddId, blocking) persistentRdds.remove(rddId) listenerBus.post(SparkListenerUnpersistRDD(rddId)) } /** * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future. * * If a jar is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. * * @note A path can be added only once. Subsequent additions of the same path are ignored. */ def addJar(path: String): Unit = { addJar(path, false) } private def addJar(path: String, addedOnSubmit: Boolean): Unit = { def addLocalJarFile(file: File): Seq[String] = { try { if (!file.exists()) { throw new FileNotFoundException(s\"Jar ${file.getAbsolutePath} not found\") } if (file.isDirectory) { throw new IllegalArgumentException( s\"Directory ${file.getAbsoluteFile} is not allowed for addJar\") } Seq(env.rpcEnv.fileServer.addJar(file)) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } def checkRemoteJarFile(path: String): Seq[String] = { val hadoopPath = new Path(path) val scheme = hadoopPath.toUri.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme)) { try { val fs = hadoopPath.getFileSystem(hadoopConfiguration) if (!fs.exists(hadoopPath)) { throw new FileNotFoundException(s\"Jar ${path} not found\") } if (fs.getFileStatus(hadoopPath).isDirectory) { throw new IllegalArgumentException( s\"Directory ${path} is not allowed for addJar\") } Seq(path) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } else { Seq(path) } } if (path == null || path.isEmpty) { logWarning(\"null or empty path specified as parameter to addJar\") } else { val (keys, scheme) = if (path.contains(\"\\\\\") && Utils.isWindows) { // For local paths with backslashes on Windows, URI throws an exception (addLocalJarFile(new File(path)), \"local\") } else { val uri = Utils.resolveURI(path) // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) val uriScheme = uri.getScheme val jarPaths = uriScheme match { // A JAR file which exists only on the driver node case null => // SPARK-22585 path without schema is not url encoded addLocalJarFile(new File(uri.getPath)) // A JAR file which exists only on the driver node case \"file\" => addLocalJarFile(new File(uri.getPath)) // A JAR file which exists locally on every worker node case \"local\" => Seq(\"file:\" + uri.getPath) case \"ivy\" => // Since `new Path(path).toUri` will lose query information, // so here we use `URI.create(path)` DependencyUtils.resolveMavenDependencies(URI.create(path)) .flatMap(jar => addLocalJarFile(new File(jar))) case _ => checkRemoteJarFile(path) } (jarPaths, uriScheme) } if (keys.nonEmpty) { val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis val (added, existed) = keys.partition(addedJars.putIfAbsent(_, timestamp).isEmpty) if (added.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"Added $jarMessage $path at ${added.mkString(\",\")} with timestamp $timestamp\") postEnvironmentUpdate() } if (existed.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" + \" Overwriting of added jar is not supported in the current version.\") } } } } /** * Returns a list of jar files that are added to resources. */ def listJars(): Seq[String] = addedJars.keySet.toSeq /** * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark * may wait for some internal threads to finish. It's better to use this method to stop * SparkContext instead. */ private[spark] def stopInNewThread(): Unit = { new Thread(\"stop-spark-context\") { setDaemon(true) override def run(): Unit = { try { SparkContext.this.stop() } catch { case e: Throwable => logError(e.getMessage, e) throw e } } }.start() } /** * Shut down the SparkContext. */ def stop(): Unit = { if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop SparkContext within listener bus thread.\") } // Use the stopping variable to ensure no contention for the stop scenario. // Still track the stopped variable for use elsewhere in the code. if (!stopped.compareAndSet(false, true)) { logInfo(\"SparkContext already stopped.\") return } if (_shutdownHookRef != null) { ShutdownHookManager.removeShutdownHook(_shutdownHookRef) } if (listenerBus != null) { Utils.tryLogNonFatalError { postApplicationEnd() } } Utils.tryLogNonFatalError { _driverLogger.foreach(_.stop()) } Utils.tryLogNonFatalError { _ui.foreach(_.stop()) } Utils.tryLogNonFatalError { _cleaner.foreach(_.stop()) } Utils.tryLogNonFatalError { _executorAllocationManager.foreach(_.stop()) } if (_dagScheduler != null) { Utils.tryLogNonFatalError { _dagScheduler.stop() } _dagScheduler = null } if (_listenerBusStarted) { Utils.tryLogNonFatalError { listenerBus.stop() _listenerBusStarted = false } } if (env != null) { Utils.tryLogNonFatalError { env.metricsSystem.report() } } Utils.tryLogNonFatalError { _plugins.foreach(_.shutdown()) } FallbackStorage.cleanUp(_conf, _hadoopConfiguration) Utils.tryLogNonFatalError { _eventLogger.foreach(_.stop()) } if (_heartbeater != null) { Utils.tryLogNonFatalError { _heartbeater.stop() } _heartbeater = null } if (_shuffleDriverComponents != null) { Utils.tryLogNonFatalError { _shuffleDriverComponents.cleanupApplication() } } if (env != null && _heartbeatReceiver != null) { Utils.tryLogNonFatalError { env.rpcEnv.stop(_heartbeatReceiver) } } Utils.tryLogNonFatalError { _progressBar.foreach(_.stop()) } _taskScheduler = null // TODO: Cache.stop()? if (_env != null) { Utils.tryLogNonFatalError { _env.stop() } SparkEnv.set(null) } if (_statusStore != null) { _statusStore.close() } // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this // `SparkContext` is stopped. localProperties.remove() ResourceProfile.clearDefaultProfile() // Unset YARN mode system env variable, to allow switching between cluster types. SparkContext.clearActiveContext() logInfo(\"Successfully stopped SparkContext\") } /** * Get Spark's home location from either a value set through the constructor, * or the spark.home Java property, or the SPARK_HOME environment variable * (in that order of preference). If neither of these is set, return None. */ private[spark] def getSparkHome(): Option[String] = { conf.getOption(\"spark.home\").orElse(Option(System.getenv(\"SPARK_HOME\"))) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs. */ def setCallSite(shortCallSite: String): Unit = { setLocalProperty(CallSite.SHORT_FORM, shortCallSite) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs. */ private[spark] def setCallSite(callSite: CallSite): Unit = { setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm) setLocalProperty(CallSite.LONG_FORM, callSite.longForm) } /** * Clear the thread-local property for overriding the call sites * of actions and RDDs. */ def clearCallSite(): Unit = { setLocalProperty(CallSite.SHORT_FORM, null) setLocalProperty(CallSite.LONG_FORM, null) } /** * Capture the current user callsite and return a formatted version for printing. If the user * has overridden the call site using `setCallSite()`, this will return the user's version. */ private[spark] def getCallSite(): CallSite = { lazy val callSite = Utils.getCallSite() CallSite( Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm), Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm) ) } /** * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to */ def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * The function that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */ def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res) results } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */ def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions) } /** * Run a job on all partitions in an RDD and return the results in an array. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */ def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and return the results in an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */ def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to */ def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: (TaskContext, Iterator[T]) => U, resultHandler: (Int, U) => Unit): Unit = { runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to */ def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: Iterator[T] => U, resultHandler: (Int, U) => Unit): Unit = { val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter) runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler) } /** * :: DeveloperApi :: * Run a job that can return approximate results. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param evaluator `ApproximateEvaluator` to receive the partial results * @param timeout maximum time to wait for the job, in milliseconds * @return partial result (how partial depends on whether the job was finished before or * after timeout) */ @DeveloperApi def runApproximateJob[T, U, R]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, evaluator: ApproximateEvaluator[U, R], timeout: Long): PartialResult[R] = { assertNotStopped() val callSite = getCallSite logInfo(\"Starting job: \" + callSite.shortForm) val start = System.nanoTime val cleanedFunc = clean(func) val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout, localProperties.get) logInfo( \"Job finished: \" + callSite.shortForm + \", took \" + (System.nanoTime - start) / 1e9 + \" s\") result } /** * Submit a job for execution and return a FutureJob holding the result. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to * @param resultFunc function to be executed when the result is ready */ def submitJob[T, U, R]( rdd: RDD[T], processPartition: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit, resultFunc: => R): SimpleFutureAction[R] = { assertNotStopped() val cleanF = clean(processPartition) val callSite = getCallSite val waiter = dagScheduler.submitJob( rdd, (context: TaskContext, iter: Iterator[T]) => cleanF(iter), partitions, callSite, resultHandler, localProperties.get) new SimpleFutureAction(waiter, resultFunc) } /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future. */ private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } /** * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup` * for more information. */ def cancelJobGroup(groupId: String): Unit = { assertNotStopped() dagScheduler.cancelJobGroup(groupId) } /** Cancel all jobs that have been scheduled or are running. */ def cancelAllJobs(): Unit = { assertNotStopped() dagScheduler.cancelAllJobs() } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @param reason optional reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent */ def cancelJob(jobId: Int, reason: String): Unit = { dagScheduler.cancelJob(jobId, Option(reason)) } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent */ def cancelJob(jobId: Int): Unit = { dagScheduler.cancelJob(jobId, None) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @param reason reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent */ def cancelStage(stageId: Int, reason: String): Unit = { dagScheduler.cancelStage(stageId, Option(reason)) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent */ def cancelStage(stageId: Int): Unit = { dagScheduler.cancelStage(stageId, None) } /** * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI * or through SparkListener.onTaskStart. * * @param taskId the task ID to kill. This id uniquely identifies the task attempt. * @param interruptThread whether to interrupt the thread running the task. * @param reason the reason for killing the task, which should be a short string. If a task * is killed multiple times with different reasons, only one reason will be reported. * * @return Whether the task was successfully killed. */ def killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean = { dagScheduler.killTaskAttempt(taskId, interruptThread, reason) } /** * Clean a closure to make it ready to be serialized and sent to tasks * (removes unreferenced variables in $outer's, updates REPL variables) * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt> * if not. * * @param f the closure to clean * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not * serializable * @return the cleaned closure */ private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = { ClosureCleaner.clean(f, checkSerializable) f } /** * Set the directory under which RDDs are going to be checkpointed. * @param directory path to the directory where checkpoint files will be stored * (must be HDFS path if running in cluster) */ def setCheckpointDir(directory: String): Unit = { // If we are running on a cluster, log a warning if the directory is local. // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from // its own local file system, which is incorrect because the checkpoint files // are actually on the executor machines. if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) { logWarning(\"Spark is not running in local mode, therefore the checkpoint directory \" + s\"must not be on the local filesystem. Directory '$directory' \" + \"appears to be on the local filesystem.\") } checkpointDir = Option(directory).map { dir => val path = new Path(dir, UUID.randomUUID().toString) val fs = path.getFileSystem(hadoopConfiguration) fs.mkdirs(path) fs.getFileStatus(path).getPath.toString } } def getCheckpointDir: Option[String] = checkpointDir /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). */ def defaultParallelism: Int = { assertNotStopped() taskScheduler.defaultParallelism } /** * Default min number of partitions for Hadoop RDDs when not given by user * Notice that we use math.min so the \"defaultMinPartitions\" cannot be higher than 2. * The reasons for this are discussed in https://github.com/mesos/spark/pull/718 */ def defaultMinPartitions: Int = math.min(defaultParallelism, 2) private val nextShuffleId = new AtomicInteger(0) private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement() private val nextRddId = new AtomicInteger(0) /** Register a new RDD, returning its RDD ID */ private[spark] def newRddId(): Int = nextRddId.getAndIncrement() /** * Registers listeners specified in spark.extraListeners, then starts the listener bus. * This should be called after all internal listeners have been registered with the listener bus * (e.g. after the web UI and event logging listeners have been registered). */ private def setupAndStartListenerBus(): Unit = { try { conf.get(EXTRA_LISTENERS).foreach { classNames => val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf) listeners.foreach { listener => listenerBus.addToSharedQueue(listener) logInfo(s\"Registered listener ${listener.getClass().getName()}\") } } } catch { case e: Exception => try { stop() } finally { throw new SparkException(s\"Exception when registering SparkListener\", e) } } listenerBus.start(this, _env.metricsSystem) _listenerBusStarted = true } /** Post the application start event */ private def postApplicationStart(): Unit = { // Note: this code assumes that the task scheduler has been initialized and has contacted // the cluster manager to get an application ID (in case the cluster manager provides one). listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId), startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls, schedulerBackend.getDriverAttributes)) _driverLogger.foreach(_.startSync(_hadoopConfiguration)) } /** Post the application end event */ private def postApplicationEnd(): Unit = { listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis)) } /** Post the environment update event once the task scheduler is ready */ private def postEnvironmentUpdate(): Unit = { if (taskScheduler != null) { val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val addedArchivePaths = addedArchives.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, hadoopConfiguration, schedulingMode, addedJarPaths, addedFilePaths, addedArchivePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) } } /** Reports heartbeat metrics for the driver. */ private def reportHeartBeat(executorMetricsSource: Option[ExecutorMetricsSource]): Unit = { val currentMetrics = ExecutorMetrics.getCurrentMetrics(env.memoryManager) executorMetricsSource.foreach(_.updateMetricsSnapshot(currentMetrics)) val driverUpdates = new HashMap[(Int, Int), ExecutorMetrics] // In the driver, we do not track per-stage metrics, so use a dummy stage for the key driverUpdates.put(EventLoggingListener.DRIVER_STAGE_KEY, new ExecutorMetrics(currentMetrics)) val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0) listenerBus.post(SparkListenerExecutorMetricsUpdate(\"driver\", accumUpdates, driverUpdates)) } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having finished construction. // NOTE: this must be placed at the end of the SparkContext constructor. SparkContext.setActiveContext(this) } /** * The SparkContext object contains a number of implicit conversions and parameters for use with * various Spark features. */ object SparkContext extends Logging { private val VALID_LOG_LEVELS = Set(\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\") /** * Lock that guards access to global variables that track SparkContext construction. */ private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object() /** * The active, fully-constructed SparkContext. If no SparkContext is active, then this is `null`. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`. */ private val activeContext: AtomicReference[SparkContext] = new AtomicReference[SparkContext](null) /** * Points to a partially-constructed SparkContext if another thread is in the SparkContext * constructor, or `None` if no SparkContext is being constructed. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`. */ private var contextBeingConstructed: Option[SparkContext] = None /** * Called to ensure that no other SparkContext is running in this JVM. * * Throws an exception if a running context is detected and logs a warning if another thread is * constructing a SparkContext. This warning is necessary because the current locking scheme * prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception. */ private def assertNoOtherContextIsRunning(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()).filter(_ ne sc).foreach { ctx => val errMsg = \"Only one SparkContext should be running in this JVM (see SPARK-2243).\" + s\"The currently running SparkContext was created at:\\n${ctx.creationSite.longForm}\" throw new SparkException(errMsg) } contextBeingConstructed.filter(_ ne sc).foreach { otherContext => // Since otherContext might point to a partially-constructed context, guard against // its creationSite field being null: val otherContextCreationSite = Option(otherContext.creationSite).map(_.longForm).getOrElse(\"unknown location\") val warnMsg = \"Another SparkContext is being constructed (or threw an exception in its\" + \" constructor). This may indicate an error, since only one SparkContext should be\" + \" running in this JVM (see SPARK-2243).\" + s\" The other SparkContext was created at:\\n$otherContextCreationSite\" logWarning(warnMsg) } } } /** * Called to ensure that SparkContext is created or accessed only on the Driver. * * Throws an exception if a SparkContext is about to be created in executors. */ private def assertOnDriver(): Unit = { if (Utils.isInRunningSparkTask) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkContext should only be created and accessed on the driver.\") } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * @param config `SparkConfig` that will be used for initialisation of the `SparkContext` * @return current `SparkContext` (or a new one if it wasn't created before the function call) */ def getOrCreate(config: SparkConf): SparkContext = { // Synchronize to ensure that multiple create requests don't trigger an exception // from assertNoOtherContextIsRunning within setActiveContext SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext(config)) } else { if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") } } activeContext.get() } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * This method allows not passing a SparkConf (useful if just retrieving). * * @return current `SparkContext` (or a new one if wasn't created before the function call) */ def getOrCreate(): SparkContext = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext()) } activeContext.get() } } /** Return the current active [[SparkContext]] if any. */ private[spark] def getActive: Option[SparkContext] = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()) } } /** * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is * running. Throws an exception if a running context is detected and logs a warning if another * thread is constructing a SparkContext. This warning is necessary because the current locking * scheme prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception. */ private[spark] def markPartiallyConstructed(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = Some(sc) } } /** * Called at the end of the SparkContext constructor to ensure that no other SparkContext has * raced with this constructor and started. */ private[spark] def setActiveContext(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = None activeContext.set(sc) } } /** * Clears the active SparkContext metadata. This is called by `SparkContext#stop()`. It's * also called in unit tests to prevent a flood of warnings from test suites that don't / can't * properly clean up their SparkContexts. */ private[spark] def clearActiveContext(): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { activeContext.set(null) } } private[spark] val SPARK_JOB_DESCRIPTION = \"spark.job.description\" private[spark] val SPARK_JOB_GROUP_ID = \"spark.jobGroup.id\" private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = \"spark.job.interruptOnCancel\" private[spark] val SPARK_SCHEDULER_POOL = \"spark.scheduler.pool\" private[spark] val RDD_SCOPE_KEY = \"spark.rdd.scope\" private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = \"spark.rdd.scope.noOverride\" /** * Executor id for the driver. In earlier versions of Spark, this was `<driver>`, but this was * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see * SPARK-6716 for more details). */ private[spark] val DRIVER_IDENTIFIER = \"driver\" private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Iterable[T]) : ArrayWritable = { def anyToWritable[U <: Writable](u: U): Writable = u new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]], arr.map(x => anyToWritable(x)).toArray) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to SparkContext. * * @param cls class that should be inside of the jar * @return jar that contains the Class, `None` if not found */ def jarOfClass(cls: Class[_]): Option[String] = { val uri = cls.getResource(\"/\" + cls.getName.replace('.', '/') + \".class\") if (uri != null) { val uriStr = uri.toString if (uriStr.startsWith(\"jar:file:\")) { // URI will be of the form \"jar:file:/path/foo.jar!/package/cls.class\", // so pull out the /path/foo.jar Some(uriStr.substring(\"jar:file:\".length, uriStr.indexOf('!'))) } else { None } } else { None } } /** * Find the JAR that contains the class of a particular object, to make it easy for users * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in * your driver program. * * @param obj reference to an instance which class should be inside of the jar * @return jar that contains the class of the instance, `None` if not found */ def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass) /** * Creates a modified version of a SparkConf with the parameters that can be passed separately * to SparkContext, to make it easier to write SparkContext's constructors. This ignores * parameters that are passed as the default value of null, instead of throwing an exception * like SparkConf would. */ private[spark] def updatedConf( conf: SparkConf, master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()): SparkConf = { val res = conf.clone() res.setMaster(master) res.setAppName(appName) if (sparkHome != null) { res.setSparkHome(sparkHome) } if (jars != null && !jars.isEmpty) { res.setJars(jars) } res.setExecutorEnv(environment.toSeq) res } /** * The number of cores available to the driver to use for tasks such as I/O with Netty */ private[spark] def numDriverCores(master: String): Int = { numDriverCores(master, null) } /** * The number of cores available to the driver to use for tasks such as I/O with Netty */ private[spark] def numDriverCores(master: String, conf: SparkConf): Int = { def convertToInt(threads: String): Int = { if (threads == \"*\") Runtime.getRuntime.availableProcessors() else threads.toInt } master match { case \"local\" => 1 case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads) case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads) case \"yarn\" | SparkMasterRegex.KUBERNETES_REGEX(_) => if (conf != null && conf.get(SUBMIT_DEPLOY_MODE) == \"cluster\") { conf.getInt(DRIVER_CORES.key, 0) } else { 0 } case _ => 0 // Either driver is not being used, or its core count will be interpolated later } } /** * Create a task scheduler based on a given master URL. * Return a 2-tuple of the scheduler backend and the task scheduler. */ private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = { import SparkMasterRegex._ // When running locally, don't try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 // Ensure that default executor's resources satisfies one or more tasks requirement. // This function is for cluster managers that don't set the executor cores config, for // others its checked in ResourceProfile. def checkResourcesPerTask(executorCores: Int): Unit = { val taskCores = sc.conf.get(CPUS_PER_TASK) if (!sc.conf.get(SKIP_VALIDATE_CORES_TESTING)) { validateTaskCpusLargeEnough(sc.conf, executorCores, taskCores) } val defaultProf = sc.resourceProfileManager.defaultResourceProfile ResourceUtils.warnOnWastedResources(defaultProf, sc.conf, Some(executorCores)) } master match { case \"local\" => checkResourcesPerTask(1) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_REGEX(threads) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads. val threadCount = if (threads == \"*\") localCpuCount else threads.toInt if (threadCount <= 0) { throw new SparkException(s\"Asked to run locally with $threadCount threads\") } checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_FAILURES_REGEX(threads, maxFailures) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*, M] means the number of cores on the computer with M failures // local[N, M] means exactly N threads with M failures val threadCount = if (threads == \"*\") localCpuCount else threads.toInt checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case SPARK_REGEX(sparkUrl) => val scheduler = new TaskSchedulerImpl(sc) val masterUrls = sparkUrl.split(\",\").map(\"spark://\" + _) val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) (backend, scheduler) case LOCAL_CLUSTER_REGEX(numWorkers, coresPerWorker, memoryPerWorker) => checkResourcesPerTask(coresPerWorker.toInt) // Check to make sure memory requested <= memoryPerWorker. Otherwise Spark will just hang. val memoryPerWorkerInt = memoryPerWorker.toInt if (sc.executorMemory > memoryPerWorkerInt) { throw new SparkException( \"Asked to launch cluster with %d MiB/worker but requested %d MiB/executor\".format( memoryPerWorkerInt, sc.executorMemory)) } // For host local mode setting the default of SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED // to false because this mode is intended to be used for testing and in this case all the // executors are running on the same host. So if host local reading was enabled here then // testing of the remote fetching would be secondary as setting this config explicitly to // false would be required in most of the unit test (despite the fact that remote fetching // is much more frequent in production). sc.conf.setIfMissing(SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED, false) val scheduler = new TaskSchedulerImpl(sc) val localCluster = LocalSparkCluster( numWorkers.toInt, coresPerWorker.toInt, memoryPerWorkerInt, sc.conf) val masterUrls = localCluster.start() val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => { localCluster.stop() } (backend, scheduler) case masterUrl => val cm = getClusterManager(masterUrl) match { case Some(clusterMgr) => clusterMgr case None => throw new SparkException(\"Could not parse Master URL: '\" + master + \"'\") } try { val scheduler = cm.createTaskScheduler(sc, masterUrl) val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) cm.initialize(scheduler, backend) (backend, scheduler) } catch { case se: SparkException => throw se case NonFatal(e) => throw new SparkException(\"External scheduler cannot be instantiated\", e) } } } private def getClusterManager(url: String): Option[ExternalClusterManager] = { val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size > 1) { throw new SparkException( s\"Multiple external cluster managers registered for the url $url: $serviceLoaders\") } serviceLoaders.headOption } /** * This is a helper function to complete the missing S3A magic committer configurations * based on a single conf: `spark.hadoop.fs.s3a.bucket.<bucket>.committer.magic.enabled` */ private def fillMissingMagicCommitterConfsIfNeeded(conf: SparkConf): Unit = { val magicCommitterConfs = conf .getAllWithPrefix(\"spark.hadoop.fs.s3a.bucket.\") .filter(_._1.endsWith(\".committer.magic.enabled\")) .filter(_._2.equalsIgnoreCase(\"true\")) if (magicCommitterConfs.nonEmpty) { // Try to enable S3 magic committer if missing conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\") if (conf.get(\"spark.hadoop.fs.s3a.committer.magic.enabled\").equals(\"true\")) { conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") conf.setIfMissing(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\") conf.setIfMissing(\"spark.sql.parquet.output.committer.class\", \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\") conf.setIfMissing(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\") } } } /** * SPARK-36796: This is a helper function to supplement `--add-opens` options to * `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`. */ private def supplementJavaModuleOptions(conf: SparkConf): Unit = { def supplement(key: OptionalConfigEntry[String]): Unit = { val v = conf.get(key) match { case Some(opts) => s\"${JavaModuleOptions.defaultModuleOptions()} $opts\" case None => JavaModuleOptions.defaultModuleOptions() } conf.set(key.key, v) } supplement(DRIVER_JAVA_OPTIONS) supplement(EXECUTOR_JAVA_OPTIONS) } } /** * A collection of regexes for extracting information from the master string. */ private object SparkMasterRegex { // Regular expression used for local[N] and local[*] master formats val LOCAL_N_REGEX = \"\"\"local\\[([0-9]+|\\*)\\]\"\"\".r // Regular expression for local[N, maxRetries], used in tests with failing tasks val LOCAL_N_FAILURES_REGEX = \"\"\"local\\[([0-9]+|\\*)\\s*,\\s*([0-9]+)\\]\"\"\".r // Regular expression for simulating a Spark cluster of [N, cores, memory] locally val LOCAL_CLUSTER_REGEX = \"\"\"local-cluster\\[\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*]\"\"\".r // Regular expression for connecting to Spark deploy clusters val SPARK_REGEX = \"\"\"spark://(.*)\"\"\".r // Regular expression for connecting to kubernetes clusters val KUBERNETES_REGEX = \"\"\"k8s://(.*)\"\"\".r } /** * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`). */ private[spark] class WritableConverter[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: Writable => T) extends Serializable object WritableConverter { // Helper objects for converting common types to Writable private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T) : WritableConverter[T] = { val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]] new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W])) } // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. // The following implicit declarations have been added on top of the very similar ones // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta // expansion of zero-arg methods and thus won't match a no-arg method where it expects // an implicit that is a function of no args. implicit val intWritableConverterFn: () => WritableConverter[Int] = () => simpleWritableConverter[Int, IntWritable](_.get) implicit val longWritableConverterFn: () => WritableConverter[Long] = () => simpleWritableConverter[Long, LongWritable](_.get) implicit val doubleWritableConverterFn: () => WritableConverter[Double] = () => simpleWritableConverter[Double, DoubleWritable](_.get) implicit val floatWritableConverterFn: () => WritableConverter[Float] = () => simpleWritableConverter[Float, FloatWritable](_.get) implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] = () => simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = { () => simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit val stringWritableConverterFn: () => WritableConverter[String] = () => simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] = () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) // These implicits remain included for backwards-compatibility. They fulfill the // same role as those above. implicit def intWritableConverter(): WritableConverter[Int] = simpleWritableConverter[Int, IntWritable](_.get) implicit def longWritableConverter(): WritableConverter[Long] = simpleWritableConverter[Long, LongWritable](_.get) implicit def doubleWritableConverter(): WritableConverter[Double] = simpleWritableConverter[Double, DoubleWritable](_.get) implicit def floatWritableConverter(): WritableConverter[Float] = simpleWritableConverter[Float, FloatWritable](_.get) implicit def booleanWritableConverter(): WritableConverter[Boolean] = simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = { simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit def stringWritableConverter(): WritableConverter[String] = simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] = new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) } /** * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The `Writable` class will be used in `SequenceFileRDDFunctions`. */ private[spark] class WritableFactory[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: T => Writable) extends Serializable object WritableFactory { private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W) : WritableFactory[T] = { val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]] new WritableFactory[T](_ => writableClass, convert) } implicit def intWritableFactory: WritableFactory[Int] = simpleWritableFactory(new IntWritable(_)) implicit def longWritableFactory: WritableFactory[Long] = simpleWritableFactory(new LongWritable(_)) implicit def floatWritableFactory: WritableFactory[Float] = simpleWritableFactory(new FloatWritable(_)) implicit def doubleWritableFactory: WritableFactory[Double] = simpleWritableFactory(new DoubleWritable(_)) implicit def booleanWritableFactory: WritableFactory[Boolean] = simpleWritableFactory(new BooleanWritable(_)) implicit def bytesWritableFactory: WritableFactory[Array[Byte]] = simpleWritableFactory(new BytesWritable(_)) implicit def stringWritableFactory: WritableFactory[String] = simpleWritableFactory(new Text(_)) implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] = simpleWritableFactory(w => w) }"
      ],
      "key_entities": [
        "org/apache/spark/rdd/RDD#",
        "org/apache/spark/sql/Dataset#",
        "org/apache/spark/SparkContext#"
      ],
      "claude_stats": {
        "answer": "Based on the information retrieved, here are the **3 most important classes** in the project:\n\n## 1. **RDD** (Resilient Distributed Dataset)\n**Location:** `org/apache/spark/rdd/RDD#`\n\nRDD is the fundamental data structure in Spark. It represents an immutable, distributed collection of objects that can be processed in parallel. Key features include:\n\n- **Fault tolerance** through lineage tracking - if a partition is lost, it can be recomputed from the original data\n- **Lazy evaluation** - transformations are not computed until an action is called\n- **In-memory computing** - data can be cached in memory for faster iterative processing\n- **Partitioning** - data is automatically distributed across the cluster\n\nThe class provides numerous transformation operations (`map`, `filter`, `flatMap`, `groupBy`, etc.) and action operations (`collect`, `count`, `reduce`, `take`, etc.). It's the core abstraction that makes Spark's distributed computing model work.\n\n## 2. **Dataset**\n**Location:** `org/apache/spark/sql/Dataset#`\n\nDataset is Spark's primary structured data API, providing a strongly-typed, object-oriented interface for working with structured and semi-structured data. Key features include:\n\n- **Type safety** - compile-time type checking with encoders\n- **Optimized execution** - uses Catalyst query optimizer and Tungsten execution engine\n- **SQL integration** - can be queried using SQL or DataFrame API\n- **Unified batch and streaming** - same API works for both processing modes\n- **Rich transformations** - operations like `select`, `filter`, `join`, `groupBy`, `agg`\n\nDataset builds on top of RDD but provides higher-level abstractions and automatic optimization. When working with Row objects, it's called a DataFrame (which is an alias for `Dataset[Row]`).\n\n## 3. **SparkContext**\n**Location:** `org/apache/spark/SparkContext#`\n\nSparkContext is the main entry point for Spark functionality and represents the connection to a Spark cluster. Key responsibilities include:\n\n- **Cluster connection** - establishes connection to the cluster manager (YARN, Mesos, Kubernetes, or standalone)\n- **RDD creation** - provides methods to create RDDs from data sources (`parallelize`, `textFile`, `hadoopFile`)\n- **Resource management** - manages executors, memory, and scheduling\n- **Job execution** - coordinates job submission and task distribution\n- **Configuration** - holds Spark configuration and environment settings\n- **Broadcasting and accumulators** - provides mechanisms for sharing data and collecting metrics\n\nOnly one SparkContext can be active per JVM, and it must be stopped properly to release cluster resources. It's the foundation that enables all distributed computation in Spark.\n\n---\n\nThese three classes form the core of Apache Spark's architecture: **SparkContext** manages the execution environment, **RDD** provides the low-level distributed data abstraction, and **Dataset** offers the high-level structured data API with query optimization.",
        "prompt_time": 1.335,
        "answer_time": 28,
        "tokens": 696,
        "context_tokens": 88358,
        "used_context": [
          "## CLASS: org/apache/spark/rdd/RDD#\n abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent  def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition.  @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`  protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it.  protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences.  protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned.  @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD.  def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext).  val id: Int = sc.newRddId() /** A friendly name for this RDD  @transient var name: String = _ /** Assign a name to this RDD  def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one  private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception.  def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD.  def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set.  def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver).  private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not.  final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing.  final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not.  final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD.  @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed.  final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD.  final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned.  private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached.  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD.  def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results.  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate.  def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle.  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable.  def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition.  val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]].  def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array  def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement.  private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function.  def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally.  def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD  def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD  def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array.  def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`.  def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD  def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning.  private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other).  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition.  def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD.  def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD.  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first.  def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`.  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us.  def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator.  def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]]  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions  def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2)  def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor  def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD.  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map.  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped.  def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017.  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`.  def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD.  def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD *  def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD *  def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition.  def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements.  def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements.  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects.  def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`.  def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition  private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation.  def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used.  def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally.  def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing.  private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing.  private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized.  private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally.  def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned  @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents.  def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a>  @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD.  @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified  @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`).  @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations.  @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD  protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T]  protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on.  def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs.  private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions.  private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example.  protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging.  def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier.  private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level.  // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`.  object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE.  private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
          "## CLASS: org/apache/spark/sql/Dataset#\n@Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { @transient lazy val sparkSession: SparkSession = { if (queryExecution == null || queryExecution.sparkSession == null) { throw QueryExecutionErrors.transformationsAndActionsNotInvokedByDriverError() } queryExecution.sparkSession } // A globally unique id of this Dataset. private val id = Dataset.curId.getAndIncrement() queryExecution.assertAnalyzed() // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) } def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sqlContext.sparkSession, logicalPlan, encoder) } @transient private[sql] val logicalPlan: LogicalPlan = { val plan = queryExecution.commandExecuted if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) { val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long]) dsIds.add(id) plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds) } plan } /** * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use * it when constructing new Dataset objects that have the same object type (that will be * possibly resolved to a different schema).  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder) // The resolved `ExpressionEncoder` which can be used to turn rows to objects of type T, after // collecting rows to the driver side. private lazy val resolvedEnc = { exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer) } private implicit def classTag = exprEnc.clsTag // sqlContext must be val because a stable identifier is expected when you import implicits @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext private[sql] def resolve(colName: String): NamedExpression = { val resolver = sparkSession.sessionState.analyzer.resolver queryExecution.analyzed.resolveQuoted(colName, resolver) .getOrElse(throw resolveException(colName, schema.fieldNames)) } private def resolveException(colName: String, fields: Array[String]): AnalysisException = { val extraMsg = if (fields.exists(sparkSession.sessionState.analyzer.resolver(_, colName))) { s\"; did you mean to quote the `$colName` column?\" } else \"\" val fieldsStr = fields.mkString(\", \") QueryCompilationErrors.cannotResolveColumnNameAmongFieldsError(colName, fieldsStr, extraMsg) } private[sql] def numericColumns: Seq[Expression] = { schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n => queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get } } /** * Get rows represented in Sequence by specific truncate and vertical requirement. * * @param numRows Number of rows to return * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right.  private[sql] def getRows( numRows: Int, truncate: Int): Seq[Seq[String]] = { val newDf = toDF() val castCols = newDf.logicalPlan.output.map { col => // Since binary types in top-level schema fields have a specific format to print, // so we do not cast them to strings here. if (col.dataType == BinaryType) { Column(col) } else { Column(col).cast(StringType) } } val data = newDf.select(castCols: _*).take(numRows + 1) // For array values, replace Seq and Array with square brackets // For cells that are beyond `truncate` characters, replace it with the // first `truncate-3` and \"...\" schema.fieldNames.map(SchemaUtils.escapeMetaCharacters).toSeq +: data.map { row => row.toSeq.map { cell => val str = cell match { case null => \"null\" case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\") case _ => // Escapes meta-characters not to break the `showString` format SchemaUtils.escapeMetaCharacters(cell.toString) } if (truncate > 0 && str.length > truncate) { // do not show ellipses for strings shorter than 4 characters. if (truncate < 4) str.substring(0, truncate) else str.substring(0, truncate - 3) + \"...\" } else { str } }: Seq[String] } } /** * Compose the string representing rows for output * * @param _numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value).  private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = { val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate) val hasMoreData = tmpRows.length - 1 > numRows val rows = tmpRows.take(numRows + 1) val sb = new StringBuilder val numCols = schema.fieldNames.length // We set a minimum column width at '3' val minimumColWidth = 3 if (!vertical) { // Initialise the width of each column to a minimum value val colWidths = Array.fill(numCols)(minimumColWidth) // Compute the width of each column for (row <- rows) { for ((cell, i) <- row.zipWithIndex) { colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell)) } } val paddedRows = rows.map { row => row.zipWithIndex.map { case (cell, i) => if (truncate > 0) { StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } else { StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } } } // Create SeparateLine val sep: String = colWidths.map(\"-\" * _).addString(sb, \"+\", \"+\", \"+\\n\").toString() // column names paddedRows.head.addString(sb, \"|\", \"|\", \"|\\n\") sb.append(sep) // data paddedRows.tail.foreach(_.addString(sb, \"|\", \"|\", \"|\\n\")) sb.append(sep) } else { // Extended display mode enabled val fieldNames = rows.head val dataRows = rows.tail // Compute the width of field name and data columns val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) => math.max(curMax, Utils.stringHalfWidth(fieldName)) } val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) => math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max) } dataRows.zipWithIndex.foreach { case (row, i) => // \"+ 5\" in size means a character length except for padded names and data val rowHeader = StringUtils.rightPad( s\"-RECORD $i\", fieldNameColWidth + dataColWidth + 5, \"-\") sb.append(rowHeader).append(\"\\n\") row.zipWithIndex.map { case (cell, j) => val fieldName = StringUtils.rightPad(fieldNames(j), fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length) val data = StringUtils.rightPad(cell, dataColWidth - Utils.stringHalfWidth(cell) + cell.length) s\" $fieldName | $data \" }.addString(sb, \"\", \"\\n\", \"\\n\") } } // Print a footer if (vertical && rows.tail.isEmpty) { // In a vertical mode, print an empty row set explicitly sb.append(\"(0 rows)\\n\") } else if (hasMoreData) { // For Data that has more than \"numRows\" records val rowsString = if (numRows == 1) \"row\" else \"rows\" sb.append(s\"only showing top $numRows $rowsString\\n\") } sb.toString() } override def toString: String = { try { val builder = new StringBuilder val fields = schema.take(2).map { case f => s\"${f.name}: ${f.dataType.simpleString(2)}\" } builder.append(\"[\") builder.append(fields.mkString(\", \")) if (schema.length > 2) { if (schema.length - fields.size == 1) { builder.append(\" ... 1 more field\") } else { builder.append(\" ... \" + (schema.length - 2) + \" more fields\") } } builder.append(\"]\").toString() } catch { case NonFatal(e) => s\"Invalid tree; ${e.getMessage}:\\n$queryExecution\" } } /** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * * @group basic * @since 1.6.0  // This is declared with parentheses to prevent the Scala compiler from treating // `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame. def toDF(): DataFrame = new Dataset[Row](queryExecution, RowEncoder(schema)) /** * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * <ul> * <li>When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`).</li> * <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will * be assigned to `_1`).</li> * <li>When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used.</li> * </ul> * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * Note that `as[]` only changes the view of the data that is passed into typed operations, * such as `map()`, and does not eagerly project away any columns that are not present in * the specified class. * * @group basic * @since 1.6.0  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * {{{ * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` * rdd.toDF(\"id\", \"name\") // this creates a DataFrame with column name \"id\" and \"name\" * }}} * * @group basic * @since 2.0.0  @scala.annotation.varargs def toDF(colNames: String*): DataFrame = { require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names (${colNames.size}): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) => Column(oldAttribute).as(newName) } select(newCols : _*) } /** * Returns the schema of this Dataset. * * @group basic * @since 1.6.0  def schema: StructType = sparkSession.withActive { queryExecution.analyzed.schema } /** * Prints the schema to the console in a nice tree format. * * @group basic * @since 1.6.0  def printSchema(): Unit = printSchema(Int.MaxValue) // scalastyle:off println /** * Prints the schema up to the given level to the console in a nice tree format. * * @group basic * @since 3.0.0  def printSchema(level: Int): Unit = println(schema.treeString(level)) // scalastyle:on println /** * Prints the plans (logical and physical) with a format specified by a given explain mode. * * @param mode specifies the expected output format of plans. * <ul> * <li>`simple` Print only a physical plan.</li> * <li>`extended`: Print both logical and physical plans.</li> * <li>`codegen`: Print a physical plan and generated codes if they are * available.</li> * <li>`cost`: Print a logical plan and statistics if they are available.</li> * <li>`formatted`: Split explain output into two sections: a physical plan outline * and node details.</li> * </ul> * @group basic * @since 3.0.0  def explain(mode: String): Unit = sparkSession.withActive { // Because temporary views are resolved during analysis when we create a Dataset, and // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using // `ExplainCommand` here will probably output different query plans, compared to the results // of evaluation of the Dataset. So just output QueryExecution's query plans here. // scalastyle:off println println(queryExecution.explainString(ExplainMode.fromString(mode))) // scalastyle:on println } /** * Prints the plans (logical and physical) to the console for debugging purposes. * * @param extended default `false`. If `false`, prints only the physical plan. * * @group basic * @since 1.6.0  def explain(extended: Boolean): Unit = if (extended) { explain(ExtendedMode.name) } else { explain(SimpleMode.name) } /** * Prints the physical plan to the console for debugging purposes. * * @group basic * @since 1.6.0  def explain(): Unit = explain(SimpleMode.name) /** * Returns all column names and their data types as an array. * * @group basic * @since 1.6.0  def dtypes: Array[(String, String)] = schema.fields.map { field => (field.name, field.dataType.toString) } /** * Returns all column names as an array. * * @group basic * @since 1.6.0  def columns: Array[String] = schema.fields.map(_.name) /** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * * @group basic * @since 1.6.0  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] || logicalPlan.isInstanceOf[CommandResult] /** * Returns true if the `Dataset` is empty. * * @group basic * @since 2.4.0  def isEmpty: Boolean = withAction(\"isEmpty\", select().queryExecution) { plan => plan.executeTake(1).isEmpty } /** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * @group streaming * @since 2.0.0  def isStreaming: Boolean = logicalPlan.isStreaming /** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true) /** * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be * used to truncate the logical plan of this Dataset, which is especially useful in iterative * algorithms where the plan may grow exponentially. Local checkpoints are written to executor * storage and despite potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false) /** * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. Local checkpoints are written to executor storage and despite * potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint( eager = eager, reliableCheckpoint = false ) /** * Returns a checkpointed version of this Dataset. * * @param eager Whether to checkpoint this dataframe immediately * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the * checkpoint directory. If false creates a local checkpoint using * the caching subsystem  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) \"checkpoint\" else \"localCheckpoint\" withAction(actionName, queryExecution) { physicalPlan => val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { internalRdd.checkpoint() } else { internalRdd.localCheckpoint() } if (eager) { internalRdd.doCheckpoint() } // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. @scala.annotation.tailrec def firstLeafPartitioning(partitioning: Partitioning): Partitioning = { partitioning match { case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head) case p => p } } val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering, isStreaming )(sparkSession)).as[T] } } /** * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * Spark will use this watermark for several purposes: * <ul> * <li>To know when a given time window aggregation can be finalized and thus can be emitted * when using output modes that do not allow updates.</li> * <li>To minimize the amount of state that we need to keep for on-going aggregations, * `mapGroupsWithState` and `dropDuplicates` operators.</li> * </ul> * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * @param eventTime the name of the column that contains the event time of the row. * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). NOTE: This should not be negative. * * @group streaming * @since 2.1.0  // We only accept an existing column name, not a derived column here as a watermark that is // defined on a derived column cannot referenced elsewhere in the plan. def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan { val parsedDelay = IntervalUtils.fromIntervalString(delayThreshold) require(!IntervalUtils.isNegative(parsedDelay), s\"delay threshold ($delayThreshold) should not be negative.\") EliminateEventTimeWatermark( EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)) } /** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * * @group action * @since 1.6.0  def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * * @group action * @since 1.6.0  def show(): Unit = show(20) /** * Displays the top 20 rows of Dataset in a tabular form. * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0  def show(truncate: Boolean): Unit = show(20, truncate) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * @param numRows Number of rows to show * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0  // scalastyle:off println def show(numRows: Int, truncate: Boolean): Unit = if (truncate) { println(showString(numRows, truncate = 20)) } else { println(showString(numRows, truncate = 0)) } /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @group action * @since 1.6.0  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * If `vertical` enabled, this command prints output rows vertically (one line per column value)? * * {{{ * -RECORD 0------------------- * year | 1980 * month | 12 * AVG('Adj Close) | 0.503218 * AVG('Adj Close) | 0.595103 * -RECORD 1------------------- * year | 1981 * month | 01 * AVG('Adj Close) | 0.523289 * AVG('Adj Close) | 0.570307 * -RECORD 2------------------- * year | 1982 * month | 02 * AVG('Adj Close) | 0.436504 * AVG('Adj Close) | 0.475256 * -RECORD 3------------------- * year | 1983 * month | 03 * AVG('Adj Close) | 0.410516 * AVG('Adj Close) | 0.442194 * -RECORD 4------------------- * year | 1984 * month | 04 * AVG('Adj Close) | 0.450090 * AVG('Adj Close) | 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). * @group action * @since 2.3.0  // scalastyle:off println def show(numRows: Int, truncate: Int, vertical: Boolean): Unit = println(showString(numRows, truncate, vertical)) // scalastyle:on println /** * Returns a [[DataFrameNaFunctions]] for working with missing data. * {{{ * // Dropping rows containing any null values. * ds.na.drop() * }}} * * @group untypedrel * @since 1.6.0  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) /** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * {{{ * // Finding frequent items in column with name 'a'. * ds.stat.freqItems(Seq(\"a\")) * }}} * * @group untypedrel * @since 1.6.0  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) /** * Join with another `DataFrame`. * * Behaves as an INNER JOIN and requires a subsequent join predicate. * * @param right Right side of the join operation. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE) } /** * Inner equi-join with another `DataFrame` using the given column. * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the column \"user_id\" * df1.join(df2, \"user_id\") * }}} * * @param right Right side of the join operation. * @param usingColumn Name of the column to join on. This column must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumn: String): DataFrame = { join(right, Seq(usingColumn)) } /** * Inner equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * }}} * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = { join(right, usingColumns, \"inner\") } /** * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate * is specified as an inner join. If you would explicitly like to perform a cross join use the * `crossJoin` method. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = { // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None, JoinHint.NONE)) .analyzed.asInstanceOf[Join] withPlan { Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None, JoinHint.NONE) } } /** * Inner join with another `DataFrame`, using the given join expression. * * {{{ * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * }}} * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * find the trivially true predicates and automatically resolves them to both sides.  private def resolveSelfJoinCondition(plan: Join): Join = { val resolver = sparkSession.sessionState.analyzer.resolver val cond = plan.condition.map { _.transform { case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualTo( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualNullSafe( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) }} plan.copy(condition = cond) } /** * find the trivially true predicates and automatically resolves them to both sides.  private def resolveSelfJoinCondition( right: Dataset[_], joinExprs: Option[Column], joinType: String): Join = { // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), joinExprs.map(_.expr), JoinHint.NONE)) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) { return plan } // If left/right have no output set intersection, return the plan. val lanalyzed = this.queryExecution.analyzed val ranalyzed = right.queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) { return plan } // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. resolveSelfJoinCondition(plan) } /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * {{{ * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * }}} * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = { withPlan { resolveSelfJoinCondition(right, Some(joinExprs), joinType) } } /** * Explicit cartesian join with another `DataFrame`. * * @param right Right side of the join operation. * * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * * @group untypedrel * @since 2.1.0  def crossJoin(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Cross, None, JoinHint.NONE) } /** * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * * @param other Right side of the join. * @param condition Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`,`full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`. * * @group typedrel * @since 1.6.0  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = { // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // etc. var joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr), JoinHint.NONE)).analyzed.asInstanceOf[Join] if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) { throw QueryCompilationErrors.invalidJoinTypeInJoinWithError(joined.joinType) } // If auto self join alias is enable if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) { joined = resolveSelfJoinCondition(joined) } implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) val leftResultExpr = { if (!this.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() } else { Alias(CreateStruct(joined.left.output), \"_1\")() } } val rightResultExpr = { if (!other.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() } else { Alias(CreateStruct(joined.right.output), \"_2\")() } } if (joined.joinType.isInstanceOf[InnerLike]) { // For inner joins, we can directly perform the join and then can project the join // results into structs. This ensures that data remains flat during shuffles / // exchanges (unlike the outer join path, which nests the data before shuffling). withTypedPlan(Project(Seq(leftResultExpr, rightResultExpr), joined)) } else { // outer joins // For both join sides, combine all outputs into a single column and alias it with \"_1 // or \"_2\", to match the schema for the encoder of the join result. // Note that we do this before joining them, to enable the join operator to return null // for one side, in cases like outer-join. val left = Project(leftResultExpr :: Nil, joined.left) val right = Project(rightResultExpr :: Nil, joined.right) // Rewrites the join condition to make the attribute point to correct column/field, // after we combine the outputs of each join side. val conditionExpr = joined.condition.get transformUp { case a: Attribute if joined.left.outputSet.contains(a) => if (!this.exprEnc.isSerializedAsStructForTopLevel) { left.output.head } else { val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) } case a: Attribute if joined.right.outputSet.contains(a) => if (!other.exprEnc.isSerializedAsStructForTopLevel) { right.output.head } else { val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) } } withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr), JoinHint.NONE)) } } /** * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * @param other Right side of the join. * @param condition Join expression. * * @group typedrel * @since 1.6.0  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = { joinWith(other, condition, \"inner\") } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, usingColumns: Seq[String], joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joinExprs = usingColumns.map { column => EqualTo(resolve(column), other.resolve(column)) }.reduceOption(And).map(Column.apply).orNull joinAsOf(other, leftAsOf, rightAsOf, joinExprs, joinType, tolerance, allowExactMatches, direction) } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, joinExprs: Column, joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joined = resolveSelfJoinCondition(other, Option(joinExprs), joinType) val leftAsOfExpr = leftAsOf.expr.transformUp { case a: AttributeReference if logicalPlan.outputSet.contains(a) => val index = logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.left.output(index) } val rightAsOfExpr = rightAsOf.expr.transformUp { case a: AttributeReference if other.logicalPlan.outputSet.contains(a) => val index = other.logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.right.output(index) } withPlan { AsOfJoin( joined.left, joined.right, leftAsOfExpr, rightAsOfExpr, joined.condition, joined.joinType, Option(tolerance).map(_.expr), allowExactMatches, AsOfJoinDirection(direction) ) } } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = { sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sortWithinPartitions(sortExprs: Column*): Dataset[T] = { sortInternal(global = false, sortExprs) } /** * Returns a new Dataset sorted by the specified column, all in ascending order. * {{{ * // The following 3 are equivalent * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * }}} * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = { sort((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset sorted by the given expressions. For example: * {{{ * ds.sort($\"col1\", $\"col2\".desc) * }}} * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = { sortInternal(global = true, sortExprs) } /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*) /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0  def apply(colName: String): Column = col(colName) /** * Specifies some hint on the current Dataset. As an example, the following code specifies * that one of the plan can be broadcasted: * * {{{ * df1.join(df2.hint(\"broadcast\")) * }}} * * @group basic * @since 2.2.0  @scala.annotation.varargs def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan { UnresolvedHint(name, parameters, logicalPlan) } /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0  def col(colName: String): Column = colName match { case \"*\" => Column(ResolvedStar(queryExecution.analyzed.output)) case _ => if (sqlContext.conf.supportQuotedRegexColumnName) { colRegex(colName) } else { Column(addDataFrameIdToCol(resolve(colName))) } } // Attach the dataset id and column position to the column reference, so that we can detect // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`. // This must be called before we return a `Column` that contains `AttributeReference`. // Note that, the metadata added here are only available in the analyzer, as the analyzer rule // `DetectAmbiguousSelfJoin` will remove it. private def addDataFrameIdToCol(expr: NamedExpression): NamedExpression = { val newExpr = expr transform { case a: AttributeReference if sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) => val metadata = new MetadataBuilder() .withMetadata(a.metadata) .putLong(Dataset.DATASET_ID_KEY, id) .putLong(Dataset.COL_POS_KEY, logicalPlan.output.indexWhere(a.semanticEquals)) .build() a.withMetadata(metadata) } newExpr.asInstanceOf[NamedExpression] } /** * Selects column based on the column name specified as a regex and returns it as [[Column]]. * @group untypedrel * @since 2.3.0  def colRegex(colName: String): Column = { val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis colName match { case ParserUtils.escapedIdentifier(columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, None, caseSensitive)) case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive)) case _ => Column(addDataFrameIdToCol(resolve(colName))) } } /** * Returns a new Dataset with an alias set. * * @group typedrel * @since 1.6.0  def as(alias: String): Dataset[T] = withTypedPlan { SubqueryAlias(alias, logicalPlan) } /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0  def as(alias: Symbol): Dataset[T] = as(alias.name) /** * Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0  def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0  def alias(alias: Symbol): Dataset[T] = as(alias) /** * Selects a set of column based expressions. * {{{ * ds.select($\"colA\", $\"colB\" + 1) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def select(cols: Column*): DataFrame = withPlan { val untypedCols = cols.map { case typedCol: TypedColumn[_, _] => // Checks if a `TypedColumn` has been inserted with // specific input type and schema by `withInputType`. val needInputType = typedCol.expr.exists { case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty => true case _ => false } if (!needInputType) { typedCol } else { throw QueryCompilationErrors.cannotPassTypedColumnInUntypedSelectError(typedCol.toString) } case other => other } Project(untypedCols.map(_.named), logicalPlan) } /** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * {{{ * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*) /** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * {{{ * // The following are equivalent: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def selectExpr(exprs: String*): DataFrame = { select(exprs.map { expr => Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) }: _*) } /** * Returns a new Dataset by computing the given [[Column]] expression for each element. * * {{{ * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * }}} * * @group typedrel * @since 1.6.0  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = { implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (!encoder.isSerializedAsStructForTopLevel) { new Dataset[U1](sparkSession, project, encoder) } else { // Flattens inner fields of U1 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) } } /** * Internal helper function for building typed selects that return tuples. For simplicity and * code reuse, we do this without the help of the type system and then use helper functions * that cast appropriately for the user facing interface.  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = { val encoders = columns.map(_.encoder) val namedColumns = columns.map(_.withInputType(exprEnc, logicalPlan.output).named) val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan)) new Dataset(execution, ExpressionEncoder.tuple(encoders)) } /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] /** * Filters rows using the given condition. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0  def filter(condition: Column): Dataset[T] = withTypedPlan { Filter(condition.expr, logicalPlan) } /** * Filters rows using the given SQL expression. * {{{ * peopleDs.filter(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0  def filter(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Filters rows using the given condition. This is an alias for `filter`. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0  def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * {{{ * peopleDs.where(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0  def where(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def groupBy(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType) } /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) } /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def groupBy(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType) } /** * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0  def reduce(func: (T, T) => T): T = withNewRDDExecutionId { rdd.reduce(func) } /** * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) /** * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = { val withGroupingKey = AppendColumns(func, logicalPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, logicalPlan.output, withGroupingKey.newColumns) } /** * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * }}} * * @group untypedrel * @since 2.0.0  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = { groupBy().agg(aggExpr, aggExprs : _*) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*) /** * Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset * that returns the same result as the input, with the following guarantees: * <ul> * <li>It will compute the defined aggregates (metrics) on all the data that is flowing through * the Dataset at that point.</li> * <li>It will report the value of the defined aggregate columns as soon as we reach a completion * point. A completion point is either the end of a query (batch mode) or the end of a streaming * epoch. The value of the aggregates only reflects the data processed since the previous * completion point.</li> * </ul> * Please note that continuous execution is currently not supported. * * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that * contain references to the input Dataset's columns must always be wrapped in an aggregate * function. * * A user can observe these metrics by either adding * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session. * * {{{ * // Monitor the metrics using a listener. * spark.streams.addListener(new StreamingQueryListener() { * override def onQueryStarted(event: QueryStartedEvent): Unit = {} * override def onQueryProgress(event: QueryProgressEvent): Unit = { * event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row => * // Trigger if the number of errors exceeds 5 percent * val num_rows = row.getAs[Long](\"rc\") * val num_error_rows = row.getAs[Long](\"erc\") * val ratio = num_error_rows.toDouble / num_rows * if (ratio > 0.05) { * // Trigger alert * } * } * } * override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {} * }) * // Observe row count (rc) and error row count (erc) in the streaming Dataset * val observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\")) * observed_ds.writeStream.format(\"...\").start() * }}} * * @group typedrel * @since 3.0.0  @varargs def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan { CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan) } /** * Observe (named) metrics through an `org.apache.spark.sql.Observation` instance. * This is equivalent to calling `observe(String, Column, Column*)` but does not require * adding `org.apache.spark.sql.util.QueryExecutionListener` to the spark session. * This method does not support streaming datasets. * * A user can retrieve the metrics by accessing `org.apache.spark.sql.Observation.get`. * * {{{ * // Observe row count (rows) and highest id (maxid) in the Dataset while writing it * val observation = Observation(\"my_metrics\") * val observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\")) * observed_ds.write.parquet(\"ds.parquet\") * val metrics = observation.get * }}} * * @throws IllegalArgumentException If this is a streaming Dataset (this.isStreaming == true) * * @group typedrel * @since 3.3.0  @varargs def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = { observation.on(this, expr, exprs: _*) } /** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * @group typedrel * @since 2.0.0  def limit(n: Int): Dataset[T] = withTypedPlan { Limit(Literal(n), logicalPlan) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.union(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 4| 5| 6| * // +----+----+----+ * }}} * * Notice that the column positions in the schema aren't necessarily matched with the * fields in the strongly typed objects in a Dataset. This function resolves columns * by their positions in the schema, not the fields in the strongly typed objects. Use * [[unionByName]] to resolve columns by field name in the typed objects. * * @group typedrel * @since 2.0.0  def union(other: Dataset[T]): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan, other.logicalPlan)) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is an alias for `union`. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.0.0  def unionAll(other: Dataset[T]): Dataset[T] = union(other) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set * union (that does deduplication of elements), use this function followed by a [[distinct]]. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.unionByName(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 6| 4| 5| * // +----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. Nested columns in map types * are not currently supported. * * @group typedrel * @since 2.3.0  def unionByName(other: Dataset[T]): Dataset[T] = unionByName(other, false) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position). * * When the parameter `allowMissingColumns` is `true`, the set of column names * in this and other `Dataset` can differ; missing columns will be filled with null. * Further, the missing columns of this `Dataset` will be added at the end * in the schema of the union result: * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col0\", \"col3\") * df1.unionByName(df2, true).show * * // output: \"col3\" is missing at left df1 and added at the end of schema. * // +----+----+----+----+ * // |col0|col1|col2|col3| * // +----+----+----+----+ * // | 1| 2| 3|null| * // | 5| 4|null| 6| * // +----+----+----+----+ * * df2.unionByName(df1, true).show * * // output: \"col2\" is missing at left df2 and added at the end of schema. * // +----+----+----+----+ * // |col1|col0|col3|col2| * // +----+----+----+----+ * // | 4| 5| 6|null| * // | 2| 1|null| 3| * // +----+----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. With `allowMissingColumns`, * missing nested columns of struct columns with the same name will also be filled with null * values and added to the end of struct. Nested columns in map types are not currently * supported. * * @group typedrel * @since 3.1.0  def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns)) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 1.6.0  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset while * preserving the duplicates. * This is equivalent to `INTERSECT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard * in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT DISTINCT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0  def except(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset while * preserving the duplicates. * This is equivalent to `EXCEPT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in * SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a user-supplied seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0  def sample(fraction: Double, seed: Long): Dataset[T] = { sample(withReplacement = false, fraction = fraction, seed = seed) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a random seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0  def sample(fraction: Double): Dataset[T] = { sample(withReplacement = false, fraction = fraction) } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = { withTypedPlan { Sample(0.0, fraction, withReplacement, seed, logicalPlan) } } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = { sample(withReplacement, fraction, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * For Java API, use [[randomSplitAsList]]. * * @group typedrel * @since 2.0.0  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out // from the sort order. val sortOrder = logicalPlan.output .filter(attr => RowOrdering.isOrderable(attr.dataType)) .map(SortOrder(_, Ascending)) val plan = if (sortOrder.nonEmpty) { Sort(sortOrder, global = false, logicalPlan) } else { // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism cache() logicalPlan } val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder) }.toArray } /** * Returns a Java list that contains randomly split Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * @group typedrel * @since 2.0.0  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = { val values = randomSplit(weights, seed) java.util.Arrays.asList(values : _*) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @group typedrel * @since 2.0.0  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = { randomSplit(weights, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling.  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = { randomSplit(weights.toArray, seed) } /** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * {{{ * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select($\"title\", explode(split($\"words\", \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(count_distinct(\"title\")) * }}} * * Using `flatMap()` this can similarly be exploded as: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = { val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * {{{ * ds.select(explode(split($\"words\", \" \")).as(\"word\")) * }}} * * or `flatMap()`: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B]) : DataFrame = { val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = { val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o))) } val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * * `column`'s expression must only refer to attributes supplied by this Dataset. It is an * error to add a column that refers to some other Dataset. * * @note this method introduces a projection internally. Therefore, calling it multiple times, * for instance, via loops in order to add multiple columns can generate big plans which * can cause performance issues and even `StackOverflowException`. To avoid this, * use `select` with the multiple columns at once. * * @group untypedrel * @since 2.0.0  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col)) /** * (Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attributes * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0  def withColumns(colsMap: Map[String, Column]): DataFrame = { val (colNames, newCols) = colsMap.toSeq.unzip withColumns(colNames, newCols) } /** * (Java-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attribute * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0  def withColumns(colsMap: java.util.Map[String, Column]): DataFrame = withColumns( colsMap.asScala.toMap ) /** * Returns a new Dataset by adding columns or replacing the existing columns that has * the same names.  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = { require(colNames.size == cols.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of columns: ${cols.size}\") SchemaUtils.checkColumnNameDuplication( colNames, \"in given column names\", sparkSession.sessionState.conf.caseSensitiveAnalysis) val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val columnSeq = colNames.zip(cols) val replacedAndExistingColumns = output.map { field => columnSeq.find { case (colName, _) => resolver(field.name, colName) } match { case Some((colName: String, col: Column)) => col.as(colName) case _ => Column(field) } } val newColumns = columnSeq.filter { case (colName, col) => !output.exists(f => resolver(f.name, colName)) }.map { case (colName, col) => col.as(colName) } select(replacedAndExistingColumns ++ newColumns : _*) } /** * Returns a new Dataset by adding columns with metadata.  private[spark] def withColumns( colNames: Seq[String], cols: Seq[Column], metadata: Seq[Metadata]): DataFrame = { require(colNames.size == metadata.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of metadata elements: ${metadata.size}\") val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) => col.as(colName, metadata) } withColumns(colNames, newCols) } /** * Returns a new Dataset by adding a column with metadata.  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = withColumns(Seq(colName), Seq(col), Seq(metadata)) /** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * * @group untypedrel * @since 2.0.0  def withColumnRenamed(existingName: String, newName: String): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f => resolver(f.name, existingName)) if (shouldRename) { val columns = output.map { col => if (resolver(col.name, existingName)) { Column(col).as(newName) } else { Column(col) } } select(columns : _*) } else { toDF() } } /** * Returns a new Dataset by updating an existing column with metadata. * * @group untypedrel * @since 3.3.0  def withMetadata(columnName: String, metadata: Metadata): DataFrame = { withColumn(columnName, col(columnName), metadata) } /** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * @group untypedrel * @since 2.0.0  def drop(colName: String): DataFrame = { drop(Seq(colName) : _*) } /** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def drop(colNames: String*): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter { attribute => colNames.forall(n => !resolver(attribute.name, n)) }.map(attribute => Column(attribute)) if (remainingCols.size == allColumns.size) { toDF() } else { this.select(remainingCols: _*) } } /** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * @group untypedrel * @since 2.0.0  def drop(col: Column): DataFrame = { val expression = col match { case Column(u: UnresolvedAttribute) => queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) => expr } val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter { attr => !attr.semanticEquals(expression) }.map(attr => Column(attr)) select(colsAfterDrop : _*) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns) /** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue // (the Streaming's state store depends on the `groupCols` order). val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) => // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col => resolver(col.name, colName)) if (cols.isEmpty) { throw QueryCompilationErrors.cannotResolveColumnNameAmongAttributesError( colName, schema.fieldNames.mkString(\", \")) } cols } Deduplicate(groupCols, logicalPlan) } /** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq) /** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def dropDuplicates(col1: String, cols: String*): Dataset[T] = { val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames) } /** * Computes basic statistics for numeric and string columns, including count, mean, stddev, min, * and max. If no columns are given, this function computes statistics for all numerical or * string columns. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * }}} * * Use [[summary]] for expanded statistics and control over which statistics to compute. * * @param cols Columns to compute statistics on. * * @group action * @since 1.6.0  @scala.annotation.varargs def describe(cols: String*): DataFrame = { val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*) selected.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\") } /** * Computes specified statistics for numeric and string columns. Available statistics are: * <ul> * <li>count</li> * <li>mean</li> * <li>stddev</li> * <li>min</li> * <li>max</li> * <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li> * <li>count_distinct</li> * <li>approx_count_distinct</li> * </ul> * * If no statistics are given, this function computes count, mean, stddev, min, * approximate quartiles (percentiles at 25%, 50%, and 75%), and max. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.summary().show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 50% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * {{{ * ds.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show() * * // output: * // summary age height * // count 10.0 10.0 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * To do a summary for specific columns first select them: * * {{{ * ds.select(\"age\", \"height\").summary().show() * }}} * * Specify statistics to output custom summaries: * * {{{ * ds.summary(\"count\", \"count_distinct\").show() * }}} * * The distinct count isn't included by default. * * You can also run approximate distinct counts which are faster: * * {{{ * ds.summary(\"count\", \"approx_count_distinct\").show() * }}} * * See also [[describe]] for basic statistics. * * @param statistics Statistics from above list to be computed. * * @group action * @since 2.3.0  @scala.annotation.varargs def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq) /** * Returns the first `n` rows. * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @group action * @since 1.6.0  def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) /** * Returns the first row. * @group action * @since 1.6.0  def head(): T = head(1).head /** * Returns the first row. Alias for head(). * @group action * @since 1.6.0  def first(): T = head() /** * Concise syntax for chaining custom transformations. * {{{ * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * }}} * * @group typedrel * @since 1.6.0  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this) /** * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0  def filter(func: T => Boolean): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0  def filter(func: FilterFunction[T]): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan { MapElements[T, U](func, logicalPlan) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * @group typedrel * @since 1.6.0  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = { new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]]) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * @group typedrel * @since 1.6.0  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala mapPartitions(func)(encoder) } /** * Returns a new `DataFrame` that contains the result of applying a serialized R function * `func` to each partition.  private[sql] def mapPartitionsInR( func: Array[Byte], packageNames: Array[Byte], broadcastVars: Array[Broadcast[Object]], schema: StructType): DataFrame = { val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]] Dataset.ofRows( sparkSession, MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan)) } /** * Applies a Scalar iterator Pandas UDF to each partition. The user-defined function * defines a transformation: `iter(pandas.DataFrame)` -> `iter(pandas.DataFrame)`. * Each partition is each iterator consisting of DataFrames as batches. * * This function uses Apache Arrow as serialization format between Java executors and Python * workers.  private[sql] def mapInPandas(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, MapInPandas( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * Applies a function to each partition in Arrow format. The user-defined function * defines a transformation: `iter(pyarrow.RecordBatch)` -> `iter(pyarrow.RecordBatch)`. * Each partition is each iterator consisting of `pyarrow.RecordBatch`s as batches.  private[sql] def pythonMapInArrow(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, PythonMapInArrow( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func)) /** * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (T) => Iterator[U] = x => f.call(x).asScala flatMap(func)(encoder) } /** * Applies a function `f` to all rows. * * @group action * @since 1.6.0  def foreach(f: T => Unit): Unit = withNewRDDExecutionId { rdd.foreach(f) } /** * (Java-specific) * Runs `func` on each element of this Dataset. * * @group action * @since 1.6.0  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) /** * Applies a function `f` to each partition of this Dataset. * * @group action * @since 1.6.0  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId { rdd.foreachPartition(f) } /** * (Java-specific) * Runs `func` on each partition of this Dataset. * * @group action * @since 1.6.0  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = { foreachPartition((it: Iterator[T]) => func.call(it.asJava)) } /** * Returns the first `n` rows in the Dataset. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def take(n: Int): Array[T] = head(n) /** * Returns the last `n` rows in the Dataset. * * Running tail requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 3.0.0  def tail(n: Int): Array[T] = withAction( \"tail\", withTypedPlan(Tail(Literal(n), logicalPlan)).queryExecution)(collectFromPlan) /** * Returns the first `n` rows in the Dataset as a list. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*) /** * Returns an array that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0  def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) /** * Returns a Java list that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan => val values = collectFromPlan(plan) java.util.Arrays.asList(values : _*) } /** * Returns an iterator that contains all rows in this Dataset. * * The iterator will consume as much memory as the largest partition in this Dataset. * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * * @group action * @since 2.0.0  def toLocalIterator(): java.util.Iterator[T] = { withAction(\"toLocalIterator\", queryExecution) { plan => val fromRow = resolvedEnc.createDeserializer() plan.executeToIterator().map(fromRow).asJava } } /** * Returns the number of rows in the Dataset. * @group action * @since 1.6.0  def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan => plan.executeCollect().head.getLong(0) } /** * Returns a new Dataset that has exactly `numPartitions` partitions. * * @group typedrel * @since 1.6.0  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = true, logicalPlan) } private def repartitionByExpression( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments. // However, we don't want to complicate the semantics of this API method. // Instead, let's give users a friendly error message, pointing them to the new method. val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder]) if (sortOrders.nonEmpty) throw new IllegalArgumentException( s\"\"\"Invalid partitionExprs specified: $sortOrders |For range partitioning use repartitionByRange(...) instead. \"\"\".stripMargin) withTypedPlan { RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByExpression(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = { repartitionByExpression(None, partitionExprs) } private def repartitionByRange( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { require(partitionExprs.nonEmpty, \"At least one partition-by expression must be specified.\") val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) }) withTypedPlan { RepartitionByExpression(sortOrder, logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0  @scala.annotation.varargs def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByRange(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0  @scala.annotation.varargs def repartitionByRange(partitionExprs: Column*): Dataset[T] = { repartitionByRange(None, partitionExprs) } /** * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions * are requested. If a larger number of partitions is requested, it will stay at the current * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can call repartition. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @group typedrel * @since 1.6.0  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = false, logicalPlan) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * Note that for a streaming [[Dataset]], this method returns distinct rows only once * regardless of the output mode, which the behavior may not be same with `DISTINCT` in SQL * against streaming [[Dataset]]. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0  def distinct(): Dataset[T] = dropDuplicates() /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0  def persist(): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this) this } /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0  def cache(): this.type = persist() /** * Persist this Dataset with the given storage level. * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, * `MEMORY_AND_DISK_2`, etc. * * @group basic * @since 1.6.0  def persist(newLevel: StorageLevel): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this } /** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * @group basic * @since 2.1.0  def storageLevel: StorageLevel = { sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData => cachedData.cachedRepresentation.cacheBuilder.storageLevel }.getOrElse(StorageLevel.NONE) } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @param blocking Whether to block until all blocks are deleted. * * @group basic * @since 1.6.0  def unpersist(blocking: Boolean): this.type = { sparkSession.sharedState.cacheManager.uncacheQuery( sparkSession, logicalPlan, cascade = false, blocking) this } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @group basic * @since 1.6.0  def unpersist(): this.type = unpersist(blocking = false) // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`. @transient private lazy val rddQueryExecution: QueryExecution = { val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized) } /** * Represents the content of the Dataset as an `RDD` of `T`. * * @group basic * @since 1.6.0  lazy val rdd: RDD[T] = { val objectType = exprEnc.deserializer.dataType rddQueryExecution.toRdd.mapPartitions { rows => rows.map(_.get(0, objectType).asInstanceOf[T]) } } /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0  def javaRDD: JavaRDD[T] = toJavaRDD /** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 1.6.0  @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\") def registerTempTable(tableName: String): Unit = { createOrReplaceTempView(tableName) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.0.0  @throws[AnalysisException] def createTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = false) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 2.0.0  def createOrReplaceTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = false) } /** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.1.0  @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) } /** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) } private def createTempViewCommand( viewName: String, replace: Boolean, global: Boolean): CreateViewCommand = { val viewType = if (global) GlobalTempView else LocalTempView val tableIdentifier = try { sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName) } catch { case _: ParseException => throw QueryCompilationErrors.invalidViewNameError(viewName) } CreateViewCommand( name = tableIdentifier, userSpecifiedColumns = Nil, comment = None, properties = Map.empty, originalText = None, plan = logicalPlan, allowExisting = false, replace = replace, viewType = viewType, isAnalyzed = true) } /** * Interface for saving the content of the non-streaming Dataset out into external storage. * * @group basic * @since 1.6.0  def write: DataFrameWriter[T] = { if (isStreaming) { logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriter[T](this) } /** * Create a write configuration builder for v2 sources. * * This builder is used to configure and execute write operations. For example, to append to an * existing table, run: * * {{{ * df.writeTo(\"catalog.db.table\").append() * }}} * * This can also be used to create or replace existing tables: * * {{{ * df.writeTo(\"catalog.db.table\").partitionedBy($\"col\").createOrReplace() * }}} * * @group basic * @since 3.0.0  def writeTo(table: String): DataFrameWriterV2[T] = { // TODO: streaming could be adapted to use this interface if (isStreaming) { logicalPlan.failAnalysis( \"'writeTo' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriterV2[T](table, this) } /** * Interface for saving the content of the streaming Dataset out into external storage. * * @group basic * @since 2.0.0  def writeStream: DataStreamWriter[T] = { if (!isStreaming) { logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") } new DataStreamWriter[T](this) } /** * Returns the content of the Dataset as a Dataset of JSON strings. * @since 2.0.0  def toJSON: Dataset[String] = { val rowSchema = this.schema val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone mapPartitions { iter => val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer, new JSONOptions(Map.empty[String, String], sessionLocalTimeZone)) new Iterator[String] { private val toRow = exprEnc.createSerializer() override def hasNext: Boolean = iter.hasNext override def next(): String = { gen.write(toRow(iter.next())) gen.flush() val json = writer.toString if (hasNext) { writer.reset() } else { gen.close() } json } } } (Encoders.STRING) } /** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * @group basic * @since 2.0.0  def inputFiles: Array[String] = { val files: Seq[String] = queryExecution.optimizedPlan.collect { case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) => fsBasedRelation.inputFiles case fr: FileRelation => fr.inputFiles case r: HiveTableRelation => r.tableMeta.storage.locationUri.map(_.toString).toArray case DataSourceV2ScanRelation(DataSourceV2Relation(table: FileTable, _, _, _, _), _, _, _) => table.fileIndex.inputFiles }.flatten files.toSet.toArray } /** * Returns `true` when the logical query plans inside both [[Dataset]]s are equal and * therefore return same results. * * @note The equality comparison here is simplified by tolerating the cosmetic differences * such as attribute names. * @note This API can compare both [[Dataset]]s very fast but can still return `false` on * the [[Dataset]] that return the same results, for instance, from different plans. Such * false negative semantic can be useful when caching as an example. * @since 3.1.0  @DeveloperApi def sameSemantics(other: Dataset[T]): Boolean = { queryExecution.analyzed.sameResult(other.queryExecution.analyzed) } /** * Returns a `hashCode` of the logical query plan against this [[Dataset]]. * * @note Unlike the standard `hashCode`, the hash is calculated against the query plan * simplified by tolerating the cosmetic differences such as attribute names. * @since 3.1.0  @DeveloperApi def semanticHash(): Int = { queryExecution.analyzed.semanticHash() } //////////////////////////////////////////////////////////////////////////// // For Python API //////////////////////////////////////////////////////////////////////////// /** * It adds a new long column with the name `name` that increases one by one. * This is for 'distributed-sequence' default index in pandas API on Spark.  private[sql] def withSequenceColumn(name: String) = { Dataset.ofRows( sparkSession, AttachDistributedSequence( AttributeReference(name, LongType, nullable = false)(), logicalPlan)) } /** * Converts a JavaRDD to a PythonRDD.  private[sql] def javaToPython: JavaRDD[Array[Byte]] = { val structType = schema // capture it for closure val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType)) EvaluatePython.javaToPython(rdd) } private[sql] def collectToPython(): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"collectToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeCollect().iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def tailToPython(n: Int): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"tailToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeTail(n).iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def getRowsToPython( _numRows: Int, truncate: Int): Array[Any] = { EvaluatePython.registerPicklers() val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) val rows = getRows(numRows, truncate).map(_.toArray).toArray val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType))) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( rows.iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-GetRows\") } /** * Collect a Dataset as Arrow batches and serve stream to SparkR. It sends * arrow batches in an ordered manner with buffering. This is inevitable * due to missing R API that reads batches from socket directly. See ARROW-4512. * Eventually, this code should be deduplicated by `collectAsArrowToPython`.  private[sql] def collectAsArrowToR(): Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone RRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToR\", queryExecution) { plan => val buffer = new ByteArrayOutputStream() val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, buffer, timeZoneId) val arrowBatchRdd = toArrowBatchRdd(plan) val numPartitions = arrowBatchRdd.partitions.length // Store collection results for worst case of 1 to N-1 partitions val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1)) var lastIndex = -1 // index of last partition written // Handler to eagerly write partitions to Python in order def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = { // If result is from next partition in order if (index - 1 == lastIndex) { batchWriter.writeBatches(arrowBatches.iterator) lastIndex += 1 // Write stored partitions that come next in order while (lastIndex < results.length && results(lastIndex) != null) { batchWriter.writeBatches(results(lastIndex).iterator) results(lastIndex) = null lastIndex += 1 } // After last batch, end the stream if (lastIndex == results.length) { batchWriter.end() val batches = buffer.toByteArray out.writeInt(batches.length) out.write(batches) } } else { // Store partitions received out of order results(index - 1) = arrowBatches } } sparkSession.sparkContext.runJob( arrowBatchRdd, (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray, 0 until numPartitions, handlePartitionBatches) } } } /** * Collect a Dataset as Arrow batches and serve stream to PySpark. It sends * arrow batches in an un-ordered manner without buffering, and then batch order * information at the end. The batches should be reordered at Python side.  private[sql] def collectAsArrowToPython: Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone PythonRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToPython\", queryExecution) { plan => val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId) // Batches ordered by (index of partition, batch index in that partition) tuple val batchOrder = ArrayBuffer.empty[(Int, Int)] // Handler to eagerly write batches to Python as they arrive, un-ordered val handlePartitionBatches = (index: Int, arrowBatches: Array[Array[Byte]]) => if (arrowBatches.nonEmpty) { // Write all batches (can be more than 1) in the partition, store the batch order tuple batchWriter.writeBatches(arrowBatches.iterator) arrowBatches.indices.foreach { partitionBatchIndex => batchOrder.append((index, partitionBatchIndex)) } } Utils.tryWithSafeFinally { val arrowBatchRdd = toArrowBatchRdd(plan) sparkSession.sparkContext.runJob( arrowBatchRdd, (it: Iterator[Array[Byte]]) => it.toArray, handlePartitionBatches) } { // After processing all partitions, end the batch stream batchWriter.end() // Write batch order indices out.writeInt(batchOrder.length) // Sort by (index of partition, batch index in that partition) tuple to get the // overall_batch_index from 0 to N-1 batches, which can be used to put the // transferred batches in the correct order batchOrder.zipWithIndex.sortBy(_._1).foreach { case (_, overallBatchIndex) => out.writeInt(overallBatchIndex) } } } } } private[sql] def toPythonIterator(prefetchPartitions: Boolean = false): Array[Any] = { withNewExecutionId { PythonRDD.toLocalIteratorAndServe(javaToPython.rdd, prefetchPartitions) } } //////////////////////////////////////////////////////////////////////////// // Private Helpers //////////////////////////////////////////////////////////////////////////// /** * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with * an execution.  private def withNewExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(queryExecution)(body) } /** * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect * them with an execution. Before performing the action, the metrics of the executed plan will be * reset.  private def withNewRDDExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(rddQueryExecution) { rddQueryExecution.executedPlan.resetMetrics() body } } /** * Wrap a Dataset action to track the QueryExecution and time cost, then report to the * user-registered callback functions, and also to convert asserts/NPE to * the internal error exception.  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } /** * Collect all elements from a spark plan.  private def collectFromPlan(plan: SparkPlan): Array[T] = { val fromRow = resolvedEnc.createDeserializer() plan.executeCollect().map(fromRow) } private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = { val sortOrder: Seq[SortOrder] = sortExprs.map { col => col.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) } } withTypedPlan { Sort(sortOrder, global = global, logicalPlan) } } /** A convenient function to wrap a logical plan and produce a DataFrame.  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = { Dataset.ofRows(sparkSession, logicalPlan) } /** A convenient function to wrap a logical plan and produce a Dataset.  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { Dataset(sparkSession, logicalPlan) } /** A convenient function to wrap a set based logical plan and produce a Dataset.  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) { // Set operators widen types (change the schema), so we cannot reuse the row encoder. Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]] } else { Dataset(sparkSession, logicalPlan) } } /** Convert to an RDD of serialized ArrowRecordBatches.  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = { val schemaCaptured = this.schema val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone plan.execute().mapPartitionsInternal { iter => val context = TaskContext.get() ArrowConverters.toBatchIterator( iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context) } } // This is only used in tests, for now. private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = { toArrowBatchRdd(queryExecution.executedPlan) } }",
          "## CLASS: org/apache/spark/SparkContext#\n* this config overrides the default configs as well as system properties.  class SparkContext(config: SparkConf) extends Logging { // The call site where this SparkContext was constructed. private val creationSite: CallSite = Utils.getCallSite() if (!config.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) { // In order to prevent SparkContext from being created in executors. SparkContext.assertOnDriver() } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having started construction. // NOTE: this must be placed at the beginning of the SparkContext constructor. SparkContext.markPartiallyConstructed(this) val startTime = System.currentTimeMillis() private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false) private[spark] def assertNotStopped(): Unit = { if (stopped.get()) { val activeContext = SparkContext.activeContext.get() val activeCreationSite = if (activeContext == null) { \"(No active SparkContext.)\" } else { activeContext.creationSite.longForm } throw new IllegalStateException( s\"\"\"Cannot call methods on a stopped SparkContext. |This stopped SparkContext was created at: | |${creationSite.longForm} | |The currently active SparkContext was created at: | |$activeCreationSite \"\"\".stripMargin) } } /** * Create a SparkContext that loads settings from system properties (for instance, when * launching with ./bin/spark-submit).  def this() = this(new SparkConf()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters  def this(master: String, appName: String, conf: SparkConf) = this(SparkContext.updatedConf(conf, master, appName)) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs. * @param environment Environment variables to set on worker nodes.  def this( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment)) } // The following constructors are required when Java code accesses SparkContext directly. // Please see SI-4278 /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI.  private[spark] def this(master: String, appName: String) = this(master, appName, null, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes.  private[spark] def this(master: String, appName: String, sparkHome: String) = this(master, appName, sparkHome, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs.  private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) = this(master, appName, sparkHome, jars, Map()) // log out Spark Version in Spark driver log logInfo(s\"Running Spark version $SPARK_VERSION\") /* ------------------------------------------------------------------------------------- * | Private variables. These variables keep the internal state of the context, and are | | not accessible by the outside world. They're mutable since we want to initialize all | | of them to some neutral value ahead of time, so that calling \"stop()\" while the | | constructor is still running is safe. | * -------------------------------------------------------------------------------------  private var _conf: SparkConf = _ private var _eventLogDir: Option[URI] = None private var _eventLogCodec: Option[String] = None private var _listenerBus: LiveListenerBus = _ private var _env: SparkEnv = _ private var _statusTracker: SparkStatusTracker = _ private var _progressBar: Option[ConsoleProgressBar] = None private var _ui: Option[SparkUI] = None private var _hadoopConfiguration: Configuration = _ private var _executorMemory: Int = _ private var _schedulerBackend: SchedulerBackend = _ private var _taskScheduler: TaskScheduler = _ private var _heartbeatReceiver: RpcEndpointRef = _ @volatile private var _dagScheduler: DAGScheduler = _ private var _applicationId: String = _ private var _applicationAttemptId: Option[String] = None private var _eventLogger: Option[EventLoggingListener] = None private var _driverLogger: Option[DriverLogger] = None private var _executorAllocationManager: Option[ExecutorAllocationManager] = None private var _cleaner: Option[ContextCleaner] = None private var _listenerBusStarted: Boolean = false private var _jars: Seq[String] = _ private var _files: Seq[String] = _ private var _archives: Seq[String] = _ private var _shutdownHookRef: AnyRef = _ private var _statusStore: AppStatusStore = _ private var _heartbeater: Heartbeater = _ private var _resources: immutable.Map[String, ResourceInformation] = _ private var _shuffleDriverComponents: ShuffleDriverComponents = _ private var _plugins: Option[PluginContainer] = None private var _resourceProfileManager: ResourceProfileManager = _ /* ------------------------------------------------------------------------------------- * | Accessors and public fields. These provide access to the internal state of the | | context. | * -------------------------------------------------------------------------------------  private[spark] def conf: SparkConf = _conf /** * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be * changed at runtime.  def getConf: SparkConf = conf.clone() def resources: Map[String, ResourceInformation] = _resources def jars: Seq[String] = _jars def files: Seq[String] = _files def archives: Seq[String] = _archives def master: String = _conf.get(\"spark.master\") def deployMode: String = _conf.get(SUBMIT_DEPLOY_MODE) def appName: String = _conf.get(\"spark.app.name\") private[spark] def isEventLogEnabled: Boolean = _conf.get(EVENT_LOG_ENABLED) private[spark] def eventLogDir: Option[URI] = _eventLogDir private[spark] def eventLogCodec: Option[String] = _eventLogCodec def isLocal: Boolean = Utils.isLocalMaster(_conf) /** * @return true if context is stopped or in the midst of stopping.  def isStopped: Boolean = stopped.get() private[spark] def statusStore: AppStatusStore = _statusStore // An asynchronous listener bus for Spark events private[spark] def listenerBus: LiveListenerBus = _listenerBus // This function allows components created by SparkEnv to be mocked in unit tests: private[spark] def createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv = { SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf)) } private[spark] def env: SparkEnv = _env // Used to store a URL for each static file/jar together with the file's local timestamp private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedArchives = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala // Keeps track of all persisted RDDs private[spark] val persistentRdds = { val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]() map.asScala } def statusTracker: SparkStatusTracker = _statusTracker private[spark] def progressBar: Option[ConsoleProgressBar] = _progressBar private[spark] def ui: Option[SparkUI] = _ui def uiWebUrl: Option[String] = _ui.map(_.webUrl) /** * A default Hadoop Configuration for the Hadoop code (e.g. file systems) that we reuse. * * @note As it will be reused in all Hadoop RDDs, it's better not to modify it unless you * plan to set some global configurations for all Hadoop RDDs.  def hadoopConfiguration: Configuration = _hadoopConfiguration private[spark] def executorMemory: Int = _executorMemory // Environment variables to pass to our executors. private[spark] val executorEnvs = HashMap[String, String]() // Set SPARK_USER for user who is running SparkContext. val sparkUser = Utils.getCurrentUserName() private[spark] def schedulerBackend: SchedulerBackend = _schedulerBackend private[spark] def taskScheduler: TaskScheduler = _taskScheduler private[spark] def taskScheduler_=(ts: TaskScheduler): Unit = { _taskScheduler = ts } private[spark] def dagScheduler: DAGScheduler = _dagScheduler private[spark] def dagScheduler_=(ds: DAGScheduler): Unit = { _dagScheduler = ds } private[spark] def shuffleDriverComponents: ShuffleDriverComponents = _shuffleDriverComponents /** * A unique identifier for the Spark application. * Its format depends on the scheduler implementation. * (i.e. * in case of local spark app something like 'local-1433865536131' * in case of YARN something like 'application_1433865536131_34483' * in case of MESOS something like 'driver-20170926223339-0001' * )  def applicationId: String = _applicationId def applicationAttemptId: Option[String] = _applicationAttemptId private[spark] def eventLogger: Option[EventLoggingListener] = _eventLogger private[spark] def executorAllocationManager: Option[ExecutorAllocationManager] = _executorAllocationManager private[spark] def resourceProfileManager: ResourceProfileManager = _resourceProfileManager private[spark] def cleaner: Option[ContextCleaner] = _cleaner private[spark] var checkpointDir: Option[String] = None // Thread Local variable that can be used by users to pass information down the stack protected[spark] val localProperties = new InheritableThreadLocal[Properties] { override def childValue(parent: Properties): Properties = { // Note: make a clone such that changes in the parent properties aren't reflected in // the those of the children threads, which has confusing semantics (SPARK-10563). Utils.cloneProperties(parent) } override protected def initialValue(): Properties = new Properties() } /* ------------------------------------------------------------------------------------- * | Initialization. This code initializes the context in a manner that is exception-safe. | | All internal fields holding state are initialized here, and any error prompts the | | stop() method to be called. | * -------------------------------------------------------------------------------------  private def warnSparkMem(value: String): String = { logWarning(\"Using SPARK_MEM to set amount of memory to use per executor process is \" + \"deprecated, please use spark.executor.memory instead.\") value } /** Control our logLevel. This overrides any user-defined log settings. * @param logLevel The desired log level as a string. * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN  def setLogLevel(logLevel: String): Unit = { // let's allow lowercase or mixed case too val upperCased = logLevel.toUpperCase(Locale.ROOT) require(SparkContext.VALID_LOG_LEVELS.contains(upperCased), s\"Supplied level $logLevel did not match one of:\" + s\" ${SparkContext.VALID_LOG_LEVELS.mkString(\",\")}\") Utils.setLogLevel(Level.toLevel(upperCased)) } try { _conf = config.clone() _conf.validateSettings() _conf.set(\"spark.app.startTime\", startTime.toString) if (!_conf.contains(\"spark.master\")) { throw new SparkException(\"A master URL must be set in your configuration\") } if (!_conf.contains(\"spark.app.name\")) { throw new SparkException(\"An application name must be set in your configuration\") } // This should be set as early as possible. SparkContext.fillMissingMagicCommitterConfsIfNeeded(_conf) SparkContext.supplementJavaModuleOptions(_conf) _driverLogger = DriverLogger(_conf) val resourcesFileOpt = conf.get(DRIVER_RESOURCES_FILE) _resources = getOrDiscoverAllResources(_conf, SPARK_DRIVER_PREFIX, resourcesFileOpt) logResourceInfo(SPARK_DRIVER_PREFIX, _resources) // log out spark.app.name in the Spark driver logs logInfo(s\"Submitted application: $appName\") // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster if (master == \"yarn\" && deployMode == \"cluster\" && !_conf.contains(\"spark.yarn.app.id\")) { throw new SparkException(\"Detected yarn cluster mode, but isn't running on a cluster. \" + \"Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.\") } if (_conf.getBoolean(\"spark.logConf\", false)) { logInfo(\"Spark configuration:\\n\" + _conf.toDebugString) } // Set Spark driver host and port system properties. This explicitly sets the configuration // instead of relying on the default value of the config constant. _conf.set(DRIVER_HOST_ADDRESS, _conf.get(DRIVER_HOST_ADDRESS)) _conf.setIfMissing(DRIVER_PORT, 0) _conf.set(EXECUTOR_ID, SparkContext.DRIVER_IDENTIFIER) _jars = Utils.getUserJars(_conf) _files = _conf.getOption(FILES.key).map(_.split(\",\")).map(_.filter(_.nonEmpty)) .toSeq.flatten _archives = _conf.getOption(ARCHIVES.key).map(Utils.stringToSeq).toSeq.flatten _eventLogDir = if (isEventLogEnabled) { val unresolvedDir = conf.get(EVENT_LOG_DIR).stripSuffix(\"/\") Some(Utils.resolveURI(unresolvedDir)) } else { None } _eventLogCodec = { val compress = _conf.get(EVENT_LOG_COMPRESS) if (compress && isEventLogEnabled) { Some(_conf.get(EVENT_LOG_COMPRESSION_CODEC)).map(CompressionCodec.getShortName) } else { None } } _listenerBus = new LiveListenerBus(_conf) _resourceProfileManager = new ResourceProfileManager(_conf, _listenerBus) // Initialize the app status store and listener before SparkEnv is created so that it gets // all events. val appStatusSource = AppStatusSource.createSource(conf) _statusStore = AppStatusStore.createLiveStore(conf, appStatusSource) listenerBus.addToStatusQueue(_statusStore.listener.get) // Create the Spark execution environment (cache, map output tracker, etc) _env = createSparkEnv(_conf, isLocal, listenerBus) SparkEnv.set(_env) // If running the REPL, register the repl's output dir with the file server. _conf.getOption(\"spark.repl.class.outputDir\").foreach { path => val replUri = _env.rpcEnv.fileServer.addDirectory(\"/classes\", new File(path)) _conf.set(\"spark.repl.class.uri\", replUri) } _statusTracker = new SparkStatusTracker(this, _statusStore) _progressBar = if (_conf.get(UI_SHOW_CONSOLE_PROGRESS)) { Some(new ConsoleProgressBar(this)) } else { None } _ui = if (conf.get(UI_ENABLED)) { Some(SparkUI.create(Some(this), _statusStore, _conf, _env.securityManager, appName, \"\", startTime)) } else { // For tests, do not enable the UI None } // Bind the UI before starting the task scheduler to communicate // the bound port to the cluster manager properly _ui.foreach(_.bind()) _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf) // Performance optimization: this dummy call to .size() triggers eager evaluation of // Configuration's internal `properties` field, guaranteeing that it will be computed and // cached before SessionState.newHadoopConf() uses `sc.hadoopConfiguration` to create // a new per-session Configuration. If `properties` has not been computed by that time // then each newly-created Configuration will perform its own expensive IO and XML // parsing to load configuration defaults and populate its own properties. By ensuring // that we've pre-computed the parent's properties, the child Configuration will simply // clone the parent's properties. _hadoopConfiguration.size() // Add each JAR given through the constructor if (jars != null) { jars.foreach(jar => addJar(jar, true)) if (addedJars.nonEmpty) { _conf.set(\"spark.app.initial.jar.urls\", addedJars.keys.toSeq.mkString(\",\")) } } if (files != null) { files.foreach(file => addFile(file, false, true)) if (addedFiles.nonEmpty) { _conf.set(\"spark.app.initial.file.urls\", addedFiles.keys.toSeq.mkString(\",\")) } } if (archives != null) { archives.foreach(file => addFile(file, false, true, isArchive = true)) if (addedArchives.nonEmpty) { _conf.set(\"spark.app.initial.archive.urls\", addedArchives.keys.toSeq.mkString(\",\")) } } _executorMemory = _conf.getOption(EXECUTOR_MEMORY.key) .orElse(Option(System.getenv(\"SPARK_EXECUTOR_MEMORY\"))) .orElse(Option(System.getenv(\"SPARK_MEM\")) .map(warnSparkMem)) .map(Utils.memoryStringToMb) .getOrElse(1024) // Convert java options to env vars as a work around // since we can't set env vars directly in sbt. for { (envKey, propKey) <- Seq((\"SPARK_TESTING\", IS_TESTING.key)) value <- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))} { executorEnvs(envKey) = value } Option(System.getenv(\"SPARK_PREPEND_CLASSES\")).foreach { v => executorEnvs(\"SPARK_PREPEND_CLASSES\") = v } // The Mesos scheduler backend relies on this environment variable to set executor memory. // TODO: Set this only in the Mesos scheduler. executorEnvs(\"SPARK_EXECUTOR_MEMORY\") = executorMemory + \"m\" executorEnvs ++= _conf.getExecutorEnv executorEnvs(\"SPARK_USER\") = sparkUser _shuffleDriverComponents = ShuffleDataIOUtils.loadShuffleDataIO(config).driver() _shuffleDriverComponents.initializeApplication().asScala.foreach { case (k, v) => _conf.set(ShuffleDataIOUtils.SHUFFLE_SPARK_CONF_PREFIX + k, v) } // We need to register \"HeartbeatReceiver\" before \"createTaskScheduler\" because Executor will // retrieve \"HeartbeatReceiver\" in the constructor. (SPARK-6640) _heartbeatReceiver = env.rpcEnv.setupEndpoint( HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this)) // Initialize any plugins before the task scheduler is initialized. _plugins = PluginContainer(this, _resources.asJava) // Create and start the scheduler val (sched, ts) = SparkContext.createTaskScheduler(this, master) _schedulerBackend = sched _taskScheduler = ts _dagScheduler = new DAGScheduler(this) _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet) val _executorMetricsSource = if (_conf.get(METRICS_EXECUTORMETRICS_SOURCE_ENABLED)) { Some(new ExecutorMetricsSource) } else { None } // create and start the heartbeater for collecting memory metrics _heartbeater = new Heartbeater( () => SparkContext.this.reportHeartBeat(_executorMetricsSource), \"driver-heartbeater\", conf.get(EXECUTOR_HEARTBEAT_INTERVAL)) _heartbeater.start() // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's // constructor _taskScheduler.start() _applicationId = _taskScheduler.applicationId() _applicationAttemptId = _taskScheduler.applicationAttemptId() _conf.set(\"spark.app.id\", _applicationId) _applicationAttemptId.foreach { attemptId => _conf.set(APP_ATTEMPT_ID, attemptId) _env.blockManager.blockStoreClient.setAppAttemptId(attemptId) } if (_conf.get(UI_REVERSE_PROXY)) { val proxyUrl = _conf.get(UI_REVERSE_PROXY_URL.key, \"\").stripSuffix(\"/\") + \"/proxy/\" + _applicationId System.setProperty(\"spark.ui.proxyBase\", proxyUrl) } _ui.foreach(_.setAppId(_applicationId)) _env.blockManager.initialize(_applicationId) FallbackStorage.registerBlockManagerIfNeeded(_env.blockManager.master, _conf) // The metrics system for Driver need to be set spark.app.id to app ID. // So it should start after we get app ID from the task scheduler and set spark.app.id. _env.metricsSystem.start(_conf.get(METRICS_STATIC_SOURCES_ENABLED)) _eventLogger = if (isEventLogEnabled) { val logger = new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get, _conf, _hadoopConfiguration) logger.start() listenerBus.addToEventLogQueue(logger) Some(logger) } else { None } _cleaner = if (_conf.get(CLEANER_REFERENCE_TRACKING)) { Some(new ContextCleaner(this, _shuffleDriverComponents)) } else { None } _cleaner.foreach(_.start()) val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) _executorAllocationManager = if (dynamicAllocationEnabled) { schedulerBackend match { case b: ExecutorAllocationClient => Some(new ExecutorAllocationManager( schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf, cleaner = cleaner, resourceProfileManager = resourceProfileManager)) case _ => None } } else { None } _executorAllocationManager.foreach(_.start()) setupAndStartListenerBus() postEnvironmentUpdate() postApplicationStart() // After application started, attach handlers to started server and start handler. _ui.foreach(_.attachAllHandler()) // Attach the driver metrics servlet handler to the web ui after the metrics system is started. _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler))) // Make sure the context is stopped if the user forgets about it. This avoids leaving // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM // is killed, though. logDebug(\"Adding shutdown hook\") // force eager creation of logger _shutdownHookRef = ShutdownHookManager.addShutdownHook( ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () => logInfo(\"Invoking stop() from shutdown hook\") try { stop() } catch { case e: Throwable => logWarning(\"Ignoring Exception while stopping SparkContext from shutdown hook\", e) } } // Post init _taskScheduler.postStartHook() if (isLocal) { _env.metricsSystem.registerSource(Executor.executorSourceLocalModeOnly) } _env.metricsSystem.registerSource(_dagScheduler.metricsSource) _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager)) _env.metricsSystem.registerSource(new JVMCPUSource()) _executorMetricsSource.foreach(_.register(_env.metricsSystem)) _executorAllocationManager.foreach { e => _env.metricsSystem.registerSource(e.executorAllocationManagerSource) } appStatusSource.foreach(_env.metricsSystem.registerSource(_)) _plugins.foreach(_.registerMetrics(applicationId)) } catch { case NonFatal(e) => logError(\"Error initializing SparkContext.\", e) try { stop() } catch { case NonFatal(inner) => logError(\"Error stopping SparkContext after init error.\", inner) } finally { throw e } } /** * Called by the web UI to obtain executor thread dumps. This method may be expensive. * Logs an error and returns None if we failed to obtain a thread dump, which could occur due * to an executor being dead or unresponsive or due to network issues while sending the thread * dump message back to the driver.  private[spark] def getExecutorThreadDump(executorId: String): Option[Array[ThreadStackTrace]] = { try { if (executorId == SparkContext.DRIVER_IDENTIFIER) { Some(Utils.getThreadDump()) } else { env.blockManager.master.getExecutorEndpointRef(executorId) match { case Some(endpointRef) => Some(endpointRef.askSync[Array[ThreadStackTrace]](TriggerThreadDump)) case None => logWarning(s\"Executor $executorId might already have stopped and \" + \"can not request thread dump from it.\") None } } } catch { case e: Exception => logError(s\"Exception getting thread dump from executor $executorId\", e) None } } private[spark] def getLocalProperties: Properties = localProperties.get() private[spark] def setLocalProperties(props: Properties): Unit = { localProperties.set(props) } /** * Set a local property that affects jobs submitted from this thread, such as the Spark fair * scheduler pool. User-defined properties may also be set here. These properties are propagated * through to worker tasks and can be accessed there via * [[org.apache.spark.TaskContext#getLocalProperty]]. * * These properties are inherited by child threads spawned from this thread. This * may have unexpected consequences when working with thread pools. The standard java * implementation of thread pools have worker threads spawn other worker threads. * As a result, local properties may propagate unpredictably.  def setLocalProperty(key: String, value: String): Unit = { if (value == null) { localProperties.get.remove(key) } else { localProperties.get.setProperty(key, value) } } /** * Get a local property set in this thread, or null if it is missing. See * `org.apache.spark.SparkContext.setLocalProperty`.  def getLocalProperty(key: String): String = Option(localProperties.get).map(_.getProperty(key)).orNull /** Set a human readable description of the current job.  def setJobDescription(value: String): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, value) } /** * Assigns a group ID to all the jobs started by this thread until the group ID is set to a * different value or cleared. * * Often, a unit of execution in an application consists of multiple Spark actions or jobs. * Application programmers can use this method to group all those jobs together and give a * group description. Once set, the Spark web UI will associate such jobs with this group. * * The application can also use `org.apache.spark.SparkContext.cancelJobGroup` to cancel all * running jobs in this group. For example, * {{{ * // In the main thread: * sc.setJobGroup(\"some_job_to_cancel\", \"some job description\") * sc.parallelize(1 to 10000, 2).map { i => Thread.sleep(10); i }.count() * * // In a separate thread: * sc.cancelJobGroup(\"some_job_to_cancel\") * }}} * * @param interruptOnCancel If true, then job cancellation will result in `Thread.interrupt()` * being called on the job's executor threads. This is useful to help ensure that the tasks * are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS * may respond to Thread.interrupt() by marking nodes as dead.  def setJobGroup(groupId: String, description: String, interruptOnCancel: Boolean = false): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId) // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup // APIs to also take advantage of this property (e.g., internal job failures or canceling from // JobProgressTab UI) on a per-job basis. setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString) } /** Clear the current thread's job group ID and its description.  def clearJobGroup(): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, null) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, null) setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, null) } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * @note Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](this)(body) // Methods for creating RDDs /** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection. Pass a copy of the argument to avoid this. * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection  def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } /** * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by * `step` every element. * * @note if we need to cache this RDD, we should make sure each partition does not exceed limit. * * @param start the start value. * @param end the end value. * @param step the incremental step * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed range  def range( start: Long, end: Long, step: Long = 1, numSlices: Int = defaultParallelism): RDD[Long] = withScope { assertNotStopped() // when step is 0, range will run infinitely require(step != 0, \"step cannot be 0\") val numElements: BigInt = { val safeStart = BigInt(start) val safeEnd = BigInt(end) if ((safeEnd - safeStart) % step == 0 || (safeEnd > safeStart) != (step > 0)) { (safeEnd - safeStart) / step } else { // the remainder has the same sign with range, could add 1 more (safeEnd - safeStart) / step + 1 } } parallelize(0 until numSlices, numSlices).mapPartitionsWithIndex { (i, _) => val partitionStart = (i * numElements) / numSlices * step + start val partitionEnd = (((i + 1) * numElements) / numSlices) * step + start def getSafeMargin(bi: BigInt): Long = if (bi.isValidLong) { bi.toLong } else if (bi > 0) { Long.MaxValue } else { Long.MinValue } val safePartitionStart = getSafeMargin(partitionStart) val safePartitionEnd = getSafeMargin(partitionEnd) new Iterator[Long] { private[this] var number: Long = safePartitionStart private[this] var overflow: Boolean = false override def hasNext = if (!overflow) { if (step > 0) { number < safePartitionEnd } else { number > safePartitionEnd } } else false override def next() = { val ret = number number += step if (number < ret ^ step < 0) { // we have Long.MaxValue + Long.MaxValue < Long.MaxValue // and Long.MinValue + Long.MinValue > Long.MinValue, so iff the step causes a step // back, we are pretty sure that we have an overflow. overflow = true } ret } } } } /** Distribute a local Scala collection to form an RDD. * * This method is identical to `parallelize`. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection  def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices) } /** * Distribute a local Scala collection to form an RDD, with one or more * location preferences (hostnames of Spark nodes) for each object. * Create a new partition for each collection item. * @param seq list of tuples of data and location preferences (hostnames of Spark nodes) * @return RDD representing data partitioned according to location preferences  def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope { assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs) } /** * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * The text files must be encoded as UTF-8. * * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file  def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope { assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair => pair._2.toString).setName(path) } /** * Read a directory of text files from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI. Each file is read as a single record and returned in a * key-value pair, where the key is the path of each file, the value is the content of each file. * The text files must be encoded as UTF-8. * * <p> For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do `val rdd = sparkContext.wholeTextFile(\"hdfs://a-hdfs-path\")`, * * <p> then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred, large file is also allowable, but may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and the corresponding file content  def wholeTextFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, String)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that wholeTextFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new WholeTextFileRDD( this, classOf[WholeTextFileInputFormat], classOf[Text], classOf[Text], updateConf, minPartitions).map(record => (record._1.toString, record._2.toString)).setName(path) } /** * Get an RDD for a Hadoop-readable dataset as PortableDataStream for each file * (useful for binary data) * * For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do * `val rdd = sparkContext.binaryFiles(\"hdfs://a-hdfs-path\")`, * * then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred; very large files may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and corresponding file content  def binaryFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, PortableDataStream)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that binaryFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new BinaryFileRDD( this, classOf[StreamInputFormat], classOf[String], classOf[PortableDataStream], updateConf, minPartitions).setName(path) } /** * Load data from a flat binary file, assuming the length of each record is constant. * * @note We ensure that the byte array for each record in the resulting RDD * has the provided record length. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param recordLength The length at which to split the records * @param conf Configuration for setting up the dataset. * * @return An RDD of data with values, represented as byte arrays  def binaryRecords( path: String, recordLength: Int, conf: Configuration = hadoopConfiguration): RDD[Array[Byte]] = withScope { assertNotStopped() conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = newAPIHadoopFile[LongWritable, BytesWritable, FixedLengthBinaryInputFormat](path, classOf[FixedLengthBinaryInputFormat], classOf[LongWritable], classOf[BytesWritable], conf = conf) br.map { case (k, v) => val bytes = v.copyBytes() assert(bytes.length == recordLength, \"Byte array does not have correct length\") bytes } } /** * Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf given its InputFormat and other * necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable), * using the older MapReduce API (`org.apache.hadoop.mapred`). * * @param conf JobConf for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions Minimum number of Hadoop Splits to generate. * @return RDD of tuples of key and corresponding value * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function.  def hadoopRDD[K, V]( conf: JobConf, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf before broadcasting it. SparkHadoopUtil.get.addCredentials(conf) new HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minPartitions) } /** Get an RDD for a Hadoop file with an arbitrary InputFormat * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V]( path: String, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // A Hadoop configuration can be about 10 KiB, which is pretty big, so broadcast it. val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration)) val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path) new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minPartitions) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V, F <: InputFormat[K, V]] (path: String, minPartitions: Int) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile(path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]], minPartitions) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths as * a list of inputs * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V, F <: InputFormat[K, V]](path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile[K, V, F](path, defaultMinPartitions) } /** * Smarter version of `newApiHadoopFile` that uses class tags to figure out the classes of keys, * values and the `org.apache.hadoop.mapreduce.InputFormat` (new MapReduce API) so that user * don't need to pass them directly. Instead, callers can just write, for example: * ``` * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * ``` * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @return RDD of tuples of key and corresponding value  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]] (path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { newAPIHadoopFile( path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]]) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * @param conf Hadoop configuration * @return RDD of tuples of key and corresponding value  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( path: String, fClass: Class[F], kClass: Class[K], vClass: Class[V], conf: Configuration = hadoopConfiguration): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // The call to NewHadoopJob automatically adds security credentials to conf, // so we don't need to explicitly add them ourselves val job = NewHadoopJob.getInstance(conf) // Use setInputPaths so that newAPIHadoopFile aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updatedConf = job.getConfiguration new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @param conf Configuration for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function.  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( conf: Configuration = hadoopConfiguration, fClass: Class[F], kClass: Class[K], vClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf. Required to access secure HDFS. val jconf = new JobConf(conf) SparkHadoopUtil.get.addCredentials(jconf) new NewHadoopRDD(this, fClass, kClass, vClass, jconf) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V], minPartitions: Int ): RDD[(K, V)] = withScope { assertNotStopped() val inputFormatClass = classOf[SequenceFileInputFormat[K, V]] hadoopFile(path, inputFormatClass, keyClass, valueClass, minPartitions) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V]( path: String, keyClass: Class[K], valueClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() sequenceFile(path, keyClass, valueClass, defaultMinPartitions) } /** * Version of sequenceFile() for types implicitly convertible to Writables through a * WritableConverter. For example, to access a SequenceFile where the keys are Text and the * values are IntWritable, you could simply write * {{{ * sparkContext.sequenceFile[String, Int](path, ...) * }}} * * WritableConverters are provided in a somewhat strange way (by an implicit function) to support * both subclasses of Writable and types for which we define a converter (e.g. Int to * IntWritable). The most natural thing would've been to have implicit objects for the * converters, but then we couldn't have an object for every subclass of Writable (you can't * have a parameterized singleton object). We use functions instead to create a new converter * for the appropriate type. In addition, we pass the converter a ClassTag of its type to * allow it to figure out the Writable class to use in the subclass case. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V] (path: String, minPartitions: Int = defaultMinPartitions) (implicit km: ClassTag[K], vm: ClassTag[V], kcf: () => WritableConverter[K], vcf: () => WritableConverter[V]): RDD[(K, V)] = { withScope { assertNotStopped() val kc = clean(kcf)() val vc = clean(vcf)() val format = classOf[SequenceFileInputFormat[Writable, Writable]] val writables = hadoopFile(path, format, kc.writableClass(km).asInstanceOf[Class[Writable]], vc.writableClass(vm).asInstanceOf[Class[Writable]], minPartitions) writables.map { case (k, v) => (kc.convert(k), vc.convert(v)) } } } /** * Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and * BytesWritable values that contain a serialized partition. This is still an experimental * storage format and may not be supported exactly as is in future Spark releases. It will also * be pretty slow if you use the default serializer (Java serialization), * though the nice thing about it is that there's very little effort required to save arbitrary * objects. * * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD representing deserialized data from the file(s)  def objectFile[T: ClassTag]( path: String, minPartitions: Int = defaultMinPartitions): RDD[T] = withScope { assertNotStopped() sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions) .flatMap(x => Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader)) } protected[spark] def checkpointFile[T: ClassTag](path: String): RDD[T] = withScope { new ReliableCheckpointRDD[T](this, path) } /** Build the union of a list of RDDs.  def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope { val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty) val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet if (nonEmptyRdds.forall(_.partitioner.isDefined) && partitioners.size == 1) { new PartitionerAwareUnionRDD(this, nonEmptyRdds) } else { new UnionRDD(this, nonEmptyRdds) } } /** Build the union of a list of RDDs passed as variable-length arguments.  def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope { union(Seq(first) ++ rest) } /** Get an RDD that has no partitions or elements.  def emptyRDD[T: ClassTag]: RDD[T] = new EmptyRDD[T](this) // Methods for creating shared variables /** * Register the given accumulator. * * @note Accumulators must be registered before use, or it will throw exception.  def register(acc: AccumulatorV2[_, _]): Unit = { acc.register(this) } /** * Register the given accumulator with given name. * * @note Accumulators must be registered before use, or it will throw exception.  def register(acc: AccumulatorV2[_, _], name: String): Unit = { acc.register(this, name = Option(name)) } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.  def longAccumulator: LongAccumulator = { val acc = new LongAccumulator register(acc) acc } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.  def longAccumulator(name: String): LongAccumulator = { val acc = new LongAccumulator register(acc, name) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.  def doubleAccumulator: DoubleAccumulator = { val acc = new DoubleAccumulator register(acc) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.  def doubleAccumulator(name: String): DoubleAccumulator = { val acc = new DoubleAccumulator register(acc, name) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list.  def collectionAccumulator[T]: CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list.  def collectionAccumulator[T](name: String): CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc, name) acc } /** * Broadcast a read-only variable to the cluster, returning a * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions. * The variable will be sent to each cluster only once. * * @param value value to broadcast to the Spark nodes * @return `Broadcast` object, a read-only variable cached on each machine  def broadcast[T: ClassTag](value: T): Broadcast[T] = { assertNotStopped() require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass), \"Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\") val bc = env.broadcastManager.newBroadcast[T](value, isLocal) val callSite = getCallSite logInfo(\"Created broadcast \" + bc.id + \" from \" + callSite.shortForm) cleaner.foreach(_.registerBroadcastForCleanup(bc)) bc } /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String): Unit = { addFile(path, false, false) } /** * Returns a list of file paths that are added to resources.  def listFiles(): Seq[String] = addedFiles.keySet.toSeq /** * :: Experimental :: * Add an archive to be downloaded and unpacked with this Spark job on every node. * * If an archive is added during execution, it will not be available until the next TaskSet * starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(paths-to-files)` to find its download/unpacked location. * The given path should be one of .zip, .tar, .tar.gz, .tgz and .jar. * * @note A path can be added only once. Subsequent additions of the same path are ignored. * * @since 3.1.0  @Experimental def addArchive(path: String): Unit = { addFile(path, false, false, isArchive = true) } /** * :: Experimental :: * Returns a list of archive paths that are added to resources. * * @since 3.1.0  @Experimental def listArchives(): Seq[String] = addedArchives.keySet.toSeq /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * @param recursive if true, a directory can be given in `path`. Currently directories are * only supported for Hadoop-supported filesystems. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String, recursive: Boolean): Unit = { addFile(path, recursive, false) } private def addFile( path: String, recursive: Boolean, addedOnSubmit: Boolean, isArchive: Boolean = false ): Unit = { val uri = Utils.resolveURI(path) val schemeCorrectedURI = uri.getScheme match { case null => new File(path).getCanonicalFile.toURI case \"local\" => logWarning(s\"File with 'local' scheme $path is not supported to add to file server, \" + s\"since it is already available on every node.\") return case _ => uri } val hadoopPath = new Path(schemeCorrectedURI) val scheme = schemeCorrectedURI.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) { val fs = hadoopPath.getFileSystem(hadoopConfiguration) val isDir = fs.getFileStatus(hadoopPath).isDirectory if (!isLocal && scheme == \"file\" && isDir) { throw new SparkException(s\"addFile does not support local directories when not running \" + \"local mode.\") } if (!recursive && isDir) { throw new SparkException(s\"Added file $hadoopPath is a directory and recursive is not \" + \"turned on.\") } } else { // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) } val key = if (!isLocal && scheme == \"file\") { env.rpcEnv.fileServer.addFile(new File(uri.getPath)) } else if (uri.getScheme == null) { schemeCorrectedURI.toString } else { uri.toString } val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis if (!isArchive && addedFiles.putIfAbsent(key, timestamp).isEmpty) { logInfo(s\"Added file $path at $key with timestamp $timestamp\") // Fetch the file locally so that closures which are run on the driver can still use the // SparkFiles API to access files. Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf, hadoopConfiguration, timestamp, useCache = false) postEnvironmentUpdate() } else if ( isArchive && addedArchives.putIfAbsent( UriBuilder.fromUri(new URI(key)).fragment(uri.getFragment).build().toString, timestamp).isEmpty) { logInfo(s\"Added archive $path at $key with timestamp $timestamp\") // If the scheme is file, use URI to simply copy instead of downloading. val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key) val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build() val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false) val dest = new File( SparkFiles.getRootDirectory(), if (uri.getFragment != null) uri.getFragment else source.getName) logInfo( s\"Unpacking an archive $path from ${source.getAbsolutePath} to ${dest.getAbsolutePath}\") Utils.deleteRecursively(dest) Utils.unpack(source, dest) postEnvironmentUpdate() } else { logWarning(s\"The path $path has been added already. Overwriting of added paths \" + \"is not supported in the current version.\") } } /** * :: DeveloperApi :: * Register a listener to receive up-calls from events that happen during execution.  @DeveloperApi def addSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.addToSharedQueue(listener) } /** * :: DeveloperApi :: * Deregister the listener from Spark's listener bus.  @DeveloperApi def removeSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.removeListener(listener) } private[spark] def getExecutorIds(): Seq[String] = { schedulerBackend match { case b: ExecutorAllocationClient => b.getExecutorIds() case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") Nil } } /** * Get the max number of tasks that can be concurrent launched based on the ResourceProfile * could be used, even if some of them are being used at the moment. * Note that please don't cache the value returned by this method, because the number can change * due to add/remove executors. * * @param rp ResourceProfile which to use to calculate max concurrent tasks. * @return The max number of tasks that can be concurrent launched currently.  private[spark] def maxNumConcurrentTasks(rp: ResourceProfile): Int = { schedulerBackend.maxNumConcurrentTasks(rp) } /** * Update the cluster manager on our scheduling needs. Three bits of information are included * to help it make decisions. This applies to the default ResourceProfile. * @param numExecutors The total number of executors we'd like to have. The cluster manager * shouldn't kill any running executor to reach this number, but, * if all existing executors were to die, this is the number of executors * we'd want to be allocated. * @param localityAwareTasks The number of tasks in all active stages that have a locality * preferences. This includes running, pending, and completed tasks. * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages * that would like to like to run on that host. * This includes running, pending, and completed tasks. * @return whether the request is acknowledged by the cluster manager.  @DeveloperApi def requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: immutable.Map[String, Int] ): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => // this is being applied to the default resource profile, would need to add api to support // others val defaultProfId = resourceProfileManager.defaultResourceProfile.id b.requestTotalExecutors(immutable.Map(defaultProfId-> numExecutors), immutable.Map(localityAwareTasks -> defaultProfId), immutable.Map(defaultProfId -> hostToLocalTaskCount)) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request an additional number of executors from the cluster manager. * @return whether the request is received.  @DeveloperApi def requestExecutors(numAdditionalExecutors: Int): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.requestExecutors(numAdditionalExecutors) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executors. * * This is not supported when dynamic allocation is turned on. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executors it kills * through this method with new ones, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutors(executorIds: Seq[String]): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => require(executorAllocationManager.isEmpty, \"killExecutors() unsupported with Dynamic Allocation turned on\") b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executor. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executor it kills * through this method with a new one, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId)) /** * Request that the cluster manager kill the specified executor without adjusting the * application resource requirements. * * The effect is that a new executor will be launched in place of the one killed by * this request. This assumes the cluster manager will automatically and eventually * fulfill all missing application resource requests. * * @note The replace is by no means guaranteed; another application on the same cluster * can steal the window of opportunity and acquire this application's resources in the * mean time. * * @return whether the request is received.  private[spark] def killAndReplaceExecutor(executorId: String): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** The version of Spark on which this application is running.  def version: String = SPARK_VERSION /** * Return a map from the block manager to the max memory available for caching and the remaining * memory available for caching.  def getExecutorMemoryStatus: Map[String, (Long, Long)] = { assertNotStopped() env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) => (blockManagerId.host + \":\" + blockManagerId.port, mem) } } /** * :: DeveloperApi :: * Return information about what RDDs are cached, if they are in mem or on disk, how much space * they take, etc.  @DeveloperApi def getRDDStorageInfo: Array[RDDInfo] = { getRDDStorageInfo(_ => true) } private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = { assertNotStopped() val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray rddInfos.foreach { rddInfo => val rddId = rddInfo.id val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId)) rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0) rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L) rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L) } rddInfos.filter(_.isCached) } /** * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call. * * @note This does not necessarily mean the caching or computation was successful.  def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap /** * :: DeveloperApi :: * Return pools for fair scheduler  @DeveloperApi def getAllPools: Seq[Schedulable] = { assertNotStopped() // TODO(xiajunluan): We should take nested pools into account taskScheduler.rootPool.schedulableQueue.asScala.toSeq } /** * :: DeveloperApi :: * Return the pool associated with the given name, if one exists  @DeveloperApi def getPoolForName(pool: String): Option[Schedulable] = { assertNotStopped() Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool)) } /** * Return current scheduling mode  def getSchedulingMode: SchedulingMode.SchedulingMode = { assertNotStopped() taskScheduler.schedulingMode } /** * Gets the locality information associated with the partition in a particular rdd * @param rdd of interest * @param partition to be looked up for locality * @return list of preferred locations for the partition  private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { dagScheduler.getPreferredLocs(rdd, partition) } /** * Register an RDD to be persisted in memory and/or disk storage  private[spark] def persistRDD(rdd: RDD[_]): Unit = { persistentRdds(rdd.id) = rdd } /** * Unpersist an RDD from memory and/or disk storage  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean): Unit = { env.blockManager.master.removeRdd(rddId, blocking) persistentRdds.remove(rddId) listenerBus.post(SparkListenerUnpersistRDD(rddId)) } /** * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future. * * If a jar is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addJar(path: String): Unit = { addJar(path, false) } private def addJar(path: String, addedOnSubmit: Boolean): Unit = { def addLocalJarFile(file: File): Seq[String] = { try { if (!file.exists()) { throw new FileNotFoundException(s\"Jar ${file.getAbsolutePath} not found\") } if (file.isDirectory) { throw new IllegalArgumentException( s\"Directory ${file.getAbsoluteFile} is not allowed for addJar\") } Seq(env.rpcEnv.fileServer.addJar(file)) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } def checkRemoteJarFile(path: String): Seq[String] = { val hadoopPath = new Path(path) val scheme = hadoopPath.toUri.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme)) { try { val fs = hadoopPath.getFileSystem(hadoopConfiguration) if (!fs.exists(hadoopPath)) { throw new FileNotFoundException(s\"Jar ${path} not found\") } if (fs.getFileStatus(hadoopPath).isDirectory) { throw new IllegalArgumentException( s\"Directory ${path} is not allowed for addJar\") } Seq(path) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } else { Seq(path) } } if (path == null || path.isEmpty) { logWarning(\"null or empty path specified as parameter to addJar\") } else { val (keys, scheme) = if (path.contains(\"\\\\\") && Utils.isWindows) { // For local paths with backslashes on Windows, URI throws an exception (addLocalJarFile(new File(path)), \"local\") } else { val uri = Utils.resolveURI(path) // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) val uriScheme = uri.getScheme val jarPaths = uriScheme match { // A JAR file which exists only on the driver node case null => // SPARK-22585 path without schema is not url encoded addLocalJarFile(new File(uri.getPath)) // A JAR file which exists only on the driver node case \"file\" => addLocalJarFile(new File(uri.getPath)) // A JAR file which exists locally on every worker node case \"local\" => Seq(\"file:\" + uri.getPath) case \"ivy\" => // Since `new Path(path).toUri` will lose query information, // so here we use `URI.create(path)` DependencyUtils.resolveMavenDependencies(URI.create(path)) .flatMap(jar => addLocalJarFile(new File(jar))) case _ => checkRemoteJarFile(path) } (jarPaths, uriScheme) } if (keys.nonEmpty) { val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis val (added, existed) = keys.partition(addedJars.putIfAbsent(_, timestamp).isEmpty) if (added.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"Added $jarMessage $path at ${added.mkString(\",\")} with timestamp $timestamp\") postEnvironmentUpdate() } if (existed.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" + \" Overwriting of added jar is not supported in the current version.\") } } } } /** * Returns a list of jar files that are added to resources.  def listJars(): Seq[String] = addedJars.keySet.toSeq /** * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark * may wait for some internal threads to finish. It's better to use this method to stop * SparkContext instead.  private[spark] def stopInNewThread(): Unit = { new Thread(\"stop-spark-context\") { setDaemon(true) override def run(): Unit = { try { SparkContext.this.stop() } catch { case e: Throwable => logError(e.getMessage, e) throw e } } }.start() } /** * Shut down the SparkContext.  def stop(): Unit = { if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop SparkContext within listener bus thread.\") } // Use the stopping variable to ensure no contention for the stop scenario. // Still track the stopped variable for use elsewhere in the code. if (!stopped.compareAndSet(false, true)) { logInfo(\"SparkContext already stopped.\") return } if (_shutdownHookRef != null) { ShutdownHookManager.removeShutdownHook(_shutdownHookRef) } if (listenerBus != null) { Utils.tryLogNonFatalError { postApplicationEnd() } } Utils.tryLogNonFatalError { _driverLogger.foreach(_.stop()) } Utils.tryLogNonFatalError { _ui.foreach(_.stop()) } Utils.tryLogNonFatalError { _cleaner.foreach(_.stop()) } Utils.tryLogNonFatalError { _executorAllocationManager.foreach(_.stop()) } if (_dagScheduler != null) { Utils.tryLogNonFatalError { _dagScheduler.stop() } _dagScheduler = null } if (_listenerBusStarted) { Utils.tryLogNonFatalError { listenerBus.stop() _listenerBusStarted = false } } if (env != null) { Utils.tryLogNonFatalError { env.metricsSystem.report() } } Utils.tryLogNonFatalError { _plugins.foreach(_.shutdown()) } FallbackStorage.cleanUp(_conf, _hadoopConfiguration) Utils.tryLogNonFatalError { _eventLogger.foreach(_.stop()) } if (_heartbeater != null) { Utils.tryLogNonFatalError { _heartbeater.stop() } _heartbeater = null } if (_shuffleDriverComponents != null) { Utils.tryLogNonFatalError { _shuffleDriverComponents.cleanupApplication() } } if (env != null && _heartbeatReceiver != null) { Utils.tryLogNonFatalError { env.rpcEnv.stop(_heartbeatReceiver) } } Utils.tryLogNonFatalError { _progressBar.foreach(_.stop()) } _taskScheduler = null // TODO: Cache.stop()? if (_env != null) { Utils.tryLogNonFatalError { _env.stop() } SparkEnv.set(null) } if (_statusStore != null) { _statusStore.close() } // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this // `SparkContext` is stopped. localProperties.remove() ResourceProfile.clearDefaultProfile() // Unset YARN mode system env variable, to allow switching between cluster types. SparkContext.clearActiveContext() logInfo(\"Successfully stopped SparkContext\") } /** * Get Spark's home location from either a value set through the constructor, * or the spark.home Java property, or the SPARK_HOME environment variable * (in that order of preference). If neither of these is set, return None.  private[spark] def getSparkHome(): Option[String] = { conf.getOption(\"spark.home\").orElse(Option(System.getenv(\"SPARK_HOME\"))) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  def setCallSite(shortCallSite: String): Unit = { setLocalProperty(CallSite.SHORT_FORM, shortCallSite) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  private[spark] def setCallSite(callSite: CallSite): Unit = { setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm) setLocalProperty(CallSite.LONG_FORM, callSite.longForm) } /** * Clear the thread-local property for overriding the call sites * of actions and RDDs.  def clearCallSite(): Unit = { setLocalProperty(CallSite.SHORT_FORM, null) setLocalProperty(CallSite.LONG_FORM, null) } /** * Capture the current user callsite and return a formatted version for printing. If the user * has overridden the call site using `setCallSite()`, this will return the user's version.  private[spark] def getCallSite(): CallSite = { lazy val callSite = Utils.getCallSite() CallSite( Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm), Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm) ) } /** * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * The function that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res) results } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions) } /** * Run a job on all partitions in an RDD and return the results in an array. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and return the results in an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: (TaskContext, Iterator[T]) => U, resultHandler: (Int, U) => Unit): Unit = { runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: Iterator[T] => U, resultHandler: (Int, U) => Unit): Unit = { val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter) runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler) } /** * :: DeveloperApi :: * Run a job that can return approximate results. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param evaluator `ApproximateEvaluator` to receive the partial results * @param timeout maximum time to wait for the job, in milliseconds * @return partial result (how partial depends on whether the job was finished before or * after timeout)  @DeveloperApi def runApproximateJob[T, U, R]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, evaluator: ApproximateEvaluator[U, R], timeout: Long): PartialResult[R] = { assertNotStopped() val callSite = getCallSite logInfo(\"Starting job: \" + callSite.shortForm) val start = System.nanoTime val cleanedFunc = clean(func) val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout, localProperties.get) logInfo( \"Job finished: \" + callSite.shortForm + \", took \" + (System.nanoTime - start) / 1e9 + \" s\") result } /** * Submit a job for execution and return a FutureJob holding the result. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to * @param resultFunc function to be executed when the result is ready  def submitJob[T, U, R]( rdd: RDD[T], processPartition: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit, resultFunc: => R): SimpleFutureAction[R] = { assertNotStopped() val cleanF = clean(processPartition) val callSite = getCallSite val waiter = dagScheduler.submitJob( rdd, (context: TaskContext, iter: Iterator[T]) => cleanF(iter), partitions, callSite, resultHandler, localProperties.get) new SimpleFutureAction(waiter, resultFunc) } /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future.  private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } /** * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup` * for more information.  def cancelJobGroup(groupId: String): Unit = { assertNotStopped() dagScheduler.cancelJobGroup(groupId) } /** Cancel all jobs that have been scheduled or are running.  def cancelAllJobs(): Unit = { assertNotStopped() dagScheduler.cancelAllJobs() } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @param reason optional reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int, reason: String): Unit = { dagScheduler.cancelJob(jobId, Option(reason)) } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int): Unit = { dagScheduler.cancelJob(jobId, None) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @param reason reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int, reason: String): Unit = { dagScheduler.cancelStage(stageId, Option(reason)) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int): Unit = { dagScheduler.cancelStage(stageId, None) } /** * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI * or through SparkListener.onTaskStart. * * @param taskId the task ID to kill. This id uniquely identifies the task attempt. * @param interruptThread whether to interrupt the thread running the task. * @param reason the reason for killing the task, which should be a short string. If a task * is killed multiple times with different reasons, only one reason will be reported. * * @return Whether the task was successfully killed.  def killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean = { dagScheduler.killTaskAttempt(taskId, interruptThread, reason) } /** * Clean a closure to make it ready to be serialized and sent to tasks * (removes unreferenced variables in $outer's, updates REPL variables) * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt> * if not. * * @param f the closure to clean * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not * serializable * @return the cleaned closure  private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = { ClosureCleaner.clean(f, checkSerializable) f } /** * Set the directory under which RDDs are going to be checkpointed. * @param directory path to the directory where checkpoint files will be stored * (must be HDFS path if running in cluster)  def setCheckpointDir(directory: String): Unit = { // If we are running on a cluster, log a warning if the directory is local. // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from // its own local file system, which is incorrect because the checkpoint files // are actually on the executor machines. if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) { logWarning(\"Spark is not running in local mode, therefore the checkpoint directory \" + s\"must not be on the local filesystem. Directory '$directory' \" + \"appears to be on the local filesystem.\") } checkpointDir = Option(directory).map { dir => val path = new Path(dir, UUID.randomUUID().toString) val fs = path.getFileSystem(hadoopConfiguration) fs.mkdirs(path) fs.getFileStatus(path).getPath.toString } } def getCheckpointDir: Option[String] = checkpointDir /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD).  def defaultParallelism: Int = { assertNotStopped() taskScheduler.defaultParallelism } /** * Default min number of partitions for Hadoop RDDs when not given by user * Notice that we use math.min so the \"defaultMinPartitions\" cannot be higher than 2. * The reasons for this are discussed in https://github.com/mesos/spark/pull/718  def defaultMinPartitions: Int = math.min(defaultParallelism, 2) private val nextShuffleId = new AtomicInteger(0) private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement() private val nextRddId = new AtomicInteger(0) /** Register a new RDD, returning its RDD ID  private[spark] def newRddId(): Int = nextRddId.getAndIncrement() /** * Registers listeners specified in spark.extraListeners, then starts the listener bus. * This should be called after all internal listeners have been registered with the listener bus * (e.g. after the web UI and event logging listeners have been registered).  private def setupAndStartListenerBus(): Unit = { try { conf.get(EXTRA_LISTENERS).foreach { classNames => val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf) listeners.foreach { listener => listenerBus.addToSharedQueue(listener) logInfo(s\"Registered listener ${listener.getClass().getName()}\") } } } catch { case e: Exception => try { stop() } finally { throw new SparkException(s\"Exception when registering SparkListener\", e) } } listenerBus.start(this, _env.metricsSystem) _listenerBusStarted = true } /** Post the application start event  private def postApplicationStart(): Unit = { // Note: this code assumes that the task scheduler has been initialized and has contacted // the cluster manager to get an application ID (in case the cluster manager provides one). listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId), startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls, schedulerBackend.getDriverAttributes)) _driverLogger.foreach(_.startSync(_hadoopConfiguration)) } /** Post the application end event  private def postApplicationEnd(): Unit = { listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis)) } /** Post the environment update event once the task scheduler is ready  private def postEnvironmentUpdate(): Unit = { if (taskScheduler != null) { val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val addedArchivePaths = addedArchives.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, hadoopConfiguration, schedulingMode, addedJarPaths, addedFilePaths, addedArchivePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) } } /** Reports heartbeat metrics for the driver.  private def reportHeartBeat(executorMetricsSource: Option[ExecutorMetricsSource]): Unit = { val currentMetrics = ExecutorMetrics.getCurrentMetrics(env.memoryManager) executorMetricsSource.foreach(_.updateMetricsSnapshot(currentMetrics)) val driverUpdates = new HashMap[(Int, Int), ExecutorMetrics] // In the driver, we do not track per-stage metrics, so use a dummy stage for the key driverUpdates.put(EventLoggingListener.DRIVER_STAGE_KEY, new ExecutorMetrics(currentMetrics)) val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0) listenerBus.post(SparkListenerExecutorMetricsUpdate(\"driver\", accumUpdates, driverUpdates)) } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having finished construction. // NOTE: this must be placed at the end of the SparkContext constructor. SparkContext.setActiveContext(this) } /** * The SparkContext object contains a number of implicit conversions and parameters for use with * various Spark features.  object SparkContext extends Logging { private val VALID_LOG_LEVELS = Set(\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\") /** * Lock that guards access to global variables that track SparkContext construction.  private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object() /** * The active, fully-constructed SparkContext. If no SparkContext is active, then this is `null`. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private val activeContext: AtomicReference[SparkContext] = new AtomicReference[SparkContext](null) /** * Points to a partially-constructed SparkContext if another thread is in the SparkContext * constructor, or `None` if no SparkContext is being constructed. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private var contextBeingConstructed: Option[SparkContext] = None /** * Called to ensure that no other SparkContext is running in this JVM. * * Throws an exception if a running context is detected and logs a warning if another thread is * constructing a SparkContext. This warning is necessary because the current locking scheme * prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private def assertNoOtherContextIsRunning(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()).filter(_ ne sc).foreach { ctx => val errMsg = \"Only one SparkContext should be running in this JVM (see SPARK-2243).\" + s\"The currently running SparkContext was created at:\\n${ctx.creationSite.longForm}\" throw new SparkException(errMsg) } contextBeingConstructed.filter(_ ne sc).foreach { otherContext => // Since otherContext might point to a partially-constructed context, guard against // its creationSite field being null: val otherContextCreationSite = Option(otherContext.creationSite).map(_.longForm).getOrElse(\"unknown location\") val warnMsg = \"Another SparkContext is being constructed (or threw an exception in its\" + \" constructor). This may indicate an error, since only one SparkContext should be\" + \" running in this JVM (see SPARK-2243).\" + s\" The other SparkContext was created at:\\n$otherContextCreationSite\" logWarning(warnMsg) } } } /** * Called to ensure that SparkContext is created or accessed only on the Driver. * * Throws an exception if a SparkContext is about to be created in executors.  private def assertOnDriver(): Unit = { if (Utils.isInRunningSparkTask) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkContext should only be created and accessed on the driver.\") } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * @param config `SparkConfig` that will be used for initialisation of the `SparkContext` * @return current `SparkContext` (or a new one if it wasn't created before the function call)  def getOrCreate(config: SparkConf): SparkContext = { // Synchronize to ensure that multiple create requests don't trigger an exception // from assertNoOtherContextIsRunning within setActiveContext SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext(config)) } else { if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") } } activeContext.get() } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * This method allows not passing a SparkConf (useful if just retrieving). * * @return current `SparkContext` (or a new one if wasn't created before the function call)  def getOrCreate(): SparkContext = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext()) } activeContext.get() } } /** Return the current active [[SparkContext]] if any.  private[spark] def getActive: Option[SparkContext] = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()) } } /** * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is * running. Throws an exception if a running context is detected and logs a warning if another * thread is constructing a SparkContext. This warning is necessary because the current locking * scheme prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private[spark] def markPartiallyConstructed(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = Some(sc) } } /** * Called at the end of the SparkContext constructor to ensure that no other SparkContext has * raced with this constructor and started.  private[spark] def setActiveContext(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = None activeContext.set(sc) } } /** * Clears the active SparkContext metadata. This is called by `SparkContext#stop()`. It's * also called in unit tests to prevent a flood of warnings from test suites that don't / can't * properly clean up their SparkContexts.  private[spark] def clearActiveContext(): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { activeContext.set(null) } } private[spark] val SPARK_JOB_DESCRIPTION = \"spark.job.description\" private[spark] val SPARK_JOB_GROUP_ID = \"spark.jobGroup.id\" private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = \"spark.job.interruptOnCancel\" private[spark] val SPARK_SCHEDULER_POOL = \"spark.scheduler.pool\" private[spark] val RDD_SCOPE_KEY = \"spark.rdd.scope\" private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = \"spark.rdd.scope.noOverride\" /** * Executor id for the driver. In earlier versions of Spark, this was `<driver>`, but this was * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see * SPARK-6716 for more details).  private[spark] val DRIVER_IDENTIFIER = \"driver\" private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Iterable[T]) : ArrayWritable = { def anyToWritable[U <: Writable](u: U): Writable = u new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]], arr.map(x => anyToWritable(x)).toArray) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to SparkContext. * * @param cls class that should be inside of the jar * @return jar that contains the Class, `None` if not found  def jarOfClass(cls: Class[_]): Option[String] = { val uri = cls.getResource(\"/\" + cls.getName.replace('.', '/') + \".class\") if (uri != null) { val uriStr = uri.toString if (uriStr.startsWith(\"jar:file:\")) { // URI will be of the form \"jar:file:/path/foo.jar!/package/cls.class\", // so pull out the /path/foo.jar Some(uriStr.substring(\"jar:file:\".length, uriStr.indexOf('!'))) } else { None } } else { None } } /** * Find the JAR that contains the class of a particular object, to make it easy for users * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in * your driver program. * * @param obj reference to an instance which class should be inside of the jar * @return jar that contains the class of the instance, `None` if not found  def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass) /** * Creates a modified version of a SparkConf with the parameters that can be passed separately * to SparkContext, to make it easier to write SparkContext's constructors. This ignores * parameters that are passed as the default value of null, instead of throwing an exception * like SparkConf would.  private[spark] def updatedConf( conf: SparkConf, master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()): SparkConf = { val res = conf.clone() res.setMaster(master) res.setAppName(appName) if (sparkHome != null) { res.setSparkHome(sparkHome) } if (jars != null && !jars.isEmpty) { res.setJars(jars) } res.setExecutorEnv(environment.toSeq) res } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String): Int = { numDriverCores(master, null) } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String, conf: SparkConf): Int = { def convertToInt(threads: String): Int = { if (threads == \"*\") Runtime.getRuntime.availableProcessors() else threads.toInt } master match { case \"local\" => 1 case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads) case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads) case \"yarn\" | SparkMasterRegex.KUBERNETES_REGEX(_) => if (conf != null && conf.get(SUBMIT_DEPLOY_MODE) == \"cluster\") { conf.getInt(DRIVER_CORES.key, 0) } else { 0 } case _ => 0 // Either driver is not being used, or its core count will be interpolated later } } /** * Create a task scheduler based on a given master URL. * Return a 2-tuple of the scheduler backend and the task scheduler.  private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = { import SparkMasterRegex._ // When running locally, don't try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 // Ensure that default executor's resources satisfies one or more tasks requirement. // This function is for cluster managers that don't set the executor cores config, for // others its checked in ResourceProfile. def checkResourcesPerTask(executorCores: Int): Unit = { val taskCores = sc.conf.get(CPUS_PER_TASK) if (!sc.conf.get(SKIP_VALIDATE_CORES_TESTING)) { validateTaskCpusLargeEnough(sc.conf, executorCores, taskCores) } val defaultProf = sc.resourceProfileManager.defaultResourceProfile ResourceUtils.warnOnWastedResources(defaultProf, sc.conf, Some(executorCores)) } master match { case \"local\" => checkResourcesPerTask(1) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_REGEX(threads) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads. val threadCount = if (threads == \"*\") localCpuCount else threads.toInt if (threadCount <= 0) { throw new SparkException(s\"Asked to run locally with $threadCount threads\") } checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_FAILURES_REGEX(threads, maxFailures) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*, M] means the number of cores on the computer with M failures // local[N, M] means exactly N threads with M failures val threadCount = if (threads == \"*\") localCpuCount else threads.toInt checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case SPARK_REGEX(sparkUrl) => val scheduler = new TaskSchedulerImpl(sc) val masterUrls = sparkUrl.split(\",\").map(\"spark://\" + _) val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) (backend, scheduler) case LOCAL_CLUSTER_REGEX(numWorkers, coresPerWorker, memoryPerWorker) => checkResourcesPerTask(coresPerWorker.toInt) // Check to make sure memory requested <= memoryPerWorker. Otherwise Spark will just hang. val memoryPerWorkerInt = memoryPerWorker.toInt if (sc.executorMemory > memoryPerWorkerInt) { throw new SparkException( \"Asked to launch cluster with %d MiB/worker but requested %d MiB/executor\".format( memoryPerWorkerInt, sc.executorMemory)) } // For host local mode setting the default of SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED // to false because this mode is intended to be used for testing and in this case all the // executors are running on the same host. So if host local reading was enabled here then // testing of the remote fetching would be secondary as setting this config explicitly to // false would be required in most of the unit test (despite the fact that remote fetching // is much more frequent in production). sc.conf.setIfMissing(SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED, false) val scheduler = new TaskSchedulerImpl(sc) val localCluster = LocalSparkCluster( numWorkers.toInt, coresPerWorker.toInt, memoryPerWorkerInt, sc.conf) val masterUrls = localCluster.start() val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => { localCluster.stop() } (backend, scheduler) case masterUrl => val cm = getClusterManager(masterUrl) match { case Some(clusterMgr) => clusterMgr case None => throw new SparkException(\"Could not parse Master URL: '\" + master + \"'\") } try { val scheduler = cm.createTaskScheduler(sc, masterUrl) val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) cm.initialize(scheduler, backend) (backend, scheduler) } catch { case se: SparkException => throw se case NonFatal(e) => throw new SparkException(\"External scheduler cannot be instantiated\", e) } } } private def getClusterManager(url: String): Option[ExternalClusterManager] = { val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size > 1) { throw new SparkException( s\"Multiple external cluster managers registered for the url $url: $serviceLoaders\") } serviceLoaders.headOption } /** * This is a helper function to complete the missing S3A magic committer configurations * based on a single conf: `spark.hadoop.fs.s3a.bucket.<bucket>.committer.magic.enabled`  private def fillMissingMagicCommitterConfsIfNeeded(conf: SparkConf): Unit = { val magicCommitterConfs = conf .getAllWithPrefix(\"spark.hadoop.fs.s3a.bucket.\") .filter(_._1.endsWith(\".committer.magic.enabled\")) .filter(_._2.equalsIgnoreCase(\"true\")) if (magicCommitterConfs.nonEmpty) { // Try to enable S3 magic committer if missing conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\") if (conf.get(\"spark.hadoop.fs.s3a.committer.magic.enabled\").equals(\"true\")) { conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") conf.setIfMissing(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\") conf.setIfMissing(\"spark.sql.parquet.output.committer.class\", \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\") conf.setIfMissing(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\") } } } /** * SPARK-36796: This is a helper function to supplement `--add-opens` options to * `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`.  private def supplementJavaModuleOptions(conf: SparkConf): Unit = { def supplement(key: OptionalConfigEntry[String]): Unit = { val v = conf.get(key) match { case Some(opts) => s\"${JavaModuleOptions.defaultModuleOptions()} $opts\" case None => JavaModuleOptions.defaultModuleOptions() } conf.set(key.key, v) } supplement(DRIVER_JAVA_OPTIONS) supplement(EXECUTOR_JAVA_OPTIONS) } } /** * A collection of regexes for extracting information from the master string.  private object SparkMasterRegex { // Regular expression used for local[N] and local[*] master formats val LOCAL_N_REGEX = \"\"\"local\\[([0-9]+|\\*)\\]\"\"\".r // Regular expression for local[N, maxRetries], used in tests with failing tasks val LOCAL_N_FAILURES_REGEX = \"\"\"local\\[([0-9]+|\\*)\\s*,\\s*([0-9]+)\\]\"\"\".r // Regular expression for simulating a Spark cluster of [N, cores, memory] locally val LOCAL_CLUSTER_REGEX = \"\"\"local-cluster\\[\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*]\"\"\".r // Regular expression for connecting to Spark deploy clusters val SPARK_REGEX = \"\"\"spark://(.*)\"\"\".r // Regular expression for connecting to kubernetes clusters val KUBERNETES_REGEX = \"\"\"k8s://(.*)\"\"\".r } /** * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`).  private[spark] class WritableConverter[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: Writable => T) extends Serializable object WritableConverter { // Helper objects for converting common types to Writable private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T) : WritableConverter[T] = { val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]] new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W])) } // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. // The following implicit declarations have been added on top of the very similar ones // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta // expansion of zero-arg methods and thus won't match a no-arg method where it expects // an implicit that is a function of no args. implicit val intWritableConverterFn: () => WritableConverter[Int] = () => simpleWritableConverter[Int, IntWritable](_.get) implicit val longWritableConverterFn: () => WritableConverter[Long] = () => simpleWritableConverter[Long, LongWritable](_.get) implicit val doubleWritableConverterFn: () => WritableConverter[Double] = () => simpleWritableConverter[Double, DoubleWritable](_.get) implicit val floatWritableConverterFn: () => WritableConverter[Float] = () => simpleWritableConverter[Float, FloatWritable](_.get) implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] = () => simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = { () => simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit val stringWritableConverterFn: () => WritableConverter[String] = () => simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] = () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) // These implicits remain included for backwards-compatibility. They fulfill the // same role as those above. implicit def intWritableConverter(): WritableConverter[Int] = simpleWritableConverter[Int, IntWritable](_.get) implicit def longWritableConverter(): WritableConverter[Long] = simpleWritableConverter[Long, LongWritable](_.get) implicit def doubleWritableConverter(): WritableConverter[Double] = simpleWritableConverter[Double, DoubleWritable](_.get) implicit def floatWritableConverter(): WritableConverter[Float] = simpleWritableConverter[Float, FloatWritable](_.get) implicit def booleanWritableConverter(): WritableConverter[Boolean] = simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = { simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit def stringWritableConverter(): WritableConverter[String] = simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] = new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) } /** * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The `Writable` class will be used in `SequenceFileRDDFunctions`.  private[spark] class WritableFactory[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: T => Writable) extends Serializable object WritableFactory { private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W) : WritableFactory[T] = { val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]] new WritableFactory[T](_ => writableClass, convert) } implicit def intWritableFactory: WritableFactory[Int] = simpleWritableFactory(new IntWritable(_)) implicit def longWritableFactory: WritableFactory[Long] = simpleWritableFactory(new LongWritable(_)) implicit def floatWritableFactory: WritableFactory[Float] = simpleWritableFactory(new FloatWritable(_)) implicit def doubleWritableFactory: WritableFactory[Double] = simpleWritableFactory(new DoubleWritable(_)) implicit def booleanWritableFactory: WritableFactory[Boolean] = simpleWritableFactory(new BooleanWritable(_)) implicit def bytesWritableFactory: WritableFactory[Array[Byte]] = simpleWritableFactory(new BytesWritable(_)) implicit def stringWritableFactory: WritableFactory[String] = simpleWritableFactory(new Text(_)) implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] = simpleWritableFactory(w => w) }"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### Summary: Top 3 most important classes in this project\n\nGenerated from repository code graph analysis on 2025-12-27 10:33 (local time).\n\nBelow are the three most important classes identified by the repositorys code graph analysis (combined metric), along with precise descriptions based on the code provided by the context tool.\n\n### 1) org.apache.spark.rdd.RDD[T]\n- What it is: The abstract foundational data structure of Sparks core, representing a resilient, partitioned dataset with a lineage of dependencies. It is Serializable and mixes in Logging.\n- Key constructor and fields:\n  - Primary constructor: RDD[T](@transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]])\n  - One-parent constructor: def this(@transient oneParent: RDD[_])\n  - Access to Spark conf via private[spark] def conf = sc.conf\n  - Validates at runtime that nested RDDs are not used (warns via logWarning).\n- Contracts for subclasses:\n  - @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T]\n  - protected def getPartitions: Array[Partition]\n  - protected def getDependencies: Seq[Dependency[_]] = deps\n  - protected def getPreferredLocations(split: Partition): Seq[String] = Nil\n- Purpose and importance:\n  - Defines the core execution and partitioning model of Spark, specifying how data is computed per Partition and how RDDs depend on each other. All transformations/actions ultimately operate on RDDs, making this class central to Sparks computation model.\n\n### 2) org.apache.spark.sql.Dataset[T]\n- What it is: The primary, strongly-typed logical dataset in Spark SQL, coupling logical plans with encoders to produce typed user-facing APIs.\n- Key constructor and fields:\n  - @Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T])\n  - Alternate constructors from SparkSession/SQLContext and LogicalPlan.\n  - @transient lazy val sparkSession: SparkSession derived from queryExecution; throws if invoked off-driver or when missing.\n  - A globally unique id for the Dataset used in self-join ambiguity tagging.\n  - queryExecution.assertAnalyzed() is invoked to ensure analysis is complete.\n  - private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)\n  - private lazy val resolvedEnc computed against logicalPlan.output and session state.\n  - logicalPlan is obtained from queryExecution.commandExecuted; conditionally tags plan with DATASET_ID_TAG when FAIL_AMBIGUOUS_SELF_JOIN_ENABLED is on.\n- Purpose and importance:\n  - Binds the SQL Catalyst planning/execution (QueryExecution, LogicalPlan) to user-level typed operations via Encoder/ExpressionEncoder. It is the core abstraction for typed structured queries, transformations, and actions in Spark SQL, bridging the optimizer with user APIs.\n\n### 3) org.apache.spark.SparkContext\n- What it is: The entry point for Spark Core, responsible for driver-side lifecycle, configuration, and job submission.\n- Key constructor and fields:\n  - class SparkContext(config: SparkConf) extends Logging\n  - Guards: prevents creation on executors (EXECUTOR_ALLOW_SPARK_CONTEXT), and enforces a single active context via SparkContext.markPartiallyConstructed and activeContext checks.\n  - Tracks creationSite to aid diagnostics; asserts not stopped via assertNotStopped with detailed error messages.\n  - Convenience constructors: def this() = this(new SparkConf()); def this(master: String, appName: String, conf: SparkConf) = this(SparkContext.updatedConf(...)).\n  - Lifecycle state: private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false)\n- Purpose and importance:\n  - Central orchestration object that wires user applications to clusters, manages configuration, and maintains the active Spark client context. Nearly all RDD-based operations originate from or pass through SparkContext.\n\n### Why these three are most important\n- RDD defines the core distributed data model and computation contract in Spark Core.\n- Dataset is the typed, optimized, SQL-integrated abstraction that most higher-level structured workloads build upon.\n- SparkContext is the lifecycle and entry point to cluster interaction, guarding correct driver behavior and configuration.\n\n",
          "prompt_time": 0.906,
          "answer_time": 74,
          "context_tokens": 75557,
          "used_context": [
            "## CLASS: org/apache/spark/rdd/RDD#\n abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent  def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition.  @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`  protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it.  protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences.  protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned.  @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD.  def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext).  val id: Int = sc.newRddId() /** A friendly name for this RDD  @transient var name: String = _ /** Assign a name to this RDD  def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one  private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception.  def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD.  def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set.  def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver).  private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not.  final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing.  final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not.  final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD.  @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed.  final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD.  final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned.  private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached.  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD.  def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results.  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate.  def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle.  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable.  def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition.  val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]].  def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array  def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement.  private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function.  def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally.  def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD  def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD  def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array.  def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`.  def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD  def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning.  private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other).  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition.  def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD.  def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD.  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first.  def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`.  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us.  def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator.  def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]]  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions  def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2)  def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor  def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD.  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map.  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped.  def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017.  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`.  def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD.  def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD *  def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD *  def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition.  def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements.  def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements.  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects.  def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`.  def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition  private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation.  def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used.  def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally.  def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing.  private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing.  private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized.  private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally.  def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned  @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents.  def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a>  @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD.  @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified  @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`).  @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations.  @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD  protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T]  protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on.  def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs.  private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions.  private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example.  protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging.  def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier.  private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level.  // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`.  object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE.  private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
            "## CLASS: org/apache/spark/sql/Dataset#\n@Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { @transient lazy val sparkSession: SparkSession = { if (queryExecution == null || queryExecution.sparkSession == null) { throw QueryExecutionErrors.transformationsAndActionsNotInvokedByDriverError() } queryExecution.sparkSession } // A globally unique id of this Dataset. private val id = Dataset.curId.getAndIncrement() queryExecution.assertAnalyzed() // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) } def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sqlContext.sparkSession, logicalPlan, encoder) } @transient private[sql] val logicalPlan: LogicalPlan = { val plan = queryExecution.commandExecuted if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) { val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long]) dsIds.add(id) plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds) } plan } /** * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use * it when constructing new Dataset objects that have the same object type (that will be * possibly resolved to a different schema).  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder) // The resolved `ExpressionEncoder` which can be used to turn rows to objects of type T, after // collecting rows to the driver side. private lazy val resolvedEnc = { exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer) } private implicit def classTag = exprEnc.clsTag // sqlContext must be val because a stable identifier is expected when you import implicits @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext private[sql] def resolve(colName: String): NamedExpression = { val resolver = sparkSession.sessionState.analyzer.resolver queryExecution.analyzed.resolveQuoted(colName, resolver) .getOrElse(throw resolveException(colName, schema.fieldNames)) } private def resolveException(colName: String, fields: Array[String]): AnalysisException = { val extraMsg = if (fields.exists(sparkSession.sessionState.analyzer.resolver(_, colName))) { s\"; did you mean to quote the `$colName` column?\" } else \"\" val fieldsStr = fields.mkString(\", \") QueryCompilationErrors.cannotResolveColumnNameAmongFieldsError(colName, fieldsStr, extraMsg) } private[sql] def numericColumns: Seq[Expression] = { schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n => queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get } } /** * Get rows represented in Sequence by specific truncate and vertical requirement. * * @param numRows Number of rows to return * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right.  private[sql] def getRows( numRows: Int, truncate: Int): Seq[Seq[String]] = { val newDf = toDF() val castCols = newDf.logicalPlan.output.map { col => // Since binary types in top-level schema fields have a specific format to print, // so we do not cast them to strings here. if (col.dataType == BinaryType) { Column(col) } else { Column(col).cast(StringType) } } val data = newDf.select(castCols: _*).take(numRows + 1) // For array values, replace Seq and Array with square brackets // For cells that are beyond `truncate` characters, replace it with the // first `truncate-3` and \"...\" schema.fieldNames.map(SchemaUtils.escapeMetaCharacters).toSeq +: data.map { row => row.toSeq.map { cell => val str = cell match { case null => \"null\" case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\") case _ => // Escapes meta-characters not to break the `showString` format SchemaUtils.escapeMetaCharacters(cell.toString) } if (truncate > 0 && str.length > truncate) { // do not show ellipses for strings shorter than 4 characters. if (truncate < 4) str.substring(0, truncate) else str.substring(0, truncate - 3) + \"...\" } else { str } }: Seq[String] } } /** * Compose the string representing rows for output * * @param _numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value).  private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = { val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate) val hasMoreData = tmpRows.length - 1 > numRows val rows = tmpRows.take(numRows + 1) val sb = new StringBuilder val numCols = schema.fieldNames.length // We set a minimum column width at '3' val minimumColWidth = 3 if (!vertical) { // Initialise the width of each column to a minimum value val colWidths = Array.fill(numCols)(minimumColWidth) // Compute the width of each column for (row <- rows) { for ((cell, i) <- row.zipWithIndex) { colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell)) } } val paddedRows = rows.map { row => row.zipWithIndex.map { case (cell, i) => if (truncate > 0) { StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } else { StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } } } // Create SeparateLine val sep: String = colWidths.map(\"-\" * _).addString(sb, \"+\", \"+\", \"+\\n\").toString() // column names paddedRows.head.addString(sb, \"|\", \"|\", \"|\\n\") sb.append(sep) // data paddedRows.tail.foreach(_.addString(sb, \"|\", \"|\", \"|\\n\")) sb.append(sep) } else { // Extended display mode enabled val fieldNames = rows.head val dataRows = rows.tail // Compute the width of field name and data columns val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) => math.max(curMax, Utils.stringHalfWidth(fieldName)) } val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) => math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max) } dataRows.zipWithIndex.foreach { case (row, i) => // \"+ 5\" in size means a character length except for padded names and data val rowHeader = StringUtils.rightPad( s\"-RECORD $i\", fieldNameColWidth + dataColWidth + 5, \"-\") sb.append(rowHeader).append(\"\\n\") row.zipWithIndex.map { case (cell, j) => val fieldName = StringUtils.rightPad(fieldNames(j), fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length) val data = StringUtils.rightPad(cell, dataColWidth - Utils.stringHalfWidth(cell) + cell.length) s\" $fieldName | $data \" }.addString(sb, \"\", \"\\n\", \"\\n\") } } // Print a footer if (vertical && rows.tail.isEmpty) { // In a vertical mode, print an empty row set explicitly sb.append(\"(0 rows)\\n\") } else if (hasMoreData) { // For Data that has more than \"numRows\" records val rowsString = if (numRows == 1) \"row\" else \"rows\" sb.append(s\"only showing top $numRows $rowsString\\n\") } sb.toString() } override def toString: String = { try { val builder = new StringBuilder val fields = schema.take(2).map { case f => s\"${f.name}: ${f.dataType.simpleString(2)}\" } builder.append(\"[\") builder.append(fields.mkString(\", \")) if (schema.length > 2) { if (schema.length - fields.size == 1) { builder.append(\" ... 1 more field\") } else { builder.append(\" ... \" + (schema.length - 2) + \" more fields\") } } builder.append(\"]\").toString() } catch { case NonFatal(e) => s\"Invalid tree; ${e.getMessage}:\\n$queryExecution\" } } /** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * * @group basic * @since 1.6.0  // This is declared with parentheses to prevent the Scala compiler from treating // `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame. def toDF(): DataFrame = new Dataset[Row](queryExecution, RowEncoder(schema)) /** * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * <ul> * <li>When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`).</li> * <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will * be assigned to `_1`).</li> * <li>When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used.</li> * </ul> * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * Note that `as[]` only changes the view of the data that is passed into typed operations, * such as `map()`, and does not eagerly project away any columns that are not present in * the specified class. * * @group basic * @since 1.6.0  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * {{{ * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` * rdd.toDF(\"id\", \"name\") // this creates a DataFrame with column name \"id\" and \"name\" * }}} * * @group basic * @since 2.0.0  @scala.annotation.varargs def toDF(colNames: String*): DataFrame = { require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names (${colNames.size}): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) => Column(oldAttribute).as(newName) } select(newCols : _*) } /** * Returns the schema of this Dataset. * * @group basic * @since 1.6.0  def schema: StructType = sparkSession.withActive { queryExecution.analyzed.schema } /** * Prints the schema to the console in a nice tree format. * * @group basic * @since 1.6.0  def printSchema(): Unit = printSchema(Int.MaxValue) // scalastyle:off println /** * Prints the schema up to the given level to the console in a nice tree format. * * @group basic * @since 3.0.0  def printSchema(level: Int): Unit = println(schema.treeString(level)) // scalastyle:on println /** * Prints the plans (logical and physical) with a format specified by a given explain mode. * * @param mode specifies the expected output format of plans. * <ul> * <li>`simple` Print only a physical plan.</li> * <li>`extended`: Print both logical and physical plans.</li> * <li>`codegen`: Print a physical plan and generated codes if they are * available.</li> * <li>`cost`: Print a logical plan and statistics if they are available.</li> * <li>`formatted`: Split explain output into two sections: a physical plan outline * and node details.</li> * </ul> * @group basic * @since 3.0.0  def explain(mode: String): Unit = sparkSession.withActive { // Because temporary views are resolved during analysis when we create a Dataset, and // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using // `ExplainCommand` here will probably output different query plans, compared to the results // of evaluation of the Dataset. So just output QueryExecution's query plans here. // scalastyle:off println println(queryExecution.explainString(ExplainMode.fromString(mode))) // scalastyle:on println } /** * Prints the plans (logical and physical) to the console for debugging purposes. * * @param extended default `false`. If `false`, prints only the physical plan. * * @group basic * @since 1.6.0  def explain(extended: Boolean): Unit = if (extended) { explain(ExtendedMode.name) } else { explain(SimpleMode.name) } /** * Prints the physical plan to the console for debugging purposes. * * @group basic * @since 1.6.0  def explain(): Unit = explain(SimpleMode.name) /** * Returns all column names and their data types as an array. * * @group basic * @since 1.6.0  def dtypes: Array[(String, String)] = schema.fields.map { field => (field.name, field.dataType.toString) } /** * Returns all column names as an array. * * @group basic * @since 1.6.0  def columns: Array[String] = schema.fields.map(_.name) /** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * * @group basic * @since 1.6.0  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] || logicalPlan.isInstanceOf[CommandResult] /** * Returns true if the `Dataset` is empty. * * @group basic * @since 2.4.0  def isEmpty: Boolean = withAction(\"isEmpty\", select().queryExecution) { plan => plan.executeTake(1).isEmpty } /** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * @group streaming * @since 2.0.0  def isStreaming: Boolean = logicalPlan.isStreaming /** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true) /** * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be * used to truncate the logical plan of this Dataset, which is especially useful in iterative * algorithms where the plan may grow exponentially. Local checkpoints are written to executor * storage and despite potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false) /** * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. Local checkpoints are written to executor storage and despite * potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint( eager = eager, reliableCheckpoint = false ) /** * Returns a checkpointed version of this Dataset. * * @param eager Whether to checkpoint this dataframe immediately * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the * checkpoint directory. If false creates a local checkpoint using * the caching subsystem  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) \"checkpoint\" else \"localCheckpoint\" withAction(actionName, queryExecution) { physicalPlan => val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { internalRdd.checkpoint() } else { internalRdd.localCheckpoint() } if (eager) { internalRdd.doCheckpoint() } // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. @scala.annotation.tailrec def firstLeafPartitioning(partitioning: Partitioning): Partitioning = { partitioning match { case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head) case p => p } } val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering, isStreaming )(sparkSession)).as[T] } } /** * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * Spark will use this watermark for several purposes: * <ul> * <li>To know when a given time window aggregation can be finalized and thus can be emitted * when using output modes that do not allow updates.</li> * <li>To minimize the amount of state that we need to keep for on-going aggregations, * `mapGroupsWithState` and `dropDuplicates` operators.</li> * </ul> * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * @param eventTime the name of the column that contains the event time of the row. * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). NOTE: This should not be negative. * * @group streaming * @since 2.1.0  // We only accept an existing column name, not a derived column here as a watermark that is // defined on a derived column cannot referenced elsewhere in the plan. def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan { val parsedDelay = IntervalUtils.fromIntervalString(delayThreshold) require(!IntervalUtils.isNegative(parsedDelay), s\"delay threshold ($delayThreshold) should not be negative.\") EliminateEventTimeWatermark( EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)) } /** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * * @group action * @since 1.6.0  def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * * @group action * @since 1.6.0  def show(): Unit = show(20) /** * Displays the top 20 rows of Dataset in a tabular form. * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0  def show(truncate: Boolean): Unit = show(20, truncate) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * @param numRows Number of rows to show * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0  // scalastyle:off println def show(numRows: Int, truncate: Boolean): Unit = if (truncate) { println(showString(numRows, truncate = 20)) } else { println(showString(numRows, truncate = 0)) } /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @group action * @since 1.6.0  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * If `vertical` enabled, this command prints output rows vertically (one line per column value)? * * {{{ * -RECORD 0------------------- * year | 1980 * month | 12 * AVG('Adj Close) | 0.503218 * AVG('Adj Close) | 0.595103 * -RECORD 1------------------- * year | 1981 * month | 01 * AVG('Adj Close) | 0.523289 * AVG('Adj Close) | 0.570307 * -RECORD 2------------------- * year | 1982 * month | 02 * AVG('Adj Close) | 0.436504 * AVG('Adj Close) | 0.475256 * -RECORD 3------------------- * year | 1983 * month | 03 * AVG('Adj Close) | 0.410516 * AVG('Adj Close) | 0.442194 * -RECORD 4------------------- * year | 1984 * month | 04 * AVG('Adj Close) | 0.450090 * AVG('Adj Close) | 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). * @group action * @since 2.3.0  // scalastyle:off println def show(numRows: Int, truncate: Int, vertical: Boolean): Unit = println(showString(numRows, truncate, vertical)) // scalastyle:on println /** * Returns a [[DataFrameNaFunctions]] for working with missing data. * {{{ * // Dropping rows containing any null values. * ds.na.drop() * }}} * * @group untypedrel * @since 1.6.0  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) /** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * {{{ * // Finding frequent items in column with name 'a'. * ds.stat.freqItems(Seq(\"a\")) * }}} * * @group untypedrel * @since 1.6.0  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) /** * Join with another `DataFrame`. * * Behaves as an INNER JOIN and requires a subsequent join predicate. * * @param right Right side of the join operation. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE) } /** * Inner equi-join with another `DataFrame` using the given column. * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the column \"user_id\" * df1.join(df2, \"user_id\") * }}} * * @param right Right side of the join operation. * @param usingColumn Name of the column to join on. This column must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumn: String): DataFrame = { join(right, Seq(usingColumn)) } /** * Inner equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * }}} * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = { join(right, usingColumns, \"inner\") } /** * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate * is specified as an inner join. If you would explicitly like to perform a cross join use the * `crossJoin` method. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = { // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None, JoinHint.NONE)) .analyzed.asInstanceOf[Join] withPlan { Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None, JoinHint.NONE) } } /** * Inner join with another `DataFrame`, using the given join expression. * * {{{ * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * }}} * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * find the trivially true predicates and automatically resolves them to both sides.  private def resolveSelfJoinCondition(plan: Join): Join = { val resolver = sparkSession.sessionState.analyzer.resolver val cond = plan.condition.map { _.transform { case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualTo( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualNullSafe( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) }} plan.copy(condition = cond) } /** * find the trivially true predicates and automatically resolves them to both sides.  private def resolveSelfJoinCondition( right: Dataset[_], joinExprs: Option[Column], joinType: String): Join = { // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), joinExprs.map(_.expr), JoinHint.NONE)) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) { return plan } // If left/right have no output set intersection, return the plan. val lanalyzed = this.queryExecution.analyzed val ranalyzed = right.queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) { return plan } // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. resolveSelfJoinCondition(plan) } /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * {{{ * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * }}} * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @group untypedrel * @since 2.0.0  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = { withPlan { resolveSelfJoinCondition(right, Some(joinExprs), joinType) } } /** * Explicit cartesian join with another `DataFrame`. * * @param right Right side of the join operation. * * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * * @group untypedrel * @since 2.1.0  def crossJoin(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Cross, None, JoinHint.NONE) } /** * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * * @param other Right side of the join. * @param condition Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`,`full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`. * * @group typedrel * @since 1.6.0  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = { // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // etc. var joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr), JoinHint.NONE)).analyzed.asInstanceOf[Join] if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) { throw QueryCompilationErrors.invalidJoinTypeInJoinWithError(joined.joinType) } // If auto self join alias is enable if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) { joined = resolveSelfJoinCondition(joined) } implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) val leftResultExpr = { if (!this.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() } else { Alias(CreateStruct(joined.left.output), \"_1\")() } } val rightResultExpr = { if (!other.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() } else { Alias(CreateStruct(joined.right.output), \"_2\")() } } if (joined.joinType.isInstanceOf[InnerLike]) { // For inner joins, we can directly perform the join and then can project the join // results into structs. This ensures that data remains flat during shuffles / // exchanges (unlike the outer join path, which nests the data before shuffling). withTypedPlan(Project(Seq(leftResultExpr, rightResultExpr), joined)) } else { // outer joins // For both join sides, combine all outputs into a single column and alias it with \"_1 // or \"_2\", to match the schema for the encoder of the join result. // Note that we do this before joining them, to enable the join operator to return null // for one side, in cases like outer-join. val left = Project(leftResultExpr :: Nil, joined.left) val right = Project(rightResultExpr :: Nil, joined.right) // Rewrites the join condition to make the attribute point to correct column/field, // after we combine the outputs of each join side. val conditionExpr = joined.condition.get transformUp { case a: Attribute if joined.left.outputSet.contains(a) => if (!this.exprEnc.isSerializedAsStructForTopLevel) { left.output.head } else { val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) } case a: Attribute if joined.right.outputSet.contains(a) => if (!other.exprEnc.isSerializedAsStructForTopLevel) { right.output.head } else { val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) } } withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr), JoinHint.NONE)) } } /** * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * @param other Right side of the join. * @param condition Join expression. * * @group typedrel * @since 1.6.0  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = { joinWith(other, condition, \"inner\") } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, usingColumns: Seq[String], joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joinExprs = usingColumns.map { column => EqualTo(resolve(column), other.resolve(column)) }.reduceOption(And).map(Column.apply).orNull joinAsOf(other, leftAsOf, rightAsOf, joinExprs, joinType, tolerance, allowExactMatches, direction) } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, joinExprs: Column, joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joined = resolveSelfJoinCondition(other, Option(joinExprs), joinType) val leftAsOfExpr = leftAsOf.expr.transformUp { case a: AttributeReference if logicalPlan.outputSet.contains(a) => val index = logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.left.output(index) } val rightAsOfExpr = rightAsOf.expr.transformUp { case a: AttributeReference if other.logicalPlan.outputSet.contains(a) => val index = other.logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.right.output(index) } withPlan { AsOfJoin( joined.left, joined.right, leftAsOfExpr, rightAsOfExpr, joined.condition, joined.joinType, Option(tolerance).map(_.expr), allowExactMatches, AsOfJoinDirection(direction) ) } } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = { sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sortWithinPartitions(sortExprs: Column*): Dataset[T] = { sortInternal(global = false, sortExprs) } /** * Returns a new Dataset sorted by the specified column, all in ascending order. * {{{ * // The following 3 are equivalent * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * }}} * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = { sort((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset sorted by the given expressions. For example: * {{{ * ds.sort($\"col1\", $\"col2\".desc) * }}} * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = { sortInternal(global = true, sortExprs) } /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*) /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0  def apply(colName: String): Column = col(colName) /** * Specifies some hint on the current Dataset. As an example, the following code specifies * that one of the plan can be broadcasted: * * {{{ * df1.join(df2.hint(\"broadcast\")) * }}} * * @group basic * @since 2.2.0  @scala.annotation.varargs def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan { UnresolvedHint(name, parameters, logicalPlan) } /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0  def col(colName: String): Column = colName match { case \"*\" => Column(ResolvedStar(queryExecution.analyzed.output)) case _ => if (sqlContext.conf.supportQuotedRegexColumnName) { colRegex(colName) } else { Column(addDataFrameIdToCol(resolve(colName))) } } // Attach the dataset id and column position to the column reference, so that we can detect // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`. // This must be called before we return a `Column` that contains `AttributeReference`. // Note that, the metadata added here are only available in the analyzer, as the analyzer rule // `DetectAmbiguousSelfJoin` will remove it. private def addDataFrameIdToCol(expr: NamedExpression): NamedExpression = { val newExpr = expr transform { case a: AttributeReference if sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) => val metadata = new MetadataBuilder() .withMetadata(a.metadata) .putLong(Dataset.DATASET_ID_KEY, id) .putLong(Dataset.COL_POS_KEY, logicalPlan.output.indexWhere(a.semanticEquals)) .build() a.withMetadata(metadata) } newExpr.asInstanceOf[NamedExpression] } /** * Selects column based on the column name specified as a regex and returns it as [[Column]]. * @group untypedrel * @since 2.3.0  def colRegex(colName: String): Column = { val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis colName match { case ParserUtils.escapedIdentifier(columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, None, caseSensitive)) case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive)) case _ => Column(addDataFrameIdToCol(resolve(colName))) } } /** * Returns a new Dataset with an alias set. * * @group typedrel * @since 1.6.0  def as(alias: String): Dataset[T] = withTypedPlan { SubqueryAlias(alias, logicalPlan) } /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0  def as(alias: Symbol): Dataset[T] = as(alias.name) /** * Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0  def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0  def alias(alias: Symbol): Dataset[T] = as(alias) /** * Selects a set of column based expressions. * {{{ * ds.select($\"colA\", $\"colB\" + 1) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def select(cols: Column*): DataFrame = withPlan { val untypedCols = cols.map { case typedCol: TypedColumn[_, _] => // Checks if a `TypedColumn` has been inserted with // specific input type and schema by `withInputType`. val needInputType = typedCol.expr.exists { case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty => true case _ => false } if (!needInputType) { typedCol } else { throw QueryCompilationErrors.cannotPassTypedColumnInUntypedSelectError(typedCol.toString) } case other => other } Project(untypedCols.map(_.named), logicalPlan) } /** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * {{{ * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*) /** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * {{{ * // The following are equivalent: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def selectExpr(exprs: String*): DataFrame = { select(exprs.map { expr => Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) }: _*) } /** * Returns a new Dataset by computing the given [[Column]] expression for each element. * * {{{ * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * }}} * * @group typedrel * @since 1.6.0  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = { implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (!encoder.isSerializedAsStructForTopLevel) { new Dataset[U1](sparkSession, project, encoder) } else { // Flattens inner fields of U1 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) } } /** * Internal helper function for building typed selects that return tuples. For simplicity and * code reuse, we do this without the help of the type system and then use helper functions * that cast appropriately for the user facing interface.  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = { val encoders = columns.map(_.encoder) val namedColumns = columns.map(_.withInputType(exprEnc, logicalPlan.output).named) val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan)) new Dataset(execution, ExpressionEncoder.tuple(encoders)) } /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0  def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] /** * Filters rows using the given condition. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0  def filter(condition: Column): Dataset[T] = withTypedPlan { Filter(condition.expr, logicalPlan) } /** * Filters rows using the given SQL expression. * {{{ * peopleDs.filter(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0  def filter(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Filters rows using the given condition. This is an alias for `filter`. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0  def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * {{{ * peopleDs.where(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0  def where(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def groupBy(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType) } /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) } /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def groupBy(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType) } /** * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0  def reduce(func: (T, T) => T): T = withNewRDDExecutionId { rdd.reduce(func) } /** * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) /** * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = { val withGroupingKey = AppendColumns(func, logicalPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, logicalPlan.output, withGroupingKey.newColumns) } /** * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * }}} * * @group untypedrel * @since 2.0.0  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = { groupBy().agg(aggExpr, aggExprs : _*) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * }}} * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*) /** * Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset * that returns the same result as the input, with the following guarantees: * <ul> * <li>It will compute the defined aggregates (metrics) on all the data that is flowing through * the Dataset at that point.</li> * <li>It will report the value of the defined aggregate columns as soon as we reach a completion * point. A completion point is either the end of a query (batch mode) or the end of a streaming * epoch. The value of the aggregates only reflects the data processed since the previous * completion point.</li> * </ul> * Please note that continuous execution is currently not supported. * * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that * contain references to the input Dataset's columns must always be wrapped in an aggregate * function. * * A user can observe these metrics by either adding * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session. * * {{{ * // Monitor the metrics using a listener. * spark.streams.addListener(new StreamingQueryListener() { * override def onQueryStarted(event: QueryStartedEvent): Unit = {} * override def onQueryProgress(event: QueryProgressEvent): Unit = { * event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row => * // Trigger if the number of errors exceeds 5 percent * val num_rows = row.getAs[Long](\"rc\") * val num_error_rows = row.getAs[Long](\"erc\") * val ratio = num_error_rows.toDouble / num_rows * if (ratio > 0.05) { * // Trigger alert * } * } * } * override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {} * }) * // Observe row count (rc) and error row count (erc) in the streaming Dataset * val observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\")) * observed_ds.writeStream.format(\"...\").start() * }}} * * @group typedrel * @since 3.0.0  @varargs def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan { CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan) } /** * Observe (named) metrics through an `org.apache.spark.sql.Observation` instance. * This is equivalent to calling `observe(String, Column, Column*)` but does not require * adding `org.apache.spark.sql.util.QueryExecutionListener` to the spark session. * This method does not support streaming datasets. * * A user can retrieve the metrics by accessing `org.apache.spark.sql.Observation.get`. * * {{{ * // Observe row count (rows) and highest id (maxid) in the Dataset while writing it * val observation = Observation(\"my_metrics\") * val observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\")) * observed_ds.write.parquet(\"ds.parquet\") * val metrics = observation.get * }}} * * @throws IllegalArgumentException If this is a streaming Dataset (this.isStreaming == true) * * @group typedrel * @since 3.3.0  @varargs def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = { observation.on(this, expr, exprs: _*) } /** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * @group typedrel * @since 2.0.0  def limit(n: Int): Dataset[T] = withTypedPlan { Limit(Literal(n), logicalPlan) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.union(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 4| 5| 6| * // +----+----+----+ * }}} * * Notice that the column positions in the schema aren't necessarily matched with the * fields in the strongly typed objects in a Dataset. This function resolves columns * by their positions in the schema, not the fields in the strongly typed objects. Use * [[unionByName]] to resolve columns by field name in the typed objects. * * @group typedrel * @since 2.0.0  def union(other: Dataset[T]): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan, other.logicalPlan)) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is an alias for `union`. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.0.0  def unionAll(other: Dataset[T]): Dataset[T] = union(other) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set * union (that does deduplication of elements), use this function followed by a [[distinct]]. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.unionByName(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 6| 4| 5| * // +----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. Nested columns in map types * are not currently supported. * * @group typedrel * @since 2.3.0  def unionByName(other: Dataset[T]): Dataset[T] = unionByName(other, false) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position). * * When the parameter `allowMissingColumns` is `true`, the set of column names * in this and other `Dataset` can differ; missing columns will be filled with null. * Further, the missing columns of this `Dataset` will be added at the end * in the schema of the union result: * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col0\", \"col3\") * df1.unionByName(df2, true).show * * // output: \"col3\" is missing at left df1 and added at the end of schema. * // +----+----+----+----+ * // |col0|col1|col2|col3| * // +----+----+----+----+ * // | 1| 2| 3|null| * // | 5| 4|null| 6| * // +----+----+----+----+ * * df2.unionByName(df1, true).show * * // output: \"col2\" is missing at left df2 and added at the end of schema. * // +----+----+----+----+ * // |col1|col0|col3|col2| * // +----+----+----+----+ * // | 4| 5| 6|null| * // | 2| 1|null| 3| * // +----+----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. With `allowMissingColumns`, * missing nested columns of struct columns with the same name will also be filled with null * values and added to the end of struct. Nested columns in map types are not currently * supported. * * @group typedrel * @since 3.1.0  def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns)) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 1.6.0  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset while * preserving the duplicates. * This is equivalent to `INTERSECT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard * in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT DISTINCT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0  def except(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset while * preserving the duplicates. * This is equivalent to `EXCEPT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in * SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a user-supplied seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0  def sample(fraction: Double, seed: Long): Dataset[T] = { sample(withReplacement = false, fraction = fraction, seed = seed) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a random seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0  def sample(fraction: Double): Dataset[T] = { sample(withReplacement = false, fraction = fraction) } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = { withTypedPlan { Sample(0.0, fraction, withReplacement, seed, logicalPlan) } } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = { sample(withReplacement, fraction, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * For Java API, use [[randomSplitAsList]]. * * @group typedrel * @since 2.0.0  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out // from the sort order. val sortOrder = logicalPlan.output .filter(attr => RowOrdering.isOrderable(attr.dataType)) .map(SortOrder(_, Ascending)) val plan = if (sortOrder.nonEmpty) { Sort(sortOrder, global = false, logicalPlan) } else { // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism cache() logicalPlan } val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder) }.toArray } /** * Returns a Java list that contains randomly split Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * @group typedrel * @since 2.0.0  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = { val values = randomSplit(weights, seed) java.util.Arrays.asList(values : _*) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @group typedrel * @since 2.0.0  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = { randomSplit(weights, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling.  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = { randomSplit(weights.toArray, seed) } /** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * {{{ * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select($\"title\", explode(split($\"words\", \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(count_distinct(\"title\")) * }}} * * Using `flatMap()` this can similarly be exploded as: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = { val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * {{{ * ds.select(explode(split($\"words\", \" \")).as(\"word\")) * }}} * * or `flatMap()`: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B]) : DataFrame = { val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = { val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o))) } val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * * `column`'s expression must only refer to attributes supplied by this Dataset. It is an * error to add a column that refers to some other Dataset. * * @note this method introduces a projection internally. Therefore, calling it multiple times, * for instance, via loops in order to add multiple columns can generate big plans which * can cause performance issues and even `StackOverflowException`. To avoid this, * use `select` with the multiple columns at once. * * @group untypedrel * @since 2.0.0  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col)) /** * (Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attributes * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0  def withColumns(colsMap: Map[String, Column]): DataFrame = { val (colNames, newCols) = colsMap.toSeq.unzip withColumns(colNames, newCols) } /** * (Java-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attribute * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0  def withColumns(colsMap: java.util.Map[String, Column]): DataFrame = withColumns( colsMap.asScala.toMap ) /** * Returns a new Dataset by adding columns or replacing the existing columns that has * the same names.  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = { require(colNames.size == cols.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of columns: ${cols.size}\") SchemaUtils.checkColumnNameDuplication( colNames, \"in given column names\", sparkSession.sessionState.conf.caseSensitiveAnalysis) val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val columnSeq = colNames.zip(cols) val replacedAndExistingColumns = output.map { field => columnSeq.find { case (colName, _) => resolver(field.name, colName) } match { case Some((colName: String, col: Column)) => col.as(colName) case _ => Column(field) } } val newColumns = columnSeq.filter { case (colName, col) => !output.exists(f => resolver(f.name, colName)) }.map { case (colName, col) => col.as(colName) } select(replacedAndExistingColumns ++ newColumns : _*) } /** * Returns a new Dataset by adding columns with metadata.  private[spark] def withColumns( colNames: Seq[String], cols: Seq[Column], metadata: Seq[Metadata]): DataFrame = { require(colNames.size == metadata.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of metadata elements: ${metadata.size}\") val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) => col.as(colName, metadata) } withColumns(colNames, newCols) } /** * Returns a new Dataset by adding a column with metadata.  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = withColumns(Seq(colName), Seq(col), Seq(metadata)) /** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * * @group untypedrel * @since 2.0.0  def withColumnRenamed(existingName: String, newName: String): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f => resolver(f.name, existingName)) if (shouldRename) { val columns = output.map { col => if (resolver(col.name, existingName)) { Column(col).as(newName) } else { Column(col) } } select(columns : _*) } else { toDF() } } /** * Returns a new Dataset by updating an existing column with metadata. * * @group untypedrel * @since 3.3.0  def withMetadata(columnName: String, metadata: Metadata): DataFrame = { withColumn(columnName, col(columnName), metadata) } /** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * @group untypedrel * @since 2.0.0  def drop(colName: String): DataFrame = { drop(Seq(colName) : _*) } /** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0  @scala.annotation.varargs def drop(colNames: String*): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter { attribute => colNames.forall(n => !resolver(attribute.name, n)) }.map(attribute => Column(attribute)) if (remainingCols.size == allColumns.size) { toDF() } else { this.select(remainingCols: _*) } } /** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * @group untypedrel * @since 2.0.0  def drop(col: Column): DataFrame = { val expression = col match { case Column(u: UnresolvedAttribute) => queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) => expr } val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter { attr => !attr.semanticEquals(expression) }.map(attr => Column(attr)) select(colsAfterDrop : _*) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns) /** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue // (the Streaming's state store depends on the `groupCols` order). val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) => // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col => resolver(col.name, colName)) if (cols.isEmpty) { throw QueryCompilationErrors.cannotResolveColumnNameAmongAttributesError( colName, schema.fieldNames.mkString(\", \")) } cols } Deduplicate(groupCols, logicalPlan) } /** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq) /** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def dropDuplicates(col1: String, cols: String*): Dataset[T] = { val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames) } /** * Computes basic statistics for numeric and string columns, including count, mean, stddev, min, * and max. If no columns are given, this function computes statistics for all numerical or * string columns. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * }}} * * Use [[summary]] for expanded statistics and control over which statistics to compute. * * @param cols Columns to compute statistics on. * * @group action * @since 1.6.0  @scala.annotation.varargs def describe(cols: String*): DataFrame = { val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*) selected.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\") } /** * Computes specified statistics for numeric and string columns. Available statistics are: * <ul> * <li>count</li> * <li>mean</li> * <li>stddev</li> * <li>min</li> * <li>max</li> * <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li> * <li>count_distinct</li> * <li>approx_count_distinct</li> * </ul> * * If no statistics are given, this function computes count, mean, stddev, min, * approximate quartiles (percentiles at 25%, 50%, and 75%), and max. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.summary().show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 50% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * {{{ * ds.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show() * * // output: * // summary age height * // count 10.0 10.0 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * To do a summary for specific columns first select them: * * {{{ * ds.select(\"age\", \"height\").summary().show() * }}} * * Specify statistics to output custom summaries: * * {{{ * ds.summary(\"count\", \"count_distinct\").show() * }}} * * The distinct count isn't included by default. * * You can also run approximate distinct counts which are faster: * * {{{ * ds.summary(\"count\", \"approx_count_distinct\").show() * }}} * * See also [[describe]] for basic statistics. * * @param statistics Statistics from above list to be computed. * * @group action * @since 2.3.0  @scala.annotation.varargs def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq) /** * Returns the first `n` rows. * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @group action * @since 1.6.0  def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) /** * Returns the first row. * @group action * @since 1.6.0  def head(): T = head(1).head /** * Returns the first row. Alias for head(). * @group action * @since 1.6.0  def first(): T = head() /** * Concise syntax for chaining custom transformations. * {{{ * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * }}} * * @group typedrel * @since 1.6.0  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this) /** * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0  def filter(func: T => Boolean): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0  def filter(func: FilterFunction[T]): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan { MapElements[T, U](func, logicalPlan) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * @group typedrel * @since 1.6.0  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = { new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]]) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * @group typedrel * @since 1.6.0  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala mapPartitions(func)(encoder) } /** * Returns a new `DataFrame` that contains the result of applying a serialized R function * `func` to each partition.  private[sql] def mapPartitionsInR( func: Array[Byte], packageNames: Array[Byte], broadcastVars: Array[Broadcast[Object]], schema: StructType): DataFrame = { val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]] Dataset.ofRows( sparkSession, MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan)) } /** * Applies a Scalar iterator Pandas UDF to each partition. The user-defined function * defines a transformation: `iter(pandas.DataFrame)` -> `iter(pandas.DataFrame)`. * Each partition is each iterator consisting of DataFrames as batches. * * This function uses Apache Arrow as serialization format between Java executors and Python * workers.  private[sql] def mapInPandas(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, MapInPandas( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * Applies a function to each partition in Arrow format. The user-defined function * defines a transformation: `iter(pyarrow.RecordBatch)` -> `iter(pyarrow.RecordBatch)`. * Each partition is each iterator consisting of `pyarrow.RecordBatch`s as batches.  private[sql] def pythonMapInArrow(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, PythonMapInArrow( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func)) /** * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (T) => Iterator[U] = x => f.call(x).asScala flatMap(func)(encoder) } /** * Applies a function `f` to all rows. * * @group action * @since 1.6.0  def foreach(f: T => Unit): Unit = withNewRDDExecutionId { rdd.foreach(f) } /** * (Java-specific) * Runs `func` on each element of this Dataset. * * @group action * @since 1.6.0  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) /** * Applies a function `f` to each partition of this Dataset. * * @group action * @since 1.6.0  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId { rdd.foreachPartition(f) } /** * (Java-specific) * Runs `func` on each partition of this Dataset. * * @group action * @since 1.6.0  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = { foreachPartition((it: Iterator[T]) => func.call(it.asJava)) } /** * Returns the first `n` rows in the Dataset. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def take(n: Int): Array[T] = head(n) /** * Returns the last `n` rows in the Dataset. * * Running tail requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 3.0.0  def tail(n: Int): Array[T] = withAction( \"tail\", withTypedPlan(Tail(Literal(n), logicalPlan)).queryExecution)(collectFromPlan) /** * Returns the first `n` rows in the Dataset as a list. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*) /** * Returns an array that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0  def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) /** * Returns a Java list that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0  def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan => val values = collectFromPlan(plan) java.util.Arrays.asList(values : _*) } /** * Returns an iterator that contains all rows in this Dataset. * * The iterator will consume as much memory as the largest partition in this Dataset. * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * * @group action * @since 2.0.0  def toLocalIterator(): java.util.Iterator[T] = { withAction(\"toLocalIterator\", queryExecution) { plan => val fromRow = resolvedEnc.createDeserializer() plan.executeToIterator().map(fromRow).asJava } } /** * Returns the number of rows in the Dataset. * @group action * @since 1.6.0  def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan => plan.executeCollect().head.getLong(0) } /** * Returns a new Dataset that has exactly `numPartitions` partitions. * * @group typedrel * @since 1.6.0  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = true, logicalPlan) } private def repartitionByExpression( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments. // However, we don't want to complicate the semantics of this API method. // Instead, let's give users a friendly error message, pointing them to the new method. val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder]) if (sortOrders.nonEmpty) throw new IllegalArgumentException( s\"\"\"Invalid partitionExprs specified: $sortOrders |For range partitioning use repartitionByRange(...) instead. \"\"\".stripMargin) withTypedPlan { RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByExpression(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0  @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = { repartitionByExpression(None, partitionExprs) } private def repartitionByRange( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { require(partitionExprs.nonEmpty, \"At least one partition-by expression must be specified.\") val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) }) withTypedPlan { RepartitionByExpression(sortOrder, logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0  @scala.annotation.varargs def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByRange(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0  @scala.annotation.varargs def repartitionByRange(partitionExprs: Column*): Dataset[T] = { repartitionByRange(None, partitionExprs) } /** * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions * are requested. If a larger number of partitions is requested, it will stay at the current * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can call repartition. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @group typedrel * @since 1.6.0  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = false, logicalPlan) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * Note that for a streaming [[Dataset]], this method returns distinct rows only once * regardless of the output mode, which the behavior may not be same with `DISTINCT` in SQL * against streaming [[Dataset]]. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0  def distinct(): Dataset[T] = dropDuplicates() /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0  def persist(): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this) this } /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0  def cache(): this.type = persist() /** * Persist this Dataset with the given storage level. * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, * `MEMORY_AND_DISK_2`, etc. * * @group basic * @since 1.6.0  def persist(newLevel: StorageLevel): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this } /** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * @group basic * @since 2.1.0  def storageLevel: StorageLevel = { sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData => cachedData.cachedRepresentation.cacheBuilder.storageLevel }.getOrElse(StorageLevel.NONE) } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @param blocking Whether to block until all blocks are deleted. * * @group basic * @since 1.6.0  def unpersist(blocking: Boolean): this.type = { sparkSession.sharedState.cacheManager.uncacheQuery( sparkSession, logicalPlan, cascade = false, blocking) this } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @group basic * @since 1.6.0  def unpersist(): this.type = unpersist(blocking = false) // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`. @transient private lazy val rddQueryExecution: QueryExecution = { val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized) } /** * Represents the content of the Dataset as an `RDD` of `T`. * * @group basic * @since 1.6.0  lazy val rdd: RDD[T] = { val objectType = exprEnc.deserializer.dataType rddQueryExecution.toRdd.mapPartitions { rows => rows.map(_.get(0, objectType).asInstanceOf[T]) } } /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0  def javaRDD: JavaRDD[T] = toJavaRDD /** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 1.6.0  @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\") def registerTempTable(tableName: String): Unit = { createOrReplaceTempView(tableName) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.0.0  @throws[AnalysisException] def createTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = false) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 2.0.0  def createOrReplaceTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = false) } /** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.1.0  @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) } /** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) } private def createTempViewCommand( viewName: String, replace: Boolean, global: Boolean): CreateViewCommand = { val viewType = if (global) GlobalTempView else LocalTempView val tableIdentifier = try { sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName) } catch { case _: ParseException => throw QueryCompilationErrors.invalidViewNameError(viewName) } CreateViewCommand( name = tableIdentifier, userSpecifiedColumns = Nil, comment = None, properties = Map.empty, originalText = None, plan = logicalPlan, allowExisting = false, replace = replace, viewType = viewType, isAnalyzed = true) } /** * Interface for saving the content of the non-streaming Dataset out into external storage. * * @group basic * @since 1.6.0  def write: DataFrameWriter[T] = { if (isStreaming) { logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriter[T](this) } /** * Create a write configuration builder for v2 sources. * * This builder is used to configure and execute write operations. For example, to append to an * existing table, run: * * {{{ * df.writeTo(\"catalog.db.table\").append() * }}} * * This can also be used to create or replace existing tables: * * {{{ * df.writeTo(\"catalog.db.table\").partitionedBy($\"col\").createOrReplace() * }}} * * @group basic * @since 3.0.0  def writeTo(table: String): DataFrameWriterV2[T] = { // TODO: streaming could be adapted to use this interface if (isStreaming) { logicalPlan.failAnalysis( \"'writeTo' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriterV2[T](table, this) } /** * Interface for saving the content of the streaming Dataset out into external storage. * * @group basic * @since 2.0.0  def writeStream: DataStreamWriter[T] = { if (!isStreaming) { logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") } new DataStreamWriter[T](this) } /** * Returns the content of the Dataset as a Dataset of JSON strings. * @since 2.0.0  def toJSON: Dataset[String] = { val rowSchema = this.schema val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone mapPartitions { iter => val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer, new JSONOptions(Map.empty[String, String], sessionLocalTimeZone)) new Iterator[String] { private val toRow = exprEnc.createSerializer() override def hasNext: Boolean = iter.hasNext override def next(): String = { gen.write(toRow(iter.next())) gen.flush() val json = writer.toString if (hasNext) { writer.reset() } else { gen.close() } json } } } (Encoders.STRING) } /** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * @group basic * @since 2.0.0  def inputFiles: Array[String] = { val files: Seq[String] = queryExecution.optimizedPlan.collect { case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) => fsBasedRelation.inputFiles case fr: FileRelation => fr.inputFiles case r: HiveTableRelation => r.tableMeta.storage.locationUri.map(_.toString).toArray case DataSourceV2ScanRelation(DataSourceV2Relation(table: FileTable, _, _, _, _), _, _, _) => table.fileIndex.inputFiles }.flatten files.toSet.toArray } /** * Returns `true` when the logical query plans inside both [[Dataset]]s are equal and * therefore return same results. * * @note The equality comparison here is simplified by tolerating the cosmetic differences * such as attribute names. * @note This API can compare both [[Dataset]]s very fast but can still return `false` on * the [[Dataset]] that return the same results, for instance, from different plans. Such * false negative semantic can be useful when caching as an example. * @since 3.1.0  @DeveloperApi def sameSemantics(other: Dataset[T]): Boolean = { queryExecution.analyzed.sameResult(other.queryExecution.analyzed) } /** * Returns a `hashCode` of the logical query plan against this [[Dataset]]. * * @note Unlike the standard `hashCode`, the hash is calculated against the query plan * simplified by tolerating the cosmetic differences such as attribute names. * @since 3.1.0  @DeveloperApi def semanticHash(): Int = { queryExecution.analyzed.semanticHash() } //////////////////////////////////////////////////////////////////////////// // For Python API //////////////////////////////////////////////////////////////////////////// /** * It adds a new long column with the name `name` that increases one by one. * This is for 'distributed-sequence' default index in pandas API on Spark.  private[sql] def withSequenceColumn(name: String) = { Dataset.ofRows( sparkSession, AttachDistributedSequence( AttributeReference(name, LongType, nullable = false)(), logicalPlan)) } /** * Converts a JavaRDD to a PythonRDD.  private[sql] def javaToPython: JavaRDD[Array[Byte]] = { val structType = schema // capture it for closure val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType)) EvaluatePython.javaToPython(rdd) } private[sql] def collectToPython(): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"collectToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeCollect().iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def tailToPython(n: Int): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"tailToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeTail(n).iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def getRowsToPython( _numRows: Int, truncate: Int): Array[Any] = { EvaluatePython.registerPicklers() val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) val rows = getRows(numRows, truncate).map(_.toArray).toArray val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType))) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( rows.iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-GetRows\") } /** * Collect a Dataset as Arrow batches and serve stream to SparkR. It sends * arrow batches in an ordered manner with buffering. This is inevitable * due to missing R API that reads batches from socket directly. See ARROW-4512. * Eventually, this code should be deduplicated by `collectAsArrowToPython`.  private[sql] def collectAsArrowToR(): Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone RRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToR\", queryExecution) { plan => val buffer = new ByteArrayOutputStream() val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, buffer, timeZoneId) val arrowBatchRdd = toArrowBatchRdd(plan) val numPartitions = arrowBatchRdd.partitions.length // Store collection results for worst case of 1 to N-1 partitions val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1)) var lastIndex = -1 // index of last partition written // Handler to eagerly write partitions to Python in order def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = { // If result is from next partition in order if (index - 1 == lastIndex) { batchWriter.writeBatches(arrowBatches.iterator) lastIndex += 1 // Write stored partitions that come next in order while (lastIndex < results.length && results(lastIndex) != null) { batchWriter.writeBatches(results(lastIndex).iterator) results(lastIndex) = null lastIndex += 1 } // After last batch, end the stream if (lastIndex == results.length) { batchWriter.end() val batches = buffer.toByteArray out.writeInt(batches.length) out.write(batches) } } else { // Store partitions received out of order results(index - 1) = arrowBatches } } sparkSession.sparkContext.runJob( arrowBatchRdd, (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray, 0 until numPartitions, handlePartitionBatches) } } } /** * Collect a Dataset as Arrow batches and serve stream to PySpark. It sends * arrow batches in an un-ordered manner without buffering, and then batch order * information at the end. The batches should be reordered at Python side.  private[sql] def collectAsArrowToPython: Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone PythonRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToPython\", queryExecution) { plan => val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId) // Batches ordered by (index of partition, batch index in that partition) tuple val batchOrder = ArrayBuffer.empty[(Int, Int)] // Handler to eagerly write batches to Python as they arrive, un-ordered val handlePartitionBatches = (index: Int, arrowBatches: Array[Array[Byte]]) => if (arrowBatches.nonEmpty) { // Write all batches (can be more than 1) in the partition, store the batch order tuple batchWriter.writeBatches(arrowBatches.iterator) arrowBatches.indices.foreach { partitionBatchIndex => batchOrder.append((index, partitionBatchIndex)) } } Utils.tryWithSafeFinally { val arrowBatchRdd = toArrowBatchRdd(plan) sparkSession.sparkContext.runJob( arrowBatchRdd, (it: Iterator[Array[Byte]]) => it.toArray, handlePartitionBatches) } { // After processing all partitions, end the batch stream batchWriter.end() // Write batch order indices out.writeInt(batchOrder.length) // Sort by (index of partition, batch index in that partition) tuple to get the // overall_batch_index from 0 to N-1 batches, which can be used to put the // transferred batches in the correct order batchOrder.zipWithIndex.sortBy(_._1).foreach { case (_, overallBatchIndex) => out.writeInt(overallBatchIndex) } } } } } private[sql] def toPythonIterator(prefetchPartitions: Boolean = false): Array[Any] = { withNewExecutionId { PythonRDD.toLocalIteratorAndServe(javaToPython.rdd, prefetchPartitions) } } //////////////////////////////////////////////////////////////////////////// // Private Helpers //////////////////////////////////////////////////////////////////////////// /** * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with * an execution.  private def withNewExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(queryExecution)(body) } /** * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect * them with an execution. Before performing the action, the metrics of the executed plan will be * reset.  private def withNewRDDExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(rddQueryExecution) { rddQueryExecution.executedPlan.resetMetrics() body } } /** * Wrap a Dataset action to track the QueryExecution and time cost, then report to the * user-registered callback functions, and also to convert asserts/NPE to * the internal error exception.  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } /** * Collect all elements from a spark plan.  private def collectFromPlan(plan: SparkPlan): Array[T] = { val fromRow = resolvedEnc.createDeserializer() plan.executeCollect().map(fromRow) } private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = { val sortOrder: Seq[SortOrder] = sortExprs.map { col => col.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) } } withTypedPlan { Sort(sortOrder, global = global, logicalPlan) } } /** A convenient function to wrap a logical plan and produce a DataFrame.  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = { Dataset.ofRows(sparkSession, logicalPlan) } /** A convenient function to wrap a logical plan and produce a Dataset.  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { Dataset(sparkSession, logicalPlan) } /** A convenient function to wrap a set based logical plan and produce a Dataset.  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) { // Set operators widen types (change the schema), so we cannot reuse the row encoder. Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]] } else { Dataset(sparkSession, logicalPlan) } } /** Convert to an RDD of serialized ArrowRecordBatches.  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = { val schemaCaptured = this.schema val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone plan.execute().mapPartitionsInternal { iter => val context = TaskContext.get() ArrowConverters.toBatchIterator( iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context) } } // This is only used in tests, for now. private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = { toArrowBatchRdd(queryExecution.executedPlan) } }",
            "## CLASS: org/apache/spark/SparkContext#\n* this config overrides the default configs as well as system properties.  class SparkContext(config: SparkConf) extends Logging { // The call site where this SparkContext was constructed. private val creationSite: CallSite = Utils.getCallSite() if (!config.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) { // In order to prevent SparkContext from being created in executors. SparkContext.assertOnDriver() } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having started construction. // NOTE: this must be placed at the beginning of the SparkContext constructor. SparkContext.markPartiallyConstructed(this) val startTime = System.currentTimeMillis() private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false) private[spark] def assertNotStopped(): Unit = { if (stopped.get()) { val activeContext = SparkContext.activeContext.get() val activeCreationSite = if (activeContext == null) { \"(No active SparkContext.)\" } else { activeContext.creationSite.longForm } throw new IllegalStateException( s\"\"\"Cannot call methods on a stopped SparkContext. |This stopped SparkContext was created at: | |${creationSite.longForm} | |The currently active SparkContext was created at: | |$activeCreationSite \"\"\".stripMargin) } } /** * Create a SparkContext that loads settings from system properties (for instance, when * launching with ./bin/spark-submit).  def this() = this(new SparkConf()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters  def this(master: String, appName: String, conf: SparkConf) = this(SparkContext.updatedConf(conf, master, appName)) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs. * @param environment Environment variables to set on worker nodes.  def this( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) = { this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment)) } // The following constructors are required when Java code accesses SparkContext directly. // Please see SI-4278 /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI.  private[spark] def this(master: String, appName: String) = this(master, appName, null, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes.  private[spark] def this(master: String, appName: String, sparkHome: String) = this(master, appName, sparkHome, Nil, Map()) /** * Alternative constructor that allows setting common Spark properties directly * * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]). * @param appName A name for your application, to display on the cluster web UI. * @param sparkHome Location where Spark is installed on cluster nodes. * @param jars Collection of JARs to send to the cluster. These can be paths on the local file * system or HDFS, HTTP, HTTPS, or FTP URLs.  private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) = this(master, appName, sparkHome, jars, Map()) // log out Spark Version in Spark driver log logInfo(s\"Running Spark version $SPARK_VERSION\") /* ------------------------------------------------------------------------------------- * | Private variables. These variables keep the internal state of the context, and are | | not accessible by the outside world. They're mutable since we want to initialize all | | of them to some neutral value ahead of time, so that calling \"stop()\" while the | | constructor is still running is safe. | * -------------------------------------------------------------------------------------  private var _conf: SparkConf = _ private var _eventLogDir: Option[URI] = None private var _eventLogCodec: Option[String] = None private var _listenerBus: LiveListenerBus = _ private var _env: SparkEnv = _ private var _statusTracker: SparkStatusTracker = _ private var _progressBar: Option[ConsoleProgressBar] = None private var _ui: Option[SparkUI] = None private var _hadoopConfiguration: Configuration = _ private var _executorMemory: Int = _ private var _schedulerBackend: SchedulerBackend = _ private var _taskScheduler: TaskScheduler = _ private var _heartbeatReceiver: RpcEndpointRef = _ @volatile private var _dagScheduler: DAGScheduler = _ private var _applicationId: String = _ private var _applicationAttemptId: Option[String] = None private var _eventLogger: Option[EventLoggingListener] = None private var _driverLogger: Option[DriverLogger] = None private var _executorAllocationManager: Option[ExecutorAllocationManager] = None private var _cleaner: Option[ContextCleaner] = None private var _listenerBusStarted: Boolean = false private var _jars: Seq[String] = _ private var _files: Seq[String] = _ private var _archives: Seq[String] = _ private var _shutdownHookRef: AnyRef = _ private var _statusStore: AppStatusStore = _ private var _heartbeater: Heartbeater = _ private var _resources: immutable.Map[String, ResourceInformation] = _ private var _shuffleDriverComponents: ShuffleDriverComponents = _ private var _plugins: Option[PluginContainer] = None private var _resourceProfileManager: ResourceProfileManager = _ /* ------------------------------------------------------------------------------------- * | Accessors and public fields. These provide access to the internal state of the | | context. | * -------------------------------------------------------------------------------------  private[spark] def conf: SparkConf = _conf /** * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be * changed at runtime.  def getConf: SparkConf = conf.clone() def resources: Map[String, ResourceInformation] = _resources def jars: Seq[String] = _jars def files: Seq[String] = _files def archives: Seq[String] = _archives def master: String = _conf.get(\"spark.master\") def deployMode: String = _conf.get(SUBMIT_DEPLOY_MODE) def appName: String = _conf.get(\"spark.app.name\") private[spark] def isEventLogEnabled: Boolean = _conf.get(EVENT_LOG_ENABLED) private[spark] def eventLogDir: Option[URI] = _eventLogDir private[spark] def eventLogCodec: Option[String] = _eventLogCodec def isLocal: Boolean = Utils.isLocalMaster(_conf) /** * @return true if context is stopped or in the midst of stopping.  def isStopped: Boolean = stopped.get() private[spark] def statusStore: AppStatusStore = _statusStore // An asynchronous listener bus for Spark events private[spark] def listenerBus: LiveListenerBus = _listenerBus // This function allows components created by SparkEnv to be mocked in unit tests: private[spark] def createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv = { SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf)) } private[spark] def env: SparkEnv = _env // Used to store a URL for each static file/jar together with the file's local timestamp private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedArchives = new ConcurrentHashMap[String, Long]().asScala private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala // Keeps track of all persisted RDDs private[spark] val persistentRdds = { val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]() map.asScala } def statusTracker: SparkStatusTracker = _statusTracker private[spark] def progressBar: Option[ConsoleProgressBar] = _progressBar private[spark] def ui: Option[SparkUI] = _ui def uiWebUrl: Option[String] = _ui.map(_.webUrl) /** * A default Hadoop Configuration for the Hadoop code (e.g. file systems) that we reuse. * * @note As it will be reused in all Hadoop RDDs, it's better not to modify it unless you * plan to set some global configurations for all Hadoop RDDs.  def hadoopConfiguration: Configuration = _hadoopConfiguration private[spark] def executorMemory: Int = _executorMemory // Environment variables to pass to our executors. private[spark] val executorEnvs = HashMap[String, String]() // Set SPARK_USER for user who is running SparkContext. val sparkUser = Utils.getCurrentUserName() private[spark] def schedulerBackend: SchedulerBackend = _schedulerBackend private[spark] def taskScheduler: TaskScheduler = _taskScheduler private[spark] def taskScheduler_=(ts: TaskScheduler): Unit = { _taskScheduler = ts } private[spark] def dagScheduler: DAGScheduler = _dagScheduler private[spark] def dagScheduler_=(ds: DAGScheduler): Unit = { _dagScheduler = ds } private[spark] def shuffleDriverComponents: ShuffleDriverComponents = _shuffleDriverComponents /** * A unique identifier for the Spark application. * Its format depends on the scheduler implementation. * (i.e. * in case of local spark app something like 'local-1433865536131' * in case of YARN something like 'application_1433865536131_34483' * in case of MESOS something like 'driver-20170926223339-0001' * )  def applicationId: String = _applicationId def applicationAttemptId: Option[String] = _applicationAttemptId private[spark] def eventLogger: Option[EventLoggingListener] = _eventLogger private[spark] def executorAllocationManager: Option[ExecutorAllocationManager] = _executorAllocationManager private[spark] def resourceProfileManager: ResourceProfileManager = _resourceProfileManager private[spark] def cleaner: Option[ContextCleaner] = _cleaner private[spark] var checkpointDir: Option[String] = None // Thread Local variable that can be used by users to pass information down the stack protected[spark] val localProperties = new InheritableThreadLocal[Properties] { override def childValue(parent: Properties): Properties = { // Note: make a clone such that changes in the parent properties aren't reflected in // the those of the children threads, which has confusing semantics (SPARK-10563). Utils.cloneProperties(parent) } override protected def initialValue(): Properties = new Properties() } /* ------------------------------------------------------------------------------------- * | Initialization. This code initializes the context in a manner that is exception-safe. | | All internal fields holding state are initialized here, and any error prompts the | | stop() method to be called. | * -------------------------------------------------------------------------------------  private def warnSparkMem(value: String): String = { logWarning(\"Using SPARK_MEM to set amount of memory to use per executor process is \" + \"deprecated, please use spark.executor.memory instead.\") value } /** Control our logLevel. This overrides any user-defined log settings. * @param logLevel The desired log level as a string. * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN  def setLogLevel(logLevel: String): Unit = { // let's allow lowercase or mixed case too val upperCased = logLevel.toUpperCase(Locale.ROOT) require(SparkContext.VALID_LOG_LEVELS.contains(upperCased), s\"Supplied level $logLevel did not match one of:\" + s\" ${SparkContext.VALID_LOG_LEVELS.mkString(\",\")}\") Utils.setLogLevel(Level.toLevel(upperCased)) } try { _conf = config.clone() _conf.validateSettings() _conf.set(\"spark.app.startTime\", startTime.toString) if (!_conf.contains(\"spark.master\")) { throw new SparkException(\"A master URL must be set in your configuration\") } if (!_conf.contains(\"spark.app.name\")) { throw new SparkException(\"An application name must be set in your configuration\") } // This should be set as early as possible. SparkContext.fillMissingMagicCommitterConfsIfNeeded(_conf) SparkContext.supplementJavaModuleOptions(_conf) _driverLogger = DriverLogger(_conf) val resourcesFileOpt = conf.get(DRIVER_RESOURCES_FILE) _resources = getOrDiscoverAllResources(_conf, SPARK_DRIVER_PREFIX, resourcesFileOpt) logResourceInfo(SPARK_DRIVER_PREFIX, _resources) // log out spark.app.name in the Spark driver logs logInfo(s\"Submitted application: $appName\") // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster if (master == \"yarn\" && deployMode == \"cluster\" && !_conf.contains(\"spark.yarn.app.id\")) { throw new SparkException(\"Detected yarn cluster mode, but isn't running on a cluster. \" + \"Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.\") } if (_conf.getBoolean(\"spark.logConf\", false)) { logInfo(\"Spark configuration:\\n\" + _conf.toDebugString) } // Set Spark driver host and port system properties. This explicitly sets the configuration // instead of relying on the default value of the config constant. _conf.set(DRIVER_HOST_ADDRESS, _conf.get(DRIVER_HOST_ADDRESS)) _conf.setIfMissing(DRIVER_PORT, 0) _conf.set(EXECUTOR_ID, SparkContext.DRIVER_IDENTIFIER) _jars = Utils.getUserJars(_conf) _files = _conf.getOption(FILES.key).map(_.split(\",\")).map(_.filter(_.nonEmpty)) .toSeq.flatten _archives = _conf.getOption(ARCHIVES.key).map(Utils.stringToSeq).toSeq.flatten _eventLogDir = if (isEventLogEnabled) { val unresolvedDir = conf.get(EVENT_LOG_DIR).stripSuffix(\"/\") Some(Utils.resolveURI(unresolvedDir)) } else { None } _eventLogCodec = { val compress = _conf.get(EVENT_LOG_COMPRESS) if (compress && isEventLogEnabled) { Some(_conf.get(EVENT_LOG_COMPRESSION_CODEC)).map(CompressionCodec.getShortName) } else { None } } _listenerBus = new LiveListenerBus(_conf) _resourceProfileManager = new ResourceProfileManager(_conf, _listenerBus) // Initialize the app status store and listener before SparkEnv is created so that it gets // all events. val appStatusSource = AppStatusSource.createSource(conf) _statusStore = AppStatusStore.createLiveStore(conf, appStatusSource) listenerBus.addToStatusQueue(_statusStore.listener.get) // Create the Spark execution environment (cache, map output tracker, etc) _env = createSparkEnv(_conf, isLocal, listenerBus) SparkEnv.set(_env) // If running the REPL, register the repl's output dir with the file server. _conf.getOption(\"spark.repl.class.outputDir\").foreach { path => val replUri = _env.rpcEnv.fileServer.addDirectory(\"/classes\", new File(path)) _conf.set(\"spark.repl.class.uri\", replUri) } _statusTracker = new SparkStatusTracker(this, _statusStore) _progressBar = if (_conf.get(UI_SHOW_CONSOLE_PROGRESS)) { Some(new ConsoleProgressBar(this)) } else { None } _ui = if (conf.get(UI_ENABLED)) { Some(SparkUI.create(Some(this), _statusStore, _conf, _env.securityManager, appName, \"\", startTime)) } else { // For tests, do not enable the UI None } // Bind the UI before starting the task scheduler to communicate // the bound port to the cluster manager properly _ui.foreach(_.bind()) _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf) // Performance optimization: this dummy call to .size() triggers eager evaluation of // Configuration's internal `properties` field, guaranteeing that it will be computed and // cached before SessionState.newHadoopConf() uses `sc.hadoopConfiguration` to create // a new per-session Configuration. If `properties` has not been computed by that time // then each newly-created Configuration will perform its own expensive IO and XML // parsing to load configuration defaults and populate its own properties. By ensuring // that we've pre-computed the parent's properties, the child Configuration will simply // clone the parent's properties. _hadoopConfiguration.size() // Add each JAR given through the constructor if (jars != null) { jars.foreach(jar => addJar(jar, true)) if (addedJars.nonEmpty) { _conf.set(\"spark.app.initial.jar.urls\", addedJars.keys.toSeq.mkString(\",\")) } } if (files != null) { files.foreach(file => addFile(file, false, true)) if (addedFiles.nonEmpty) { _conf.set(\"spark.app.initial.file.urls\", addedFiles.keys.toSeq.mkString(\",\")) } } if (archives != null) { archives.foreach(file => addFile(file, false, true, isArchive = true)) if (addedArchives.nonEmpty) { _conf.set(\"spark.app.initial.archive.urls\", addedArchives.keys.toSeq.mkString(\",\")) } } _executorMemory = _conf.getOption(EXECUTOR_MEMORY.key) .orElse(Option(System.getenv(\"SPARK_EXECUTOR_MEMORY\"))) .orElse(Option(System.getenv(\"SPARK_MEM\")) .map(warnSparkMem)) .map(Utils.memoryStringToMb) .getOrElse(1024) // Convert java options to env vars as a work around // since we can't set env vars directly in sbt. for { (envKey, propKey) <- Seq((\"SPARK_TESTING\", IS_TESTING.key)) value <- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))} { executorEnvs(envKey) = value } Option(System.getenv(\"SPARK_PREPEND_CLASSES\")).foreach { v => executorEnvs(\"SPARK_PREPEND_CLASSES\") = v } // The Mesos scheduler backend relies on this environment variable to set executor memory. // TODO: Set this only in the Mesos scheduler. executorEnvs(\"SPARK_EXECUTOR_MEMORY\") = executorMemory + \"m\" executorEnvs ++= _conf.getExecutorEnv executorEnvs(\"SPARK_USER\") = sparkUser _shuffleDriverComponents = ShuffleDataIOUtils.loadShuffleDataIO(config).driver() _shuffleDriverComponents.initializeApplication().asScala.foreach { case (k, v) => _conf.set(ShuffleDataIOUtils.SHUFFLE_SPARK_CONF_PREFIX + k, v) } // We need to register \"HeartbeatReceiver\" before \"createTaskScheduler\" because Executor will // retrieve \"HeartbeatReceiver\" in the constructor. (SPARK-6640) _heartbeatReceiver = env.rpcEnv.setupEndpoint( HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this)) // Initialize any plugins before the task scheduler is initialized. _plugins = PluginContainer(this, _resources.asJava) // Create and start the scheduler val (sched, ts) = SparkContext.createTaskScheduler(this, master) _schedulerBackend = sched _taskScheduler = ts _dagScheduler = new DAGScheduler(this) _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet) val _executorMetricsSource = if (_conf.get(METRICS_EXECUTORMETRICS_SOURCE_ENABLED)) { Some(new ExecutorMetricsSource) } else { None } // create and start the heartbeater for collecting memory metrics _heartbeater = new Heartbeater( () => SparkContext.this.reportHeartBeat(_executorMetricsSource), \"driver-heartbeater\", conf.get(EXECUTOR_HEARTBEAT_INTERVAL)) _heartbeater.start() // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's // constructor _taskScheduler.start() _applicationId = _taskScheduler.applicationId() _applicationAttemptId = _taskScheduler.applicationAttemptId() _conf.set(\"spark.app.id\", _applicationId) _applicationAttemptId.foreach { attemptId => _conf.set(APP_ATTEMPT_ID, attemptId) _env.blockManager.blockStoreClient.setAppAttemptId(attemptId) } if (_conf.get(UI_REVERSE_PROXY)) { val proxyUrl = _conf.get(UI_REVERSE_PROXY_URL.key, \"\").stripSuffix(\"/\") + \"/proxy/\" + _applicationId System.setProperty(\"spark.ui.proxyBase\", proxyUrl) } _ui.foreach(_.setAppId(_applicationId)) _env.blockManager.initialize(_applicationId) FallbackStorage.registerBlockManagerIfNeeded(_env.blockManager.master, _conf) // The metrics system for Driver need to be set spark.app.id to app ID. // So it should start after we get app ID from the task scheduler and set spark.app.id. _env.metricsSystem.start(_conf.get(METRICS_STATIC_SOURCES_ENABLED)) _eventLogger = if (isEventLogEnabled) { val logger = new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get, _conf, _hadoopConfiguration) logger.start() listenerBus.addToEventLogQueue(logger) Some(logger) } else { None } _cleaner = if (_conf.get(CLEANER_REFERENCE_TRACKING)) { Some(new ContextCleaner(this, _shuffleDriverComponents)) } else { None } _cleaner.foreach(_.start()) val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) _executorAllocationManager = if (dynamicAllocationEnabled) { schedulerBackend match { case b: ExecutorAllocationClient => Some(new ExecutorAllocationManager( schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf, cleaner = cleaner, resourceProfileManager = resourceProfileManager)) case _ => None } } else { None } _executorAllocationManager.foreach(_.start()) setupAndStartListenerBus() postEnvironmentUpdate() postApplicationStart() // After application started, attach handlers to started server and start handler. _ui.foreach(_.attachAllHandler()) // Attach the driver metrics servlet handler to the web ui after the metrics system is started. _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler))) // Make sure the context is stopped if the user forgets about it. This avoids leaving // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM // is killed, though. logDebug(\"Adding shutdown hook\") // force eager creation of logger _shutdownHookRef = ShutdownHookManager.addShutdownHook( ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () => logInfo(\"Invoking stop() from shutdown hook\") try { stop() } catch { case e: Throwable => logWarning(\"Ignoring Exception while stopping SparkContext from shutdown hook\", e) } } // Post init _taskScheduler.postStartHook() if (isLocal) { _env.metricsSystem.registerSource(Executor.executorSourceLocalModeOnly) } _env.metricsSystem.registerSource(_dagScheduler.metricsSource) _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager)) _env.metricsSystem.registerSource(new JVMCPUSource()) _executorMetricsSource.foreach(_.register(_env.metricsSystem)) _executorAllocationManager.foreach { e => _env.metricsSystem.registerSource(e.executorAllocationManagerSource) } appStatusSource.foreach(_env.metricsSystem.registerSource(_)) _plugins.foreach(_.registerMetrics(applicationId)) } catch { case NonFatal(e) => logError(\"Error initializing SparkContext.\", e) try { stop() } catch { case NonFatal(inner) => logError(\"Error stopping SparkContext after init error.\", inner) } finally { throw e } } /** * Called by the web UI to obtain executor thread dumps. This method may be expensive. * Logs an error and returns None if we failed to obtain a thread dump, which could occur due * to an executor being dead or unresponsive or due to network issues while sending the thread * dump message back to the driver.  private[spark] def getExecutorThreadDump(executorId: String): Option[Array[ThreadStackTrace]] = { try { if (executorId == SparkContext.DRIVER_IDENTIFIER) { Some(Utils.getThreadDump()) } else { env.blockManager.master.getExecutorEndpointRef(executorId) match { case Some(endpointRef) => Some(endpointRef.askSync[Array[ThreadStackTrace]](TriggerThreadDump)) case None => logWarning(s\"Executor $executorId might already have stopped and \" + \"can not request thread dump from it.\") None } } } catch { case e: Exception => logError(s\"Exception getting thread dump from executor $executorId\", e) None } } private[spark] def getLocalProperties: Properties = localProperties.get() private[spark] def setLocalProperties(props: Properties): Unit = { localProperties.set(props) } /** * Set a local property that affects jobs submitted from this thread, such as the Spark fair * scheduler pool. User-defined properties may also be set here. These properties are propagated * through to worker tasks and can be accessed there via * [[org.apache.spark.TaskContext#getLocalProperty]]. * * These properties are inherited by child threads spawned from this thread. This * may have unexpected consequences when working with thread pools. The standard java * implementation of thread pools have worker threads spawn other worker threads. * As a result, local properties may propagate unpredictably.  def setLocalProperty(key: String, value: String): Unit = { if (value == null) { localProperties.get.remove(key) } else { localProperties.get.setProperty(key, value) } } /** * Get a local property set in this thread, or null if it is missing. See * `org.apache.spark.SparkContext.setLocalProperty`.  def getLocalProperty(key: String): String = Option(localProperties.get).map(_.getProperty(key)).orNull /** Set a human readable description of the current job.  def setJobDescription(value: String): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, value) } /** * Assigns a group ID to all the jobs started by this thread until the group ID is set to a * different value or cleared. * * Often, a unit of execution in an application consists of multiple Spark actions or jobs. * Application programmers can use this method to group all those jobs together and give a * group description. Once set, the Spark web UI will associate such jobs with this group. * * The application can also use `org.apache.spark.SparkContext.cancelJobGroup` to cancel all * running jobs in this group. For example, * {{{ * // In the main thread: * sc.setJobGroup(\"some_job_to_cancel\", \"some job description\") * sc.parallelize(1 to 10000, 2).map { i => Thread.sleep(10); i }.count() * * // In a separate thread: * sc.cancelJobGroup(\"some_job_to_cancel\") * }}} * * @param interruptOnCancel If true, then job cancellation will result in `Thread.interrupt()` * being called on the job's executor threads. This is useful to help ensure that the tasks * are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS * may respond to Thread.interrupt() by marking nodes as dead.  def setJobGroup(groupId: String, description: String, interruptOnCancel: Boolean = false): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId) // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup // APIs to also take advantage of this property (e.g., internal job failures or canceling from // JobProgressTab UI) on a per-job basis. setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString) } /** Clear the current thread's job group ID and its description.  def clearJobGroup(): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, null) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, null) setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, null) } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * @note Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](this)(body) // Methods for creating RDDs /** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection. Pass a copy of the argument to avoid this. * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection  def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } /** * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by * `step` every element. * * @note if we need to cache this RDD, we should make sure each partition does not exceed limit. * * @param start the start value. * @param end the end value. * @param step the incremental step * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed range  def range( start: Long, end: Long, step: Long = 1, numSlices: Int = defaultParallelism): RDD[Long] = withScope { assertNotStopped() // when step is 0, range will run infinitely require(step != 0, \"step cannot be 0\") val numElements: BigInt = { val safeStart = BigInt(start) val safeEnd = BigInt(end) if ((safeEnd - safeStart) % step == 0 || (safeEnd > safeStart) != (step > 0)) { (safeEnd - safeStart) / step } else { // the remainder has the same sign with range, could add 1 more (safeEnd - safeStart) / step + 1 } } parallelize(0 until numSlices, numSlices).mapPartitionsWithIndex { (i, _) => val partitionStart = (i * numElements) / numSlices * step + start val partitionEnd = (((i + 1) * numElements) / numSlices) * step + start def getSafeMargin(bi: BigInt): Long = if (bi.isValidLong) { bi.toLong } else if (bi > 0) { Long.MaxValue } else { Long.MinValue } val safePartitionStart = getSafeMargin(partitionStart) val safePartitionEnd = getSafeMargin(partitionEnd) new Iterator[Long] { private[this] var number: Long = safePartitionStart private[this] var overflow: Boolean = false override def hasNext = if (!overflow) { if (step > 0) { number < safePartitionEnd } else { number > safePartitionEnd } } else false override def next() = { val ret = number number += step if (number < ret ^ step < 0) { // we have Long.MaxValue + Long.MaxValue < Long.MaxValue // and Long.MinValue + Long.MinValue > Long.MinValue, so iff the step causes a step // back, we are pretty sure that we have an overflow. overflow = true } ret } } } } /** Distribute a local Scala collection to form an RDD. * * This method is identical to `parallelize`. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection  def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices) } /** * Distribute a local Scala collection to form an RDD, with one or more * location preferences (hostnames of Spark nodes) for each object. * Create a new partition for each collection item. * @param seq list of tuples of data and location preferences (hostnames of Spark nodes) * @return RDD representing data partitioned according to location preferences  def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope { assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs) } /** * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * The text files must be encoded as UTF-8. * * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file  def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope { assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair => pair._2.toString).setName(path) } /** * Read a directory of text files from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI. Each file is read as a single record and returned in a * key-value pair, where the key is the path of each file, the value is the content of each file. * The text files must be encoded as UTF-8. * * <p> For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do `val rdd = sparkContext.wholeTextFile(\"hdfs://a-hdfs-path\")`, * * <p> then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred, large file is also allowable, but may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and the corresponding file content  def wholeTextFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, String)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that wholeTextFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new WholeTextFileRDD( this, classOf[WholeTextFileInputFormat], classOf[Text], classOf[Text], updateConf, minPartitions).map(record => (record._1.toString, record._2.toString)).setName(path) } /** * Get an RDD for a Hadoop-readable dataset as PortableDataStream for each file * (useful for binary data) * * For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do * `val rdd = sparkContext.binaryFiles(\"hdfs://a-hdfs-path\")`, * * then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred; very large files may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and corresponding file content  def binaryFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, PortableDataStream)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that binaryFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new BinaryFileRDD( this, classOf[StreamInputFormat], classOf[String], classOf[PortableDataStream], updateConf, minPartitions).setName(path) } /** * Load data from a flat binary file, assuming the length of each record is constant. * * @note We ensure that the byte array for each record in the resulting RDD * has the provided record length. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param recordLength The length at which to split the records * @param conf Configuration for setting up the dataset. * * @return An RDD of data with values, represented as byte arrays  def binaryRecords( path: String, recordLength: Int, conf: Configuration = hadoopConfiguration): RDD[Array[Byte]] = withScope { assertNotStopped() conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = newAPIHadoopFile[LongWritable, BytesWritable, FixedLengthBinaryInputFormat](path, classOf[FixedLengthBinaryInputFormat], classOf[LongWritable], classOf[BytesWritable], conf = conf) br.map { case (k, v) => val bytes = v.copyBytes() assert(bytes.length == recordLength, \"Byte array does not have correct length\") bytes } } /** * Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf given its InputFormat and other * necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable), * using the older MapReduce API (`org.apache.hadoop.mapred`). * * @param conf JobConf for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions Minimum number of Hadoop Splits to generate. * @return RDD of tuples of key and corresponding value * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function.  def hadoopRDD[K, V]( conf: JobConf, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf before broadcasting it. SparkHadoopUtil.get.addCredentials(conf) new HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minPartitions) } /** Get an RDD for a Hadoop file with an arbitrary InputFormat * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V]( path: String, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // A Hadoop configuration can be about 10 KiB, which is pretty big, so broadcast it. val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration)) val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path) new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minPartitions) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V, F <: InputFormat[K, V]] (path: String, minPartitions: Int) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile(path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]], minPartitions) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths as * a list of inputs * @return RDD of tuples of key and corresponding value  def hadoopFile[K, V, F <: InputFormat[K, V]](path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile[K, V, F](path, defaultMinPartitions) } /** * Smarter version of `newApiHadoopFile` that uses class tags to figure out the classes of keys, * values and the `org.apache.hadoop.mapreduce.InputFormat` (new MapReduce API) so that user * don't need to pass them directly. Instead, callers can just write, for example: * ``` * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * ``` * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @return RDD of tuples of key and corresponding value  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]] (path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { newAPIHadoopFile( path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]]) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * @param conf Hadoop configuration * @return RDD of tuples of key and corresponding value  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( path: String, fClass: Class[F], kClass: Class[K], vClass: Class[V], conf: Configuration = hadoopConfiguration): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // The call to NewHadoopJob automatically adds security credentials to conf, // so we don't need to explicitly add them ourselves val job = NewHadoopJob.getInstance(conf) // Use setInputPaths so that newAPIHadoopFile aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updatedConf = job.getConfiguration new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @param conf Configuration for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function.  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( conf: Configuration = hadoopConfiguration, fClass: Class[F], kClass: Class[K], vClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf. Required to access secure HDFS. val jconf = new JobConf(conf) SparkHadoopUtil.get.addCredentials(jconf) new NewHadoopRDD(this, fClass, kClass, vClass, jconf) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V], minPartitions: Int ): RDD[(K, V)] = withScope { assertNotStopped() val inputFormatClass = classOf[SequenceFileInputFormat[K, V]] hadoopFile(path, inputFormatClass, keyClass, valueClass, minPartitions) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V]( path: String, keyClass: Class[K], valueClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() sequenceFile(path, keyClass, valueClass, defaultMinPartitions) } /** * Version of sequenceFile() for types implicitly convertible to Writables through a * WritableConverter. For example, to access a SequenceFile where the keys are Text and the * values are IntWritable, you could simply write * {{{ * sparkContext.sequenceFile[String, Int](path, ...) * }}} * * WritableConverters are provided in a somewhat strange way (by an implicit function) to support * both subclasses of Writable and types for which we define a converter (e.g. Int to * IntWritable). The most natural thing would've been to have implicit objects for the * converters, but then we couldn't have an object for every subclass of Writable (you can't * have a parameterized singleton object). We use functions instead to create a new converter * for the appropriate type. In addition, we pass the converter a ClassTag of its type to * allow it to figure out the Writable class to use in the subclass case. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value  def sequenceFile[K, V] (path: String, minPartitions: Int = defaultMinPartitions) (implicit km: ClassTag[K], vm: ClassTag[V], kcf: () => WritableConverter[K], vcf: () => WritableConverter[V]): RDD[(K, V)] = { withScope { assertNotStopped() val kc = clean(kcf)() val vc = clean(vcf)() val format = classOf[SequenceFileInputFormat[Writable, Writable]] val writables = hadoopFile(path, format, kc.writableClass(km).asInstanceOf[Class[Writable]], vc.writableClass(vm).asInstanceOf[Class[Writable]], minPartitions) writables.map { case (k, v) => (kc.convert(k), vc.convert(v)) } } } /** * Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and * BytesWritable values that contain a serialized partition. This is still an experimental * storage format and may not be supported exactly as is in future Spark releases. It will also * be pretty slow if you use the default serializer (Java serialization), * though the nice thing about it is that there's very little effort required to save arbitrary * objects. * * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD representing deserialized data from the file(s)  def objectFile[T: ClassTag]( path: String, minPartitions: Int = defaultMinPartitions): RDD[T] = withScope { assertNotStopped() sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions) .flatMap(x => Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader)) } protected[spark] def checkpointFile[T: ClassTag](path: String): RDD[T] = withScope { new ReliableCheckpointRDD[T](this, path) } /** Build the union of a list of RDDs.  def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope { val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty) val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet if (nonEmptyRdds.forall(_.partitioner.isDefined) && partitioners.size == 1) { new PartitionerAwareUnionRDD(this, nonEmptyRdds) } else { new UnionRDD(this, nonEmptyRdds) } } /** Build the union of a list of RDDs passed as variable-length arguments.  def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope { union(Seq(first) ++ rest) } /** Get an RDD that has no partitions or elements.  def emptyRDD[T: ClassTag]: RDD[T] = new EmptyRDD[T](this) // Methods for creating shared variables /** * Register the given accumulator. * * @note Accumulators must be registered before use, or it will throw exception.  def register(acc: AccumulatorV2[_, _]): Unit = { acc.register(this) } /** * Register the given accumulator with given name. * * @note Accumulators must be registered before use, or it will throw exception.  def register(acc: AccumulatorV2[_, _], name: String): Unit = { acc.register(this, name = Option(name)) } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.  def longAccumulator: LongAccumulator = { val acc = new LongAccumulator register(acc) acc } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.  def longAccumulator(name: String): LongAccumulator = { val acc = new LongAccumulator register(acc, name) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.  def doubleAccumulator: DoubleAccumulator = { val acc = new DoubleAccumulator register(acc) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.  def doubleAccumulator(name: String): DoubleAccumulator = { val acc = new DoubleAccumulator register(acc, name) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list.  def collectionAccumulator[T]: CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list.  def collectionAccumulator[T](name: String): CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc, name) acc } /** * Broadcast a read-only variable to the cluster, returning a * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions. * The variable will be sent to each cluster only once. * * @param value value to broadcast to the Spark nodes * @return `Broadcast` object, a read-only variable cached on each machine  def broadcast[T: ClassTag](value: T): Broadcast[T] = { assertNotStopped() require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass), \"Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\") val bc = env.broadcastManager.newBroadcast[T](value, isLocal) val callSite = getCallSite logInfo(\"Created broadcast \" + bc.id + \" from \" + callSite.shortForm) cleaner.foreach(_.registerBroadcastForCleanup(bc)) bc } /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String): Unit = { addFile(path, false, false) } /** * Returns a list of file paths that are added to resources.  def listFiles(): Seq[String] = addedFiles.keySet.toSeq /** * :: Experimental :: * Add an archive to be downloaded and unpacked with this Spark job on every node. * * If an archive is added during execution, it will not be available until the next TaskSet * starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(paths-to-files)` to find its download/unpacked location. * The given path should be one of .zip, .tar, .tar.gz, .tgz and .jar. * * @note A path can be added only once. Subsequent additions of the same path are ignored. * * @since 3.1.0  @Experimental def addArchive(path: String): Unit = { addFile(path, false, false, isArchive = true) } /** * :: Experimental :: * Returns a list of archive paths that are added to resources. * * @since 3.1.0  @Experimental def listArchives(): Seq[String] = addedArchives.keySet.toSeq /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * @param recursive if true, a directory can be given in `path`. Currently directories are * only supported for Hadoop-supported filesystems. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String, recursive: Boolean): Unit = { addFile(path, recursive, false) } private def addFile( path: String, recursive: Boolean, addedOnSubmit: Boolean, isArchive: Boolean = false ): Unit = { val uri = Utils.resolveURI(path) val schemeCorrectedURI = uri.getScheme match { case null => new File(path).getCanonicalFile.toURI case \"local\" => logWarning(s\"File with 'local' scheme $path is not supported to add to file server, \" + s\"since it is already available on every node.\") return case _ => uri } val hadoopPath = new Path(schemeCorrectedURI) val scheme = schemeCorrectedURI.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) { val fs = hadoopPath.getFileSystem(hadoopConfiguration) val isDir = fs.getFileStatus(hadoopPath).isDirectory if (!isLocal && scheme == \"file\" && isDir) { throw new SparkException(s\"addFile does not support local directories when not running \" + \"local mode.\") } if (!recursive && isDir) { throw new SparkException(s\"Added file $hadoopPath is a directory and recursive is not \" + \"turned on.\") } } else { // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) } val key = if (!isLocal && scheme == \"file\") { env.rpcEnv.fileServer.addFile(new File(uri.getPath)) } else if (uri.getScheme == null) { schemeCorrectedURI.toString } else { uri.toString } val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis if (!isArchive && addedFiles.putIfAbsent(key, timestamp).isEmpty) { logInfo(s\"Added file $path at $key with timestamp $timestamp\") // Fetch the file locally so that closures which are run on the driver can still use the // SparkFiles API to access files. Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf, hadoopConfiguration, timestamp, useCache = false) postEnvironmentUpdate() } else if ( isArchive && addedArchives.putIfAbsent( UriBuilder.fromUri(new URI(key)).fragment(uri.getFragment).build().toString, timestamp).isEmpty) { logInfo(s\"Added archive $path at $key with timestamp $timestamp\") // If the scheme is file, use URI to simply copy instead of downloading. val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key) val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build() val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false) val dest = new File( SparkFiles.getRootDirectory(), if (uri.getFragment != null) uri.getFragment else source.getName) logInfo( s\"Unpacking an archive $path from ${source.getAbsolutePath} to ${dest.getAbsolutePath}\") Utils.deleteRecursively(dest) Utils.unpack(source, dest) postEnvironmentUpdate() } else { logWarning(s\"The path $path has been added already. Overwriting of added paths \" + \"is not supported in the current version.\") } } /** * :: DeveloperApi :: * Register a listener to receive up-calls from events that happen during execution.  @DeveloperApi def addSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.addToSharedQueue(listener) } /** * :: DeveloperApi :: * Deregister the listener from Spark's listener bus.  @DeveloperApi def removeSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.removeListener(listener) } private[spark] def getExecutorIds(): Seq[String] = { schedulerBackend match { case b: ExecutorAllocationClient => b.getExecutorIds() case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") Nil } } /** * Get the max number of tasks that can be concurrent launched based on the ResourceProfile * could be used, even if some of them are being used at the moment. * Note that please don't cache the value returned by this method, because the number can change * due to add/remove executors. * * @param rp ResourceProfile which to use to calculate max concurrent tasks. * @return The max number of tasks that can be concurrent launched currently.  private[spark] def maxNumConcurrentTasks(rp: ResourceProfile): Int = { schedulerBackend.maxNumConcurrentTasks(rp) } /** * Update the cluster manager on our scheduling needs. Three bits of information are included * to help it make decisions. This applies to the default ResourceProfile. * @param numExecutors The total number of executors we'd like to have. The cluster manager * shouldn't kill any running executor to reach this number, but, * if all existing executors were to die, this is the number of executors * we'd want to be allocated. * @param localityAwareTasks The number of tasks in all active stages that have a locality * preferences. This includes running, pending, and completed tasks. * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages * that would like to like to run on that host. * This includes running, pending, and completed tasks. * @return whether the request is acknowledged by the cluster manager.  @DeveloperApi def requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: immutable.Map[String, Int] ): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => // this is being applied to the default resource profile, would need to add api to support // others val defaultProfId = resourceProfileManager.defaultResourceProfile.id b.requestTotalExecutors(immutable.Map(defaultProfId-> numExecutors), immutable.Map(localityAwareTasks -> defaultProfId), immutable.Map(defaultProfId -> hostToLocalTaskCount)) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request an additional number of executors from the cluster manager. * @return whether the request is received.  @DeveloperApi def requestExecutors(numAdditionalExecutors: Int): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.requestExecutors(numAdditionalExecutors) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executors. * * This is not supported when dynamic allocation is turned on. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executors it kills * through this method with new ones, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutors(executorIds: Seq[String]): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => require(executorAllocationManager.isEmpty, \"killExecutors() unsupported with Dynamic Allocation turned on\") b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executor. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executor it kills * through this method with a new one, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId)) /** * Request that the cluster manager kill the specified executor without adjusting the * application resource requirements. * * The effect is that a new executor will be launched in place of the one killed by * this request. This assumes the cluster manager will automatically and eventually * fulfill all missing application resource requests. * * @note The replace is by no means guaranteed; another application on the same cluster * can steal the window of opportunity and acquire this application's resources in the * mean time. * * @return whether the request is received.  private[spark] def killAndReplaceExecutor(executorId: String): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** The version of Spark on which this application is running.  def version: String = SPARK_VERSION /** * Return a map from the block manager to the max memory available for caching and the remaining * memory available for caching.  def getExecutorMemoryStatus: Map[String, (Long, Long)] = { assertNotStopped() env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) => (blockManagerId.host + \":\" + blockManagerId.port, mem) } } /** * :: DeveloperApi :: * Return information about what RDDs are cached, if they are in mem or on disk, how much space * they take, etc.  @DeveloperApi def getRDDStorageInfo: Array[RDDInfo] = { getRDDStorageInfo(_ => true) } private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = { assertNotStopped() val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray rddInfos.foreach { rddInfo => val rddId = rddInfo.id val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId)) rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0) rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L) rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L) } rddInfos.filter(_.isCached) } /** * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call. * * @note This does not necessarily mean the caching or computation was successful.  def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap /** * :: DeveloperApi :: * Return pools for fair scheduler  @DeveloperApi def getAllPools: Seq[Schedulable] = { assertNotStopped() // TODO(xiajunluan): We should take nested pools into account taskScheduler.rootPool.schedulableQueue.asScala.toSeq } /** * :: DeveloperApi :: * Return the pool associated with the given name, if one exists  @DeveloperApi def getPoolForName(pool: String): Option[Schedulable] = { assertNotStopped() Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool)) } /** * Return current scheduling mode  def getSchedulingMode: SchedulingMode.SchedulingMode = { assertNotStopped() taskScheduler.schedulingMode } /** * Gets the locality information associated with the partition in a particular rdd * @param rdd of interest * @param partition to be looked up for locality * @return list of preferred locations for the partition  private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { dagScheduler.getPreferredLocs(rdd, partition) } /** * Register an RDD to be persisted in memory and/or disk storage  private[spark] def persistRDD(rdd: RDD[_]): Unit = { persistentRdds(rdd.id) = rdd } /** * Unpersist an RDD from memory and/or disk storage  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean): Unit = { env.blockManager.master.removeRdd(rddId, blocking) persistentRdds.remove(rddId) listenerBus.post(SparkListenerUnpersistRDD(rddId)) } /** * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future. * * If a jar is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addJar(path: String): Unit = { addJar(path, false) } private def addJar(path: String, addedOnSubmit: Boolean): Unit = { def addLocalJarFile(file: File): Seq[String] = { try { if (!file.exists()) { throw new FileNotFoundException(s\"Jar ${file.getAbsolutePath} not found\") } if (file.isDirectory) { throw new IllegalArgumentException( s\"Directory ${file.getAbsoluteFile} is not allowed for addJar\") } Seq(env.rpcEnv.fileServer.addJar(file)) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } def checkRemoteJarFile(path: String): Seq[String] = { val hadoopPath = new Path(path) val scheme = hadoopPath.toUri.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme)) { try { val fs = hadoopPath.getFileSystem(hadoopConfiguration) if (!fs.exists(hadoopPath)) { throw new FileNotFoundException(s\"Jar ${path} not found\") } if (fs.getFileStatus(hadoopPath).isDirectory) { throw new IllegalArgumentException( s\"Directory ${path} is not allowed for addJar\") } Seq(path) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } else { Seq(path) } } if (path == null || path.isEmpty) { logWarning(\"null or empty path specified as parameter to addJar\") } else { val (keys, scheme) = if (path.contains(\"\\\\\") && Utils.isWindows) { // For local paths with backslashes on Windows, URI throws an exception (addLocalJarFile(new File(path)), \"local\") } else { val uri = Utils.resolveURI(path) // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) val uriScheme = uri.getScheme val jarPaths = uriScheme match { // A JAR file which exists only on the driver node case null => // SPARK-22585 path without schema is not url encoded addLocalJarFile(new File(uri.getPath)) // A JAR file which exists only on the driver node case \"file\" => addLocalJarFile(new File(uri.getPath)) // A JAR file which exists locally on every worker node case \"local\" => Seq(\"file:\" + uri.getPath) case \"ivy\" => // Since `new Path(path).toUri` will lose query information, // so here we use `URI.create(path)` DependencyUtils.resolveMavenDependencies(URI.create(path)) .flatMap(jar => addLocalJarFile(new File(jar))) case _ => checkRemoteJarFile(path) } (jarPaths, uriScheme) } if (keys.nonEmpty) { val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis val (added, existed) = keys.partition(addedJars.putIfAbsent(_, timestamp).isEmpty) if (added.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"Added $jarMessage $path at ${added.mkString(\",\")} with timestamp $timestamp\") postEnvironmentUpdate() } if (existed.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" + \" Overwriting of added jar is not supported in the current version.\") } } } } /** * Returns a list of jar files that are added to resources.  def listJars(): Seq[String] = addedJars.keySet.toSeq /** * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark * may wait for some internal threads to finish. It's better to use this method to stop * SparkContext instead.  private[spark] def stopInNewThread(): Unit = { new Thread(\"stop-spark-context\") { setDaemon(true) override def run(): Unit = { try { SparkContext.this.stop() } catch { case e: Throwable => logError(e.getMessage, e) throw e } } }.start() } /** * Shut down the SparkContext.  def stop(): Unit = { if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop SparkContext within listener bus thread.\") } // Use the stopping variable to ensure no contention for the stop scenario. // Still track the stopped variable for use elsewhere in the code. if (!stopped.compareAndSet(false, true)) { logInfo(\"SparkContext already stopped.\") return } if (_shutdownHookRef != null) { ShutdownHookManager.removeShutdownHook(_shutdownHookRef) } if (listenerBus != null) { Utils.tryLogNonFatalError { postApplicationEnd() } } Utils.tryLogNonFatalError { _driverLogger.foreach(_.stop()) } Utils.tryLogNonFatalError { _ui.foreach(_.stop()) } Utils.tryLogNonFatalError { _cleaner.foreach(_.stop()) } Utils.tryLogNonFatalError { _executorAllocationManager.foreach(_.stop()) } if (_dagScheduler != null) { Utils.tryLogNonFatalError { _dagScheduler.stop() } _dagScheduler = null } if (_listenerBusStarted) { Utils.tryLogNonFatalError { listenerBus.stop() _listenerBusStarted = false } } if (env != null) { Utils.tryLogNonFatalError { env.metricsSystem.report() } } Utils.tryLogNonFatalError { _plugins.foreach(_.shutdown()) } FallbackStorage.cleanUp(_conf, _hadoopConfiguration) Utils.tryLogNonFatalError { _eventLogger.foreach(_.stop()) } if (_heartbeater != null) { Utils.tryLogNonFatalError { _heartbeater.stop() } _heartbeater = null } if (_shuffleDriverComponents != null) { Utils.tryLogNonFatalError { _shuffleDriverComponents.cleanupApplication() } } if (env != null && _heartbeatReceiver != null) { Utils.tryLogNonFatalError { env.rpcEnv.stop(_heartbeatReceiver) } } Utils.tryLogNonFatalError { _progressBar.foreach(_.stop()) } _taskScheduler = null // TODO: Cache.stop()? if (_env != null) { Utils.tryLogNonFatalError { _env.stop() } SparkEnv.set(null) } if (_statusStore != null) { _statusStore.close() } // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this // `SparkContext` is stopped. localProperties.remove() ResourceProfile.clearDefaultProfile() // Unset YARN mode system env variable, to allow switching between cluster types. SparkContext.clearActiveContext() logInfo(\"Successfully stopped SparkContext\") } /** * Get Spark's home location from either a value set through the constructor, * or the spark.home Java property, or the SPARK_HOME environment variable * (in that order of preference). If neither of these is set, return None.  private[spark] def getSparkHome(): Option[String] = { conf.getOption(\"spark.home\").orElse(Option(System.getenv(\"SPARK_HOME\"))) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  def setCallSite(shortCallSite: String): Unit = { setLocalProperty(CallSite.SHORT_FORM, shortCallSite) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  private[spark] def setCallSite(callSite: CallSite): Unit = { setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm) setLocalProperty(CallSite.LONG_FORM, callSite.longForm) } /** * Clear the thread-local property for overriding the call sites * of actions and RDDs.  def clearCallSite(): Unit = { setLocalProperty(CallSite.SHORT_FORM, null) setLocalProperty(CallSite.LONG_FORM, null) } /** * Capture the current user callsite and return a formatted version for printing. If the user * has overridden the call site using `setCallSite()`, this will return the user's version.  private[spark] def getCallSite(): CallSite = { lazy val callSite = Utils.getCallSite() CallSite( Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm), Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm) ) } /** * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * The function that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res) results } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions) } /** * Run a job on all partitions in an RDD and return the results in an array. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and return the results in an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: (TaskContext, Iterator[T]) => U, resultHandler: (Int, U) => Unit): Unit = { runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: Iterator[T] => U, resultHandler: (Int, U) => Unit): Unit = { val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter) runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler) } /** * :: DeveloperApi :: * Run a job that can return approximate results. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param evaluator `ApproximateEvaluator` to receive the partial results * @param timeout maximum time to wait for the job, in milliseconds * @return partial result (how partial depends on whether the job was finished before or * after timeout)  @DeveloperApi def runApproximateJob[T, U, R]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, evaluator: ApproximateEvaluator[U, R], timeout: Long): PartialResult[R] = { assertNotStopped() val callSite = getCallSite logInfo(\"Starting job: \" + callSite.shortForm) val start = System.nanoTime val cleanedFunc = clean(func) val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout, localProperties.get) logInfo( \"Job finished: \" + callSite.shortForm + \", took \" + (System.nanoTime - start) / 1e9 + \" s\") result } /** * Submit a job for execution and return a FutureJob holding the result. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to * @param resultFunc function to be executed when the result is ready  def submitJob[T, U, R]( rdd: RDD[T], processPartition: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit, resultFunc: => R): SimpleFutureAction[R] = { assertNotStopped() val cleanF = clean(processPartition) val callSite = getCallSite val waiter = dagScheduler.submitJob( rdd, (context: TaskContext, iter: Iterator[T]) => cleanF(iter), partitions, callSite, resultHandler, localProperties.get) new SimpleFutureAction(waiter, resultFunc) } /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future.  private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } /** * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup` * for more information.  def cancelJobGroup(groupId: String): Unit = { assertNotStopped() dagScheduler.cancelJobGroup(groupId) } /** Cancel all jobs that have been scheduled or are running.  def cancelAllJobs(): Unit = { assertNotStopped() dagScheduler.cancelAllJobs() } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @param reason optional reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int, reason: String): Unit = { dagScheduler.cancelJob(jobId, Option(reason)) } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int): Unit = { dagScheduler.cancelJob(jobId, None) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @param reason reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int, reason: String): Unit = { dagScheduler.cancelStage(stageId, Option(reason)) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int): Unit = { dagScheduler.cancelStage(stageId, None) } /** * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI * or through SparkListener.onTaskStart. * * @param taskId the task ID to kill. This id uniquely identifies the task attempt. * @param interruptThread whether to interrupt the thread running the task. * @param reason the reason for killing the task, which should be a short string. If a task * is killed multiple times with different reasons, only one reason will be reported. * * @return Whether the task was successfully killed.  def killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean = { dagScheduler.killTaskAttempt(taskId, interruptThread, reason) } /** * Clean a closure to make it ready to be serialized and sent to tasks * (removes unreferenced variables in $outer's, updates REPL variables) * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt> * if not. * * @param f the closure to clean * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not * serializable * @return the cleaned closure  private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = { ClosureCleaner.clean(f, checkSerializable) f } /** * Set the directory under which RDDs are going to be checkpointed. * @param directory path to the directory where checkpoint files will be stored * (must be HDFS path if running in cluster)  def setCheckpointDir(directory: String): Unit = { // If we are running on a cluster, log a warning if the directory is local. // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from // its own local file system, which is incorrect because the checkpoint files // are actually on the executor machines. if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) { logWarning(\"Spark is not running in local mode, therefore the checkpoint directory \" + s\"must not be on the local filesystem. Directory '$directory' \" + \"appears to be on the local filesystem.\") } checkpointDir = Option(directory).map { dir => val path = new Path(dir, UUID.randomUUID().toString) val fs = path.getFileSystem(hadoopConfiguration) fs.mkdirs(path) fs.getFileStatus(path).getPath.toString } } def getCheckpointDir: Option[String] = checkpointDir /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD).  def defaultParallelism: Int = { assertNotStopped() taskScheduler.defaultParallelism } /** * Default min number of partitions for Hadoop RDDs when not given by user * Notice that we use math.min so the \"defaultMinPartitions\" cannot be higher than 2. * The reasons for this are discussed in https://github.com/mesos/spark/pull/718  def defaultMinPartitions: Int = math.min(defaultParallelism, 2) private val nextShuffleId = new AtomicInteger(0) private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement() private val nextRddId = new AtomicInteger(0) /** Register a new RDD, returning its RDD ID  private[spark] def newRddId(): Int = nextRddId.getAndIncrement() /** * Registers listeners specified in spark.extraListeners, then starts the listener bus. * This should be called after all internal listeners have been registered with the listener bus * (e.g. after the web UI and event logging listeners have been registered).  private def setupAndStartListenerBus(): Unit = { try { conf.get(EXTRA_LISTENERS).foreach { classNames => val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf) listeners.foreach { listener => listenerBus.addToSharedQueue(listener) logInfo(s\"Registered listener ${listener.getClass().getName()}\") } } } catch { case e: Exception => try { stop() } finally { throw new SparkException(s\"Exception when registering SparkListener\", e) } } listenerBus.start(this, _env.metricsSystem) _listenerBusStarted = true } /** Post the application start event  private def postApplicationStart(): Unit = { // Note: this code assumes that the task scheduler has been initialized and has contacted // the cluster manager to get an application ID (in case the cluster manager provides one). listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId), startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls, schedulerBackend.getDriverAttributes)) _driverLogger.foreach(_.startSync(_hadoopConfiguration)) } /** Post the application end event  private def postApplicationEnd(): Unit = { listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis)) } /** Post the environment update event once the task scheduler is ready  private def postEnvironmentUpdate(): Unit = { if (taskScheduler != null) { val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val addedArchivePaths = addedArchives.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, hadoopConfiguration, schedulingMode, addedJarPaths, addedFilePaths, addedArchivePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) } } /** Reports heartbeat metrics for the driver.  private def reportHeartBeat(executorMetricsSource: Option[ExecutorMetricsSource]): Unit = { val currentMetrics = ExecutorMetrics.getCurrentMetrics(env.memoryManager) executorMetricsSource.foreach(_.updateMetricsSnapshot(currentMetrics)) val driverUpdates = new HashMap[(Int, Int), ExecutorMetrics] // In the driver, we do not track per-stage metrics, so use a dummy stage for the key driverUpdates.put(EventLoggingListener.DRIVER_STAGE_KEY, new ExecutorMetrics(currentMetrics)) val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0) listenerBus.post(SparkListenerExecutorMetricsUpdate(\"driver\", accumUpdates, driverUpdates)) } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having finished construction. // NOTE: this must be placed at the end of the SparkContext constructor. SparkContext.setActiveContext(this) } /** * The SparkContext object contains a number of implicit conversions and parameters for use with * various Spark features.  object SparkContext extends Logging { private val VALID_LOG_LEVELS = Set(\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\") /** * Lock that guards access to global variables that track SparkContext construction.  private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object() /** * The active, fully-constructed SparkContext. If no SparkContext is active, then this is `null`. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private val activeContext: AtomicReference[SparkContext] = new AtomicReference[SparkContext](null) /** * Points to a partially-constructed SparkContext if another thread is in the SparkContext * constructor, or `None` if no SparkContext is being constructed. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private var contextBeingConstructed: Option[SparkContext] = None /** * Called to ensure that no other SparkContext is running in this JVM. * * Throws an exception if a running context is detected and logs a warning if another thread is * constructing a SparkContext. This warning is necessary because the current locking scheme * prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private def assertNoOtherContextIsRunning(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()).filter(_ ne sc).foreach { ctx => val errMsg = \"Only one SparkContext should be running in this JVM (see SPARK-2243).\" + s\"The currently running SparkContext was created at:\\n${ctx.creationSite.longForm}\" throw new SparkException(errMsg) } contextBeingConstructed.filter(_ ne sc).foreach { otherContext => // Since otherContext might point to a partially-constructed context, guard against // its creationSite field being null: val otherContextCreationSite = Option(otherContext.creationSite).map(_.longForm).getOrElse(\"unknown location\") val warnMsg = \"Another SparkContext is being constructed (or threw an exception in its\" + \" constructor). This may indicate an error, since only one SparkContext should be\" + \" running in this JVM (see SPARK-2243).\" + s\" The other SparkContext was created at:\\n$otherContextCreationSite\" logWarning(warnMsg) } } } /** * Called to ensure that SparkContext is created or accessed only on the Driver. * * Throws an exception if a SparkContext is about to be created in executors.  private def assertOnDriver(): Unit = { if (Utils.isInRunningSparkTask) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkContext should only be created and accessed on the driver.\") } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * @param config `SparkConfig` that will be used for initialisation of the `SparkContext` * @return current `SparkContext` (or a new one if it wasn't created before the function call)  def getOrCreate(config: SparkConf): SparkContext = { // Synchronize to ensure that multiple create requests don't trigger an exception // from assertNoOtherContextIsRunning within setActiveContext SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext(config)) } else { if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") } } activeContext.get() } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * This method allows not passing a SparkConf (useful if just retrieving). * * @return current `SparkContext` (or a new one if wasn't created before the function call)  def getOrCreate(): SparkContext = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext()) } activeContext.get() } } /** Return the current active [[SparkContext]] if any.  private[spark] def getActive: Option[SparkContext] = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()) } } /** * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is * running. Throws an exception if a running context is detected and logs a warning if another * thread is constructing a SparkContext. This warning is necessary because the current locking * scheme prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private[spark] def markPartiallyConstructed(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = Some(sc) } } /** * Called at the end of the SparkContext constructor to ensure that no other SparkContext has * raced with this constructor and started.  private[spark] def setActiveContext(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = None activeContext.set(sc) } } /** * Clears the active SparkContext metadata. This is called by `SparkContext#stop()`. It's * also called in unit tests to prevent a flood of warnings from test suites that don't / can't * properly clean up their SparkContexts.  private[spark] def clearActiveContext(): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { activeContext.set(null) } } private[spark] val SPARK_JOB_DESCRIPTION = \"spark.job.description\" private[spark] val SPARK_JOB_GROUP_ID = \"spark.jobGroup.id\" private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = \"spark.job.interruptOnCancel\" private[spark] val SPARK_SCHEDULER_POOL = \"spark.scheduler.pool\" private[spark] val RDD_SCOPE_KEY = \"spark.rdd.scope\" private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = \"spark.rdd.scope.noOverride\" /** * Executor id for the driver. In earlier versions of Spark, this was `<driver>`, but this was * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see * SPARK-6716 for more details).  private[spark] val DRIVER_IDENTIFIER = \"driver\" private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Iterable[T]) : ArrayWritable = { def anyToWritable[U <: Writable](u: U): Writable = u new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]], arr.map(x => anyToWritable(x)).toArray) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to SparkContext. * * @param cls class that should be inside of the jar * @return jar that contains the Class, `None` if not found  def jarOfClass(cls: Class[_]): Option[String] = { val uri = cls.getResource(\"/\" + cls.getName.replace('.', '/') + \".class\") if (uri != null) { val uriStr = uri.toString if (uriStr.startsWith(\"jar:file:\")) { // URI will be of the form \"jar:file:/path/foo.jar!/package/cls.class\", // so pull out the /path/foo.jar Some(uriStr.substring(\"jar:file:\".length, uriStr.indexOf('!'))) } else { None } } else { None } } /** * Find the JAR that contains the class of a particular object, to make it easy for users * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in * your driver program. * * @param obj reference to an instance which class should be inside of the jar * @return jar that contains the class of the instance, `None` if not found  def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass) /** * Creates a modified version of a SparkConf with the parameters that can be passed separately * to SparkContext, to make it easier to write SparkContext's constructors. This ignores * parameters that are passed as the default value of null, instead of throwing an exception * like SparkConf would.  private[spark] def updatedConf( conf: SparkConf, master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()): SparkConf = { val res = conf.clone() res.setMaster(master) res.setAppName(appName) if (sparkHome != null) { res.setSparkHome(sparkHome) } if (jars != null && !jars.isEmpty) { res.setJars(jars) } res.setExecutorEnv(environment.toSeq) res } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String): Int = { numDriverCores(master, null) } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String, conf: SparkConf): Int = { def convertToInt(threads: String): Int = { if (threads == \"*\") Runtime.getRuntime.availableProcessors() else threads.toInt } master match { case \"local\" => 1 case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads) case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads) case \"yarn\" | SparkMasterRegex.KUBERNETES_REGEX(_) => if (conf != null && conf.get(SUBMIT_DEPLOY_MODE) == \"cluster\") { conf.getInt(DRIVER_CORES.key, 0) } else { 0 } case _ => 0 // Either driver is not being used, or its core count will be interpolated later } } /** * Create a task scheduler based on a given master URL. * Return a 2-tuple of the scheduler backend and the task scheduler.  private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = { import SparkMasterRegex._ // When running locally, don't try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 // Ensure that default executor's resources satisfies one or more tasks requirement. // This function is for cluster managers that don't set the executor cores config, for // others its checked in ResourceProfile. def checkResourcesPerTask(executorCores: Int): Unit = { val taskCores = sc.conf.get(CPUS_PER_TASK) if (!sc.conf.get(SKIP_VALIDATE_CORES_TESTING)) { validateTaskCpusLargeEnough(sc.conf, executorCores, taskCores) } val defaultProf = sc.resourceProfileManager.defaultResourceProfile ResourceUtils.warnOnWastedResources(defaultProf, sc.conf, Some(executorCores)) } master match { case \"local\" => checkResourcesPerTask(1) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_REGEX(threads) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads. val threadCount = if (threads == \"*\") localCpuCount else threads.toInt if (threadCount <= 0) { throw new SparkException(s\"Asked to run locally with $threadCount threads\") } checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_FAILURES_REGEX(threads, maxFailures) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*, M] means the number of cores on the computer with M failures // local[N, M] means exactly N threads with M failures val threadCount = if (threads == \"*\") localCpuCount else threads.toInt checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case SPARK_REGEX(sparkUrl) => val scheduler = new TaskSchedulerImpl(sc) val masterUrls = sparkUrl.split(\",\").map(\"spark://\" + _) val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) (backend, scheduler) case LOCAL_CLUSTER_REGEX(numWorkers, coresPerWorker, memoryPerWorker) => checkResourcesPerTask(coresPerWorker.toInt) // Check to make sure memory requested <= memoryPerWorker. Otherwise Spark will just hang. val memoryPerWorkerInt = memoryPerWorker.toInt if (sc.executorMemory > memoryPerWorkerInt) { throw new SparkException( \"Asked to launch cluster with %d MiB/worker but requested %d MiB/executor\".format( memoryPerWorkerInt, sc.executorMemory)) } // For host local mode setting the default of SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED // to false because this mode is intended to be used for testing and in this case all the // executors are running on the same host. So if host local reading was enabled here then // testing of the remote fetching would be secondary as setting this config explicitly to // false would be required in most of the unit test (despite the fact that remote fetching // is much more frequent in production). sc.conf.setIfMissing(SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED, false) val scheduler = new TaskSchedulerImpl(sc) val localCluster = LocalSparkCluster( numWorkers.toInt, coresPerWorker.toInt, memoryPerWorkerInt, sc.conf) val masterUrls = localCluster.start() val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => { localCluster.stop() } (backend, scheduler) case masterUrl => val cm = getClusterManager(masterUrl) match { case Some(clusterMgr) => clusterMgr case None => throw new SparkException(\"Could not parse Master URL: '\" + master + \"'\") } try { val scheduler = cm.createTaskScheduler(sc, masterUrl) val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) cm.initialize(scheduler, backend) (backend, scheduler) } catch { case se: SparkException => throw se case NonFatal(e) => throw new SparkException(\"External scheduler cannot be instantiated\", e) } } } private def getClusterManager(url: String): Option[ExternalClusterManager] = { val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size > 1) { throw new SparkException( s\"Multiple external cluster managers registered for the url $url: $serviceLoaders\") } serviceLoaders.headOption } /** * This is a helper function to complete the missing S3A magic committer configurations * based on a single conf: `spark.hadoop.fs.s3a.bucket.<bucket>.committer.magic.enabled`  private def fillMissingMagicCommitterConfsIfNeeded(conf: SparkConf): Unit = { val magicCommitterConfs = conf .getAllWithPrefix(\"spark.hadoop.fs.s3a.bucket.\") .filter(_._1.endsWith(\".committer.magic.enabled\")) .filter(_._2.equalsIgnoreCase(\"true\")) if (magicCommitterConfs.nonEmpty) { // Try to enable S3 magic committer if missing conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\") if (conf.get(\"spark.hadoop.fs.s3a.committer.magic.enabled\").equals(\"true\")) { conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") conf.setIfMissing(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\") conf.setIfMissing(\"spark.sql.parquet.output.committer.class\", \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\") conf.setIfMissing(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\") } } } /** * SPARK-36796: This is a helper function to supplement `--add-opens` options to * `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`.  private def supplementJavaModuleOptions(conf: SparkConf): Unit = { def supplement(key: OptionalConfigEntry[String]): Unit = { val v = conf.get(key) match { case Some(opts) => s\"${JavaModuleOptions.defaultModuleOptions()} $opts\" case None => JavaModuleOptions.defaultModuleOptions() } conf.set(key.key, v) } supplement(DRIVER_JAVA_OPTIONS) supplement(EXECUTOR_JAVA_OPTIONS) } } /** * A collection of regexes for extracting information from the master string.  private object SparkMasterRegex { // Regular expression used for local[N] and local[*] master formats val LOCAL_N_REGEX = \"\"\"local\\[([0-9]+|\\*)\\]\"\"\".r // Regular expression for local[N, maxRetries], used in tests with failing tasks val LOCAL_N_FAILURES_REGEX = \"\"\"local\\[([0-9]+|\\*)\\s*,\\s*([0-9]+)\\]\"\"\".r // Regular expression for simulating a Spark cluster of [N, cores, memory] locally val LOCAL_CLUSTER_REGEX = \"\"\"local-cluster\\[\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*]\"\"\".r // Regular expression for connecting to Spark deploy clusters val SPARK_REGEX = \"\"\"spark://(.*)\"\"\".r // Regular expression for connecting to kubernetes clusters val KUBERNETES_REGEX = \"\"\"k8s://(.*)\"\"\".r } /** * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`).  private[spark] class WritableConverter[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: Writable => T) extends Serializable object WritableConverter { // Helper objects for converting common types to Writable private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T) : WritableConverter[T] = { val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]] new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W])) } // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. // The following implicit declarations have been added on top of the very similar ones // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta // expansion of zero-arg methods and thus won't match a no-arg method where it expects // an implicit that is a function of no args. implicit val intWritableConverterFn: () => WritableConverter[Int] = () => simpleWritableConverter[Int, IntWritable](_.get) implicit val longWritableConverterFn: () => WritableConverter[Long] = () => simpleWritableConverter[Long, LongWritable](_.get) implicit val doubleWritableConverterFn: () => WritableConverter[Double] = () => simpleWritableConverter[Double, DoubleWritable](_.get) implicit val floatWritableConverterFn: () => WritableConverter[Float] = () => simpleWritableConverter[Float, FloatWritable](_.get) implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] = () => simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = { () => simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit val stringWritableConverterFn: () => WritableConverter[String] = () => simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] = () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) // These implicits remain included for backwards-compatibility. They fulfill the // same role as those above. implicit def intWritableConverter(): WritableConverter[Int] = simpleWritableConverter[Int, IntWritable](_.get) implicit def longWritableConverter(): WritableConverter[Long] = simpleWritableConverter[Long, LongWritable](_.get) implicit def doubleWritableConverter(): WritableConverter[Double] = simpleWritableConverter[Double, DoubleWritable](_.get) implicit def floatWritableConverter(): WritableConverter[Float] = simpleWritableConverter[Float, FloatWritable](_.get) implicit def booleanWritableConverter(): WritableConverter[Boolean] = simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = { simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit def stringWritableConverter(): WritableConverter[String] = simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] = new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) } /** * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The `Writable` class will be used in `SequenceFileRDDFunctions`.  private[spark] class WritableFactory[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: T => Writable) extends Serializable object WritableFactory { private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W) : WritableFactory[T] = { val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]] new WritableFactory[T](_ => writableClass, convert) } implicit def intWritableFactory: WritableFactory[Int] = simpleWritableFactory(new IntWritable(_)) implicit def longWritableFactory: WritableFactory[Long] = simpleWritableFactory(new LongWritable(_)) implicit def floatWritableFactory: WritableFactory[Float] = simpleWritableFactory(new FloatWritable(_)) implicit def doubleWritableFactory: WritableFactory[Double] = simpleWritableFactory(new DoubleWritable(_)) implicit def booleanWritableFactory: WritableFactory[Boolean] = simpleWritableFactory(new BooleanWritable(_)) implicit def bytesWritableFactory: WritableFactory[Array[Byte]] = simpleWritableFactory(new BytesWritable(_)) implicit def stringWritableFactory: WritableFactory[String] = simpleWritableFactory(new Text(_)) implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] = simpleWritableFactory(w => w) }"
          ],
          "tokens": 953,
          "more_info": "Didn't return solution, needed to be asked for solution to save it to .md file"
        },
        "without_mcp": {
          "answer": "# Three Most Important Classes in Apache Spark 3.3.0\n\nThis document briefly describes three foundational classes that underpin Spark's core and SQL functionality: SparkContext, RDD, and SparkSession. These classes are central to application lifecycle, distributed computation, and structured APIs.\n\n## 1) SparkContext\n- Location: core/src/main/scala/org/apache/spark/SparkContext.scala\n- Role: Entry point to the Spark cluster for low-level APIs. Manages the application lifecycle, configuration, resource allocation, job submission, task scheduling, and interaction with cluster managers.\n- Why its important: Almost every Spark application initializes a SparkContext (directly or indirectly via SparkSession). It orchestrates RDD creation, accumulators/broadcasts, and coordinates with the scheduler and executors.\n- Key responsibilities:\n  - Initialize and configure the Spark application (SparkConf, environment setup).\n  - Connect to cluster managers (Standalone, YARN, Mesos, Kubernetes).\n  - Create RDDs from various sources (collections, files, Hadoop input formats).\n  - Submit jobs and stages to the scheduler; track jobs via listeners and UI.\n  - Manage shared variables (Broadcast, AccumulatorV2) and plugin lifecycle.\n- Notable methods/constructs:\n  - parallelize, textFile, wholeTextFiles, sequenceFile\n  - runJob, submitJob (job submission pipeline)\n  - broadcast, accumulator\n  - getConf, defaultParallelism\n\n## 2) RDD (Resilient Distributed Dataset)\n- Location: core/src/main/scala/org/apache/spark/rdd/RDD.scala\n- Role: Fundamental immutable distributed collection with lineage-based fault tolerance. Provides transformations and actions for low-level data-parallel computations.\n- Why its important: It is the core abstraction enabling Sparks distributed processing model, caching, partitioning, and fault recovery. Many higher-level APIs (DataFrames/Datasets) ultimately compile down to RDD operations at execution time.\n- Key responsibilities:\n  - Represent data partitions and dependencies forming a DAG (lineage).\n  - Provide lazy transformations and eager actions.\n  - Expose persistence/caching semantics and partition locality hints.\n  - Facilitate compute() for partitions and preferredLocations.\n- Notable methods/constructs:\n  - Transformations: map, flatMap, filter, union, join, groupByKey, reduceByKey\n  - Actions: collect, count, first, take, reduce, saveAsTextFile\n  - persist, cache, unpersist\n  - getNumPartitions, partitions, dependencies, toDebugString\n\n## 3) SparkSession\n- Location: sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala\n- Role: Unified entry point for Spark SQL, DataFrames, Datasets, and catalog. Wraps a SparkContext and manages SQL configuration, catalog, and session state.\n- Why its important: Modern Spark applications use SparkSession as the primary API surface for structured data processing, including reading/writing data sources, SQL queries, and function registration.\n- Key responsibilities:\n  - Provide DataFrame/Dataset creation and SQL execution (sql, table, range).\n  - Manage catalog (databases, tables, views, functions) and session state.\n  - Coordinate with SharedState for cache manager, listener, and external catalog.\n  - Offer DataFrameReader/DataFrameWriter for data sources (Parquet, CSV, etc.).\n- Notable methods/constructs:\n  - builder, getOrCreate (session lifecycle)\n  - read, readStream, sql\n  - createDataFrame, udf registration via udf, catalog operations\n\n---\n\nNotes\n- These components interact closely: SparkSession holds/uses a SparkContext; DataFrame/Dataset operations compile to logical/physical plans executed on RDDs; SparkContext drives the execution on the cluster.\n- Other important classes exist (e.g., TaskSchedulerImpl, SparkEnv, Dataset, DataFrame, SQLExecution), but the three above form the backbone across core and SQL layers.",
          "answer_time": 56,
          "tokens": 0,
          "more_info": "Identified project as spark and guessed which classes are most important, chose 2 different classes comapred to mcp"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 3,
          "reasoning": "Answer A provides a complete, well-structured response with clear explanations of each class's purpose and importance. It covers all three classes fully with logical flow and excellent readability. Answer B appears incomplete (cuts off mid-sentence) and while technically detailed, it's harder to follow with excessive technical jargon that obscures the main points. Answer A better addresses what a user would want to know about the most important classes."
        },
        "mcp_vs_no_mcp": {
          "winner": "without_mcp",
          "score_a": 4,
          "score_b": 5,
          "reasoning": "Both answers are well-structured and comprehensive. Answer A provides excellent technical detail and clear explanations of RDD, Dataset, and SparkContext. However, Answer B makes a more informed choice by selecting SparkSession instead of Dataset as the third class, which better represents the modern Spark API entry point. Answer B also provides more specific technical details like file locations, method names, and explains the relationships between classes more clearly. The structure in Answer B with bullet points and clear categorization makes it more readable and actionable for developers."
        }
      }
    },
    {
      "id": "Q006",
      "question": "What are 10 entities with the most number of neighbors?",
      "category": "top-n",
      "ground_truth_contexts": [
        "expression - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala - Metric value: 959.00 logicalplan - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala - Metric value: 874.00 utils - OBJECT - core/src/main/scala/org/apache/spark/util/Utils.scala - Metric value: 861.00 querycompilationerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala - Metric value: 789.00 rdd - CLASS - core/src/main/scala/org/apache/spark/rdd/RDD.scala - Metric value: 731.00 queryexecutionerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala - Metric value: 709.00 logging - TRAIT - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 664.00 column - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Column.scala - Metric value: 637.00 internalrow - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala - Metric value: 612.00 loginfo - METHOD - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 595.00"
      ],
      "key_entities": [
        "org/apache/spark/sql/catalyst/expressions/Expression#",
        "org/apache/spark/sql/catalyst/plans/logical/LogicalPlan#",
        "org/apache/spark/ml/impl/Utils.",
        "org/apache/spark/sql/errors/QueryCompilationErrors.",
        "org/apache/spark/rdd/RDD#",
        "org/apache/spark/sql/errors/QueryExecutionErrors.",
        "org/apache/spark/internal/Logging#",
        "org/apache/spark/sql/catalog/Column#",
        "org/apache/spark/sql/catalyst/InternalRow#",
        "org/apache/spark/internal/Logging#logInfo(+1)."
      ],
      "claude_stats": {
        "answer": "Here are the **10 entities with the most number of neighbors** in the codebase:\n\n1. **Expression** (CLASS) - 959 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala`\n\n2. **LogicalPlan** (CLASS) - 874 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala`\n\n3. **Utils** (OBJECT) - 861 neighbors\n   - Location: `core/src/main/scala/org/apache/spark/util/Utils.scala`\n\n4. **QueryCompilationErrors** (OBJECT) - 789 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala`\n\n5. **RDD** (CLASS) - 731 neighbors\n   - Location: `core/src/main/scala/org/apache/spark/rdd/RDD.scala`\n\n6. **QueryExecutionErrors** (OBJECT) - 709 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala`\n\n7. **Logging** (TRAIT) - 664 neighbors\n   - Location: `core/src/main/scala/org/apache/spark/internal/Logging.scala`\n\n8. **Column** (CLASS) - 637 neighbors\n   - Location: `sql/core/src/main/scala/org/apache/spark/sql/Column.scala`\n\n9. **InternalRow** (CLASS) - 612 neighbors\n   - Location: `sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala`\n\n10. **logInfo** (METHOD) - 595 neighbors\n    - Location: `core/src/main/scala/org/apache/spark/internal/Logging.scala`\n\nThese entities are highly connected within the codebase, which suggests they play central roles in the architecture. The **Expression** and **LogicalPlan** classes, in particular, are core abstractions in Spark's SQL catalyst framework.",
        "prompt_time": 9.489,
        "answer_time": 28,
        "context_tokens": 400,
        "tokens": 554,
        "used_context": [
          "expression - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala - Metric value: 959.00 logicalplan - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala - Metric value: 874.00 utils - OBJECT - core/src/main/scala/org/apache/spark/util/Utils.scala - Metric value: 861.00 querycompilationerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala - Metric value: 789.00 rdd - CLASS - core/src/main/scala/org/apache/spark/rdd/RDD.scala - Metric value: 731.00 queryexecutionerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala - Metric value: 709.00 logging - TRAIT - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 664.00 column - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Column.scala - Metric value: 637.00 internalrow - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala - Metric value: 612.00 loginfo - METHOD - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 595.00"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "# Top 10 Entities by Number of Neighbors\n\nThis document lists the 10 entities in this repository with the highest number of neighbors (incoming and outgoing connections in the code graph).\n\n- Metric: number_of_neighbors\n- Order: descending (most connected first)\n\n1. expression  CLASS  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala  959\n2. logicalplan  CLASS  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala  874\n3. utils  OBJECT  core/src/main/scala/org/apache/spark/util/Utils.scala  861\n4. querycompilationerrors  OBJECT  sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala  789\n5. rdd  CLASS  core/src/main/scala/org/apache/spark/rdd/RDD.scala  731\n6. queryexecutionerrors  OBJECT  sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala  709\n7. logging  TRAIT  core/src/main/scala/org/apache/spark/internal/Logging.scala  664\n8. column  CLASS  sql/core/src/main/scala/org/apache/spark/sql/Column.scala  637\n9. internalrow  CLASS  sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala  612\n10. loginfo  METHOD  core/src/main/scala/org/apache/spark/internal/Logging.scala  595\n\nNotes\n- These values come from the static code graph analysis for this repository version.\n- \"Neighbors\" includes related entities linked through declarations, calls, and type relations, as captured by the analyzer.\n\nSuggested next step\n- The top entity is `expression` (CLASS).",
          "prompt_time": 8.03,
          "answer_time": 52,
          "context_tokens": 300,
          "used_context": [
            "expression - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala - Metric value: 959.00 logicalplan - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala - Metric value: 874.00 utils - OBJECT - core/src/main/scala/org/apache/spark/util/Utils.scala - Metric value: 861.00 querycompilationerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala - Metric value: 789.00 rdd - CLASS - core/src/main/scala/org/apache/spark/rdd/RDD.scala - Metric value: 731.00 queryexecutionerrors - OBJECT - sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala - Metric value: 709.00 logging - TRAIT - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 664.00 column - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Column.scala - Metric value: 637.00 internalrow - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/InternalRow.scala - Metric value: 612.00 loginfo - METHOD - core/src/main/scala/org/apache/spark/internal/Logging.scala - Metric value: 595.00"
          ],
          "tokens": 410,
          "more_info": "Returned answer in .md file - learned it from before"
        },
        "without_mcp": {
          "answer": "# Top 10 Entities by Number of Neighbors\n\nTimestamp: 2025-12-27 13:21 (local time)\n\nThis document lists 10 entities in the Apache Spark 3.3.0 source tree that are expected to have the most related entities (neighbors). Related here broadly covers declarations, references (calls/usages), inheritance/extension, and other static relationships typically present in a code knowledge graph.\n\nNote: The repository does not ship with a precomputed entity-relationship graph, so the ranking below is based on well-established architectural roles in Spark and typical connectivity patterns of these components. It is intended to satisfy the request format (answer in a .md file) and provide a practical ranking; exact counts can vary depending on the relationship model used.\n\n1. org.apache.spark.rdd.RDD  neighbors: 300+\n2. org.apache.spark.SparkContext  neighbors: 250+\n3. org.apache.spark.sql.SparkSession  neighbors: 220+\n4. org.apache.spark.sql.DataFrame  neighbors: 200+\n5. org.apache.spark.sql.Dataset  neighbors: 200+\n6. org.apache.spark.sql.functions (package object)  neighbors: 180+\n7. org.apache.spark.scheduler.DAGScheduler  neighbors: 170+\n8. org.apache.spark.scheduler.TaskSchedulerImpl  neighbors: 160+\n9. org.apache.spark.storage.BlockManager  neighbors: 150+\n10. org.apache.spark.util.Utils  neighbors: 140+\n\nIf you need an exact, reproducible ranking from this repository, please provide (or point to) the entity graph export (e.g., JSON of nodes and edges) or enable the graph extraction step, and I will compute precise degree counts (number_of_neighbors) and update this file accordingly.",
          "answer_time": 212,
          "tokens": 362,
          "more_info": "Answer not precised, changed neighbors from question to related entities so Junie gets better context"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 4,
          "score_b": 3,
          "reasoning": "Both answers provide the same accurate data, but Answer A has better readability with proper capitalization, clearer formatting, and more informative context about the entities' roles in Spark's architecture. Answer A also flows more naturally as a response, while Answer B reads more like a technical report with overly formal structure and lowercase naming that reduces clarity."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 4,
          "score_b": 2,
          "reasoning": "Answer A provides specific, quantified data with exact neighbor counts (959, 874, etc.) and precise file locations, directly answering the question. Answer B admits it lacks actual data and provides estimates with vague ranges (300+, 250+), essentially acknowledging it cannot properly answer the question. While B has good structure, A delivers concrete results that fulfill the user's request for the top 10 entities with most neighbors."
        }
      }
    },
    {
      "id": "Q007",
      "question": "How does the project manage external data connectivity and writing?",
      "category": "general",
      "parameters": {
        "kinds": [
          "CLASS",
          "METHOD",
          "TRAIT"
        ],
        "keywords": [
          "write",
          "batch",
          "stream",
          "sink",
          "source",
          "connector",
          "jdbc",
          "provider",
          "commit",
          "options"
        ],
        "top_k": 8,
        "max_neighbors": 4
      },
      "ground_truth_contexts": [
        "class JdbcOptionsInWrite( override val parameters: CaseInsensitiveMap[String]) extends JDBCOptions(parameters) { import JDBCOptions._ def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters)) def this(url: String, table: String, parameters: Map[String, String]) = { this(CaseInsensitiveMap(parameters ++ Map( JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table))) } require( parameters.get(JDBC_TABLE_NAME).isDefined, s\"Option '$JDBC_TABLE_NAME' is required. \" + s\"Option '$JDBC_QUERY_STRING' is not applicable while writing.\") val table = parameters(JDBC_TABLE_NAME) } object JDBCOptions { private val curId = new java.util.concurrent.atomic.AtomicLong(0L) private val jdbcOptionNames = collection.mutable.Set[String]() private def newOption(name: String): String = { jdbcOptionNames += name.toLowerCase(Locale.ROOT) name } val JDBC_URL = newOption(\"url\") val JDBC_TABLE_NAME = newOption(\"dbtable\") val JDBC_QUERY_STRING = newOption(\"query\") val JDBC_DRIVER_CLASS = newOption(\"driver\") val JDBC_PARTITION_COLUMN = newOption(\"partitionColumn\") val JDBC_LOWER_BOUND = newOption(\"lowerBound\") val JDBC_UPPER_BOUND = newOption(\"upperBound\") val JDBC_NUM_PARTITIONS = newOption(\"numPartitions\") val JDBC_QUERY_TIMEOUT = newOption(\"queryTimeout\") val JDBC_BATCH_FETCH_SIZE = newOption(\"fetchsize\") val JDBC_TRUNCATE = newOption(\"truncate\") val JDBC_CASCADE_TRUNCATE = newOption(\"cascadeTruncate\") val JDBC_CREATE_TABLE_OPTIONS = newOption(\"createTableOptions\") val JDBC_CREATE_TABLE_COLUMN_TYPES = newOption(\"createTableColumnTypes\") val JDBC_CUSTOM_DATAFRAME_COLUMN_TYPES = newOption(\"customSchema\") val JDBC_BATCH_INSERT_SIZE = newOption(\"batchsize\") val JDBC_TXN_ISOLATION_LEVEL = newOption(\"isolationLevel\") val JDBC_SESSION_INIT_STATEMENT = newOption(\"sessionInitStatement\") val JDBC_PUSHDOWN_PREDICATE = newOption(\"pushDownPredicate\") val JDBC_PUSHDOWN_AGGREGATE = newOption(\"pushDownAggregate\") val JDBC_PUSHDOWN_LIMIT = newOption(\"pushDownLimit\") val JDBC_PUSHDOWN_TABLESAMPLE = newOption(\"pushDownTableSample\") val JDBC_KEYTAB = newOption(\"keytab\") val JDBC_PRINCIPAL = newOption(\"principal\") val JDBC_TABLE_COMMENT = newOption(\"tableComment\") val JDBC_REFRESH_KRB5_CONFIG = newOption(\"refreshKrb5Config\") val JDBC_CONNECTION_PROVIDER = newOption(\"connectionProvider\") }",
        "* with [[MicroBatchWrite]] before execution. */ case class WriteToMicroBatchDataSource( relation: Option[DataSourceV2Relation], table: SupportsWrite, query: LogicalPlan, queryId: String, writeOptions: Map[String, String], outputMode: OutputMode, batchId: Option[Long] = None) extends UnaryNode { override def child: LogicalPlan = query override def output: Seq[Attribute] = Nil def withNewBatchId(batchId: Long): WriteToMicroBatchDataSource = { copy(batchId = Some(batchId)) } override protected def withNewChildInternal(newChild: LogicalPlan): WriteToMicroBatchDataSource = copy(query = newChild) }",
        "*/ class MicroBatchWrite(eppchId: Long, val writeSupport: StreamingWrite) extends BatchWrite { override def commit(messages: Array[WriterCommitMessage]): Unit = { writeSupport.commit(eppchId, messages) } override def abort(messages: Array[WriterCommitMessage]): Unit = { writeSupport.abort(eppchId, messages) } override def createBatchWriterFactory(info: PhysicalWriteInfo): DataWriterFactory = { new MicroBatchWriterFactory(eppchId, writeSupport.createStreamingWriterFactory(info)) } } class MicroBatchWriterFactory(epochId: Long, streamingWriterFactory: StreamingDataWriterFactory) extends DataWriterFactory { override def createWriter(partitionId: Int, taskId: Long): DataWriter[InternalRow] = { streamingWriterFactory.createWriter(partitionId, taskId, epochId) } }",
        "*/ class JDBCOptions( val parameters: CaseInsensitiveMap[String]) extends Serializable with Logging { import JDBCOptions._ def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters)) def this(url: String, table: String, parameters: Map[String, String]) = { this(CaseInsensitiveMap(parameters ++ Map( JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table))) } /** * Returns a property with all options. */ val asProperties: Properties = { val properties = new Properties() parameters.originalMap.foreach { case (k, v) => properties.setProperty(k, v) } properties } /** * Returns a property with all options except Spark internal data source options like `url`, * `dbtable`, and `numPartition`. This should be used when invoking JDBC API like `Driver.connect` * because each DBMS vendor has its own property list for JDBC driver. See SPARK-17776. */ val asConnectionProperties: Properties = { val properties = new Properties() parameters.originalMap.filterKeys(key => !jdbcOptionNames(key.toLowerCase(Locale.ROOT))) .foreach { case (k, v) => properties.setProperty(k, v) } properties } // ------------------------------------------------------------ // Required parameters // ------------------------------------------------------------ require(parameters.isDefinedAt(JDBC_URL), s\"Option '$JDBC_URL' is required.\") // a JDBC URL val url = parameters(JDBC_URL) // table name or a table subquery. val tableOrQuery = (parameters.get(JDBC_TABLE_NAME), parameters.get(JDBC_QUERY_STRING)) match { case (Some(name), Some(subquery)) => throw QueryExecutionErrors.cannotSpecifyBothJdbcTableNameAndQueryError( JDBC_TABLE_NAME, JDBC_QUERY_STRING) case (None, None) => throw QueryExecutionErrors.missingJdbcTableNameAndQueryError( JDBC_TABLE_NAME, JDBC_QUERY_STRING) case (Some(name), None) => if (name.isEmpty) { throw QueryExecutionErrors.emptyOptionError(JDBC_TABLE_NAME) } else { name.trim } case (None, Some(subquery)) => if (subquery.isEmpty) { throw QueryExecutionErrors.emptyOptionError(JDBC_QUERY_STRING) } else { s\"(${subquery}) SPARK_GEN_SUBQ_${curId.getAndIncrement()}\" } } // ------------------------------------------------------------ // Optional parameters // ------------------------------------------------------------ val driverClass = { val userSpecifiedDriverClass = parameters.get(JDBC_DRIVER_CLASS) userSpecifiedDriverClass.foreach(DriverRegistry.register) // Performing this part of the logic on the driver guards against the corner-case where the // driver returned for a URL is different on the driver and executors due to classpath // differences. userSpecifiedDriverClass.getOrElse { DriverManager.getDriver(url).getClass.getCanonicalName } } // the number of partitions val numPartitions = parameters.get(JDBC_NUM_PARTITIONS).map(_.toInt) // the number of seconds the driver will wait for a Statement object to execute to the given // number of seconds. Zero means there is no limit. val queryTimeout = parameters.getOrElse(JDBC_QUERY_TIMEOUT, \"0\").toInt // ------------------------------------------------------------ // Optional parameters only for reading // ------------------------------------------------------------ // the column used to partition val partitionColumn = parameters.get(JDBC_PARTITION_COLUMN) // the lower bound of partition column val lowerBound = parameters.get(JDBC_LOWER_BOUND) // the upper bound of the partition column val upperBound = parameters.get(JDBC_UPPER_BOUND) // numPartitions is also used for data source writing require((partitionColumn.isEmpty && lowerBound.isEmpty && upperBound.isEmpty) || (partitionColumn.isDefined && lowerBound.isDefined && upperBound.isDefined && numPartitions.isDefined), s\"When reading JDBC data sources, users need to specify all or none for the following \" + s\"options: '$JDBC_PARTITION_COLUMN', '$JDBC_LOWER_BOUND', '$JDBC_UPPER_BOUND', \" + s\"and '$JDBC_NUM_PARTITIONS'\") require(!(parameters.get(JDBC_QUERY_STRING).isDefined && partitionColumn.isDefined), s\"\"\" |Options '$JDBC_QUERY_STRING' and '$JDBC_PARTITION_COLUMN' can not be specified together. |Please define the query using `$JDBC_TABLE_NAME` option instead and make sure to qualify |the partition columns using the supplied subquery alias to resolve any ambiguity. |Example : |spark.read.format(\"jdbc\") | .option(\"url\", jdbcUrl) | .option(\"dbtable\", \"(select c1, c2 from t1) as subq\") | .option(\"partitionColumn\", \"c1\") | .option(\"lowerBound\", \"1\") | .option(\"upperBound\", \"100\") | .option(\"numPartitions\", \"3\") | .load() \"\"\".stripMargin ) val fetchSize = parameters.getOrElse(JDBC_BATCH_FETCH_SIZE, \"0\").toInt // ------------------------------------------------------------ // Optional parameters only for writing // ------------------------------------------------------------ // if to truncate the table from the JDBC database val isTruncate = parameters.getOrElse(JDBC_TRUNCATE, \"false\").toBoolean val isCascadeTruncate: Option[Boolean] = parameters.get(JDBC_CASCADE_TRUNCATE).map(_.toBoolean) // the create table option , which can be table_options or partition_options. // E.g., \"CREATE TABLE t (name string) ENGINE=InnoDB DEFAULT CHARSET=utf8\" // TODO: to reuse the existing partition parameters for those partition specific options val createTableOptions = parameters.getOrElse(JDBC_CREATE_TABLE_OPTIONS, \"\") val createTableColumnTypes = parameters.get(JDBC_CREATE_TABLE_COLUMN_TYPES) val customSchema = parameters.get(JDBC_CUSTOM_DATAFRAME_COLUMN_TYPES) val batchSize = { val size = parameters.getOrElse(JDBC_BATCH_INSERT_SIZE, \"1000\").toInt require(size >= 1, s\"Invalid value `${size.toString}` for parameter \" + s\"`$JDBC_BATCH_INSERT_SIZE`. The minimum value is 1.\") size } val isolationLevel = parameters.getOrElse(JDBC_TXN_ISOLATION_LEVEL, \"READ_UNCOMMITTED\") match { case \"NONE\" => Connection.TRANSACTION_NONE case \"READ_UNCOMMITTED\" => Connection.TRANSACTION_READ_UNCOMMITTED case \"READ_COMMITTED\" => Connection.TRANSACTION_READ_COMMITTED case \"REPEATABLE_READ\" => Connection.TRANSACTION_REPEATABLE_READ case \"SERIALIZABLE\" => Connection.TRANSACTION_SERIALIZABLE case other => throw QueryExecutionErrors.invalidJdbcTxnIsolationLevelError( JDBC_TXN_ISOLATION_LEVEL, other) } // An option to execute custom SQL before fetching data from the remote DB val sessionInitStatement = parameters.get(JDBC_SESSION_INIT_STATEMENT) // An option to allow/disallow pushing down predicate into JDBC data source val pushDownPredicate = parameters.getOrElse(JDBC_PUSHDOWN_PREDICATE, \"true\").toBoolean // An option to allow/disallow pushing down aggregate into JDBC data source // This only applies to Data Source V2 JDBC val pushDownAggregate = parameters.getOrElse(JDBC_PUSHDOWN_AGGREGATE, \"false\").toBoolean // An option to allow/disallow pushing down LIMIT into V2 JDBC data source // This only applies to Data Source V2 JDBC val pushDownLimit = parameters.getOrElse(JDBC_PUSHDOWN_LIMIT, \"false\").toBoolean // An option to allow/disallow pushing down TABLESAMPLE into JDBC data source // This only applies to Data Source V2 JDBC val pushDownTableSample = parameters.getOrElse(JDBC_PUSHDOWN_TABLESAMPLE, \"false\").toBoolean // The local path of user's keytab file, which is assumed to be pre-uploaded to all nodes either // by --files option of spark-submit or manually val keytab = { val keytabParam = parameters.getOrElse(JDBC_KEYTAB, null) if (keytabParam != null && FilenameUtils.getPath(keytabParam).isEmpty) { val result = SparkFiles.get(keytabParam) logDebug(s\"Keytab path not found, assuming --files, file name used on executor: $result\") result } else { logDebug(\"Keytab path found, assuming manual upload\") keytabParam } } // The principal name of user's keytab file val principal = parameters.getOrElse(JDBC_PRINCIPAL, null) val tableComment = parameters.getOrElse(JDBC_TABLE_COMMENT, \"\") val refreshKrb5Config = parameters.getOrElse(JDBC_REFRESH_KRB5_CONFIG, \"false\").toBoolean // User specified JDBC connection provider name val connectionProviderName = parameters.get(JDBC_CONNECTION_PROVIDER) } class JdbcOptionsInWrite( override val parameters: CaseInsensitiveMap[String]) extends JDBCOptions(parameters) { import JDBCOptions._ def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters)) def this(url: String, table: String, parameters: Map[String, String]) = { this(CaseInsensitiveMap(parameters ++ Map( JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table))) } require( parameters.get(JDBC_TABLE_NAME).isDefined, s\"Option '$JDBC_TABLE_NAME' is required. \" + s\"Option '$JDBC_QUERY_STRING' is not applicable while writing.\") val table = parameters(JDBC_TABLE_NAME) } object JDBCOptions { private val curId = new java.util.concurrent.atomic.AtomicLong(0L) private val jdbcOptionNames = collection.mutable.Set[String]() private def newOption(name: String): String = { jdbcOptionNames += name.toLowerCase(Locale.ROOT) name } val JDBC_URL = newOption(\"url\") val JDBC_TABLE_NAME = newOption(\"dbtable\") val JDBC_QUERY_STRING = newOption(\"query\") val JDBC_DRIVER_CLASS = newOption(\"driver\") val JDBC_PARTITION_COLUMN = newOption(\"partitionColumn\") val JDBC_LOWER_BOUND = newOption(\"lowerBound\") val JDBC_UPPER_BOUND = newOption(\"upperBound\") val JDBC_NUM_PARTITIONS = newOption(\"numPartitions\") val JDBC_QUERY_TIMEOUT = newOption(\"queryTimeout\") val JDBC_BATCH_FETCH_SIZE = newOption(\"fetchsize\") val JDBC_TRUNCATE = newOption(\"truncate\") val JDBC_CASCADE_TRUNCATE = newOption(\"cascadeTruncate\") val JDBC_CREATE_TABLE_OPTIONS = newOption(\"createTableOptions\") val JDBC_CREATE_TABLE_COLUMN_TYPES = newOption(\"createTableColumnTypes\") val JDBC_CUSTOM_DATAFRAME_COLUMN_TYPES = newOption(\"customSchema\") val JDBC_BATCH_INSERT_SIZE = newOption(\"batchsize\") val JDBC_TXN_ISOLATION_LEVEL = newOption(\"isolationLevel\") val JDBC_SESSION_INIT_STATEMENT = newOption(\"sessionInitStatement\") val JDBC_PUSHDOWN_PREDICATE = newOption(\"pushDownPredicate\") val JDBC_PUSHDOWN_AGGREGATE = newOption(\"pushDownAggregate\") val JDBC_PUSHDOWN_LIMIT = newOption(\"pushDownLimit\") val JDBC_PUSHDOWN_TABLESAMPLE = newOption(\"pushDownTableSample\") val JDBC_KEYTAB = newOption(\"keytab\") val JDBC_PRINCIPAL = newOption(\"principal\") val JDBC_TABLE_COMMENT = newOption(\"tableComment\") val JDBC_REFRESH_KRB5_CONFIG = newOption(\"refreshKrb5Config\") val JDBC_CONNECTION_PROVIDER = newOption(\"connectionProvider\") }",
        "writtenRows case _ => // Table does not support writes - staged changes are also rolled back below if table // is staging. throw QueryExecutionErrors.unsupportedTableWritesError(ident) } })(catchBlock = { table match { // Failure rolls back the staged writes and metadata changes. case st: StagedTable => st.abortStagedChanges() case _ => catalog.dropTable(ident) } }) } } private[v2] case class DataWritingSparkTaskResult( numRows: Long, writerCommitMessage: WriterCommitMessage) /** * Sink progress information collected after commit. */ private[sql] case class StreamWriterCommitProgress(numOutputRows: Long)"
      ],
      "key_entities": [
        "org/apache/spark/sql/execution/datasources/jdbc/JdbcOptionsInWrite#",
        "org/apache/spark/sql/execution/streaming/sources/WriteToMicroBatchDataSource#",
        "org/apache/spark/sql/execution/streaming/sources/MicroBatchWrite#",
        "org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions#",
        "org/apache/spark/sql/execution/datasources/v2/StreamWriterCommitProgress#"
      ],
      "claude_stats": {
        "answer": "Based on the code analysis, here's how the project manages external data connectivity and writing:\n\n## Overview\n\nThe project (Apache Spark SQL) uses a **layered writer architecture** that handles both batch and streaming data writes to external systems.\n\n---\n\n## Core Components\n\n### 1. **DataFrameWriter (Batch Writes)**\nLocated in `org/apache/spark/sql/DataFrameWriter#`, this handles writing non-streaming DataFrames.\n\n**Key responsibilities:**\n- Configure write operations through method chaining (mode, format, partitioning, bucketing)\n- Support multiple save modes: Overwrite, Append, Ignore, ErrorIfExists\n- Handle both v1 (legacy) and v2 (modern) data sources\n\n**Main write methods:**\n- `save(path)` - writes to specified path\n- `saveAsTable(tableName)` - writes to a table in the catalog\n- `insertInto(tableName)` - inserts into existing table\n- Format-specific methods: `json()`, `parquet()`, `orc()`, `csv()`, `text()`, `jdbc()`\n\n### 2. **DataStreamWriter (Streaming Writes)**\nLocated in `org/apache/spark/sql/streaming/DataStreamWriter#`, handles streaming DataFrame writes.\n\n**Key responsibilities:**\n- Configure streaming writes with trigger intervals, output modes, watermarks\n- Support specialized streaming sinks: `memory`, `foreach`, `foreachBatch`\n- Handle checkpointing for fault tolerance\n\n**Main methods:**\n- `start(path)` - begins streaming to path\n- `toTable(tableName)` - streams to table\n- `foreach(writer)` - custom per-row processing\n- `foreachBatch(function)` - custom per-batch processing\n\n---\n\n## Data Source Architecture\n\n### **Dual API Support**\n\nThe project supports both:\n\n1. **V1 DataSources** (Legacy)\n   - Uses `DataSource` class to look up implementations\n   - Creates `BaseRelation` instances\n   - Uses Hadoop's input/output formats\n\n2. **V2 DataSources** (Modern)\n   - Uses `TableProvider` interface\n   - Provides better table management\n   - Supports capabilities checking (e.g., `BATCH_WRITE`, `STREAMING_WRITE`)\n   - Allows external table metadata\n\n### **Format Resolution**\n```scala\nval cls = DataSource.lookupDataSource(source, conf)\n// Checks if class is TableProvider (v2) or legacy format (v1)\n```\n\n---\n\n## Write Flow\n\n### **Batch Write Process:**\n\n1. **Configuration Phase**\n   ```scala\n   df.write\n     .mode(\"overwrite\")\n     .format(\"parquet\")\n     .partitionBy(\"year\", \"month\")\n     .save(\"/path/to/output\")\n   ```\n\n2. **Source Resolution**\n   - Looks up data source by format name\n   - Checks if v2 TableProvider or v1 source\n   - Handles disabled sources list\n\n3. **Execution**\n   - For v2: Creates logical plan nodes (`AppendData`, `OverwriteByExpression`, etc.)\n   - For v1: Uses `DataSource.planForWriting()` with Hadoop OutputFormat\n   - Runs via `SparkSession.sessionState.executePlan()`\n\n### **Streaming Write Process:**\n\n1. **Sink Selection**\n   - `memory`  `MemorySink` (for testing)\n   - `foreach`  `ForeachWriterTable`\n   - `foreachBatch`  `ForeachBatchSink`\n   - Other formats  Lookup via `DataSource`\n\n2. **Query Management**\n   ```scala\n   streamingQueryManager.startQuery(\n     queryName,\n     checkpointLocation,\n     df,\n     sink,\n     outputMode,\n     trigger\n   )\n   ```\n\n---\n\n## Key Features\n\n### **Partitioning & Bucketing**\n- File-based partitioning (Hive-style): `year=2024/month=01/`\n- Bucketing for co-location: distributes data by hash\n- Sorting within buckets\n\n### **Save Modes (Batch)**\n- `Overwrite` - replace existing data\n- `Append` - add to existing data\n- `Ignore` - no-op if exists\n- `ErrorIfExists` - fail if exists\n\n### **Output Modes (Streaming)**\n- `Append` - only new rows\n- `Complete` - all rows every trigger\n- `Update` - only updated rows\n\n### **Checkpointing**\nStreaming writes maintain checkpoints for fault tolerance:\n- Stores offset information\n- Enables exactly-once semantics\n- Configured via `checkpointLocation` option\n\n---\n\n## Example Usage\n\n**Batch:**\n```scala\ndf.write\n  .format(\"parquet\")\n  .mode(\"overwrite\")\n  .partitionBy(\"year\")\n  .save(\"s3://bucket/data\")\n```\n\n**Streaming:**\n```scala\ndf.writeStream\n  .format(\"kafka\")\n  .option(\"checkpointLocation\", \"/checkpoints\")\n  .outputMode(\"append\")\n  .trigger(Trigger.ProcessingTime(\"10 seconds\"))\n  .start()\n```\n\n**Custom Sink:**\n```scala\ndf.writeStream\n  .foreachBatch { (batchDF, batchId) =>\n    batchDF.write.jdbc(url, table, props)\n  }\n  .start()\n```\n\n---\n\n## Built-in Connectors\n\nThe system includes connectors for:\n- **File formats:** Parquet, JSON, ORC, CSV, Text\n- **Databases:** JDBC\n- **Streaming:** Kafka, console, memory\n- **Cloud storage:** S3, HDFS (via Hadoop FileSystem)\n\n",
        "prompt_time": 19.898,
        "answer_time": 60,
        "context_tokens": 104781,
        "tokens": 1015,
        "used_context": [
          "## CLASS: org/apache/spark/sql/streaming/DataStreamWriter# (implementation)\n*/ @Evolving final class DataStreamWriter[T] private[sql](ds: Dataset[T]) { import DataStreamWriter._ private val df = ds.toDF() /** * Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. * <ul> * <li> `OutputMode.Append()`: only the new rows in the streaming DataFrame/Dataset will be * written to the sink.</li> * <li> `OutputMode.Complete()`: all the rows in the streaming DataFrame/Dataset will be written * to the sink every time there are some updates.</li> * <li> `OutputMode.Update()`: only the rows that were updated in the streaming * DataFrame/Dataset will be written to the sink every time there are some updates. * If the query doesn't contain aggregations, it will be equivalent to * `OutputMode.Append()` mode.</li> * </ul> * * @since 2.0.0 */ def outputMode(outputMode: OutputMode): DataStreamWriter[T] = { this.outputMode = outputMode this } /** * Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink. * <ul> * <li> `append`: only the new rows in the streaming DataFrame/Dataset will be written to * the sink.</li> * <li> `complete`: all the rows in the streaming DataFrame/Dataset will be written to the sink * every time there are some updates.</li> * <li> `update`: only the rows that were updated in the streaming DataFrame/Dataset will * be written to the sink every time there are some updates. If the query doesn't * contain aggregations, it will be equivalent to `append` mode.</li> * </ul> * * @since 2.0.0 */ def outputMode(outputMode: String): DataStreamWriter[T] = { this.outputMode = InternalOutputModes(outputMode) this } /** * Set the trigger for the stream query. The default value is `ProcessingTime(0)` and it will run * the query as fast as possible. * * Scala Example: * {{{ * df.writeStream.trigger(ProcessingTime(\"10 seconds\")) * * import scala.concurrent.duration._ * df.writeStream.trigger(ProcessingTime(10.seconds)) * }}} * * Java Example: * {{{ * df.writeStream().trigger(ProcessingTime.create(\"10 seconds\")) * * import java.util.concurrent.TimeUnit * df.writeStream().trigger(ProcessingTime.create(10, TimeUnit.SECONDS)) * }}} * * @since 2.0.0 */ def trigger(trigger: Trigger): DataStreamWriter[T] = { this.trigger = trigger this } /** * Specifies the name of the [[StreamingQuery]] that can be started with `start()`. * This name must be unique among all the currently active queries in the associated SQLContext. * * @since 2.0.0 */ def queryName(queryName: String): DataStreamWriter[T] = { this.extraOptions += (\"queryName\" -> queryName) this } /** * Specifies the underlying output data source. * * @since 2.0.0 */ def format(source: String): DataStreamWriter[T] = { this.source = source this } /** * Partitions the output by the given columns on the file system. If specified, the output is * laid out on the file system similar to Hive's partitioning scheme. As an example, when we * partition a dataset by year and then month, the directory layout would look like: * * <ul> * <li> year=2016/month=01/</li> * <li> year=2016/month=02/</li> * </ul> * * Partitioning is one of the most widely used techniques to optimize physical data layout. * It provides a coarse-grained index for skipping unnecessary data reads when queries have * predicates on the partitioned columns. In order for partitioning to work well, the number * of distinct values in each column should typically be less than tens of thousands. * * @since 2.0.0 */ @scala.annotation.varargs def partitionBy(colNames: String*): DataStreamWriter[T] = { this.partitioningColumns = Option(colNames) this } /** * Adds an output option for the underlying data source. * * @since 2.0.0 */ def option(key: String, value: String): DataStreamWriter[T] = { this.extraOptions += (key -> value) this } /** * Adds an output option for the underlying data source. * * @since 2.0.0 */ def option(key: String, value: Boolean): DataStreamWriter[T] = option(key, value.toString) /** * Adds an output option for the underlying data source. * * @since 2.0.0 */ def option(key: String, value: Long): DataStreamWriter[T] = option(key, value.toString) /** * Adds an output option for the underlying data source. * * @since 2.0.0 */ def option(key: String, value: Double): DataStreamWriter[T] = option(key, value.toString) /** * (Scala-specific) Adds output options for the underlying data source. * * @since 2.0.0 */ def options(options: scala.collection.Map[String, String]): DataStreamWriter[T] = { this.extraOptions ++= options this } /** * Adds output options for the underlying data source. * * @since 2.0.0 */ def options(options: java.util.Map[String, String]): DataStreamWriter[T] = { this.options(options.asScala) this } /** * Starts the execution of the streaming query, which will continually output results to the given * path as new data arrives. The returned [[StreamingQuery]] object can be used to interact with * the stream. * * @since 2.0.0 */ def start(path: String): StreamingQuery = { if (!df.sparkSession.sessionState.conf.legacyPathOptionBehavior && extraOptions.contains(\"path\")) { throw QueryCompilationErrors.setPathOptionAndCallWithPathParameterError(\"start\") } startInternal(Some(path)) } /** * Starts the execution of the streaming query, which will continually output results to the given * path as new data arrives. The returned [[StreamingQuery]] object can be used to interact with * the stream. Throws a `TimeoutException` if the following conditions are met: * - Another run of the same streaming query, that is a streaming query * sharing the same checkpoint location, is already active on the same * Spark Driver * - The SQL configuration `spark.sql.streaming.stopActiveRunOnRestart` * is enabled * - The active run cannot be stopped within the timeout controlled by * the SQL configuration `spark.sql.streaming.stopTimeout` * * @since 2.0.0 */ @throws[TimeoutException] def start(): StreamingQuery = startInternal(None) /** * Starts the execution of the streaming query, which will continually output results to the given * table as new data arrives. The returned [[StreamingQuery]] object can be used to interact with * the stream. * * For v1 table, partitioning columns provided by `partitionBy` will be respected no matter the * table exists or not. A new table will be created if the table not exists. * * For v2 table, `partitionBy` will be ignored if the table already exists. `partitionBy` will be * respected only if the v2 table does not exist. Besides, the v2 table created by this API lacks * some functionalities (e.g., customized properties, options, and serde info). If you need them, * please create the v2 table manually before the execution to avoid creating a table with * incomplete information. * * @since 3.1.0 */ @Evolving @throws[TimeoutException] def toTable(tableName: String): StreamingQuery = { this.tableName = tableName import df.sparkSession.sessionState.analyzer.CatalogAndIdentifier import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._ val originalMultipartIdentifier = df.sparkSession.sessionState.sqlParser .parseMultipartIdentifier(tableName) val CatalogAndIdentifier(catalog, identifier) = originalMultipartIdentifier // Currently we don't create a logical streaming writer node in logical plan, so cannot rely // on analyzer to resolve it. Directly lookup only for temp view to provide clearer message. // TODO (SPARK-27484): we should add the writing node before the plan is analyzed. if (df.sparkSession.sessionState.catalog.isTempView(originalMultipartIdentifier)) { throw QueryCompilationErrors.tempViewNotSupportStreamingWriteError(tableName) } if (!catalog.asTableCatalog.tableExists(identifier)) { import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._ /** * Note, currently the new table creation by this API doesn't fully cover the V2 table. * TODO (SPARK-33638): Full support of v2 table creation */ val tableSpec = TableSpec( Map.empty[String, String], Some(source), Map.empty[String, String], extraOptions.get(\"path\"), None, None, false) val cmd = CreateTable( UnresolvedDBObjectName( originalMultipartIdentifier, isNamespace = false), df.schema.asNullable, partitioningColumns.getOrElse(Nil).asTransforms.toSeq, tableSpec, ignoreIfExists = false) Dataset.ofRows(df.sparkSession, cmd) } val tableInstance = catalog.asTableCatalog.loadTable(identifier) def writeToV1Table(table: CatalogTable): StreamingQuery = { if (table.tableType == CatalogTableType.VIEW) { throw QueryCompilationErrors.streamingIntoViewNotSupportedError(tableName) } require(table.provider.isDefined) if (source != table.provider.get) { throw QueryCompilationErrors.inputSourceDiffersFromDataSourceProviderError( source, tableName, table) } format(table.provider.get) .option(\"path\", new Path(table.location).toString).start() } import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Implicits._ tableInstance match { case t: SupportsWrite if t.supports(STREAMING_WRITE) => startQuery(t, extraOptions, catalogAndIdent = Some(catalog.asTableCatalog, identifier)) case t: V2TableWithV1Fallback => writeToV1Table(t.v1Table) case t: V1Table => writeToV1Table(t.v1Table) case t => throw QueryCompilationErrors.tableNotSupportStreamingWriteError(tableName, t) } } private def startInternal(path: Option[String]): StreamingQuery = { if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) { throw QueryCompilationErrors.cannotOperateOnHiveDataSourceFilesError(\"write\") } if (source == SOURCE_NAME_MEMORY) { assertNotPartitioned(SOURCE_NAME_MEMORY) if (extraOptions.get(\"queryName\").isEmpty) { throw QueryCompilationErrors.queryNameNotSpecifiedForMemorySinkError() } val sink = new MemorySink() val resultDf = Dataset.ofRows(df.sparkSession, new MemoryPlan(sink, df.schema.toAttributes)) val recoverFromCheckpoint = outputMode == OutputMode.Complete() val query = startQuery(sink, extraOptions, recoverFromCheckpoint = recoverFromCheckpoint) resultDf.createOrReplaceTempView(query.name) query } else if (source == SOURCE_NAME_FOREACH) { assertNotPartitioned(SOURCE_NAME_FOREACH) val sink = ForeachWriterTable[T](foreachWriter, ds.exprEnc) startQuery(sink, extraOptions) } else if (source == SOURCE_NAME_FOREACH_BATCH) { assertNotPartitioned(SOURCE_NAME_FOREACH_BATCH) if (trigger.isInstanceOf[ContinuousTrigger]) { throw QueryCompilationErrors.sourceNotSupportedWithContinuousTriggerError(source) } val sink = new ForeachBatchSink[T](foreachBatchWriter, ds.exprEnc) startQuery(sink, extraOptions) } else { val cls = DataSource.lookupDataSource(source, df.sparkSession.sessionState.conf) val disabledSources = Utils.stringToSeq(df.sparkSession.sqlContext.conf.disabledV2StreamingWriters) val useV1Source = disabledSources.contains(cls.getCanonicalName) || // file source v2 does not support streaming yet. classOf[FileDataSourceV2].isAssignableFrom(cls) val optionsWithPath = if (path.isEmpty) { extraOptions } else { extraOptions + (\"path\" -> path.get) } val sink = if (classOf[TableProvider].isAssignableFrom(cls) && !useV1Source) { val provider = cls.getConstructor().newInstance().asInstanceOf[TableProvider] val sessionOptions = DataSourceV2Utils.extractSessionConfigs( source = provider, conf = df.sparkSession.sessionState.conf) val finalOptions = sessionOptions.filterKeys(!optionsWithPath.contains(_)).toMap ++ optionsWithPath.originalMap val dsOptions = new CaseInsensitiveStringMap(finalOptions.asJava) // If the source accepts external table metadata, here we pass the schema of input query // to `getTable`. This is for avoiding schema inference, which can be very expensive. // If the query schema is not compatible with the existing data, the behavior is undefined. val outputSchema = if (provider.supportsExternalMetadata()) { Some(df.schema) } else { None } val table = DataSourceV2Utils.getTableFromProvider( provider, dsOptions, userSpecifiedSchema = outputSchema) import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Implicits._ table match { case table: SupportsWrite if table.supports(STREAMING_WRITE) => table case _ => createV1Sink(optionsWithPath) } } else { createV1Sink(optionsWithPath) } startQuery(sink, optionsWithPath) } } private def startQuery( sink: Table, newOptions: CaseInsensitiveMap[String], recoverFromCheckpoint: Boolean = true, catalogAndIdent: Option[(TableCatalog, Identifier)] = None): StreamingQuery = { val useTempCheckpointLocation = SOURCES_ALLOW_ONE_TIME_QUERY.contains(source) df.sparkSession.sessionState.streamingQueryManager.startQuery( newOptions.get(\"queryName\"), newOptions.get(\"checkpointLocation\"), df, newOptions.originalMap, sink, outputMode, useTempCheckpointLocation = useTempCheckpointLocation, recoverFromCheckpointLocation = recoverFromCheckpoint, trigger = trigger, catalogAndIdent = catalogAndIdent) } private def createV1Sink(optionsWithPath: CaseInsensitiveMap[String]): Sink = { val ds = DataSource( df.sparkSession, className = source, options = optionsWithPath.originalMap, partitionColumns = normalizedParCols.getOrElse(Nil)) ds.createSink(outputMode) } /** * Sets the output of the streaming query to be processed using the provided writer object. * object. See [[org.apache.spark.sql.ForeachWriter]] for more details on the lifecycle and * semantics. * @since 2.0.0 */ def foreach(writer: ForeachWriter[T]): DataStreamWriter[T] = { this.source = SOURCE_NAME_FOREACH this.foreachWriter = if (writer != null) { ds.sparkSession.sparkContext.clean(writer) } else { throw new IllegalArgumentException(\"foreach writer cannot be null\") } this } /** * :: Experimental :: * * (Scala-specific) Sets the output of the streaming query to be processed using the provided * function. This is supported only in the micro-batch execution modes (that is, when the * trigger is not continuous). In every micro-batch, the provided function will be called in * every micro-batch with (i) the output rows as a Dataset and (ii) the batch identifier. * The batchId can be used to deduplicate and transactionally write the output * (that is, the provided Dataset) to external systems. The output Dataset is guaranteed * to be exactly the same for the same batchId (assuming all operations are deterministic * in the query). * * @since 2.4.0 */ @Evolving def foreachBatch(function: (Dataset[T], Long) => Unit): DataStreamWriter[T] = { this.source = SOURCE_NAME_FOREACH_BATCH if (function == null) throw new IllegalArgumentException(\"foreachBatch function cannot be null\") this.foreachBatchWriter = function this } /** * :: Experimental :: * * (Java-specific) Sets the output of the streaming query to be processed using the provided * function. This is supported only in the micro-batch execution modes (that is, when the * trigger is not continuous). In every micro-batch, the provided function will be called in * every micro-batch with (i) the output rows as a Dataset and (ii) the batch identifier. * The batchId can be used to deduplicate and transactionally write the output * (that is, the provided Dataset) to external systems. The output Dataset is guaranteed * to be exactly the same for the same batchId (assuming all operations are deterministic * in the query). * * @since 2.4.0 */ @Evolving def foreachBatch(function: VoidFunction2[Dataset[T], java.lang.Long]): DataStreamWriter[T] = { foreachBatch((batchDs: Dataset[T], batchId: Long) => function.call(batchDs, batchId)) } private def normalizedParCols: Option[Seq[String]] = partitioningColumns.map { cols => cols.map(normalize(_, \"Partition\")) } /** * The given column name may not be equal to any of the existing column names if we were in * case-insensitive context. Normalize the given column name to the real one so that we don't * need to care about case sensitivity afterwards. */ private def normalize(columnName: String, columnType: String): String = { val validColumnNames = df.logicalPlan.output.map(_.name) validColumnNames.find(df.sparkSession.sessionState.analyzer.resolver(_, columnName)) .getOrElse(throw QueryCompilationErrors.columnNotFoundInExistingColumnsError( columnType, columnName, validColumnNames)) } private def assertNotPartitioned(operation: String): Unit = { if (partitioningColumns.isDefined) { throw QueryCompilationErrors.operationNotSupportPartitioningError(operation) } } /////////////////////////////////////////////////////////////////////////////////////// // Builder pattern config options /////////////////////////////////////////////////////////////////////////////////////// private var source: String = df.sparkSession.sessionState.conf.defaultDataSourceName private var tableName: String = null private var outputMode: OutputMode = OutputMode.Append private var trigger: Trigger = Trigger.ProcessingTime(0L) private var extraOptions = CaseInsensitiveMap[String](Map.empty) private var foreachWriter: ForeachWriter[T] = null private var foreachBatchWriter: (Dataset[T], Long) => Unit = null private var partitioningColumns: Option[Seq[String]] = None } object DataStreamWriter { val SOURCE_NAME_MEMORY = \"memory\" val SOURCE_NAME_FOREACH = \"foreach\" val SOURCE_NAME_FOREACH_BATCH = \"foreachBatch\" val SOURCE_NAME_CONSOLE = \"console\" val SOURCE_NAME_TABLE = \"table\" val SOURCE_NAME_NOOP = \"noop\" // these writer sources are also used for one-time query, hence allow temp checkpoint location val SOURCES_ALLOW_ONE_TIME_QUERY = Seq(SOURCE_NAME_MEMORY, SOURCE_NAME_FOREACH, SOURCE_NAME_FOREACH_BATCH, SOURCE_NAME_CONSOLE, SOURCE_NAME_NOOP) }",
          "## CLASS: org/apache/spark/sql/DataFrameWriter# (implementation)\n@Stable final class DataFrameWriter[T] private[sql](ds: Dataset[T]) { private val df = ds.toDF() /** * Specifies the behavior when data or table already exists. Options include: * <ul> * <li>`SaveMode.Overwrite`: overwrite the existing data.</li> * <li>`SaveMode.Append`: append the data.</li> * <li>`SaveMode.Ignore`: ignore the operation (i.e. no-op).</li> * <li>`SaveMode.ErrorIfExists`: throw an exception at runtime.</li> * </ul> * <p> * The default option is `ErrorIfExists`. * * @since 1.4.0 */ def mode(saveMode: SaveMode): DataFrameWriter[T] = { this.mode = saveMode this } /** * Specifies the behavior when data or table already exists. Options include: * <ul> * <li>`overwrite`: overwrite the existing data.</li> * <li>`append`: append the data.</li> * <li>`ignore`: ignore the operation (i.e. no-op).</li> * <li>`error` or `errorifexists`: default option, throw an exception at runtime.</li> * </ul> * * @since 1.4.0 */ def mode(saveMode: String): DataFrameWriter[T] = { saveMode.toLowerCase(Locale.ROOT) match { case \"overwrite\" => mode(SaveMode.Overwrite) case \"append\" => mode(SaveMode.Append) case \"ignore\" => mode(SaveMode.Ignore) case \"error\" | \"errorifexists\" | \"default\" => mode(SaveMode.ErrorIfExists) case _ => throw new IllegalArgumentException(s\"Unknown save mode: $saveMode. Accepted \" + \"save modes are 'overwrite', 'append', 'ignore', 'error', 'errorifexists', 'default'.\") } } /** * Specifies the underlying output data source. Built-in options include \"parquet\", \"json\", etc. * * @since 1.4.0 */ def format(source: String): DataFrameWriter[T] = { this.source = source this } /** * Adds an output option for the underlying data source. * * All options are maintained in a case-insensitive way in terms of key names. * If a new option has the same key case-insensitively, it will override the existing option. * * @since 1.4.0 */ def option(key: String, value: String): DataFrameWriter[T] = { this.extraOptions = this.extraOptions + (key -> value) this } /** * Adds an output option for the underlying data source. * * All options are maintained in a case-insensitive way in terms of key names. * If a new option has the same key case-insensitively, it will override the existing option. * * @since 2.0.0 */ def option(key: String, value: Boolean): DataFrameWriter[T] = option(key, value.toString) /** * Adds an output option for the underlying data source. * * All options are maintained in a case-insensitive way in terms of key names. * If a new option has the same key case-insensitively, it will override the existing option. * * @since 2.0.0 */ def option(key: String, value: Long): DataFrameWriter[T] = option(key, value.toString) /** * Adds an output option for the underlying data source. * * All options are maintained in a case-insensitive way in terms of key names. * If a new option has the same key case-insensitively, it will override the existing option. * * @since 2.0.0 */ def option(key: String, value: Double): DataFrameWriter[T] = option(key, value.toString) /** * (Scala-specific) Adds output options for the underlying data source. * * All options are maintained in a case-insensitive way in terms of key names. * If a new option has the same key case-insensitively, it will override the existing option. * * @since 1.4.0 */ def options(options: scala.collection.Map[String, String]): DataFrameWriter[T] = { this.extraOptions ++= options this } /** * Adds output options for the underlying data source. * * All options are maintained in a case-insensitive way in terms of key names. * If a new option has the same key case-insensitively, it will override the existing option. * * @since 1.4.0 */ def options(options: java.util.Map[String, String]): DataFrameWriter[T] = { this.options(options.asScala) this } /** * Partitions the output by the given columns on the file system. If specified, the output is * laid out on the file system similar to Hive's partitioning scheme. As an example, when we * partition a dataset by year and then month, the directory layout would look like: * <ul> * <li>year=2016/month=01/</li> * <li>year=2016/month=02/</li> * </ul> * * Partitioning is one of the most widely used techniques to optimize physical data layout. * It provides a coarse-grained index for skipping unnecessary data reads when queries have * predicates on the partitioned columns. In order for partitioning to work well, the number * of distinct values in each column should typically be less than tens of thousands. * * This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark * 2.1.0. * * @since 1.4.0 */ @scala.annotation.varargs def partitionBy(colNames: String*): DataFrameWriter[T] = { this.partitioningColumns = Option(colNames) this } /** * Buckets the output by the given columns. If specified, the output is laid out on the file * system similar to Hive's bucketing scheme, but with a different bucket hash function * and is not compatible with Hive's bucketing. * * This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark * 2.1.0. * * @since 2.0 */ @scala.annotation.varargs def bucketBy(numBuckets: Int, colName: String, colNames: String*): DataFrameWriter[T] = { this.numBuckets = Option(numBuckets) this.bucketColumnNames = Option(colName +: colNames) this } /** * Sorts the output in each bucket by the given columns. * * This is applicable for all file-based data sources (e.g. Parquet, JSON) starting with Spark * 2.1.0. * * @since 2.0 */ @scala.annotation.varargs def sortBy(colName: String, colNames: String*): DataFrameWriter[T] = { this.sortColumnNames = Option(colName +: colNames) this } /** * Saves the content of the `DataFrame` at the specified path. * * @since 1.4.0 */ def save(path: String): Unit = { if (!df.sparkSession.sessionState.conf.legacyPathOptionBehavior && extraOptions.contains(\"path\")) { throw QueryCompilationErrors.pathOptionNotSetCorrectlyWhenWritingError() } saveInternal(Some(path)) } /** * Saves the content of the `DataFrame` as the specified table. * * @since 1.4.0 */ def save(): Unit = saveInternal(None) private def saveInternal(path: Option[String]): Unit = { if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) { throw QueryCompilationErrors.cannotOperateOnHiveDataSourceFilesError(\"write\") } assertNotBucketed(\"save\") val maybeV2Provider = lookupV2Provider() if (maybeV2Provider.isDefined) { val provider = maybeV2Provider.get val sessionOptions = DataSourceV2Utils.extractSessionConfigs( provider, df.sparkSession.sessionState.conf) val optionsWithPath = getOptionsWithPath(path) val finalOptions = sessionOptions.filterKeys(!optionsWithPath.contains(_)).toMap ++ optionsWithPath.originalMap val dsOptions = new CaseInsensitiveStringMap(finalOptions.asJava) def getTable: Table = { // If the source accepts external table metadata, here we pass the schema of input query // and the user-specified partitioning to `getTable`. This is for avoiding // schema/partitioning inference, which can be very expensive. // If the query schema is not compatible with the existing data, the behavior is undefined. // For example, writing file source will success but the following reads will fail. if (provider.supportsExternalMetadata()) { provider.getTable( df.schema.asNullable, partitioningAsV2.toArray, dsOptions.asCaseSensitiveMap()) } else { DataSourceV2Utils.getTableFromProvider(provider, dsOptions, userSpecifiedSchema = None) } } import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Implicits._ val catalogManager = df.sparkSession.sessionState.catalogManager mode match { case SaveMode.Append | SaveMode.Overwrite => val (table, catalog, ident) = provider match { case supportsExtract: SupportsCatalogOptions => val ident = supportsExtract.extractIdentifier(dsOptions) val catalog = CatalogV2Util.getTableProviderCatalog( supportsExtract, catalogManager, dsOptions) (catalog.loadTable(ident), Some(catalog), Some(ident)) case _: TableProvider => val t = getTable if (t.supports(BATCH_WRITE)) { (t, None, None) } else { // Streaming also uses the data source V2 API. So it may be that the data source // implements v2, but has no v2 implementation for batch writes. In that case, we // fall back to saving as though it's a V1 source. return saveToV1Source(path) } } val relation = DataSourceV2Relation.create(table, catalog, ident, dsOptions) checkPartitioningMatchesV2Table(table) if (mode == SaveMode.Append) { runCommand(df.sparkSession) { AppendData.byName(relation, df.logicalPlan, finalOptions) } } else { // Truncate the table. TableCapabilityCheck will throw a nice exception if this // isn't supported runCommand(df.sparkSession) { OverwriteByExpression.byName( relation, df.logicalPlan, Literal(true), finalOptions) } } case createMode => provider match { case supportsExtract: SupportsCatalogOptions => val ident = supportsExtract.extractIdentifier(dsOptions) val catalog = CatalogV2Util.getTableProviderCatalog( supportsExtract, catalogManager, dsOptions) val tableSpec = TableSpec( properties = Map.empty, provider = Some(source), options = Map.empty, location = extraOptions.get(\"path\"), comment = extraOptions.get(TableCatalog.PROP_COMMENT), serde = None, external = false) runCommand(df.sparkSession) { CreateTableAsSelect( UnresolvedDBObjectName( catalog.name +: ident.namespace.toSeq :+ ident.name, isNamespace = false ), partitioningAsV2, df.queryExecution.analyzed, tableSpec, finalOptions, ignoreIfExists = createMode == SaveMode.Ignore) } case _: TableProvider => if (getTable.supports(BATCH_WRITE)) { throw QueryCompilationErrors.writeWithSaveModeUnsupportedBySourceError( source, createMode.name()) } else { // Streaming also uses the data source V2 API. So it may be that the data source // implements v2, but has no v2 implementation for batch writes. In that case, we // fallback to saving as though it's a V1 source. saveToV1Source(path) } } } } else { saveToV1Source(path) } } private def getOptionsWithPath(path: Option[String]): CaseInsensitiveMap[String] = { if (path.isEmpty) { extraOptions } else { extraOptions + (\"path\" -> path.get) } } private def saveToV1Source(path: Option[String]): Unit = { partitioningColumns.foreach { columns => extraOptions = extraOptions + ( DataSourceUtils.PARTITIONING_COLUMNS_KEY -> DataSourceUtils.encodePartitioningColumns(columns)) } val optionsWithPath = getOptionsWithPath(path) // Code path for data source v1. runCommand(df.sparkSession) { DataSource( sparkSession = df.sparkSession, className = source, partitionColumns = partitioningColumns.getOrElse(Nil), options = optionsWithPath.originalMap).planForWriting(mode, df.logicalPlan) } } /** * Inserts the content of the `DataFrame` to the specified table. It requires that * the schema of the `DataFrame` is the same as the schema of the table. * * @note Unlike `saveAsTable`, `insertInto` ignores the column names and just uses position-based * resolution. For example: * * @note SaveMode.ErrorIfExists and SaveMode.Ignore behave as SaveMode.Append in `insertInto` as * `insertInto` is not a table creating operation. * * {{{ * scala> Seq((1, 2)).toDF(\"i\", \"j\").write.mode(\"overwrite\").saveAsTable(\"t1\") * scala> Seq((3, 4)).toDF(\"j\", \"i\").write.insertInto(\"t1\") * scala> Seq((5, 6)).toDF(\"a\", \"b\").write.insertInto(\"t1\") * scala> sql(\"select * from t1\").show * +---+---+ * | i| j| * +---+---+ * | 5| 6| * | 3| 4| * | 1| 2| * +---+---+ * }}} * * Because it inserts data to an existing table, format or options will be ignored. * * @since 1.4.0 */ def insertInto(tableName: String): Unit = { import df.sparkSession.sessionState.analyzer.{AsTableIdentifier, NonSessionCatalogAndIdentifier, SessionCatalogAndIdentifier} import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._ assertNotBucketed(\"insertInto\") if (partitioningColumns.isDefined) { throw QueryCompilationErrors.partitionByDoesNotAllowedWhenUsingInsertIntoError() } val session = df.sparkSession val canUseV2 = lookupV2Provider().isDefined session.sessionState.sqlParser.parseMultipartIdentifier(tableName) match { case NonSessionCatalogAndIdentifier(catalog, ident) => insertInto(catalog, ident) case SessionCatalogAndIdentifier(catalog, ident) if canUseV2 && ident.namespace().length <= 1 => insertInto(catalog, ident) case AsTableIdentifier(tableIdentifier) => insertInto(tableIdentifier) case other => throw QueryCompilationErrors.cannotFindCatalogToHandleIdentifierError(other.quoted) } } private def insertInto(catalog: CatalogPlugin, ident: Identifier): Unit = { import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._ val table = catalog.asTableCatalog.loadTable(ident) match { case _: V1Table => return insertInto(TableIdentifier(ident.name(), ident.namespace().headOption)) case t => DataSourceV2Relation.create(t, Some(catalog), Some(ident)) } val command = mode match { case SaveMode.Append | SaveMode.ErrorIfExists | SaveMode.Ignore => AppendData.byPosition(table, df.logicalPlan, extraOptions.toMap) case SaveMode.Overwrite => val conf = df.sparkSession.sessionState.conf val dynamicPartitionOverwrite = table.table.partitioning.size > 0 && conf.partitionOverwriteMode == PartitionOverwriteMode.DYNAMIC if (dynamicPartitionOverwrite) { OverwritePartitionsDynamic.byPosition(table, df.logicalPlan, extraOptions.toMap) } else { OverwriteByExpression.byPosition(table, df.logicalPlan, Literal(true), extraOptions.toMap) } } runCommand(df.sparkSession) { command } } private def insertInto(tableIdent: TableIdentifier): Unit = { runCommand(df.sparkSession) { InsertIntoStatement( table = UnresolvedRelation(tableIdent), partitionSpec = Map.empty[String, Option[String]], Nil, query = df.logicalPlan, overwrite = mode == SaveMode.Overwrite, ifPartitionNotExists = false) } } private def getBucketSpec: Option[BucketSpec] = { if (sortColumnNames.isDefined && numBuckets.isEmpty) { throw QueryCompilationErrors.sortByNotUsedWithBucketByError() } numBuckets.map { n => BucketSpec(n, bucketColumnNames.get, sortColumnNames.getOrElse(Nil)) } } private def assertNotBucketed(operation: String): Unit = { if (getBucketSpec.isDefined) { if (sortColumnNames.isEmpty) { throw QueryCompilationErrors.bucketByUnsupportedByOperationError(operation) } else { throw QueryCompilationErrors.bucketByAndSortByUnsupportedByOperationError(operation) } } } private def assertNotPartitioned(operation: String): Unit = { if (partitioningColumns.isDefined) { throw QueryCompilationErrors.operationNotSupportPartitioningError(operation) } } /** * Saves the content of the `DataFrame` as the specified table. * * In the case the table already exists, behavior of this function depends on the * save mode, specified by the `mode` function (default to throwing an exception). * When `mode` is `Overwrite`, the schema of the `DataFrame` does not need to be * the same as that of the existing table. * * When `mode` is `Append`, if there is an existing table, we will use the format and options of * the existing table. The column order in the schema of the `DataFrame` doesn't need to be same * as that of the existing table. Unlike `insertInto`, `saveAsTable` will use the column names to * find the correct column positions. For example: * * {{{ * scala> Seq((1, 2)).toDF(\"i\", \"j\").write.mode(\"overwrite\").saveAsTable(\"t1\") * scala> Seq((3, 4)).toDF(\"j\", \"i\").write.mode(\"append\").saveAsTable(\"t1\") * scala> sql(\"select * from t1\").show * +---+---+ * | i| j| * +---+---+ * | 1| 2| * | 4| 3| * +---+---+ * }}} * * In this method, save mode is used to determine the behavior if the data source table exists in * Spark catalog. We will always overwrite the underlying data of data source (e.g. a table in * JDBC data source) if the table doesn't exist in Spark catalog, and will always append to the * underlying data of data source if the table already exists. * * When the DataFrame is created from a non-partitioned `HadoopFsRelation` with a single input * path, and the data source provider can be mapped to an existing Hive builtin SerDe (i.e. ORC * and Parquet), the table is persisted in a Hive compatible format, which means other systems * like Hive will be able to read this table. Otherwise, the table is persisted in a Spark SQL * specific format. * * @since 1.4.0 */ def saveAsTable(tableName: String): Unit = { import df.sparkSession.sessionState.analyzer.{AsTableIdentifier, NonSessionCatalogAndIdentifier, SessionCatalogAndIdentifier} import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._ val session = df.sparkSession val canUseV2 = lookupV2Provider().isDefined session.sessionState.sqlParser.parseMultipartIdentifier(tableName) match { case nameParts @ NonSessionCatalogAndIdentifier(catalog, ident) => saveAsTable(catalog.asTableCatalog, ident, nameParts) case nameParts @ SessionCatalogAndIdentifier(catalog, ident) if canUseV2 && ident.namespace().length <= 1 => saveAsTable(catalog.asTableCatalog, ident, nameParts) case AsTableIdentifier(tableIdentifier) => saveAsTable(tableIdentifier) case other => throw QueryCompilationErrors.cannotFindCatalogToHandleIdentifierError(other.quoted) } } private def saveAsTable( catalog: TableCatalog, ident: Identifier, nameParts: Seq[String]): Unit = { val tableOpt = try Option(catalog.loadTable(ident)) catch { case _: NoSuchTableException => None } val command = (mode, tableOpt) match { case (_, Some(_: V1Table)) => return saveAsTable(TableIdentifier(ident.name(), ident.namespace().headOption)) case (SaveMode.Append, Some(table)) => checkPartitioningMatchesV2Table(table) val v2Relation = DataSourceV2Relation.create(table, Some(catalog), Some(ident)) AppendData.byName(v2Relation, df.logicalPlan, extraOptions.toMap) case (SaveMode.Overwrite, _) => val tableSpec = TableSpec( properties = Map.empty, provider = Some(source), options = Map.empty, location = extraOptions.get(\"path\"), comment = extraOptions.get(TableCatalog.PROP_COMMENT), serde = None, external = false) ReplaceTableAsSelect( UnresolvedDBObjectName(nameParts, isNamespace = false), partitioningAsV2, df.queryExecution.analyzed, tableSpec, writeOptions = extraOptions.toMap, orCreate = true) // Create the table if it doesn't exist case (other, _) => // We have a potential race condition here in AppendMode, if the table suddenly gets // created between our existence check and physical execution, but this can't be helped // in any case. val tableSpec = TableSpec( properties = Map.empty, provider = Some(source), options = Map.empty, location = extraOptions.get(\"path\"), comment = extraOptions.get(TableCatalog.PROP_COMMENT), serde = None, external = false) CreateTableAsSelect( UnresolvedDBObjectName(nameParts, isNamespace = false), partitioningAsV2, df.queryExecution.analyzed, tableSpec, writeOptions = extraOptions.toMap, other == SaveMode.Ignore) } runCommand(df.sparkSession) { command } } private def saveAsTable(tableIdent: TableIdentifier): Unit = { val catalog = df.sparkSession.sessionState.catalog val tableExists = catalog.tableExists(tableIdent) val db = tableIdent.database.getOrElse(catalog.getCurrentDatabase) val tableIdentWithDB = tableIdent.copy(database = Some(db)) val tableName = tableIdentWithDB.unquotedString (tableExists, mode) match { case (true, SaveMode.Ignore) => // Do nothing case (true, SaveMode.ErrorIfExists) => throw QueryCompilationErrors.tableAlreadyExistsError(tableIdent) case (true, SaveMode.Overwrite) => // Get all input data source or hive relations of the query. val srcRelations = df.logicalPlan.collect { case LogicalRelation(src: BaseRelation, _, _, _) => src case relation: HiveTableRelation => relation.tableMeta.identifier } val tableRelation = df.sparkSession.table(tableIdentWithDB).queryExecution.analyzed EliminateSubqueryAliases(tableRelation) match { // check if the table is a data source table (the relation is a BaseRelation). case LogicalRelation(dest: BaseRelation, _, _, _) if srcRelations.contains(dest) => throw QueryCompilationErrors.cannotOverwriteTableThatIsBeingReadFromError(tableName) // check hive table relation when overwrite mode case relation: HiveTableRelation if srcRelations.contains(relation.tableMeta.identifier) => throw QueryCompilationErrors.cannotOverwriteTableThatIsBeingReadFromError(tableName) case _ => // OK } // Drop the existing table catalog.dropTable(tableIdentWithDB, ignoreIfNotExists = true, purge = false) createTable(tableIdentWithDB) // Refresh the cache of the table in the catalog. catalog.refreshTable(tableIdentWithDB) case _ => createTable(tableIdent) } } private def createTable(tableIdent: TableIdentifier): Unit = { val storage = DataSource.buildStorageFormatFromOptions(extraOptions.toMap) val tableType = if (storage.locationUri.isDefined) { CatalogTableType.EXTERNAL } else { CatalogTableType.MANAGED } val tableDesc = CatalogTable( identifier = tableIdent, tableType = tableType, storage = storage, schema = new StructType, provider = Some(source), partitionColumnNames = partitioningColumns.getOrElse(Nil), bucketSpec = getBucketSpec) runCommand(df.sparkSession)( CreateTable(tableDesc, mode, Some(df.logicalPlan))) } /** Converts the provided partitioning and bucketing information to DataSourceV2 Transforms. */ private def partitioningAsV2: Seq[Transform] = { val partitioning = partitioningColumns.map { colNames => colNames.map(name => IdentityTransform(FieldReference(name))) }.getOrElse(Seq.empty[Transform]) val bucketing = getBucketSpec.map(spec => CatalogV2Implicits.BucketSpecHelper(spec).asTransform).toSeq partitioning ++ bucketing } /** * For V2 DataSources, performs if the provided partitioning matches that of the table. * Partitioning information is not required when appending data to V2 tables. */ private def checkPartitioningMatchesV2Table(existingTable: Table): Unit = { val v2Partitions = partitioningAsV2 if (v2Partitions.isEmpty) return require(v2Partitions.sameElements(existingTable.partitioning()), \"The provided partitioning does not match of the table.\\n\" + s\" - provided: ${v2Partitions.mkString(\", \")}\\n\" + s\" - table: ${existingTable.partitioning().mkString(\", \")}\") } /** * Saves the content of the `DataFrame` to an external database table via JDBC. In the case the * table already exists in the external database, behavior of this function depends on the * save mode, specified by the `mode` function (default to throwing an exception). * * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash * your external database systems. * * JDBC-specific option and parameter documentation for storing tables via JDBC in * <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option\"> * Data Source Option</a> in the version you use. * * @param table Name of the table in the external database. * @param connectionProperties JDBC database connection arguments, a list of arbitrary string * tag/value. Normally at least a \"user\" and \"password\" property * should be included. \"batchsize\" can be used to control the * number of rows per insert. \"isolationLevel\" can be one of * \"NONE\", \"READ_COMMITTED\", \"READ_UNCOMMITTED\", \"REPEATABLE_READ\", * or \"SERIALIZABLE\", corresponding to standard transaction * isolation levels defined by JDBC's Connection object, with default * of \"READ_UNCOMMITTED\". * @since 1.4.0 */ def jdbc(url: String, table: String, connectionProperties: Properties): Unit = { assertNotPartitioned(\"jdbc\") assertNotBucketed(\"jdbc\") // connectionProperties should override settings in extraOptions. this.extraOptions ++= connectionProperties.asScala // explicit url and dbtable should override all this.extraOptions ++= Seq(\"url\" -> url, \"dbtable\" -> table) format(\"jdbc\").save() } /** * Saves the content of the `DataFrame` in JSON format (<a href=\"http://jsonlines.org/\"> * JSON Lines text format or newline-delimited JSON</a>) at the specified path. * This is equivalent to: * {{{ * format(\"json\").save(path) * }}} * * You can find the JSON-specific options for writing JSON files in * <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option\"> * Data Source Option</a> in the version you use. * * @since 1.4.0 */ def json(path: String): Unit = { format(\"json\").save(path) } /** * Saves the content of the `DataFrame` in Parquet format at the specified path. * This is equivalent to: * {{{ * format(\"parquet\").save(path) * }}} * * Parquet-specific option(s) for writing Parquet files can be found in * <a href= * \"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option\"> * Data Source Option</a> in the version you use. * * @since 1.4.0 */ def parquet(path: String): Unit = { format(\"parquet\").save(path) } /** * Saves the content of the `DataFrame` in ORC format at the specified path. * This is equivalent to: * {{{ * format(\"orc\").save(path) * }}} * * ORC-specific option(s) for writing ORC files can be found in * <a href= * \"https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option\"> * Data Source Option</a> in the version you use. * * @since 1.5.0 */ def orc(path: String): Unit = { format(\"orc\").save(path) } /** * Saves the content of the `DataFrame` in a text file at the specified path. * The DataFrame must have only one column that is of string type. * Each row becomes a new line in the output file. For example: * {{{ * // Scala: * df.write.text(\"/path/to/output\") * * // Java: * df.write().text(\"/path/to/output\") * }}} * The text files will be encoded as UTF-8. * * You can find the text-specific options for writing text files in * <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option\"> * Data Source Option</a> in the version you use. * * @since 1.6.0 */ def text(path: String): Unit = { format(\"text\").save(path) } /** * Saves the content of the `DataFrame` in CSV format at the specified path. * This is equivalent to: * {{{ * format(\"csv\").save(path) * }}} * * You can find the CSV-specific options for writing CSV files in * <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option\"> * Data Source Option</a> in the version you use. * * @since 2.0.0 */ def csv(path: String): Unit = { format(\"csv\").save(path) } /** * Wrap a DataFrameWriter action to track the QueryExecution and time cost, then report to the * user-registered callback functions. */ private def runCommand(session: SparkSession)(command: LogicalPlan): Unit = { val qe = session.sessionState.executePlan(command) qe.assertCommandExecuted() } private def lookupV2Provider(): Option[TableProvider] = { DataSource.lookupDataSourceV2(source, df.sparkSession.sessionState.conf) match { // TODO(SPARK-28396): File source v2 write path is currently broken. case Some(_: FileDataSourceV2) => None case other => other } } /////////////////////////////////////////////////////////////////////////////////////// // Builder pattern config options /////////////////////////////////////////////////////////////////////////////////////// private var source: String = df.sparkSession.sessionState.conf.defaultDataSourceName private var mode: SaveMode = SaveMode.ErrorIfExists private var extraOptions = CaseInsensitiveMap[String](Map.empty) private var partitioningColumns: Option[Seq[String]] = None private var bucketColumnNames: Option[Seq[String]] = None private var numBuckets: Option[Int] = None private var sortColumnNames: Option[Seq[String]] = None }",
          "## CLASS: org/apache/spark/sql/Dataset# (implementation)\n@Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { @transient lazy val sparkSession: SparkSession = { if (queryExecution == null || queryExecution.sparkSession == null) { throw QueryExecutionErrors.transformationsAndActionsNotInvokedByDriverError() } queryExecution.sparkSession } // A globally unique id of this Dataset. private val id = Dataset.curId.getAndIncrement() queryExecution.assertAnalyzed() // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) } def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sqlContext.sparkSession, logicalPlan, encoder) } @transient private[sql] val logicalPlan: LogicalPlan = { val plan = queryExecution.commandExecuted if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) { val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long]) dsIds.add(id) plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds) } plan } /** * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use * it when constructing new Dataset objects that have the same object type (that will be * possibly resolved to a different schema). */ private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder) // The resolved `ExpressionEncoder` which can be used to turn rows to objects of type T, after // collecting rows to the driver side. private lazy val resolvedEnc = { exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer) } private implicit def classTag = exprEnc.clsTag // sqlContext must be val because a stable identifier is expected when you import implicits @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext private[sql] def resolve(colName: String): NamedExpression = { val resolver = sparkSession.sessionState.analyzer.resolver queryExecution.analyzed.resolveQuoted(colName, resolver) .getOrElse(throw resolveException(colName, schema.fieldNames)) } private def resolveException(colName: String, fields: Array[String]): AnalysisException = { val extraMsg = if (fields.exists(sparkSession.sessionState.analyzer.resolver(_, colName))) { s\"; did you mean to quote the `$colName` column?\" } else \"\" val fieldsStr = fields.mkString(\", \") QueryCompilationErrors.cannotResolveColumnNameAmongFieldsError(colName, fieldsStr, extraMsg) } private[sql] def numericColumns: Seq[Expression] = { schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n => queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get } } /** * Get rows represented in Sequence by specific truncate and vertical requirement. * * @param numRows Number of rows to return * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. */ private[sql] def getRows( numRows: Int, truncate: Int): Seq[Seq[String]] = { val newDf = toDF() val castCols = newDf.logicalPlan.output.map { col => // Since binary types in top-level schema fields have a specific format to print, // so we do not cast them to strings here. if (col.dataType == BinaryType) { Column(col) } else { Column(col).cast(StringType) } } val data = newDf.select(castCols: _*).take(numRows + 1) // For array values, replace Seq and Array with square brackets // For cells that are beyond `truncate` characters, replace it with the // first `truncate-3` and \"...\" schema.fieldNames.map(SchemaUtils.escapeMetaCharacters).toSeq +: data.map { row => row.toSeq.map { cell => val str = cell match { case null => \"null\" case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\") case _ => // Escapes meta-characters not to break the `showString` format SchemaUtils.escapeMetaCharacters(cell.toString) } if (truncate > 0 && str.length > truncate) { // do not show ellipses for strings shorter than 4 characters. if (truncate < 4) str.substring(0, truncate) else str.substring(0, truncate - 3) + \"...\" } else { str } }: Seq[String] } } /** * Compose the string representing rows for output * * @param _numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). */ private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = { val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate) val hasMoreData = tmpRows.length - 1 > numRows val rows = tmpRows.take(numRows + 1) val sb = new StringBuilder val numCols = schema.fieldNames.length // We set a minimum column width at '3' val minimumColWidth = 3 if (!vertical) { // Initialise the width of each column to a minimum value val colWidths = Array.fill(numCols)(minimumColWidth) // Compute the width of each column for (row <- rows) { for ((cell, i) <- row.zipWithIndex) { colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell)) } } val paddedRows = rows.map { row => row.zipWithIndex.map { case (cell, i) => if (truncate > 0) { StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } else { StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } } } // Create SeparateLine val sep: String = colWidths.map(\"-\" * _).addString(sb, \"+\", \"+\", \"+\\n\").toString() // column names paddedRows.head.addString(sb, \"|\", \"|\", \"|\\n\") sb.append(sep) // data paddedRows.tail.foreach(_.addString(sb, \"|\", \"|\", \"|\\n\")) sb.append(sep) } else { // Extended display mode enabled val fieldNames = rows.head val dataRows = rows.tail // Compute the width of field name and data columns val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) => math.max(curMax, Utils.stringHalfWidth(fieldName)) } val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) => math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max) } dataRows.zipWithIndex.foreach { case (row, i) => // \"+ 5\" in size means a character length except for padded names and data val rowHeader = StringUtils.rightPad( s\"-RECORD $i\", fieldNameColWidth + dataColWidth + 5, \"-\") sb.append(rowHeader).append(\"\\n\") row.zipWithIndex.map { case (cell, j) => val fieldName = StringUtils.rightPad(fieldNames(j), fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length) val data = StringUtils.rightPad(cell, dataColWidth - Utils.stringHalfWidth(cell) + cell.length) s\" $fieldName | $data \" }.addString(sb, \"\", \"\\n\", \"\\n\") } } // Print a footer if (vertical && rows.tail.isEmpty) { // In a vertical mode, print an empty row set explicitly sb.append(\"(0 rows)\\n\") } else if (hasMoreData) { // For Data that has more than \"numRows\" records val rowsString = if (numRows == 1) \"row\" else \"rows\" sb.append(s\"only showing top $numRows $rowsString\\n\") } sb.toString() } override def toString: String = { try { val builder = new StringBuilder val fields = schema.take(2).map { case f => s\"${f.name}: ${f.dataType.simpleString(2)}\" } builder.append(\"[\") builder.append(fields.mkString(\", \")) if (schema.length > 2) { if (schema.length - fields.size == 1) { builder.append(\" ... 1 more field\") } else { builder.append(\" ... \" + (schema.length - 2) + \" more fields\") } } builder.append(\"]\").toString() } catch { case NonFatal(e) => s\"Invalid tree; ${e.getMessage}:\\n$queryExecution\" } } /** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * * @group basic * @since 1.6.0 */ // This is declared with parentheses to prevent the Scala compiler from treating // `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame. def toDF(): DataFrame = new Dataset[Row](queryExecution, RowEncoder(schema)) /** * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * <ul> * <li>When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`).</li> * <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will * be assigned to `_1`).</li> * <li>When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used.</li> * </ul> * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * Note that `as[]` only changes the view of the data that is passed into typed operations, * such as `map()`, and does not eagerly project away any columns that are not present in * the specified class. * * @group basic * @since 1.6.0 */ def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * {{{ * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` * rdd.toDF(\"id\", \"name\") // this creates a DataFrame with column name \"id\" and \"name\" * }}} * * @group basic * @since 2.0.0 */ @scala.annotation.varargs def toDF(colNames: String*): DataFrame = { require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names (${colNames.size}): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) => Column(oldAttribute).as(newName) } select(newCols : _*) } /** * Returns the schema of this Dataset. * * @group basic * @since 1.6.0 */ def schema: StructType = sparkSession.withActive { queryExecution.analyzed.schema } /** * Prints the schema to the console in a nice tree format. * * @group basic * @since 1.6.0 */ def printSchema(): Unit = printSchema(Int.MaxValue) // scalastyle:off println /** * Prints the schema up to the given level to the console in a nice tree format. * * @group basic * @since 3.0.0 */ def printSchema(level: Int): Unit = println(schema.treeString(level)) // scalastyle:on println /** * Prints the plans (logical and physical) with a format specified by a given explain mode. * * @param mode specifies the expected output format of plans. * <ul> * <li>`simple` Print only a physical plan.</li> * <li>`extended`: Print both logical and physical plans.</li> * <li>`codegen`: Print a physical plan and generated codes if they are * available.</li> * <li>`cost`: Print a logical plan and statistics if they are available.</li> * <li>`formatted`: Split explain output into two sections: a physical plan outline * and node details.</li> * </ul> * @group basic * @since 3.0.0 */ def explain(mode: String): Unit = sparkSession.withActive { // Because temporary views are resolved during analysis when we create a Dataset, and // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using // `ExplainCommand` here will probably output different query plans, compared to the results // of evaluation of the Dataset. So just output QueryExecution's query plans here. // scalastyle:off println println(queryExecution.explainString(ExplainMode.fromString(mode))) // scalastyle:on println } /** * Prints the plans (logical and physical) to the console for debugging purposes. * * @param extended default `false`. If `false`, prints only the physical plan. * * @group basic * @since 1.6.0 */ def explain(extended: Boolean): Unit = if (extended) { explain(ExtendedMode.name) } else { explain(SimpleMode.name) } /** * Prints the physical plan to the console for debugging purposes. * * @group basic * @since 1.6.0 */ def explain(): Unit = explain(SimpleMode.name) /** * Returns all column names and their data types as an array. * * @group basic * @since 1.6.0 */ def dtypes: Array[(String, String)] = schema.fields.map { field => (field.name, field.dataType.toString) } /** * Returns all column names as an array. * * @group basic * @since 1.6.0 */ def columns: Array[String] = schema.fields.map(_.name) /** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * * @group basic * @since 1.6.0 */ def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] || logicalPlan.isInstanceOf[CommandResult] /** * Returns true if the `Dataset` is empty. * * @group basic * @since 2.4.0 */ def isEmpty: Boolean = withAction(\"isEmpty\", select().queryExecution) { plan => plan.executeTake(1).isEmpty } /** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * @group streaming * @since 2.0.0 */ def isStreaming: Boolean = logicalPlan.isStreaming /** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0 */ def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0 */ def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true) /** * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be * used to truncate the logical plan of this Dataset, which is especially useful in iterative * algorithms where the plan may grow exponentially. Local checkpoints are written to executor * storage and despite potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0 */ def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false) /** * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. Local checkpoints are written to executor storage and despite * potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0 */ def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint( eager = eager, reliableCheckpoint = false ) /** * Returns a checkpointed version of this Dataset. * * @param eager Whether to checkpoint this dataframe immediately * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the * checkpoint directory. If false creates a local checkpoint using * the caching subsystem */ private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) \"checkpoint\" else \"localCheckpoint\" withAction(actionName, queryExecution) { physicalPlan => val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { internalRdd.checkpoint() } else { internalRdd.localCheckpoint() } if (eager) { internalRdd.doCheckpoint() } // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. @scala.annotation.tailrec def firstLeafPartitioning(partitioning: Partitioning): Partitioning = { partitioning match { case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head) case p => p } } val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering, isStreaming )(sparkSession)).as[T] } } /** * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * Spark will use this watermark for several purposes: * <ul> * <li>To know when a given time window aggregation can be finalized and thus can be emitted * when using output modes that do not allow updates.</li> * <li>To minimize the amount of state that we need to keep for on-going aggregations, * `mapGroupsWithState` and `dropDuplicates` operators.</li> * </ul> * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * @param eventTime the name of the column that contains the event time of the row. * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). NOTE: This should not be negative. * * @group streaming * @since 2.1.0 */ // We only accept an existing column name, not a derived column here as a watermark that is // defined on a derived column cannot referenced elsewhere in the plan. def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan { val parsedDelay = IntervalUtils.fromIntervalString(delayThreshold) require(!IntervalUtils.isNegative(parsedDelay), s\"delay threshold ($delayThreshold) should not be negative.\") EliminateEventTimeWatermark( EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)) } /** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * * @group action * @since 1.6.0 */ def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * * @group action * @since 1.6.0 */ def show(): Unit = show(20) /** * Displays the top 20 rows of Dataset in a tabular form. * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0 */ def show(truncate: Boolean): Unit = show(20, truncate) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * @param numRows Number of rows to show * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0 */ // scalastyle:off println def show(numRows: Int, truncate: Boolean): Unit = if (truncate) { println(showString(numRows, truncate = 20)) } else { println(showString(numRows, truncate = 0)) } /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @group action * @since 1.6.0 */ def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * If `vertical` enabled, this command prints output rows vertically (one line per column value)? * * {{{ * -RECORD 0------------------- * year | 1980 * month | 12 * AVG('Adj Close) | 0.503218 * AVG('Adj Close) | 0.595103 * -RECORD 1------------------- * year | 1981 * month | 01 * AVG('Adj Close) | 0.523289 * AVG('Adj Close) | 0.570307 * -RECORD 2------------------- * year | 1982 * month | 02 * AVG('Adj Close) | 0.436504 * AVG('Adj Close) | 0.475256 * -RECORD 3------------------- * year | 1983 * month | 03 * AVG('Adj Close) | 0.410516 * AVG('Adj Close) | 0.442194 * -RECORD 4------------------- * year | 1984 * month | 04 * AVG('Adj Close) | 0.450090 * AVG('Adj Close) | 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). * @group action * @since 2.3.0 */ // scalastyle:off println def show(numRows: Int, truncate: Int, vertical: Boolean): Unit = println(showString(numRows, truncate, vertical)) // scalastyle:on println /** * Returns a [[DataFrameNaFunctions]] for working with missing data. * {{{ * // Dropping rows containing any null values. * ds.na.drop() * }}} * * @group untypedrel * @since 1.6.0 */ def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) /** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * {{{ * // Finding frequent items in column with name 'a'. * ds.stat.freqItems(Seq(\"a\")) * }}} * * @group untypedrel * @since 1.6.0 */ def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) /** * Join with another `DataFrame`. * * Behaves as an INNER JOIN and requires a subsequent join predicate. * * @param right Right side of the join operation. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE) } /** * Inner equi-join with another `DataFrame` using the given column. * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the column \"user_id\" * df1.join(df2, \"user_id\") * }}} * * @param right Right side of the join operation. * @param usingColumn Name of the column to join on. This column must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumn: String): DataFrame = { join(right, Seq(usingColumn)) } /** * Inner equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * }}} * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = { join(right, usingColumns, \"inner\") } /** * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate * is specified as an inner join. If you would explicitly like to perform a cross join use the * `crossJoin` method. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = { // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None, JoinHint.NONE)) .analyzed.asInstanceOf[Join] withPlan { Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None, JoinHint.NONE) } } /** * Inner join with another `DataFrame`, using the given join expression. * * {{{ * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * }}} * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * find the trivially true predicates and automatically resolves them to both sides. */ private def resolveSelfJoinCondition(plan: Join): Join = { val resolver = sparkSession.sessionState.analyzer.resolver val cond = plan.condition.map { _.transform { case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualTo( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualNullSafe( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) }} plan.copy(condition = cond) } /** * find the trivially true predicates and automatically resolves them to both sides. */ private def resolveSelfJoinCondition( right: Dataset[_], joinExprs: Option[Column], joinType: String): Join = { // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), joinExprs.map(_.expr), JoinHint.NONE)) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) { return plan } // If left/right have no output set intersection, return the plan. val lanalyzed = this.queryExecution.analyzed val ranalyzed = right.queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) { return plan } // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. resolveSelfJoinCondition(plan) } /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * {{{ * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * }}} * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = { withPlan { resolveSelfJoinCondition(right, Some(joinExprs), joinType) } } /** * Explicit cartesian join with another `DataFrame`. * * @param right Right side of the join operation. * * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * * @group untypedrel * @since 2.1.0 */ def crossJoin(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Cross, None, JoinHint.NONE) } /** * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * * @param other Right side of the join. * @param condition Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`,`full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`. * * @group typedrel * @since 1.6.0 */ def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = { // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // etc. var joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr), JoinHint.NONE)).analyzed.asInstanceOf[Join] if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) { throw QueryCompilationErrors.invalidJoinTypeInJoinWithError(joined.joinType) } // If auto self join alias is enable if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) { joined = resolveSelfJoinCondition(joined) } implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) val leftResultExpr = { if (!this.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() } else { Alias(CreateStruct(joined.left.output), \"_1\")() } } val rightResultExpr = { if (!other.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() } else { Alias(CreateStruct(joined.right.output), \"_2\")() } } if (joined.joinType.isInstanceOf[InnerLike]) { // For inner joins, we can directly perform the join and then can project the join // results into structs. This ensures that data remains flat during shuffles / // exchanges (unlike the outer join path, which nests the data before shuffling). withTypedPlan(Project(Seq(leftResultExpr, rightResultExpr), joined)) } else { // outer joins // For both join sides, combine all outputs into a single column and alias it with \"_1 // or \"_2\", to match the schema for the encoder of the join result. // Note that we do this before joining them, to enable the join operator to return null // for one side, in cases like outer-join. val left = Project(leftResultExpr :: Nil, joined.left) val right = Project(rightResultExpr :: Nil, joined.right) // Rewrites the join condition to make the attribute point to correct column/field, // after we combine the outputs of each join side. val conditionExpr = joined.condition.get transformUp { case a: Attribute if joined.left.outputSet.contains(a) => if (!this.exprEnc.isSerializedAsStructForTopLevel) { left.output.head } else { val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) } case a: Attribute if joined.right.outputSet.contains(a) => if (!other.exprEnc.isSerializedAsStructForTopLevel) { right.output.head } else { val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) } } withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr), JoinHint.NONE)) } } /** * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * @param other Right side of the join. * @param condition Join expression. * * @group typedrel * @since 1.6.0 */ def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = { joinWith(other, condition, \"inner\") } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, usingColumns: Seq[String], joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joinExprs = usingColumns.map { column => EqualTo(resolve(column), other.resolve(column)) }.reduceOption(And).map(Column.apply).orNull joinAsOf(other, leftAsOf, rightAsOf, joinExprs, joinType, tolerance, allowExactMatches, direction) } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, joinExprs: Column, joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joined = resolveSelfJoinCondition(other, Option(joinExprs), joinType) val leftAsOfExpr = leftAsOf.expr.transformUp { case a: AttributeReference if logicalPlan.outputSet.contains(a) => val index = logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.left.output(index) } val rightAsOfExpr = rightAsOf.expr.transformUp { case a: AttributeReference if other.logicalPlan.outputSet.contains(a) => val index = other.logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.right.output(index) } withPlan { AsOfJoin( joined.left, joined.right, leftAsOfExpr, rightAsOfExpr, joined.condition, joined.joinType, Option(tolerance).map(_.expr), allowExactMatches, AsOfJoinDirection(direction) ) } } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = { sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sortWithinPartitions(sortExprs: Column*): Dataset[T] = { sortInternal(global = false, sortExprs) } /** * Returns a new Dataset sorted by the specified column, all in ascending order. * {{{ * // The following 3 are equivalent * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * }}} * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = { sort((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset sorted by the given expressions. For example: * {{{ * ds.sort($\"col1\", $\"col2\".desc) * }}} * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = { sortInternal(global = true, sortExprs) } /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*) /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0 */ def apply(colName: String): Column = col(colName) /** * Specifies some hint on the current Dataset. As an example, the following code specifies * that one of the plan can be broadcasted: * * {{{ * df1.join(df2.hint(\"broadcast\")) * }}} * * @group basic * @since 2.2.0 */ @scala.annotation.varargs def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan { UnresolvedHint(name, parameters, logicalPlan) } /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0 */ def col(colName: String): Column = colName match { case \"*\" => Column(ResolvedStar(queryExecution.analyzed.output)) case _ => if (sqlContext.conf.supportQuotedRegexColumnName) { colRegex(colName) } else { Column(addDataFrameIdToCol(resolve(colName))) } } // Attach the dataset id and column position to the column reference, so that we can detect // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`. // This must be called before we return a `Column` that contains `AttributeReference`. // Note that, the metadata added here are only available in the analyzer, as the analyzer rule // `DetectAmbiguousSelfJoin` will remove it. private def addDataFrameIdToCol(expr: NamedExpression): NamedExpression = { val newExpr = expr transform { case a: AttributeReference if sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) => val metadata = new MetadataBuilder() .withMetadata(a.metadata) .putLong(Dataset.DATASET_ID_KEY, id) .putLong(Dataset.COL_POS_KEY, logicalPlan.output.indexWhere(a.semanticEquals)) .build() a.withMetadata(metadata) } newExpr.asInstanceOf[NamedExpression] } /** * Selects column based on the column name specified as a regex and returns it as [[Column]]. * @group untypedrel * @since 2.3.0 */ def colRegex(colName: String): Column = { val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis colName match { case ParserUtils.escapedIdentifier(columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, None, caseSensitive)) case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive)) case _ => Column(addDataFrameIdToCol(resolve(colName))) } } /** * Returns a new Dataset with an alias set. * * @group typedrel * @since 1.6.0 */ def as(alias: String): Dataset[T] = withTypedPlan { SubqueryAlias(alias, logicalPlan) } /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0 */ def as(alias: Symbol): Dataset[T] = as(alias.name) /** * Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: Symbol): Dataset[T] = as(alias) /** * Selects a set of column based expressions. * {{{ * ds.select($\"colA\", $\"colB\" + 1) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def select(cols: Column*): DataFrame = withPlan { val untypedCols = cols.map { case typedCol: TypedColumn[_, _] => // Checks if a `TypedColumn` has been inserted with // specific input type and schema by `withInputType`. val needInputType = typedCol.expr.exists { case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty => true case _ => false } if (!needInputType) { typedCol } else { throw QueryCompilationErrors.cannotPassTypedColumnInUntypedSelectError(typedCol.toString) } case other => other } Project(untypedCols.map(_.named), logicalPlan) } /** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * {{{ * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*) /** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * {{{ * // The following are equivalent: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def selectExpr(exprs: String*): DataFrame = { select(exprs.map { expr => Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) }: _*) } /** * Returns a new Dataset by computing the given [[Column]] expression for each element. * * {{{ * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * }}} * * @group typedrel * @since 1.6.0 */ def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = { implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (!encoder.isSerializedAsStructForTopLevel) { new Dataset[U1](sparkSession, project, encoder) } else { // Flattens inner fields of U1 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) } } /** * Internal helper function for building typed selects that return tuples. For simplicity and * code reuse, we do this without the help of the type system and then use helper functions * that cast appropriately for the user facing interface. */ protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = { val encoders = columns.map(_.encoder) val namedColumns = columns.map(_.withInputType(exprEnc, logicalPlan.output).named) val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan)) new Dataset(execution, ExpressionEncoder.tuple(encoders)) } /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] /** * Filters rows using the given condition. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0 */ def filter(condition: Column): Dataset[T] = withTypedPlan { Filter(condition.expr, logicalPlan) } /** * Filters rows using the given SQL expression. * {{{ * peopleDs.filter(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0 */ def filter(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Filters rows using the given condition. This is an alias for `filter`. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0 */ def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * {{{ * peopleDs.where(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0 */ def where(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def groupBy(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType) } /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) } /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def groupBy(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType) } /** * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0 */ def reduce(func: (T, T) => T): T = withNewRDDExecutionId { rdd.reduce(func) } /** * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0 */ def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) /** * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0 */ def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = { val withGroupingKey = AppendColumns(func, logicalPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, logicalPlan.output, withGroupingKey.newColumns) } /** * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0 */ def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * }}} * * @group untypedrel * @since 2.0.0 */ def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = { groupBy().agg(aggExpr, aggExprs : _*) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0 */ def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0 */ def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*) /** * Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset * that returns the same result as the input, with the following guarantees: * <ul> * <li>It will compute the defined aggregates (metrics) on all the data that is flowing through * the Dataset at that point.</li> * <li>It will report the value of the defined aggregate columns as soon as we reach a completion * point. A completion point is either the end of a query (batch mode) or the end of a streaming * epoch. The value of the aggregates only reflects the data processed since the previous * completion point.</li> * </ul> * Please note that continuous execution is currently not supported. * * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that * contain references to the input Dataset's columns must always be wrapped in an aggregate * function. * * A user can observe these metrics by either adding * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session. * * {{{ * // Monitor the metrics using a listener. * spark.streams.addListener(new StreamingQueryListener() { * override def onQueryStarted(event: QueryStartedEvent): Unit = {} * override def onQueryProgress(event: QueryProgressEvent): Unit = { * event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row => * // Trigger if the number of errors exceeds 5 percent * val num_rows = row.getAs[Long](\"rc\") * val num_error_rows = row.getAs[Long](\"erc\") * val ratio = num_error_rows.toDouble / num_rows * if (ratio > 0.05) { * // Trigger alert * } * } * } * override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {} * }) * // Observe row count (rc) and error row count (erc) in the streaming Dataset * val observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\")) * observed_ds.writeStream.format(\"...\").start() * }}} * * @group typedrel * @since 3.0.0 */ @varargs def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan { CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan) } /** * Observe (named) metrics through an `org.apache.spark.sql.Observation` instance. * This is equivalent to calling `observe(String, Column, Column*)` but does not require * adding `org.apache.spark.sql.util.QueryExecutionListener` to the spark session. * This method does not support streaming datasets. * * A user can retrieve the metrics by accessing `org.apache.spark.sql.Observation.get`. * * {{{ * // Observe row count (rows) and highest id (maxid) in the Dataset while writing it * val observation = Observation(\"my_metrics\") * val observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\")) * observed_ds.write.parquet(\"ds.parquet\") * val metrics = observation.get * }}} * * @throws IllegalArgumentException If this is a streaming Dataset (this.isStreaming == true) * * @group typedrel * @since 3.3.0 */ @varargs def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = { observation.on(this, expr, exprs: _*) } /** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * @group typedrel * @since 2.0.0 */ def limit(n: Int): Dataset[T] = withTypedPlan { Limit(Literal(n), logicalPlan) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.union(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 4| 5| 6| * // +----+----+----+ * }}} * * Notice that the column positions in the schema aren't necessarily matched with the * fields in the strongly typed objects in a Dataset. This function resolves columns * by their positions in the schema, not the fields in the strongly typed objects. Use * [[unionByName]] to resolve columns by field name in the typed objects. * * @group typedrel * @since 2.0.0 */ def union(other: Dataset[T]): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan, other.logicalPlan)) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is an alias for `union`. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.0.0 */ def unionAll(other: Dataset[T]): Dataset[T] = union(other) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set * union (that does deduplication of elements), use this function followed by a [[distinct]]. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.unionByName(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 6| 4| 5| * // +----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. Nested columns in map types * are not currently supported. * * @group typedrel * @since 2.3.0 */ def unionByName(other: Dataset[T]): Dataset[T] = unionByName(other, false) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position). * * When the parameter `allowMissingColumns` is `true`, the set of column names * in this and other `Dataset` can differ; missing columns will be filled with null. * Further, the missing columns of this `Dataset` will be added at the end * in the schema of the union result: * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col0\", \"col3\") * df1.unionByName(df2, true).show * * // output: \"col3\" is missing at left df1 and added at the end of schema. * // +----+----+----+----+ * // |col0|col1|col2|col3| * // +----+----+----+----+ * // | 1| 2| 3|null| * // | 5| 4|null| 6| * // +----+----+----+----+ * * df2.unionByName(df1, true).show * * // output: \"col2\" is missing at left df2 and added at the end of schema. * // +----+----+----+----+ * // |col1|col0|col3|col2| * // +----+----+----+----+ * // | 4| 5| 6|null| * // | 2| 1|null| 3| * // +----+----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. With `allowMissingColumns`, * missing nested columns of struct columns with the same name will also be filled with null * values and added to the end of struct. Nested columns in map types are not currently * supported. * * @group typedrel * @since 3.1.0 */ def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns)) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 1.6.0 */ def intersect(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset while * preserving the duplicates. * This is equivalent to `INTERSECT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard * in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0 */ def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT DISTINCT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0 */ def except(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset while * preserving the duplicates. * This is equivalent to `EXCEPT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in * SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0 */ def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a user-supplied seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0 */ def sample(fraction: Double, seed: Long): Dataset[T] = { sample(withReplacement = false, fraction = fraction, seed = seed) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a random seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0 */ def sample(fraction: Double): Dataset[T] = { sample(withReplacement = false, fraction = fraction) } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = { withTypedPlan { Sample(0.0, fraction, withReplacement, seed, logicalPlan) } } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = { sample(withReplacement, fraction, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * For Java API, use [[randomSplitAsList]]. * * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out // from the sort order. val sortOrder = logicalPlan.output .filter(attr => RowOrdering.isOrderable(attr.dataType)) .map(SortOrder(_, Ascending)) val plan = if (sortOrder.nonEmpty) { Sort(sortOrder, global = false, logicalPlan) } else { // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism cache() logicalPlan } val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder) }.toArray } /** * Returns a Java list that contains randomly split Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * @group typedrel * @since 2.0.0 */ def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = { val values = randomSplit(weights, seed) java.util.Arrays.asList(values : _*) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double]): Array[Dataset[T]] = { randomSplit(weights, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. */ private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = { randomSplit(weights.toArray, seed) } /** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * {{{ * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select($\"title\", explode(split($\"words\", \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(count_distinct(\"title\")) * }}} * * Using `flatMap()` this can similarly be exploded as: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0 */ @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = { val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * {{{ * ds.select(explode(split($\"words\", \" \")).as(\"word\")) * }}} * * or `flatMap()`: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0 */ @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B]) : DataFrame = { val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = { val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o))) } val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * * `column`'s expression must only refer to attributes supplied by this Dataset. It is an * error to add a column that refers to some other Dataset. * * @note this method introduces a projection internally. Therefore, calling it multiple times, * for instance, via loops in order to add multiple columns can generate big plans which * can cause performance issues and even `StackOverflowException`. To avoid this, * use `select` with the multiple columns at once. * * @group untypedrel * @since 2.0.0 */ def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col)) /** * (Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attributes * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0 */ def withColumns(colsMap: Map[String, Column]): DataFrame = { val (colNames, newCols) = colsMap.toSeq.unzip withColumns(colNames, newCols) } /** * (Java-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attribute * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0 */ def withColumns(colsMap: java.util.Map[String, Column]): DataFrame = withColumns( colsMap.asScala.toMap ) /** * Returns a new Dataset by adding columns or replacing the existing columns that has * the same names. */ private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = { require(colNames.size == cols.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of columns: ${cols.size}\") SchemaUtils.checkColumnNameDuplication( colNames, \"in given column names\", sparkSession.sessionState.conf.caseSensitiveAnalysis) val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val columnSeq = colNames.zip(cols) val replacedAndExistingColumns = output.map { field => columnSeq.find { case (colName, _) => resolver(field.name, colName) } match { case Some((colName: String, col: Column)) => col.as(colName) case _ => Column(field) } } val newColumns = columnSeq.filter { case (colName, col) => !output.exists(f => resolver(f.name, colName)) }.map { case (colName, col) => col.as(colName) } select(replacedAndExistingColumns ++ newColumns : _*) } /** * Returns a new Dataset by adding columns with metadata. */ private[spark] def withColumns( colNames: Seq[String], cols: Seq[Column], metadata: Seq[Metadata]): DataFrame = { require(colNames.size == metadata.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of metadata elements: ${metadata.size}\") val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) => col.as(colName, metadata) } withColumns(colNames, newCols) } /** * Returns a new Dataset by adding a column with metadata. */ private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = withColumns(Seq(colName), Seq(col), Seq(metadata)) /** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * * @group untypedrel * @since 2.0.0 */ def withColumnRenamed(existingName: String, newName: String): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f => resolver(f.name, existingName)) if (shouldRename) { val columns = output.map { col => if (resolver(col.name, existingName)) { Column(col).as(newName) } else { Column(col) } } select(columns : _*) } else { toDF() } } /** * Returns a new Dataset by updating an existing column with metadata. * * @group untypedrel * @since 3.3.0 */ def withMetadata(columnName: String, metadata: Metadata): DataFrame = { withColumn(columnName, col(columnName), metadata) } /** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * @group untypedrel * @since 2.0.0 */ def drop(colName: String): DataFrame = { drop(Seq(colName) : _*) } /** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def drop(colNames: String*): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter { attribute => colNames.forall(n => !resolver(attribute.name, n)) }.map(attribute => Column(attribute)) if (remainingCols.size == allColumns.size) { toDF() } else { this.select(remainingCols: _*) } } /** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * @group untypedrel * @since 2.0.0 */ def drop(col: Column): DataFrame = { val expression = col match { case Column(u: UnresolvedAttribute) => queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) => expr } val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter { attr => !attr.semanticEquals(expression) }.map(attr => Column(attr)) select(colsAfterDrop : _*) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns) /** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue // (the Streaming's state store depends on the `groupCols` order). val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) => // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col => resolver(col.name, colName)) if (cols.isEmpty) { throw QueryCompilationErrors.cannotResolveColumnNameAmongAttributesError( colName, schema.fieldNames.mkString(\", \")) } cols } Deduplicate(groupCols, logicalPlan) } /** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq) /** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def dropDuplicates(col1: String, cols: String*): Dataset[T] = { val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames) } /** * Computes basic statistics for numeric and string columns, including count, mean, stddev, min, * and max. If no columns are given, this function computes statistics for all numerical or * string columns. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * }}} * * Use [[summary]] for expanded statistics and control over which statistics to compute. * * @param cols Columns to compute statistics on. * * @group action * @since 1.6.0 */ @scala.annotation.varargs def describe(cols: String*): DataFrame = { val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*) selected.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\") } /** * Computes specified statistics for numeric and string columns. Available statistics are: * <ul> * <li>count</li> * <li>mean</li> * <li>stddev</li> * <li>min</li> * <li>max</li> * <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li> * <li>count_distinct</li> * <li>approx_count_distinct</li> * </ul> * * If no statistics are given, this function computes count, mean, stddev, min, * approximate quartiles (percentiles at 25%, 50%, and 75%), and max. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.summary().show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 50% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * {{{ * ds.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show() * * // output: * // summary age height * // count 10.0 10.0 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * To do a summary for specific columns first select them: * * {{{ * ds.select(\"age\", \"height\").summary().show() * }}} * * Specify statistics to output custom summaries: * * {{{ * ds.summary(\"count\", \"count_distinct\").show() * }}} * * The distinct count isn't included by default. * * You can also run approximate distinct counts which are faster: * * {{{ * ds.summary(\"count\", \"approx_count_distinct\").show() * }}} * * See also [[describe]] for basic statistics. * * @param statistics Statistics from above list to be computed. * * @group action * @since 2.3.0 */ @scala.annotation.varargs def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq) /** * Returns the first `n` rows. * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @group action * @since 1.6.0 */ def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) /** * Returns the first row. * @group action * @since 1.6.0 */ def head(): T = head(1).head /** * Returns the first row. Alias for head(). * @group action * @since 1.6.0 */ def first(): T = head() /** * Concise syntax for chaining custom transformations. * {{{ * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * }}} * * @group typedrel * @since 1.6.0 */ def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this) /** * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0 */ def filter(func: T => Boolean): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0 */ def filter(func: FilterFunction[T]): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0 */ def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan { MapElements[T, U](func, logicalPlan) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0 */ def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * @group typedrel * @since 1.6.0 */ def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = { new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]]) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * @group typedrel * @since 1.6.0 */ def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala mapPartitions(func)(encoder) } /** * Returns a new `DataFrame` that contains the result of applying a serialized R function * `func` to each partition. */ private[sql] def mapPartitionsInR( func: Array[Byte], packageNames: Array[Byte], broadcastVars: Array[Broadcast[Object]], schema: StructType): DataFrame = { val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]] Dataset.ofRows( sparkSession, MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan)) } /** * Applies a Scalar iterator Pandas UDF to each partition. The user-defined function * defines a transformation: `iter(pandas.DataFrame)` -> `iter(pandas.DataFrame)`. * Each partition is each iterator consisting of DataFrames as batches. * * This function uses Apache Arrow as serialization format between Java executors and Python * workers. */ private[sql] def mapInPandas(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, MapInPandas( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * Applies a function to each partition in Arrow format. The user-defined function * defines a transformation: `iter(pyarrow.RecordBatch)` -> `iter(pyarrow.RecordBatch)`. * Each partition is each iterator consisting of `pyarrow.RecordBatch`s as batches. */ private[sql] def pythonMapInArrow(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, PythonMapInArrow( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0 */ def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func)) /** * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0 */ def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (T) => Iterator[U] = x => f.call(x).asScala flatMap(func)(encoder) } /** * Applies a function `f` to all rows. * * @group action * @since 1.6.0 */ def foreach(f: T => Unit): Unit = withNewRDDExecutionId { rdd.foreach(f) } /** * (Java-specific) * Runs `func` on each element of this Dataset. * * @group action * @since 1.6.0 */ def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) /** * Applies a function `f` to each partition of this Dataset. * * @group action * @since 1.6.0 */ def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId { rdd.foreachPartition(f) } /** * (Java-specific) * Runs `func` on each partition of this Dataset. * * @group action * @since 1.6.0 */ def foreachPartition(func: ForeachPartitionFunction[T]): Unit = { foreachPartition((it: Iterator[T]) => func.call(it.asJava)) } /** * Returns the first `n` rows in the Dataset. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def take(n: Int): Array[T] = head(n) /** * Returns the last `n` rows in the Dataset. * * Running tail requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 3.0.0 */ def tail(n: Int): Array[T] = withAction( \"tail\", withTypedPlan(Tail(Literal(n), logicalPlan)).queryExecution)(collectFromPlan) /** * Returns the first `n` rows in the Dataset as a list. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*) /** * Returns an array that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0 */ def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) /** * Returns a Java list that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan => val values = collectFromPlan(plan) java.util.Arrays.asList(values : _*) } /** * Returns an iterator that contains all rows in this Dataset. * * The iterator will consume as much memory as the largest partition in this Dataset. * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * * @group action * @since 2.0.0 */ def toLocalIterator(): java.util.Iterator[T] = { withAction(\"toLocalIterator\", queryExecution) { plan => val fromRow = resolvedEnc.createDeserializer() plan.executeToIterator().map(fromRow).asJava } } /** * Returns the number of rows in the Dataset. * @group action * @since 1.6.0 */ def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan => plan.executeCollect().head.getLong(0) } /** * Returns a new Dataset that has exactly `numPartitions` partitions. * * @group typedrel * @since 1.6.0 */ def repartition(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = true, logicalPlan) } private def repartitionByExpression( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments. // However, we don't want to complicate the semantics of this API method. // Instead, let's give users a friendly error message, pointing them to the new method. val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder]) if (sortOrders.nonEmpty) throw new IllegalArgumentException( s\"\"\"Invalid partitionExprs specified: $sortOrders |For range partitioning use repartitionByRange(...) instead. \"\"\".stripMargin) withTypedPlan { RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByExpression(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = { repartitionByExpression(None, partitionExprs) } private def repartitionByRange( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { require(partitionExprs.nonEmpty, \"At least one partition-by expression must be specified.\") val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) }) withTypedPlan { RepartitionByExpression(sortOrder, logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0 */ @scala.annotation.varargs def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByRange(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0 */ @scala.annotation.varargs def repartitionByRange(partitionExprs: Column*): Dataset[T] = { repartitionByRange(None, partitionExprs) } /** * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions * are requested. If a larger number of partitions is requested, it will stay at the current * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can call repartition. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @group typedrel * @since 1.6.0 */ def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = false, logicalPlan) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * Note that for a streaming [[Dataset]], this method returns distinct rows only once * regardless of the output mode, which the behavior may not be same with `DISTINCT` in SQL * against streaming [[Dataset]]. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0 */ def distinct(): Dataset[T] = dropDuplicates() /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */ def persist(): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this) this } /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */ def cache(): this.type = persist() /** * Persist this Dataset with the given storage level. * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, * `MEMORY_AND_DISK_2`, etc. * * @group basic * @since 1.6.0 */ def persist(newLevel: StorageLevel): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this } /** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * @group basic * @since 2.1.0 */ def storageLevel: StorageLevel = { sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData => cachedData.cachedRepresentation.cacheBuilder.storageLevel }.getOrElse(StorageLevel.NONE) } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @param blocking Whether to block until all blocks are deleted. * * @group basic * @since 1.6.0 */ def unpersist(blocking: Boolean): this.type = { sparkSession.sharedState.cacheManager.uncacheQuery( sparkSession, logicalPlan, cascade = false, blocking) this } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @group basic * @since 1.6.0 */ def unpersist(): this.type = unpersist(blocking = false) // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`. @transient private lazy val rddQueryExecution: QueryExecution = { val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized) } /** * Represents the content of the Dataset as an `RDD` of `T`. * * @group basic * @since 1.6.0 */ lazy val rdd: RDD[T] = { val objectType = exprEnc.deserializer.dataType rddQueryExecution.toRdd.mapPartitions { rows => rows.map(_.get(0, objectType).asInstanceOf[T]) } } /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0 */ def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0 */ def javaRDD: JavaRDD[T] = toJavaRDD /** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 1.6.0 */ @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\") def registerTempTable(tableName: String): Unit = { createOrReplaceTempView(tableName) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.0.0 */ @throws[AnalysisException] def createTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = false) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 2.0.0 */ def createOrReplaceTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = false) } /** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.1.0 */ @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) } /** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0 */ def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) } private def createTempViewCommand( viewName: String, replace: Boolean, global: Boolean): CreateViewCommand = { val viewType = if (global) GlobalTempView else LocalTempView val tableIdentifier = try { sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName) } catch { case _: ParseException => throw QueryCompilationErrors.invalidViewNameError(viewName) } CreateViewCommand( name = tableIdentifier, userSpecifiedColumns = Nil, comment = None, properties = Map.empty, originalText = None, plan = logicalPlan, allowExisting = false, replace = replace, viewType = viewType, isAnalyzed = true) } /** * Interface for saving the content of the non-streaming Dataset out into external storage. * * @group basic * @since 1.6.0 */ def write: DataFrameWriter[T] = { if (isStreaming) { logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriter[T](this) } /** * Create a write configuration builder for v2 sources. * * This builder is used to configure and execute write operations. For example, to append to an * existing table, run: * * {{{ * df.writeTo(\"catalog.db.table\").append() * }}} * * This can also be used to create or replace existing tables: * * {{{ * df.writeTo(\"catalog.db.table\").partitionedBy($\"col\").createOrReplace() * }}} * * @group basic * @since 3.0.0 */ def writeTo(table: String): DataFrameWriterV2[T] = { // TODO: streaming could be adapted to use this interface if (isStreaming) { logicalPlan.failAnalysis( \"'writeTo' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriterV2[T](table, this) } /** * Interface for saving the content of the streaming Dataset out into external storage. * * @group basic * @since 2.0.0 */ def writeStream: DataStreamWriter[T] = { if (!isStreaming) { logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") } new DataStreamWriter[T](this) } /** * Returns the content of the Dataset as a Dataset of JSON strings. * @since 2.0.0 */ def toJSON: Dataset[String] = { val rowSchema = this.schema val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone mapPartitions { iter => val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer, new JSONOptions(Map.empty[String, String], sessionLocalTimeZone)) new Iterator[String] { private val toRow = exprEnc.createSerializer() override def hasNext: Boolean = iter.hasNext override def next(): String = { gen.write(toRow(iter.next())) gen.flush() val json = writer.toString if (hasNext) { writer.reset() } else { gen.close() } json } } } (Encoders.STRING) } /** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * @group basic * @since 2.0.0 */ def inputFiles: Array[String] = { val files: Seq[String] = queryExecution.optimizedPlan.collect { case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) => fsBasedRelation.inputFiles case fr: FileRelation => fr.inputFiles case r: HiveTableRelation => r.tableMeta.storage.locationUri.map(_.toString).toArray case DataSourceV2ScanRelation(DataSourceV2Relation(table: FileTable, _, _, _, _), _, _, _) => table.fileIndex.inputFiles }.flatten files.toSet.toArray } /** * Returns `true` when the logical query plans inside both [[Dataset]]s are equal and * therefore return same results. * * @note The equality comparison here is simplified by tolerating the cosmetic differences * such as attribute names. * @note This API can compare both [[Dataset]]s very fast but can still return `false` on * the [[Dataset]] that return the same results, for instance, from different plans. Such * false negative semantic can be useful when caching as an example. * @since 3.1.0 */ @DeveloperApi def sameSemantics(other: Dataset[T]): Boolean = { queryExecution.analyzed.sameResult(other.queryExecution.analyzed) } /** * Returns a `hashCode` of the logical query plan against this [[Dataset]]. * * @note Unlike the standard `hashCode`, the hash is calculated against the query plan * simplified by tolerating the cosmetic differences such as attribute names. * @since 3.1.0 */ @DeveloperApi def semanticHash(): Int = { queryExecution.analyzed.semanticHash() } //////////////////////////////////////////////////////////////////////////// // For Python API //////////////////////////////////////////////////////////////////////////// /** * It adds a new long column with the name `name` that increases one by one. * This is for 'distributed-sequence' default index in pandas API on Spark. */ private[sql] def withSequenceColumn(name: String) = { Dataset.ofRows( sparkSession, AttachDistributedSequence( AttributeReference(name, LongType, nullable = false)(), logicalPlan)) } /** * Converts a JavaRDD to a PythonRDD. */ private[sql] def javaToPython: JavaRDD[Array[Byte]] = { val structType = schema // capture it for closure val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType)) EvaluatePython.javaToPython(rdd) } private[sql] def collectToPython(): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"collectToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeCollect().iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def tailToPython(n: Int): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"tailToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeTail(n).iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def getRowsToPython( _numRows: Int, truncate: Int): Array[Any] = { EvaluatePython.registerPicklers() val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) val rows = getRows(numRows, truncate).map(_.toArray).toArray val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType))) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( rows.iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-GetRows\") } /** * Collect a Dataset as Arrow batches and serve stream to SparkR. It sends * arrow batches in an ordered manner with buffering. This is inevitable * due to missing R API that reads batches from socket directly. See ARROW-4512. * Eventually, this code should be deduplicated by `collectAsArrowToPython`. */ private[sql] def collectAsArrowToR(): Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone RRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToR\", queryExecution) { plan => val buffer = new ByteArrayOutputStream() val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, buffer, timeZoneId) val arrowBatchRdd = toArrowBatchRdd(plan) val numPartitions = arrowBatchRdd.partitions.length // Store collection results for worst case of 1 to N-1 partitions val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1)) var lastIndex = -1 // index of last partition written // Handler to eagerly write partitions to Python in order def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = { // If result is from next partition in order if (index - 1 == lastIndex) { batchWriter.writeBatches(arrowBatches.iterator) lastIndex += 1 // Write stored partitions that come next in order while (lastIndex < results.length && results(lastIndex) != null) { batchWriter.writeBatches(results(lastIndex).iterator) results(lastIndex) = null lastIndex += 1 } // After last batch, end the stream if (lastIndex == results.length) { batchWriter.end() val batches = buffer.toByteArray out.writeInt(batches.length) out.write(batches) } } else { // Store partitions received out of order results(index - 1) = arrowBatches } } sparkSession.sparkContext.runJob( arrowBatchRdd, (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray, 0 until numPartitions, handlePartitionBatches) } } } /** * Collect a Dataset as Arrow batches and serve stream to PySpark. It sends * arrow batches in an un-ordered manner without buffering, and then batch order * information at the end. The batches should be reordered at Python side. */ private[sql] def collectAsArrowToPython: Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone PythonRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToPython\", queryExecution) { plan => val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId) // Batches ordered by (index of partition, batch index in that partition) tuple val batchOrder = ArrayBuffer.empty[(Int, Int)] // Handler to eagerly write batches to Python as they arrive, un-ordered val handlePartitionBatches = (index: Int, arrowBatches: Array[Array[Byte]]) => if (arrowBatches.nonEmpty) { // Write all batches (can be more than 1) in the partition, store the batch order tuple batchWriter.writeBatches(arrowBatches.iterator) arrowBatches.indices.foreach { partitionBatchIndex => batchOrder.append((index, partitionBatchIndex)) } } Utils.tryWithSafeFinally { val arrowBatchRdd = toArrowBatchRdd(plan) sparkSession.sparkContext.runJob( arrowBatchRdd, (it: Iterator[Array[Byte]]) => it.toArray, handlePartitionBatches) } { // After processing all partitions, end the batch stream batchWriter.end() // Write batch order indices out.writeInt(batchOrder.length) // Sort by (index of partition, batch index in that partition) tuple to get the // overall_batch_index from 0 to N-1 batches, which can be used to put the // transferred batches in the correct order batchOrder.zipWithIndex.sortBy(_._1).foreach { case (_, overallBatchIndex) => out.writeInt(overallBatchIndex) } } } } } private[sql] def toPythonIterator(prefetchPartitions: Boolean = false): Array[Any] = { withNewExecutionId { PythonRDD.toLocalIteratorAndServe(javaToPython.rdd, prefetchPartitions) } } //////////////////////////////////////////////////////////////////////////// // Private Helpers //////////////////////////////////////////////////////////////////////////// /** * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with * an execution. */ private def withNewExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(queryExecution)(body) } /** * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect * them with an execution. Before performing the action, the metrics of the executed plan will be * reset. */ private def withNewRDDExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(rddQueryExecution) { rddQueryExecution.executedPlan.resetMetrics() body } } /** * Wrap a Dataset action to track the QueryExecution and time cost, then report to the * user-registered callback functions, and also to convert asserts/NPE to * the internal error exception. */ private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } /** * Collect all elements from a spark plan. */ private def collectFromPlan(plan: SparkPlan): Array[T] = { val fromRow = resolvedEnc.createDeserializer() plan.executeCollect().map(fromRow) } private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = { val sortOrder: Seq[SortOrder] = sortExprs.map { col => col.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) } } withTypedPlan { Sort(sortOrder, global = global, logicalPlan) } } /** A convenient function to wrap a logical plan and produce a DataFrame. */ @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = { Dataset.ofRows(sparkSession, logicalPlan) } /** A convenient function to wrap a logical plan and produce a Dataset. */ @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { Dataset(sparkSession, logicalPlan) } /** A convenient function to wrap a set based logical plan and produce a Dataset. */ @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) { // Set operators widen types (change the schema), so we cannot reuse the row encoder. Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]] } else { Dataset(sparkSession, logicalPlan) } } /** Convert to an RDD of serialized ArrowRecordBatches. */ private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = { val schemaCaptured = this.schema val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone plan.execute().mapPartitionsInternal { iter => val context = TaskContext.get() ArrowConverters.toBatchIterator( iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context) } } // This is only used in tests, for now. private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = { toArrowBatchRdd(queryExecution.executedPlan) } }",
          "## CLASS: org/apache/spark/sql/SparkSession# (implementation)\n@Stable class SparkSession private( @transient val sparkContext: SparkContext, @transient private val existingSharedState: Option[SharedState], @transient private val parentSessionState: Option[SessionState], @transient private[sql] val extensions: SparkSessionExtensions, @transient private[sql] val initialSessionOptions: Map[String, String]) extends Serializable with Closeable with Logging { self => // The call site where this SparkSession was constructed. private val creationSite: CallSite = Utils.getCallSite() /** * Constructor used in Pyspark. Contains explicit application of Spark Session Extensions * which otherwise only occurs during getOrCreate. We cannot add this to the default constructor * since that would cause every new session to reinvoke Spark Session Extensions on the currently * running extensions. */ private[sql] def this( sc: SparkContext, initialSessionOptions: java.util.HashMap[String, String]) = { this(sc, None, None, SparkSession.applyExtensions( sc.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty), new SparkSessionExtensions), initialSessionOptions.asScala.toMap) } private[sql] def this(sc: SparkContext) = this(sc, new java.util.HashMap[String, String]()) private[sql] val sessionUUID: String = UUID.randomUUID.toString sparkContext.assertNotStopped() // If there is no active SparkSession, uses the default SQL conf. Otherwise, use the session's. SQLConf.setSQLConfGetter(() => { SparkSession.getActiveSession.filterNot(_.sparkContext.isStopped).map(_.sessionState.conf) .getOrElse(SQLConf.getFallbackConf) }) /** * The version of Spark on which this application is running. * * @since 2.0.0 */ def version: String = SPARK_VERSION /* ----------------------- * | Session-related state | * ----------------------- */ /** * State shared across sessions, including the `SparkContext`, cached data, listener, * and a catalog that interacts with external systems. * * This is internal to Spark and there is no guarantee on interface stability. * * @since 2.2.0 */ @Unstable @transient lazy val sharedState: SharedState = { existingSharedState.getOrElse(new SharedState(sparkContext, initialSessionOptions)) } /** * State isolated across sessions, including SQL configurations, temporary tables, registered * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]]. * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent. * * This is internal to Spark and there is no guarantee on interface stability. * * @since 2.2.0 */ @Unstable @transient lazy val sessionState: SessionState = { parentSessionState .map(_.clone(this)) .getOrElse { val state = SparkSession.instantiateSessionState( SparkSession.sessionStateClassName(sharedState.conf), self) state } } /** * A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility. * * @since 2.0.0 */ @transient val sqlContext: SQLContext = new SQLContext(this) /** * Runtime configuration interface for Spark. * * This is the interface through which the user can get and set all Spark and Hadoop * configurations that are relevant to Spark SQL. When getting the value of a config, * this defaults to the value set in the underlying `SparkContext`, if any. * * @since 2.0.0 */ @transient lazy val conf: RuntimeConfig = new RuntimeConfig(sessionState.conf) /** * An interface to register custom [[org.apache.spark.sql.util.QueryExecutionListener]]s * that listen for execution metrics. * * @since 2.0.0 */ def listenerManager: ExecutionListenerManager = sessionState.listenerManager /** * :: Experimental :: * A collection of methods that are considered experimental, but can be used to hook into * the query planner for advanced functionality. * * @since 2.0.0 */ @Experimental @Unstable def experimental: ExperimentalMethods = sessionState.experimentalMethods /** * A collection of methods for registering user-defined functions (UDF). * * The following example registers a Scala closure as UDF: * {{{ * sparkSession.udf.register(\"myUDF\", (arg1: Int, arg2: String) => arg2 + arg1) * }}} * * The following example registers a UDF in Java: * {{{ * sparkSession.udf().register(\"myUDF\", * (Integer arg1, String arg2) -> arg2 + arg1, * DataTypes.StringType); * }}} * * @note The user-defined functions must be deterministic. Due to optimization, * duplicate invocations may be eliminated or the function may even be invoked more times than * it is present in the query. * * @since 2.0.0 */ def udf: UDFRegistration = sessionState.udfRegistration /** * Returns a `StreamingQueryManager` that allows managing all the * `StreamingQuery`s active on `this`. * * @since 2.0.0 */ @Unstable def streams: StreamingQueryManager = sessionState.streamingQueryManager /** * Start a new session with isolated SQL configurations, temporary tables, registered * functions are isolated, but sharing the underlying `SparkContext` and cached data. * * @note Other than the `SparkContext`, all shared state is initialized lazily. * This method will force the initialization of the shared state to ensure that parent * and child sessions are set up with the same shared state. If the underlying catalog * implementation is Hive, this will initialize the metastore, which may take some time. * * @since 2.0.0 */ def newSession(): SparkSession = { new SparkSession( sparkContext, Some(sharedState), parentSessionState = None, extensions, initialSessionOptions) } /** * Create an identical copy of this `SparkSession`, sharing the underlying `SparkContext` * and shared state. All the state of this session (i.e. SQL configurations, temporary tables, * registered functions) is copied over, and the cloned session is set up with the same shared * state as this session. The cloned session is independent of this session, that is, any * non-global change in either session is not reflected in the other. * * @note Other than the `SparkContext`, all shared state is initialized lazily. * This method will force the initialization of the shared state to ensure that parent * and child sessions are set up with the same shared state. If the underlying catalog * implementation is Hive, this will initialize the metastore, which may take some time. */ private[sql] def cloneSession(): SparkSession = { val result = new SparkSession( sparkContext, Some(sharedState), Some(sessionState), extensions, Map.empty) result.sessionState // force copy of SessionState result } /* --------------------------------- * | Methods for creating DataFrames | * --------------------------------- */ /** * Returns a `DataFrame` with no rows or columns. * * @since 2.0.0 */ @transient lazy val emptyDataFrame: DataFrame = Dataset.ofRows(self, LocalRelation()) /** * Creates a new [[Dataset]] of type T containing zero elements. * * @return 2.0.0 */ def emptyDataset[T: Encoder]: Dataset[T] = { val encoder = implicitly[Encoder[T]] new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder) } /** * Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples). * * @since 2.0.0 */ def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = withActive { val encoder = Encoders.product[A] Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder)) } /** * Creates a `DataFrame` from a local Seq of Product. * * @since 2.0.0 */ def createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame = withActive { val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val attributeSeq = schema.toAttributes Dataset.ofRows(self, LocalRelation.fromProduct(attributeSeq, data)) } /** * :: DeveloperApi :: * Creates a `DataFrame` from an `RDD` containing [[Row]]s using the given schema. * It is important to make sure that the structure of every [[Row]] of the provided RDD matches * the provided schema. Otherwise, there will be runtime exception. * Example: * {{{ * import org.apache.spark.sql._ * import org.apache.spark.sql.types._ * val sparkSession = new org.apache.spark.sql.SparkSession(sc) * * val schema = * StructType( * StructField(\"name\", StringType, false) :: * StructField(\"age\", IntegerType, true) :: Nil) * * val people = * sc.textFile(\"examples/src/main/resources/people.txt\").map( * _.split(\",\")).map(p => Row(p(0), p(1).trim.toInt)) * val dataFrame = sparkSession.createDataFrame(people, schema) * dataFrame.printSchema * // root * // |-- name: string (nullable = false) * // |-- age: integer (nullable = true) * * dataFrame.createOrReplaceTempView(\"people\") * sparkSession.sql(\"select name from people\").collect.foreach(println) * }}} * * @since 2.0.0 */ @DeveloperApi def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame = withActive { val replaced = CharVarcharUtils.failIfHasCharVarchar(schema).asInstanceOf[StructType] // TODO: use MutableProjection when rowRDD is another DataFrame and the applied // schema differs from the existing schema on any field data type. val encoder = RowEncoder(replaced) val toRow = encoder.createSerializer() val catalystRows = rowRDD.map(toRow) internalCreateDataFrame(catalystRows.setName(rowRDD.name), schema) } /** * :: DeveloperApi :: * Creates a `DataFrame` from a `JavaRDD` containing [[Row]]s using the given schema. * It is important to make sure that the structure of every [[Row]] of the provided RDD matches * the provided schema. Otherwise, there will be runtime exception. * * @since 2.0.0 */ @DeveloperApi def createDataFrame(rowRDD: JavaRDD[Row], schema: StructType): DataFrame = { val replaced = CharVarcharUtils.failIfHasCharVarchar(schema).asInstanceOf[StructType] createDataFrame(rowRDD.rdd, replaced) } /** * :: DeveloperApi :: * Creates a `DataFrame` from a `java.util.List` containing [[Row]]s using the given schema. * It is important to make sure that the structure of every [[Row]] of the provided List matches * the provided schema. Otherwise, there will be runtime exception. * * @since 2.0.0 */ @DeveloperApi def createDataFrame(rows: java.util.List[Row], schema: StructType): DataFrame = withActive { val replaced = CharVarcharUtils.failIfHasCharVarchar(schema).asInstanceOf[StructType] Dataset.ofRows(self, LocalRelation.fromExternalRows(replaced.toAttributes, rows.asScala.toSeq)) } /** * Applies a schema to an RDD of Java Beans. * * WARNING: Since there is no guaranteed ordering for fields in a Java Bean, * SELECT * queries will return the columns in an undefined order. * * @since 2.0.0 */ def createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame = withActive { val attributeSeq: Seq[AttributeReference] = getSchema(beanClass) val className = beanClass.getName val rowRdd = rdd.mapPartitions { iter => // BeanInfo is not serializable so we must rediscover it remotely for each partition. SQLContext.beansToRows(iter, Utils.classForName(className), attributeSeq) } Dataset.ofRows(self, LogicalRDD(attributeSeq, rowRdd.setName(rdd.name))(self)) } /** * Applies a schema to an RDD of Java Beans. * * WARNING: Since there is no guaranteed ordering for fields in a Java Bean, * SELECT * queries will return the columns in an undefined order. * * @since 2.0.0 */ def createDataFrame(rdd: JavaRDD[_], beanClass: Class[_]): DataFrame = { createDataFrame(rdd.rdd, beanClass) } /** * Applies a schema to a List of Java Beans. * * WARNING: Since there is no guaranteed ordering for fields in a Java Bean, * SELECT * queries will return the columns in an undefined order. * @since 1.6.0 */ def createDataFrame(data: java.util.List[_], beanClass: Class[_]): DataFrame = withActive { val attrSeq = getSchema(beanClass) val rows = SQLContext.beansToRows(data.asScala.iterator, beanClass, attrSeq) Dataset.ofRows(self, LocalRelation(attrSeq, rows.toSeq)) } /** * Convert a `BaseRelation` created for external data sources into a `DataFrame`. * * @since 2.0.0 */ def baseRelationToDataFrame(baseRelation: BaseRelation): DataFrame = { Dataset.ofRows(self, LogicalRelation(baseRelation)) } /* ------------------------------- * | Methods for creating DataSets | * ------------------------------- */ /** * Creates a [[Dataset]] from a local Seq of data of a given type. This method requires an * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation) * that is generally created automatically through implicits from a `SparkSession`, or can be * created explicitly by calling static methods on [[Encoders]]. * * == Example == * * {{{ * * import spark.implicits._ * case class Person(name: String, age: Long) * val data = Seq(Person(\"Michael\", 29), Person(\"Andy\", 30), Person(\"Justin\", 19)) * val ds = spark.createDataset(data) * * ds.show() * // +-------+---+ * // | name|age| * // +-------+---+ * // |Michael| 29| * // | Andy| 30| * // | Justin| 19| * // +-------+---+ * }}} * * @since 2.0.0 */ def createDataset[T : Encoder](data: Seq[T]): Dataset[T] = { val enc = encoderFor[T] val toRow = enc.createSerializer() val attributes = enc.schema.toAttributes val encoded = data.map(d => toRow(d).copy()) val plan = new LocalRelation(attributes, encoded) Dataset[T](self, plan) } /** * Creates a [[Dataset]] from an RDD of a given type. This method requires an * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation) * that is generally created automatically through implicits from a `SparkSession`, or can be * created explicitly by calling static methods on [[Encoders]]. * * @since 2.0.0 */ def createDataset[T : Encoder](data: RDD[T]): Dataset[T] = { Dataset[T](self, ExternalRDD(data, self)) } /** * Creates a [[Dataset]] from a `java.util.List` of a given type. This method requires an * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation) * that is generally created automatically through implicits from a `SparkSession`, or can be * created explicitly by calling static methods on [[Encoders]]. * * == Java Example == * * {{{ * List<String> data = Arrays.asList(\"hello\", \"world\"); * Dataset<String> ds = spark.createDataset(data, Encoders.STRING()); * }}} * * @since 2.0.0 */ def createDataset[T : Encoder](data: java.util.List[T]): Dataset[T] = { createDataset(data.asScala.toSeq) } /** * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements * in a range from 0 to `end` (exclusive) with step value 1. * * @since 2.0.0 */ def range(end: Long): Dataset[java.lang.Long] = range(0, end) /** * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements * in a range from `start` to `end` (exclusive) with step value 1. * * @since 2.0.0 */ def range(start: Long, end: Long): Dataset[java.lang.Long] = { range(start, end, step = 1, numPartitions = leafNodeDefaultParallelism) } /** * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements * in a range from `start` to `end` (exclusive) with a step value. * * @since 2.0.0 */ def range(start: Long, end: Long, step: Long): Dataset[java.lang.Long] = { range(start, end, step, numPartitions = leafNodeDefaultParallelism) } /** * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements * in a range from `start` to `end` (exclusive) with a step value, with partition number * specified. * * @since 2.0.0 */ def range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long] = { new Dataset(self, Range(start, end, step, numPartitions), Encoders.LONG) } /** * Creates a `DataFrame` from an `RDD[InternalRow]`. */ private[sql] def internalCreateDataFrame( catalystRows: RDD[InternalRow], schema: StructType, isStreaming: Boolean = false): DataFrame = { // TODO: use MutableProjection when rowRDD is another DataFrame and the applied // schema differs from the existing schema on any field data type. val logicalPlan = LogicalRDD( schema.toAttributes, catalystRows, isStreaming = isStreaming)(self) Dataset.ofRows(self, logicalPlan) } /* ------------------------- * | Catalog-related methods | * ------------------------- */ /** * Interface through which the user may create, drop, alter or query underlying * databases, tables, functions etc. * * @since 2.0.0 */ @transient lazy val catalog: Catalog = new CatalogImpl(self) /** * Returns the specified table/view as a `DataFrame`. If it's a table, it must support batch * reading and the returned DataFrame is the batch scan query plan of this table. If it's a view, * the returned DataFrame is simply the query plan of the view, which can either be a batch or * streaming query plan. * * @param tableName is either a qualified or unqualified name that designates a table or view. * If a database is specified, it identifies the table/view from the database. * Otherwise, it first attempts to find a temporary view with the given name * and then match the table/view from the current database. * Note that, the global temporary view database is also valid here. * @since 2.0.0 */ def table(tableName: String): DataFrame = { read.table(tableName) } private[sql] def table(tableIdent: TableIdentifier): DataFrame = { Dataset.ofRows(self, UnresolvedRelation(tableIdent)) } /* ----------------- * | Everything else | * ----------------- */ /** * Executes a SQL query using Spark, returning the result as a `DataFrame`. * This API eagerly runs DDL/DML commands, but not for SELECT queries. * * @since 2.0.0 */ def sql(sqlText: String): DataFrame = withActive { val tracker = new QueryPlanningTracker val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) { sessionState.sqlParser.parsePlan(sqlText) } Dataset.ofRows(self, plan, tracker) } /** * Execute an arbitrary string command inside an external execution engine rather than Spark. * This could be useful when user wants to execute some commands out of Spark. For * example, executing custom DDL/DML command for JDBC, creating index for ElasticSearch, * creating cores for Solr and so on. * * The command will be eagerly executed after this method is called and the returned * DataFrame will contain the output of the command(if any). * * @param runner The class name of the runner that implements `ExternalCommandRunner`. * @param command The target command to be executed * @param options The options for the runner. * * @since 3.0.0 */ @Unstable def executeCommand(runner: String, command: String, options: Map[String, String]): DataFrame = { DataSource.lookupDataSource(runner, sessionState.conf) match { case source if classOf[ExternalCommandRunner].isAssignableFrom(source) => Dataset.ofRows(self, ExternalCommandExecutor( source.newInstance().asInstanceOf[ExternalCommandRunner], command, options)) case _ => throw QueryCompilationErrors.commandExecutionInRunnerUnsupportedError(runner) } } /** * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a * `DataFrame`. * {{{ * sparkSession.read.parquet(\"/path/to/file.parquet\") * sparkSession.read.schema(schema).json(\"/path/to/file.json\") * }}} * * @since 2.0.0 */ def read: DataFrameReader = new DataFrameReader(self) /** * Returns a `DataStreamReader` that can be used to read streaming data in as a `DataFrame`. * {{{ * sparkSession.readStream.parquet(\"/path/to/directory/of/parquet/files\") * sparkSession.readStream.schema(schema).json(\"/path/to/directory/of/json/files\") * }}} * * @since 2.0.0 */ def readStream: DataStreamReader = new DataStreamReader(self) /** * Executes some code block and prints to stdout the time taken to execute the block. This is * available in Scala only and is used primarily for interactive testing and debugging. * * @since 2.1.0 */ def time[T](f: => T): T = { val start = System.nanoTime() val ret = f val end = System.nanoTime() // scalastyle:off println println(s\"Time taken: ${NANOSECONDS.toMillis(end - start)} ms\") // scalastyle:on println ret } // scalastyle:off // Disable style checker so \"implicits\" object can start with lowercase i /** * (Scala-specific) Implicit methods available in Scala for converting * common Scala objects into `DataFrame`s. * * {{{ * val sparkSession = SparkSession.builder.getOrCreate() * import sparkSession.implicits._ * }}} * * @since 2.0.0 */ object implicits extends SQLImplicits with Serializable { protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext } // scalastyle:on /** * Stop the underlying `SparkContext`. * * @since 2.0.0 */ def stop(): Unit = { sparkContext.stop() } /** * Synonym for `stop()`. * * @since 2.1.0 */ override def close(): Unit = stop() /** * Parses the data type in our internal string representation. The data type string should * have the same format as the one generated by `toString` in scala. * It is only used by PySpark. */ protected[sql] def parseDataType(dataTypeString: String): DataType = { DataType.fromJson(dataTypeString) } /** * Apply a schema defined by the schemaString to an RDD. It is only used by PySpark. */ private[sql] def applySchemaToPythonRDD( rdd: RDD[Array[Any]], schemaString: String): DataFrame = { val schema = DataType.fromJson(schemaString).asInstanceOf[StructType] applySchemaToPythonRDD(rdd, schema) } /** * Apply `schema` to an RDD. * * @note Used by PySpark only */ private[sql] def applySchemaToPythonRDD( rdd: RDD[Array[Any]], schema: StructType): DataFrame = { val rowRdd = rdd.mapPartitions { iter => val fromJava = python.EvaluatePython.makeFromJava(schema) iter.map(r => fromJava(r).asInstanceOf[InternalRow]) } internalCreateDataFrame(rowRdd, schema) } /** * Returns a Catalyst Schema for the given java bean class. */ private def getSchema(beanClass: Class[_]): Seq[AttributeReference] = { val (dataType, _) = JavaTypeInference.inferDataType(beanClass) dataType.asInstanceOf[StructType].fields.map { f => AttributeReference(f.name, f.dataType, f.nullable)() } } /** * Execute a block of code with the this session set as the active session, and restore the * previous session on completion. */ private[sql] def withActive[T](block: => T): T = { // Use the active session thread local directly to make sure we get the session that is actually // set and not the default session. This to prevent that we promote the default session to the // active session once we are done. val old = SparkSession.activeThreadSession.get() SparkSession.setActiveSession(this) try block finally { SparkSession.setActiveSession(old) } } private[sql] def leafNodeDefaultParallelism: Int = { conf.get(SQLConf.LEAF_NODE_DEFAULT_PARALLELISM).getOrElse(sparkContext.defaultParallelism) } } @Stable object SparkSession extends Logging { /** * Builder for [[SparkSession]]. */ @Stable class Builder extends Logging { private[this] val options = new scala.collection.mutable.HashMap[String, String] private[this] val extensions = new SparkSessionExtensions private[this] var userSuppliedContext: Option[SparkContext] = None private[spark] def sparkContext(sparkContext: SparkContext): Builder = synchronized { userSuppliedContext = Option(sparkContext) this } /** * Sets a name for the application, which will be shown in the Spark web UI. * If no application name is set, a randomly generated name will be used. * * @since 2.0.0 */ def appName(name: String): Builder = config(\"spark.app.name\", name) /** * Sets a config option. Options set using this method are automatically propagated to * both `SparkConf` and SparkSession's own configuration. * * @since 2.0.0 */ def config(key: String, value: String): Builder = synchronized { options += key -> value this } /** * Sets a config option. Options set using this method are automatically propagated to * both `SparkConf` and SparkSession's own configuration. * * @since 2.0.0 */ def config(key: String, value: Long): Builder = synchronized { options += key -> value.toString this } /** * Sets a config option. Options set using this method are automatically propagated to * both `SparkConf` and SparkSession's own configuration. * * @since 2.0.0 */ def config(key: String, value: Double): Builder = synchronized { options += key -> value.toString this } /** * Sets a config option. Options set using this method are automatically propagated to * both `SparkConf` and SparkSession's own configuration. * * @since 2.0.0 */ def config(key: String, value: Boolean): Builder = synchronized { options += key -> value.toString this } /** * Sets a list of config options based on the given `SparkConf`. * * @since 2.0.0 */ def config(conf: SparkConf): Builder = synchronized { conf.getAll.foreach { case (k, v) => options += k -> v } this } /** * Sets the Spark master URL to connect to, such as \"local\" to run locally, \"local[4]\" to * run locally with 4 cores, or \"spark://master:7077\" to run on a Spark standalone cluster. * * @since 2.0.0 */ def master(master: String): Builder = config(\"spark.master\", master) /** * Enables Hive support, including connectivity to a persistent Hive metastore, support for * Hive serdes, and Hive user-defined functions. * * @since 2.0.0 */ def enableHiveSupport(): Builder = synchronized { if (hiveClassesArePresent) { config(CATALOG_IMPLEMENTATION.key, \"hive\") } else { throw new IllegalArgumentException( \"Unable to instantiate SparkSession with Hive support because \" + \"Hive classes are not found.\") } } /** * Inject extensions into the [[SparkSession]]. This allows a user to add Analyzer rules, * Optimizer rules, Planning Strategies or a customized parser. * * @since 2.2.0 */ def withExtensions(f: SparkSessionExtensions => Unit): Builder = synchronized { f(extensions) this } /** * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new * one based on the options set in this builder. * * This method first checks whether there is a valid thread-local SparkSession, * and if yes, return that one. It then checks whether there is a valid global * default SparkSession, and if yes, return that one. If no valid global default * SparkSession exists, the method creates a new SparkSession and assigns the * newly created SparkSession as the global default. * * In case an existing SparkSession is returned, the non-static config options specified in * this builder will be applied to the existing SparkSession. * * @since 2.0.0 */ def getOrCreate(): SparkSession = synchronized { val sparkConf = new SparkConf() options.foreach { case (k, v) => sparkConf.set(k, v) } if (!sparkConf.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) { assertOnDriver() } // Get the session from current thread's active session. var session = activeThreadSession.get() if ((session ne null) && !session.sparkContext.isStopped) { applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava)) return session } // Global synchronization so we will only set the default session once. SparkSession.synchronized { // If the current thread does not have an active session, get it from the global session. session = defaultSession.get() if ((session ne null) && !session.sparkContext.isStopped) { applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava)) return session } // No active nor global default session. Create a new one. val sparkContext = userSuppliedContext.getOrElse { // set a random app name if not given. if (!sparkConf.contains(\"spark.app.name\")) { sparkConf.setAppName(java.util.UUID.randomUUID().toString) } SparkContext.getOrCreate(sparkConf) // Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions. } loadExtensions(extensions) applyExtensions( sparkContext.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty), extensions) session = new SparkSession(sparkContext, None, None, extensions, options.toMap) setDefaultSession(session) setActiveSession(session) registerContextListener(sparkContext) } return session } } /** * Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]]. * * @since 2.0.0 */ def builder(): Builder = new Builder /** * Changes the SparkSession that will be returned in this thread and its children when * SparkSession.getOrCreate() is called. This can be used to ensure that a given thread receives * a SparkSession with an isolated session, instead of the global (first created) context. * * @since 2.0.0 */ def setActiveSession(session: SparkSession): Unit = { activeThreadSession.set(session) } /** * Clears the active SparkSession for current thread. Subsequent calls to getOrCreate will * return the first created context instead of a thread-local override. * * @since 2.0.0 */ def clearActiveSession(): Unit = { activeThreadSession.remove() } /** * Sets the default SparkSession that is returned by the builder. * * @since 2.0.0 */ def setDefaultSession(session: SparkSession): Unit = { defaultSession.set(session) } /** * Clears the default SparkSession that is returned by the builder. * * @since 2.0.0 */ def clearDefaultSession(): Unit = { defaultSession.set(null) } /** * Returns the active SparkSession for the current thread, returned by the builder. * * @note Return None, when calling this function on executors * * @since 2.2.0 */ def getActiveSession: Option[SparkSession] = { if (Utils.isInRunningSparkTask) { // Return None when running on executors. None } else { Option(activeThreadSession.get) } } /** * Returns the default SparkSession that is returned by the builder. * * @note Return None, when calling this function on executors * * @since 2.2.0 */ def getDefaultSession: Option[SparkSession] = { if (Utils.isInRunningSparkTask) { // Return None when running on executors. None } else { Option(defaultSession.get) } } /** * Returns the currently active SparkSession, otherwise the default one. If there is no default * SparkSession, throws an exception. * * @since 2.4.0 */ def active: SparkSession = { getActiveSession.getOrElse(getDefaultSession.getOrElse( throw new IllegalStateException(\"No active or default Spark session found\"))) } /** * Apply modifiable settings to an existing [[SparkSession]]. This method are used * both in Scala and Python, so put this under [[SparkSession]] object. */ private[sql] def applyModifiableSettings( session: SparkSession, options: java.util.HashMap[String, String]): Unit = { // Lazy val to avoid an unnecessary session state initialization lazy val conf = session.sessionState.conf val dedupOptions = if (options.isEmpty) Map.empty[String, String] else ( options.asScala.toSet -- conf.getAllConfs.toSet).toMap val (staticConfs, otherConfs) = dedupOptions.partition(kv => SQLConf.isStaticConfigKey(kv._1)) otherConfs.foreach { case (k, v) => conf.setConfString(k, v) } // Note that other runtime SQL options, for example, for other third-party datasource // can be marked as an ignored configuration here. val maybeIgnoredConfs = otherConfs.filterNot { case (k, _) => conf.isModifiable(k) } if (staticConfs.nonEmpty || maybeIgnoredConfs.nonEmpty) { logWarning( \"Using an existing Spark session; only runtime SQL configurations will take effect.\") } if (staticConfs.nonEmpty) { logDebug(\"Ignored static SQL configurations:\\n \" + conf.redactOptions(staticConfs).toSeq.map { case (k, v) => s\"$k=$v\" }.mkString(\"\\n \")) } if (maybeIgnoredConfs.nonEmpty) { // Only print out non-static and non-runtime SQL configurations. // Note that this might show core configurations or source specific // options defined in the third-party datasource. logDebug(\"Configurations that might not take effect:\\n \" + conf.redactOptions( maybeIgnoredConfs).toSeq.map { case (k, v) => s\"$k=$v\" }.mkString(\"\\n \")) } } /** * Returns a cloned SparkSession with all specified configurations disabled, or * the original SparkSession if all configurations are already disabled. */ private[sql] def getOrCloneSessionWithConfigsOff( session: SparkSession, configurations: Seq[ConfigEntry[Boolean]]): SparkSession = { val configsEnabled = configurations.filter(session.sessionState.conf.getConf(_)) if (configsEnabled.isEmpty) { session } else { val newSession = session.cloneSession() configsEnabled.foreach(conf => { newSession.sessionState.conf.setConf(conf, false) }) newSession } } //////////////////////////////////////////////////////////////////////////////////////// // Private methods from now on //////////////////////////////////////////////////////////////////////////////////////// private val listenerRegistered: AtomicBoolean = new AtomicBoolean(false) /** Register the AppEnd listener onto the Context */ private def registerContextListener(sparkContext: SparkContext): Unit = { if (!listenerRegistered.get()) { sparkContext.addSparkListener(new SparkListener { override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = { defaultSession.set(null) listenerRegistered.set(false) } }) listenerRegistered.set(true) } } /** The active SparkSession for the current thread. */ private val activeThreadSession = new InheritableThreadLocal[SparkSession] /** Reference to the root SparkSession. */ private val defaultSession = new AtomicReference[SparkSession] private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME = \"org.apache.spark.sql.hive.HiveSessionStateBuilder\" private def sessionStateClassName(conf: SparkConf): String = { conf.get(CATALOG_IMPLEMENTATION) match { case \"hive\" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME case \"in-memory\" => classOf[SessionStateBuilder].getCanonicalName } } private def assertOnDriver(): Unit = { if (TaskContext.get != null) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkSession should only be created and accessed on the driver.\") } } /** * Helper method to create an instance of `SessionState` based on `className` from conf. * The result is either `SessionState` or a Hive based `SessionState`. */ private def instantiateSessionState( className: String, sparkSession: SparkSession): SessionState = { try { // invoke new [Hive]SessionStateBuilder( // SparkSession, // Option[SessionState]) val clazz = Utils.classForName(className) val ctor = clazz.getConstructors.head ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build() } catch { case NonFatal(e) => throw new IllegalArgumentException(s\"Error while instantiating '$className':\", e) } } /** * @return true if Hive classes can be loaded, otherwise false. */ private[spark] def hiveClassesArePresent: Boolean = { try { Utils.classForName(HIVE_SESSION_STATE_BUILDER_CLASS_NAME) Utils.classForName(\"org.apache.hadoop.hive.conf.HiveConf\") true } catch { case _: ClassNotFoundException | _: NoClassDefFoundError => false } } private[spark] def cleanupAnyExistingSession(): Unit = { val session = getActiveSession.orElse(getDefaultSession) if (session.isDefined) { logWarning( s\"\"\"An existing Spark session exists as the active or default session. |This probably means another suite leaked it. Attempting to stop it before continuing. |This existing Spark session was created at: | |${session.get.creationSite.longForm} | \"\"\".stripMargin) session.get.stop() SparkSession.clearActiveSession() SparkSession.clearDefaultSession() } } /** * Initialize extensions for given extension classnames. The classes will be applied to the * extensions passed into this function. */ private def applyExtensions( extensionConfClassNames: Seq[String], extensions: SparkSessionExtensions): SparkSessionExtensions = { extensionConfClassNames.foreach { extensionConfClassName => try { val extensionConfClass = Utils.classForName(extensionConfClassName) val extensionConf = extensionConfClass.getConstructor().newInstance() .asInstanceOf[SparkSessionExtensions => Unit] extensionConf(extensions) } catch { // Ignore the error if we cannot find the class or when the class has the wrong type. case e@(_: ClassCastException | _: ClassNotFoundException | _: NoClassDefFoundError) => logWarning(s\"Cannot use $extensionConfClassName to configure session extensions.\", e) } } extensions } /** * Load extensions from [[ServiceLoader]] and use them */ private def loadExtensions(extensions: SparkSessionExtensions): Unit = { val loader = ServiceLoader.load(classOf[SparkSessionExtensionsProvider], Utils.getContextOrSparkClassLoader) val loadedExts = loader.iterator() while (loadedExts.hasNext) { try { val ext = loadedExts.next() ext(extensions) } catch { case e: Throwable => logWarning(\"Failed to load session extension\", e) } } } }",
          "## CLASS: org/apache/spark/rdd/RDD# (implementation)\n*/ abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent */ def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition. */ @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }` */ protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it. */ protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences. */ protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned. */ @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD. */ def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext). */ val id: Int = sc.newRddId() /** A friendly name for this RDD */ @transient var name: String = _ /** Assign a name to this RDD */ def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one */ private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception. */ def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD. */ def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set. */ def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver). */ private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed */ private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not. */ final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing. */ final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not. */ final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD. */ @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed. */ final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD. */ final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned. */ private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing. */ private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached. */ private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body. */ private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD. */ def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */ def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate. */ def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle. */ def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable. */ def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition. */ val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]]. */ def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array */ def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement. */ private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function. */ def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. */ def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD */ def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD */ def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array. */ def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. */ def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process. */ def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process. */ def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD */ def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive. */ private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning. */ private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive. */ private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other). */ def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition. */ def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD. */ def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD. */ def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first. */ def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`. */ def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us. */ def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator. */ def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]] */ def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions */ def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions */ def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2) */ def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor */ def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD. */ def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds */ def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map. */ def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds */ def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped. */ def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. */ def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file. */ def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file. */ def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`. */ def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD. */ def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD * */ def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD * */ def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition. */ def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements. */ def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements. */ def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects. */ def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`. */ def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition */ private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation. */ def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used. */ def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. */ def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing. */ private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing. */ private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized. */ private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally. */ def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned */ @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents. */ def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a> */ @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD. */ @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified */ @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`). */ @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations. */ @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD */ protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T] */ protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on. */ def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility. */ private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility. */ private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs. */ private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions. */ private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example. */ protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging. */ def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier. */ private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level. */ // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`. */ object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE. */ private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
          "## CLASS: org/apache/spark/SparkConf# (implementation)\n*/ class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging with Serializable { import SparkConf._ /** Create a SparkConf that loads defaults from system properties and the classpath */ def this() = this(true) private val settings = new ConcurrentHashMap[String, String]() @transient private lazy val reader: ConfigReader = { val _reader = new ConfigReader(new SparkConfigProvider(settings)) _reader.bindEnv((key: String) => Option(getenv(key))) _reader } if (loadDefaults) { loadFromSystemProperties(false) } private[spark] def loadFromSystemProperties(silent: Boolean): SparkConf = { // Load any spark.* system properties for ((key, value) <- Utils.getSystemProperties if key.startsWith(\"spark.\")) { set(key, value, silent) } this } /** Set a configuration variable. */ def set(key: String, value: String): SparkConf = { set(key, value, false) } private[spark] def set(key: String, value: String, silent: Boolean): SparkConf = { if (key == null) { throw new NullPointerException(\"null key\") } if (value == null) { throw new NullPointerException(\"null value for \" + key) } if (!silent) { logDeprecationWarning(key) } settings.put(key, value) this } private[spark] def set[T](entry: ConfigEntry[T], value: T): SparkConf = { set(entry.key, entry.stringConverter(value)) this } private[spark] def set[T](entry: OptionalConfigEntry[T], value: T): SparkConf = { set(entry.key, entry.rawStringConverter(value)) this } /** * The master URL to connect to, such as \"local\" to run locally with one thread, \"local[4]\" to * run locally with 4 cores, or \"spark://master:7077\" to run on a Spark standalone cluster. */ def setMaster(master: String): SparkConf = { set(\"spark.master\", master) } /** Set a name for your application. Shown in the Spark web UI. */ def setAppName(name: String): SparkConf = { set(\"spark.app.name\", name) } /** Set JAR files to distribute to the cluster. */ def setJars(jars: Seq[String]): SparkConf = { for (jar <- jars if (jar == null)) logWarning(\"null jar passed to SparkContext constructor\") set(JARS, jars.filter(_ != null)) } /** Set JAR files to distribute to the cluster. (Java-friendly version.) */ def setJars(jars: Array[String]): SparkConf = { setJars(jars.toSeq) } /** * Set an environment variable to be used when launching executors for this application. * These variables are stored as properties of the form spark.executorEnv.VAR_NAME * (for example spark.executorEnv.PATH) but this method makes them easier to set. */ def setExecutorEnv(variable: String, value: String): SparkConf = { set(\"spark.executorEnv.\" + variable, value) } /** * Set multiple environment variables to be used when launching executors. * These variables are stored as properties of the form spark.executorEnv.VAR_NAME * (for example spark.executorEnv.PATH) but this method makes them easier to set. */ def setExecutorEnv(variables: Seq[(String, String)]): SparkConf = { for ((k, v) <- variables) { setExecutorEnv(k, v) } this } /** * Set multiple environment variables to be used when launching executors. * (Java-friendly version.) */ def setExecutorEnv(variables: Array[(String, String)]): SparkConf = { setExecutorEnv(variables.toSeq) } /** * Set the location where Spark is installed on worker nodes. */ def setSparkHome(home: String): SparkConf = { set(\"spark.home\", home) } /** Set multiple parameters together */ def setAll(settings: Iterable[(String, String)]): SparkConf = { settings.foreach { case (k, v) => set(k, v) } this } /** Set a parameter if it isn't already configured */ def setIfMissing(key: String, value: String): SparkConf = { if (settings.putIfAbsent(key, value) == null) { logDeprecationWarning(key) } this } private[spark] def setIfMissing[T](entry: ConfigEntry[T], value: T): SparkConf = { if (settings.putIfAbsent(entry.key, entry.stringConverter(value)) == null) { logDeprecationWarning(entry.key) } this } private[spark] def setIfMissing[T](entry: OptionalConfigEntry[T], value: T): SparkConf = { if (settings.putIfAbsent(entry.key, entry.rawStringConverter(value)) == null) { logDeprecationWarning(entry.key) } this } /** * Use Kryo serialization and register the given set of classes with Kryo. * If called multiple times, this will append the classes from all calls together. */ def registerKryoClasses(classes: Array[Class[_]]): SparkConf = { val allClassNames = new LinkedHashSet[String]() allClassNames ++= get(KRYO_CLASSES_TO_REGISTER).map(_.trim) .filter(!_.isEmpty) allClassNames ++= classes.map(_.getName) set(KRYO_CLASSES_TO_REGISTER, allClassNames.toSeq) set(SERIALIZER, classOf[KryoSerializer].getName) this } private final val avroNamespace = \"avro.schema.\" /** * Use Kryo serialization and register the given set of Avro schemas so that the generic * record serializer can decrease network IO */ def registerAvroSchemas(schemas: Schema*): SparkConf = { for (schema <- schemas) { set(avroNamespace + SchemaNormalization.parsingFingerprint64(schema), schema.toString) } this } /** Gets all the avro schemas in the configuration used in the generic Avro record serializer */ def getAvroSchema: Map[Long, String] = { getAll.filter { case (k, v) => k.startsWith(avroNamespace) } .map { case (k, v) => (k.substring(avroNamespace.length).toLong, v) } .toMap } /** Remove a parameter from the configuration */ def remove(key: String): SparkConf = { settings.remove(key) this } private[spark] def remove(entry: ConfigEntry[_]): SparkConf = { remove(entry.key) } /** Get a parameter; throws a NoSuchElementException if it's not set */ def get(key: String): String = { getOption(key).getOrElse(throw new NoSuchElementException(key)) } /** Get a parameter, falling back to a default if not set */ def get(key: String, defaultValue: String): String = { getOption(key).getOrElse(defaultValue) } /** * Retrieves the value of a pre-defined configuration entry. * * - This is an internal Spark API. * - The return type if defined by the configuration entry. * - This will throw an exception is the config is not optional and the value is not set. */ private[spark] def get[T](entry: ConfigEntry[T]): T = { entry.readFrom(reader) } /** * Get a time parameter as seconds; throws a NoSuchElementException if it's not set. If no * suffix is provided then seconds are assumed. * @throws java.util.NoSuchElementException If the time parameter is not set * @throws NumberFormatException If the value cannot be interpreted as seconds */ def getTimeAsSeconds(key: String): Long = catchIllegalValue(key) { Utils.timeStringAsSeconds(get(key)) } /** * Get a time parameter as seconds, falling back to a default if not set. If no * suffix is provided then seconds are assumed. * @throws NumberFormatException If the value cannot be interpreted as seconds */ def getTimeAsSeconds(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.timeStringAsSeconds(get(key, defaultValue)) } /** * Get a time parameter as milliseconds; throws a NoSuchElementException if it's not set. If no * suffix is provided then milliseconds are assumed. * @throws java.util.NoSuchElementException If the time parameter is not set * @throws NumberFormatException If the value cannot be interpreted as milliseconds */ def getTimeAsMs(key: String): Long = catchIllegalValue(key) { Utils.timeStringAsMs(get(key)) } /** * Get a time parameter as milliseconds, falling back to a default if not set. If no * suffix is provided then milliseconds are assumed. * @throws NumberFormatException If the value cannot be interpreted as milliseconds */ def getTimeAsMs(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.timeStringAsMs(get(key, defaultValue)) } /** * Get a size parameter as bytes; throws a NoSuchElementException if it's not set. If no * suffix is provided then bytes are assumed. * @throws java.util.NoSuchElementException If the size parameter is not set * @throws NumberFormatException If the value cannot be interpreted as bytes */ def getSizeAsBytes(key: String): Long = catchIllegalValue(key) { Utils.byteStringAsBytes(get(key)) } /** * Get a size parameter as bytes, falling back to a default if not set. If no * suffix is provided then bytes are assumed. * @throws NumberFormatException If the value cannot be interpreted as bytes */ def getSizeAsBytes(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.byteStringAsBytes(get(key, defaultValue)) } /** * Get a size parameter as bytes, falling back to a default if not set. * @throws NumberFormatException If the value cannot be interpreted as bytes */ def getSizeAsBytes(key: String, defaultValue: Long): Long = catchIllegalValue(key) { Utils.byteStringAsBytes(get(key, defaultValue + \"B\")) } /** * Get a size parameter as Kibibytes; throws a NoSuchElementException if it's not set. If no * suffix is provided then Kibibytes are assumed. * @throws java.util.NoSuchElementException If the size parameter is not set * @throws NumberFormatException If the value cannot be interpreted as Kibibytes */ def getSizeAsKb(key: String): Long = catchIllegalValue(key) { Utils.byteStringAsKb(get(key)) } /** * Get a size parameter as Kibibytes, falling back to a default if not set. If no * suffix is provided then Kibibytes are assumed. * @throws NumberFormatException If the value cannot be interpreted as Kibibytes */ def getSizeAsKb(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.byteStringAsKb(get(key, defaultValue)) } /** * Get a size parameter as Mebibytes; throws a NoSuchElementException if it's not set. If no * suffix is provided then Mebibytes are assumed. * @throws java.util.NoSuchElementException If the size parameter is not set * @throws NumberFormatException If the value cannot be interpreted as Mebibytes */ def getSizeAsMb(key: String): Long = catchIllegalValue(key) { Utils.byteStringAsMb(get(key)) } /** * Get a size parameter as Mebibytes, falling back to a default if not set. If no * suffix is provided then Mebibytes are assumed. * @throws NumberFormatException If the value cannot be interpreted as Mebibytes */ def getSizeAsMb(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.byteStringAsMb(get(key, defaultValue)) } /** * Get a size parameter as Gibibytes; throws a NoSuchElementException if it's not set. If no * suffix is provided then Gibibytes are assumed. * @throws java.util.NoSuchElementException If the size parameter is not set * @throws NumberFormatException If the value cannot be interpreted as Gibibytes */ def getSizeAsGb(key: String): Long = catchIllegalValue(key) { Utils.byteStringAsGb(get(key)) } /** * Get a size parameter as Gibibytes, falling back to a default if not set. If no * suffix is provided then Gibibytes are assumed. * @throws NumberFormatException If the value cannot be interpreted as Gibibytes */ def getSizeAsGb(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.byteStringAsGb(get(key, defaultValue)) } /** Get a parameter as an Option */ def getOption(key: String): Option[String] = { Option(settings.get(key)).orElse(getDeprecatedConfig(key, settings)) } /** Get an optional value, applying variable substitution. */ private[spark] def getWithSubstitution(key: String): Option[String] = { getOption(key).map(reader.substitute) } /** Get all parameters as a list of pairs */ def getAll: Array[(String, String)] = { settings.entrySet().asScala.map(x => (x.getKey, x.getValue)).toArray } /** * Get all parameters that start with `prefix` */ def getAllWithPrefix(prefix: String): Array[(String, String)] = { getAll.filter { case (k, v) => k.startsWith(prefix) } .map { case (k, v) => (k.substring(prefix.length), v) } } /** * Get a parameter as an integer, falling back to a default if not set * @throws NumberFormatException If the value cannot be interpreted as an integer */ def getInt(key: String, defaultValue: Int): Int = catchIllegalValue(key) { getOption(key).map(_.toInt).getOrElse(defaultValue) } /** * Get a parameter as a long, falling back to a default if not set * @throws NumberFormatException If the value cannot be interpreted as a long */ def getLong(key: String, defaultValue: Long): Long = catchIllegalValue(key) { getOption(key).map(_.toLong).getOrElse(defaultValue) } /** * Get a parameter as a double, falling back to a default if not ste * @throws NumberFormatException If the value cannot be interpreted as a double */ def getDouble(key: String, defaultValue: Double): Double = catchIllegalValue(key) { getOption(key).map(_.toDouble).getOrElse(defaultValue) } /** * Get a parameter as a boolean, falling back to a default if not set * @throws IllegalArgumentException If the value cannot be interpreted as a boolean */ def getBoolean(key: String, defaultValue: Boolean): Boolean = catchIllegalValue(key) { getOption(key).map(_.toBoolean).getOrElse(defaultValue) } /** Get all executor environment variables set on this SparkConf */ def getExecutorEnv: Seq[(String, String)] = { getAllWithPrefix(\"spark.executorEnv.\") } /** * Returns the Spark application id, valid in the Driver after TaskScheduler registration and * from the start in the Executor. */ def getAppId: String = get(\"spark.app.id\") /** Does the configuration contain a given parameter? */ def contains(key: String): Boolean = { settings.containsKey(key) || configsWithAlternatives.get(key).toSeq.flatten.exists { alt => contains(alt.key) } } private[spark] def contains(entry: ConfigEntry[_]): Boolean = contains(entry.key) /** Copy this object */ override def clone: SparkConf = { val cloned = new SparkConf(false) settings.entrySet().asScala.foreach { e => cloned.set(e.getKey(), e.getValue(), true) } cloned } /** * By using this instead of System.getenv(), environment variables can be mocked * in unit tests. */ private[spark] def getenv(name: String): String = System.getenv(name) /** * Wrapper method for get() methods which require some specific value format. This catches * any [[NumberFormatException]] or [[IllegalArgumentException]] and re-raises it with the * incorrectly configured key in the exception message. */ private def catchIllegalValue[T](key: String)(getValue: => T): T = { try { getValue } catch { case e: NumberFormatException => // NumberFormatException doesn't have a constructor that takes a cause for some reason. throw new NumberFormatException(s\"Illegal value for config key $key: ${e.getMessage}\") .initCause(e) case e: IllegalArgumentException => throw new IllegalArgumentException(s\"Illegal value for config key $key: ${e.getMessage}\", e) } } /** * Checks for illegal or deprecated config settings. Throws an exception for the former. Not * idempotent - may mutate this conf object to convert deprecated settings to supported ones. */ private[spark] def validateSettings(): Unit = { if (contains(\"spark.local.dir\")) { val msg = \"Note that spark.local.dir will be overridden by the value set by \" + \"the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS\" + \" in YARN).\" logWarning(msg) } val executorOptsKey = EXECUTOR_JAVA_OPTIONS.key // Used by Yarn in 1.1 and before sys.props.get(\"spark.driver.libraryPath\").foreach { value => val warning = s\"\"\" |spark.driver.libraryPath was detected (set to '$value'). |This is deprecated in Spark 1.2+. | |Please instead use: ${DRIVER_LIBRARY_PATH.key} \"\"\".stripMargin logWarning(warning) } // Validate spark.executor.extraJavaOptions getOption(executorOptsKey).foreach { javaOpts => if (javaOpts.contains(\"-Dspark\")) { val msg = s\"$executorOptsKey is not allowed to set Spark options (was '$javaOpts'). \" + \"Set them directly on a SparkConf or in a properties file when using ./bin/spark-submit.\" throw new Exception(msg) } if (javaOpts.contains(\"-Xmx\")) { val msg = s\"$executorOptsKey is not allowed to specify max heap memory settings \" + s\"(was '$javaOpts'). Use spark.executor.memory instead.\" throw new Exception(msg) } } // Validate memory fractions for (key <- Seq(MEMORY_FRACTION.key, MEMORY_STORAGE_FRACTION.key)) { val value = getDouble(key, 0.5) if (value > 1 || value < 0) { throw new IllegalArgumentException(s\"$key should be between 0 and 1 (was '$value').\") } } if (contains(SUBMIT_DEPLOY_MODE)) { get(SUBMIT_DEPLOY_MODE) match { case \"cluster\" | \"client\" => case e => throw new SparkException(s\"${SUBMIT_DEPLOY_MODE.key} can only be \" + \"\\\"cluster\\\" or \\\"client\\\".\") } } if (contains(CORES_MAX) && contains(EXECUTOR_CORES)) { val totalCores = getInt(CORES_MAX.key, 1) val executorCores = get(EXECUTOR_CORES) val leftCores = totalCores % executorCores if (leftCores != 0) { logWarning(s\"Total executor cores: ${totalCores} is not \" + s\"divisible by cores per executor: ${executorCores}, \" + s\"the left cores: ${leftCores} will not be allocated\") } } val encryptionEnabled = get(NETWORK_CRYPTO_ENABLED) || get(SASL_ENCRYPTION_ENABLED) require(!encryptionEnabled || get(NETWORK_AUTH_ENABLED), s\"${NETWORK_AUTH_ENABLED.key} must be enabled when enabling encryption.\") val executorTimeoutThresholdMs = get(NETWORK_TIMEOUT) * 1000 val executorHeartbeatIntervalMs = get(EXECUTOR_HEARTBEAT_INTERVAL) val networkTimeout = NETWORK_TIMEOUT.key // If spark.executor.heartbeatInterval bigger than spark.network.timeout, // it will almost always cause ExecutorLostFailure. See SPARK-22754. require(executorTimeoutThresholdMs > executorHeartbeatIntervalMs, \"The value of \" + s\"${networkTimeout}=${executorTimeoutThresholdMs}ms must be greater than the value of \" + s\"${EXECUTOR_HEARTBEAT_INTERVAL.key}=${executorHeartbeatIntervalMs}ms.\") } /** * Return a string listing all keys and values, one per line. This is useful to print the * configuration out for debugging. */ def toDebugString: String = { Utils.redact(this, getAll).sorted.map { case (k, v) => k + \"=\" + v }.mkString(\"\\n\") } } private[spark] object SparkConf extends Logging { /** * Maps deprecated config keys to information about the deprecation. * * The extra information is logged as a warning when the config is present in the user's * configuration. */ private val deprecatedConfigs: Map[String, DeprecatedConfig] = { val configs = Seq( DeprecatedConfig(\"spark.cache.class\", \"0.8\", \"The spark.cache.class property is no longer being used! Specify storage levels using \" + \"the RDD.persist() method instead.\"), DeprecatedConfig(\"spark.yarn.user.classpath.first\", \"1.3\", \"Please use spark.{driver,executor}.userClassPathFirst instead.\"), DeprecatedConfig(\"spark.kryoserializer.buffer.mb\", \"1.4\", \"Please use spark.kryoserializer.buffer instead. The default value for \" + \"spark.kryoserializer.buffer.mb was previously specified as '0.064'. Fractional values \" + \"are no longer accepted. To specify the equivalent now, one may use '64k'.\"), DeprecatedConfig(\"spark.rpc\", \"2.0\", \"Not used anymore.\"), DeprecatedConfig(\"spark.scheduler.executorTaskBlacklistTime\", \"2.1.0\", \"Please use the new excludedOnFailure options, spark.excludeOnFailure.*\"), DeprecatedConfig(\"spark.yarn.am.port\", \"2.0.0\", \"Not used anymore\"), DeprecatedConfig(\"spark.executor.port\", \"2.0.0\", \"Not used anymore\"), DeprecatedConfig(\"spark.shuffle.service.index.cache.entries\", \"2.3.0\", \"Not used anymore. Please use spark.shuffle.service.index.cache.size\"), DeprecatedConfig(\"spark.yarn.credentials.file.retention.count\", \"2.4.0\", \"Not used anymore.\"), DeprecatedConfig(\"spark.yarn.credentials.file.retention.days\", \"2.4.0\", \"Not used anymore.\"), DeprecatedConfig(\"spark.yarn.services\", \"3.0.0\", \"Feature no longer available.\"), DeprecatedConfig(\"spark.executor.plugins\", \"3.0.0\", \"Feature replaced with new plugin API. See Monitoring documentation.\"), DeprecatedConfig(\"spark.blacklist.enabled\", \"3.1.0\", \"Please use spark.excludeOnFailure.enabled\"), DeprecatedConfig(\"spark.blacklist.task.maxTaskAttemptsPerExecutor\", \"3.1.0\", \"Please use spark.excludeOnFailure.task.maxTaskAttemptsPerExecutor\"), DeprecatedConfig(\"spark.blacklist.task.maxTaskAttemptsPerNode\", \"3.1.0\", \"Please use spark.excludeOnFailure.task.maxTaskAttemptsPerNode\"), DeprecatedConfig(\"spark.blacklist.application.maxFailedTasksPerExecutor\", \"3.1.0\", \"Please use spark.excludeOnFailure.application.maxFailedTasksPerExecutor\"), DeprecatedConfig(\"spark.blacklist.stage.maxFailedTasksPerExecutor\", \"3.1.0\", \"Please use spark.excludeOnFailure.stage.maxFailedTasksPerExecutor\"), DeprecatedConfig(\"spark.blacklist.application.maxFailedExecutorsPerNode\", \"3.1.0\", \"Please use spark.excludeOnFailure.application.maxFailedExecutorsPerNode\"), DeprecatedConfig(\"spark.blacklist.stage.maxFailedExecutorsPerNode\", \"3.1.0\", \"Please use spark.excludeOnFailure.stage.maxFailedExecutorsPerNode\"), DeprecatedConfig(\"spark.blacklist.timeout\", \"3.1.0\", \"Please use spark.excludeOnFailure.timeout\"), DeprecatedConfig(\"spark.blacklist.application.fetchFailure.enabled\", \"3.1.0\", \"Please use spark.excludeOnFailure.application.fetchFailure.enabled\"), DeprecatedConfig(\"spark.scheduler.blacklist.unschedulableTaskSetTimeout\", \"3.1.0\", \"Please use spark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout\"), DeprecatedConfig(\"spark.blacklist.killBlacklistedExecutors\", \"3.1.0\", \"Please use spark.excludeOnFailure.killExcludedExecutors\"), DeprecatedConfig(\"spark.yarn.blacklist.executor.launch.blacklisting.enabled\", \"3.1.0\", \"Please use spark.yarn.executor.launch.excludeOnFailure.enabled\") ) Map(configs.map { cfg => (cfg.key -> cfg) } : _*) } /** * Maps a current config key to alternate keys that were used in previous version of Spark. * * The alternates are used in the order defined in this map. If deprecated configs are * present in the user's configuration, a warning is logged. * * TODO: consolidate it with `ConfigBuilder.withAlternative`. */ private val configsWithAlternatives = Map[String, Seq[AlternateConfig]]( EXECUTOR_USER_CLASS_PATH_FIRST.key -> Seq( AlternateConfig(\"spark.files.userClassPathFirst\", \"1.3\")), UPDATE_INTERVAL_S.key -> Seq( AlternateConfig(\"spark.history.fs.update.interval.seconds\", \"1.4\"), AlternateConfig(\"spark.history.fs.updateInterval\", \"1.3\"), AlternateConfig(\"spark.history.updateInterval\", \"1.3\")), CLEANER_INTERVAL_S.key -> Seq( AlternateConfig(\"spark.history.fs.cleaner.interval.seconds\", \"1.4\")), MAX_LOG_AGE_S.key -> Seq( AlternateConfig(\"spark.history.fs.cleaner.maxAge.seconds\", \"1.4\")), \"spark.yarn.am.waitTime\" -> Seq( AlternateConfig(\"spark.yarn.applicationMaster.waitTries\", \"1.3\", // Translate old value to a duration, with 10s wait time per try. translation = s => s\"${s.toLong * 10}s\")), REDUCER_MAX_SIZE_IN_FLIGHT.key -> Seq( AlternateConfig(\"spark.reducer.maxMbInFlight\", \"1.4\")), KRYO_SERIALIZER_BUFFER_SIZE.key -> Seq( AlternateConfig(\"spark.kryoserializer.buffer.mb\", \"1.4\", translation = s => s\"${(s.toDouble * 1000).toInt}k\")), KRYO_SERIALIZER_MAX_BUFFER_SIZE.key -> Seq( AlternateConfig(\"spark.kryoserializer.buffer.max.mb\", \"1.4\")), SHUFFLE_FILE_BUFFER_SIZE.key -> Seq( AlternateConfig(\"spark.shuffle.file.buffer.kb\", \"1.4\")), EXECUTOR_LOGS_ROLLING_MAX_SIZE.key -> Seq( AlternateConfig(\"spark.executor.logs.rolling.size.maxBytes\", \"1.4\")), IO_COMPRESSION_SNAPPY_BLOCKSIZE.key -> Seq( AlternateConfig(\"spark.io.compression.snappy.block.size\", \"1.4\")), IO_COMPRESSION_LZ4_BLOCKSIZE.key -> Seq( AlternateConfig(\"spark.io.compression.lz4.block.size\", \"1.4\")), RPC_NUM_RETRIES.key -> Seq( AlternateConfig(\"spark.akka.num.retries\", \"1.4\")), RPC_RETRY_WAIT.key -> Seq( AlternateConfig(\"spark.akka.retry.wait\", \"1.4\")), RPC_ASK_TIMEOUT.key -> Seq( AlternateConfig(\"spark.akka.askTimeout\", \"1.4\")), RPC_LOOKUP_TIMEOUT.key -> Seq( AlternateConfig(\"spark.akka.lookupTimeout\", \"1.4\")), \"spark.streaming.fileStream.minRememberDuration\" -> Seq( AlternateConfig(\"spark.streaming.minRememberDuration\", \"1.5\")), \"spark.yarn.max.executor.failures\" -> Seq( AlternateConfig(\"spark.yarn.max.worker.failures\", \"1.5\")), MEMORY_OFFHEAP_ENABLED.key -> Seq( AlternateConfig(\"spark.unsafe.offHeap\", \"1.6\")), RPC_MESSAGE_MAX_SIZE.key -> Seq( AlternateConfig(\"spark.akka.frameSize\", \"1.6\")), \"spark.yarn.jars\" -> Seq( AlternateConfig(\"spark.yarn.jar\", \"2.0\")), MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM.key -> Seq( AlternateConfig(\"spark.reducer.maxReqSizeShuffleToMem\", \"2.3\"), AlternateConfig(\"spark.maxRemoteBlockSizeFetchToMem\", \"3.0\")), LISTENER_BUS_EVENT_QUEUE_CAPACITY.key -> Seq( AlternateConfig(\"spark.scheduler.listenerbus.eventqueue.size\", \"2.3\")), DRIVER_MEMORY_OVERHEAD.key -> Seq( AlternateConfig(\"spark.yarn.driver.memoryOverhead\", \"2.3\")), EXECUTOR_MEMORY_OVERHEAD.key -> Seq( AlternateConfig(\"spark.yarn.executor.memoryOverhead\", \"2.3\")), KEYTAB.key -> Seq( AlternateConfig(\"spark.yarn.keytab\", \"3.0\")), PRINCIPAL.key -> Seq( AlternateConfig(\"spark.yarn.principal\", \"3.0\")), KERBEROS_RELOGIN_PERIOD.key -> Seq( AlternateConfig(\"spark.yarn.kerberos.relogin.period\", \"3.0\")), KERBEROS_FILESYSTEMS_TO_ACCESS.key -> Seq( AlternateConfig(\"spark.yarn.access.namenodes\", \"2.2\"), AlternateConfig(\"spark.yarn.access.hadoopFileSystems\", \"3.0\")), \"spark.kafka.consumer.cache.capacity\" -> Seq( AlternateConfig(\"spark.sql.kafkaConsumerCache.capacity\", \"3.0\")) ) /** * A view of `configsWithAlternatives` that makes it more efficient to look up deprecated * config keys. * * Maps the deprecated config name to a 2-tuple (new config name, alternate config info). */ private val allAlternatives: Map[String, (String, AlternateConfig)] = { configsWithAlternatives.keys.flatMap { key => configsWithAlternatives(key).map { cfg => (cfg.key -> (key -> cfg)) } }.toMap } /** * Return whether the given config should be passed to an executor on start-up. * * Certain authentication configs are required from the executor when it connects to * the scheduler, while the rest of the spark configs can be inherited from the driver later. */ def isExecutorStartupConf(name: String): Boolean = { (name.startsWith(\"spark.auth\") && name != SecurityManager.SPARK_AUTH_SECRET_CONF) || name.startsWith(\"spark.rpc\") || name.startsWith(\"spark.network\") || isSparkPortConf(name) } /** * Return true if the given config matches either `spark.*.port` or `spark.port.*`. */ def isSparkPortConf(name: String): Boolean = { (name.startsWith(\"spark.\") && name.endsWith(\".port\")) || name.startsWith(\"spark.port.\") } /** * Looks for available deprecated keys for the given config option, and return the first * value available. */ def getDeprecatedConfig(key: String, conf: JMap[String, String]): Option[String] = { configsWithAlternatives.get(key).flatMap { alts => alts.collectFirst { case alt if conf.containsKey(alt.key) => val value = conf.get(alt.key) if (alt.translation != null) alt.translation(value) else value } } } /** * Logs a warning message if the given config key is deprecated. */ def logDeprecationWarning(key: String): Unit = { deprecatedConfigs.get(key).foreach { cfg => logWarning( s\"The configuration key '$key' has been deprecated as of Spark ${cfg.version} and \" + s\"may be removed in the future. ${cfg.deprecationMessage}\") return } allAlternatives.get(key).foreach { case (newKey, cfg) => logWarning( s\"The configuration key '$key' has been deprecated as of Spark ${cfg.version} and \" + s\"may be removed in the future. Please use the new key '$newKey' instead.\") return } if (key.startsWith(\"spark.akka\") || key.startsWith(\"spark.ssl.akka\")) { logWarning( s\"The configuration key $key is not supported anymore \" + s\"because Spark doesn't use Akka since 2.0\") } } /** * Holds information about keys that have been deprecated and do not have a replacement. * * @param key The deprecated key. * @param version Version of Spark where key was deprecated. * @param deprecationMessage Message to include in the deprecation warning. */ private case class DeprecatedConfig( key: String, version: String, deprecationMessage: String) /** * Information about an alternate configuration key that has been deprecated. * * @param key The deprecated config key. * @param version The Spark version in which the key was deprecated. * @param translation A translation function for converting old config values into new ones. */ private case class AlternateConfig( key: String, version: String, translation: String => String = null) }"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "# How external data connectivity and writing are managed (based only on visible code)\n\n_Last updated: 2026-01-03 11:08 local time_\n\nThis document summarizes what can be concluded from the visible source code in this repository snapshot about how Apache Spark manages external data connectivity and writing. It focuses strictly on what is present in the examined files and comments therein.\n\n## Responsibilities (from code only)\n\n- SparkSession\n  - Owns session/runtime state and wiring.\n  - Its shared state includes a catalog that interacts with external systems (per comment in SparkSession implementation).\n  - Provides configuration exposure to SQL layers via SQLConf getter.\n\n- Dataset\n  - Wraps a QueryExecution and a type Encoder.\n  - Asserts the plan is analyzed and exposes the session.\n  - No concrete external IO logic is present in the visible Dataset code.\n\n- RDD / JavaPairRDD\n  - Core distributed data structures and transformations/persistence helpers.\n  - No explicit external connector or write path is shown in the retrieved snippets.\n\n- SparkConf\n  - Stores key/value configuration (e.g., spark.master, app name, jars) and binds a ConfigReader.\n  - This is the mechanism through which connectivity-related configs would be supplied, although specific connector keys are not shown here.\n\n## Data flow (how inputs transform to outputs in the shown code)\n\n- SparkConf\n  - Collects spark.* system properties into an internal ConcurrentHashMap via loadFromSystemProperties and set.\n  - Exposes setters like setMaster and setAppName.\n\n- SparkSession\n  - Captures a SparkContext and initializes a SparkSessionExtensions instance.\n  - Registers SQLConf.getter to route SQL configs to the active sessions SessionState.\n  - Maintains shared/session state, including a catalog that interacts with external systems (per code comment), but the concrete catalog or connectors are not shown in the retrieved body.\n\n- Dataset\n  - Constructed with a QueryExecution and Encoder; asserts analysis via queryExecution.assertAnalyzed.\n  - Exposes sparkSession from the QueryExecution and materializes logicalPlan from queryExecution.commandExecuted.\n  - No reader/writer APIs or data source invocations are visible in the provided snippet.\n\n- RDD / JavaPairRDD\n  - Provide transformations (filter, distinct, coalesce) and persistence (cache, persist, unpersist), not the external read/write pipeline in the shown code.\n\n## Key methods and logic (visible details)\n\n- SparkConf.set(key, value)\n  - Validates input, logs deprecations, stores in settings.\n  - Convenience wrappers: setMaster, setAppName, setJars.\n\n- SparkSession constructor\n  - Applies extensions and wires SQLConf getter.\n  - Comment explicitly states shared state has a catalog that interacts with external systems.\n\n- Dataset.logicalPlan\n  - Pulls plan from queryExecution.commandExecuted.\n  - Optionally tags with DATASET_ID_TAG when FAIL_AMBIGUOUS_SELF_JOIN_ENABLED is set.\n\n- Dataset.exprEnc / resolvedEnc\n  - Manages ExpressionEncoder resolution against plan output (no IO interactions shown).\n\n- JavaPairRDD transformations and persistence\n  - cache/persist/unpersist and common transformations; no external data sinks/sources in view.\n\n## Whats missing in the visible code\n\nThe concrete external connectivity and write path elements are not present in the inspected snippets. Specifically, not found among the visible code were:\n\n- DataFrameReader and DataFrameWriter entry points\n- Data Source V1/V2 interfaces and implementations (e.g., DataSource V2 WriteBuilder, DataWritingCommand)\n- Format-specific connectors (CSV/Parquet/ORC/JSON), and JDBC reader/writer\n- Catalog implementations and external metastore connectors\n\nTherefore, from the currently visible code, we can only confirm:\n\n- External system interaction is mediated by a catalog within SparkSessions shared state (comment evidence).\n- Configuration flows through SparkConf/SQLConf to sessions, which would govern connectors.\n- The actual implementations that perform reads/writes are outside the snippets provided.\n\n## Implications\n\n- Configuration path: Users provide connector-specific and cluster settings via SparkConf (and SQLConf for SQL-related configs). These are made available to the session and its subsystems (e.g., catalogs, planners).\n- IO path (not visible here): In typical Spark architecture, DataFrameReader/Writer and DataSource V1/V2 are responsible for resolving formats, building scans/writes, and materializing relations/commands. While this is expected, the concrete code is not in the visible set.\n\n## Next steps (if a deeper end-to-end explanation is desired)\n\nIf you would like a complete explanation of how external data connectivity and writing are implemented in this repository, we can examine the following components specifically:\n\n- DataFrameReader and DataFrameWriter\n- Data Source V2 write path (WriteBuilder, DataWriterFactory, DataWriter, and DataWritingCommand)\n- Built-in formats (CSV/Parquet/JSON/ORC) and JDBC\n- Catalog implementations used by SparkSessions shared state\n\n",
          "prompt_time": 32.781,
          "answer_time": 182,
          "context_tokens": 97514,
          "used_context": [
            "## CLASS: org/apache/spark/sql/Dataset# (implementation)\n@Stable class Dataset[T] private[sql]( @DeveloperApi @Unstable @transient val queryExecution: QueryExecution, @DeveloperApi @Unstable @transient val encoder: Encoder[T]) extends Serializable { @transient lazy val sparkSession: SparkSession = { if (queryExecution == null || queryExecution.sparkSession == null) { throw QueryExecutionErrors.transformationsAndActionsNotInvokedByDriverError() } queryExecution.sparkSession } // A globally unique id of this Dataset. private val id = Dataset.curId.getAndIncrement() queryExecution.assertAnalyzed() // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure // you wrap it with `withNewExecutionId` if this actions doesn't call other action. def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sparkSession.sessionState.executePlan(logicalPlan), encoder) } def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = { this(sqlContext.sparkSession, logicalPlan, encoder) } @transient private[sql] val logicalPlan: LogicalPlan = { val plan = queryExecution.commandExecuted if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) { val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long]) dsIds.add(id) plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds) } plan } /** * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use * it when constructing new Dataset objects that have the same object type (that will be * possibly resolved to a different schema). */ private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder) // The resolved `ExpressionEncoder` which can be used to turn rows to objects of type T, after // collecting rows to the driver side. private lazy val resolvedEnc = { exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer) } private implicit def classTag = exprEnc.clsTag // sqlContext must be val because a stable identifier is expected when you import implicits @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext private[sql] def resolve(colName: String): NamedExpression = { val resolver = sparkSession.sessionState.analyzer.resolver queryExecution.analyzed.resolveQuoted(colName, resolver) .getOrElse(throw resolveException(colName, schema.fieldNames)) } private def resolveException(colName: String, fields: Array[String]): AnalysisException = { val extraMsg = if (fields.exists(sparkSession.sessionState.analyzer.resolver(_, colName))) { s\"; did you mean to quote the `$colName` column?\" } else \"\" val fieldsStr = fields.mkString(\", \") QueryCompilationErrors.cannotResolveColumnNameAmongFieldsError(colName, fieldsStr, extraMsg) } private[sql] def numericColumns: Seq[Expression] = { schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n => queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get } } /** * Get rows represented in Sequence by specific truncate and vertical requirement. * * @param numRows Number of rows to return * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. */ private[sql] def getRows( numRows: Int, truncate: Int): Seq[Seq[String]] = { val newDf = toDF() val castCols = newDf.logicalPlan.output.map { col => // Since binary types in top-level schema fields have a specific format to print, // so we do not cast them to strings here. if (col.dataType == BinaryType) { Column(col) } else { Column(col).cast(StringType) } } val data = newDf.select(castCols: _*).take(numRows + 1) // For array values, replace Seq and Array with square brackets // For cells that are beyond `truncate` characters, replace it with the // first `truncate-3` and \"...\" schema.fieldNames.map(SchemaUtils.escapeMetaCharacters).toSeq +: data.map { row => row.toSeq.map { cell => val str = cell match { case null => \"null\" case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\") case _ => // Escapes meta-characters not to break the `showString` format SchemaUtils.escapeMetaCharacters(cell.toString) } if (truncate > 0 && str.length > truncate) { // do not show ellipses for strings shorter than 4 characters. if (truncate < 4) str.substring(0, truncate) else str.substring(0, truncate - 3) + \"...\" } else { str } }: Seq[String] } } /** * Compose the string representing rows for output * * @param _numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). */ private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = { val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate) val hasMoreData = tmpRows.length - 1 > numRows val rows = tmpRows.take(numRows + 1) val sb = new StringBuilder val numCols = schema.fieldNames.length // We set a minimum column width at '3' val minimumColWidth = 3 if (!vertical) { // Initialise the width of each column to a minimum value val colWidths = Array.fill(numCols)(minimumColWidth) // Compute the width of each column for (row <- rows) { for ((cell, i) <- row.zipWithIndex) { colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell)) } } val paddedRows = rows.map { row => row.zipWithIndex.map { case (cell, i) => if (truncate > 0) { StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } else { StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length) } } } // Create SeparateLine val sep: String = colWidths.map(\"-\" * _).addString(sb, \"+\", \"+\", \"+\\n\").toString() // column names paddedRows.head.addString(sb, \"|\", \"|\", \"|\\n\") sb.append(sep) // data paddedRows.tail.foreach(_.addString(sb, \"|\", \"|\", \"|\\n\")) sb.append(sep) } else { // Extended display mode enabled val fieldNames = rows.head val dataRows = rows.tail // Compute the width of field name and data columns val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) => math.max(curMax, Utils.stringHalfWidth(fieldName)) } val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) => math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max) } dataRows.zipWithIndex.foreach { case (row, i) => // \"+ 5\" in size means a character length except for padded names and data val rowHeader = StringUtils.rightPad( s\"-RECORD $i\", fieldNameColWidth + dataColWidth + 5, \"-\") sb.append(rowHeader).append(\"\\n\") row.zipWithIndex.map { case (cell, j) => val fieldName = StringUtils.rightPad(fieldNames(j), fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length) val data = StringUtils.rightPad(cell, dataColWidth - Utils.stringHalfWidth(cell) + cell.length) s\" $fieldName | $data \" }.addString(sb, \"\", \"\\n\", \"\\n\") } } // Print a footer if (vertical && rows.tail.isEmpty) { // In a vertical mode, print an empty row set explicitly sb.append(\"(0 rows)\\n\") } else if (hasMoreData) { // For Data that has more than \"numRows\" records val rowsString = if (numRows == 1) \"row\" else \"rows\" sb.append(s\"only showing top $numRows $rowsString\\n\") } sb.toString() } override def toString: String = { try { val builder = new StringBuilder val fields = schema.take(2).map { case f => s\"${f.name}: ${f.dataType.simpleString(2)}\" } builder.append(\"[\") builder.append(fields.mkString(\", \")) if (schema.length > 2) { if (schema.length - fields.size == 1) { builder.append(\" ... 1 more field\") } else { builder.append(\" ... \" + (schema.length - 2) + \" more fields\") } } builder.append(\"]\").toString() } catch { case NonFatal(e) => s\"Invalid tree; ${e.getMessage}:\\n$queryExecution\" } } /** * Converts this strongly typed collection of data to generic Dataframe. In contrast to the * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]] * objects that allow fields to be accessed by ordinal or name. * * @group basic * @since 1.6.0 */ // This is declared with parentheses to prevent the Scala compiler from treating // `ds.toDF(\"1\")` as invoking this toDF and then apply on the returned DataFrame. def toDF(): DataFrame = new Dataset[Row](queryExecution, RowEncoder(schema)) /** * Returns a new Dataset where each record has been mapped on to the specified type. The * method used to map columns depend on the type of `U`: * <ul> * <li>When `U` is a class, fields for the class will be mapped to columns of the same name * (case sensitivity is determined by `spark.sql.caseSensitive`).</li> * <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will * be assigned to `_1`).</li> * <li>When `U` is a primitive type (i.e. String, Int, etc), then the first column of the * `DataFrame` will be used.</li> * </ul> * * If the schema of the Dataset does not match the desired `U` type, you can use `select` * along with `alias` or `as` to rearrange or rename as required. * * Note that `as[]` only changes the view of the data that is passed into typed operations, * such as `map()`, and does not eagerly project away any columns that are not present in * the specified class. * * @group basic * @since 1.6.0 */ def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan) /** * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed. * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with * meaningful names. For example: * {{{ * val rdd: RDD[(Int, String)] = ... * rdd.toDF() // this implicit conversion creates a DataFrame with column name `_1` and `_2` * rdd.toDF(\"id\", \"name\") // this creates a DataFrame with column name \"id\" and \"name\" * }}} * * @group basic * @since 2.0.0 */ @scala.annotation.varargs def toDF(colNames: String*): DataFrame = { require(schema.size == colNames.size, \"The number of columns doesn't match.\\n\" + s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" + s\"New column names (${colNames.size}): \" + colNames.mkString(\", \")) val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) => Column(oldAttribute).as(newName) } select(newCols : _*) } /** * Returns the schema of this Dataset. * * @group basic * @since 1.6.0 */ def schema: StructType = sparkSession.withActive { queryExecution.analyzed.schema } /** * Prints the schema to the console in a nice tree format. * * @group basic * @since 1.6.0 */ def printSchema(): Unit = printSchema(Int.MaxValue) // scalastyle:off println /** * Prints the schema up to the given level to the console in a nice tree format. * * @group basic * @since 3.0.0 */ def printSchema(level: Int): Unit = println(schema.treeString(level)) // scalastyle:on println /** * Prints the plans (logical and physical) with a format specified by a given explain mode. * * @param mode specifies the expected output format of plans. * <ul> * <li>`simple` Print only a physical plan.</li> * <li>`extended`: Print both logical and physical plans.</li> * <li>`codegen`: Print a physical plan and generated codes if they are * available.</li> * <li>`cost`: Print a logical plan and statistics if they are available.</li> * <li>`formatted`: Split explain output into two sections: a physical plan outline * and node details.</li> * </ul> * @group basic * @since 3.0.0 */ def explain(mode: String): Unit = sparkSession.withActive { // Because temporary views are resolved during analysis when we create a Dataset, and // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using // `ExplainCommand` here will probably output different query plans, compared to the results // of evaluation of the Dataset. So just output QueryExecution's query plans here. // scalastyle:off println println(queryExecution.explainString(ExplainMode.fromString(mode))) // scalastyle:on println } /** * Prints the plans (logical and physical) to the console for debugging purposes. * * @param extended default `false`. If `false`, prints only the physical plan. * * @group basic * @since 1.6.0 */ def explain(extended: Boolean): Unit = if (extended) { explain(ExtendedMode.name) } else { explain(SimpleMode.name) } /** * Prints the physical plan to the console for debugging purposes. * * @group basic * @since 1.6.0 */ def explain(): Unit = explain(SimpleMode.name) /** * Returns all column names and their data types as an array. * * @group basic * @since 1.6.0 */ def dtypes: Array[(String, String)] = schema.fields.map { field => (field.name, field.dataType.toString) } /** * Returns all column names as an array. * * @group basic * @since 1.6.0 */ def columns: Array[String] = schema.fields.map(_.name) /** * Returns true if the `collect` and `take` methods can be run locally * (without any Spark executors). * * @group basic * @since 1.6.0 */ def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] || logicalPlan.isInstanceOf[CommandResult] /** * Returns true if the `Dataset` is empty. * * @group basic * @since 2.4.0 */ def isEmpty: Boolean = withAction(\"isEmpty\", select().queryExecution) { plan => plan.executeTake(1).isEmpty } /** * Returns true if this Dataset contains one or more sources that continuously * return data as it arrives. A Dataset that reads data from a streaming source * must be executed as a `StreamingQuery` using the `start()` method in * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or * `collect()`, will throw an [[AnalysisException]] when there is a streaming * source present. * * @group streaming * @since 2.0.0 */ def isStreaming: Boolean = logicalPlan.isStreaming /** * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0 */ def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true) /** * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the * logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. It will be saved to files inside the checkpoint * directory set with `SparkContext#setCheckpointDir`. * * @group basic * @since 2.1.0 */ def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true) /** * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be * used to truncate the logical plan of this Dataset, which is especially useful in iterative * algorithms where the plan may grow exponentially. Local checkpoints are written to executor * storage and despite potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0 */ def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false) /** * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate * the logical plan of this Dataset, which is especially useful in iterative algorithms where the * plan may grow exponentially. Local checkpoints are written to executor storage and despite * potentially faster they are unreliable and may compromise job completion. * * @group basic * @since 2.3.0 */ def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint( eager = eager, reliableCheckpoint = false ) /** * Returns a checkpointed version of this Dataset. * * @param eager Whether to checkpoint this dataframe immediately * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the * checkpoint directory. If false creates a local checkpoint using * the caching subsystem */ private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = { val actionName = if (reliableCheckpoint) \"checkpoint\" else \"localCheckpoint\" withAction(actionName, queryExecution) { physicalPlan => val internalRdd = physicalPlan.execute().map(_.copy()) if (reliableCheckpoint) { internalRdd.checkpoint() } else { internalRdd.localCheckpoint() } if (eager) { internalRdd.doCheckpoint() } // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the // size of `PartitioningCollection` may grow exponentially for queries involving deep inner // joins. @scala.annotation.tailrec def firstLeafPartitioning(partitioning: Partitioning): Partitioning = { partitioning match { case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head) case p => p } } val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning) Dataset.ofRows( sparkSession, LogicalRDD( logicalPlan.output, internalRdd, outputPartitioning, physicalPlan.outputOrdering, isStreaming )(sparkSession)).as[T] } } /** * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time * before which we assume no more late data is going to arrive. * * Spark will use this watermark for several purposes: * <ul> * <li>To know when a given time window aggregation can be finalized and thus can be emitted * when using output modes that do not allow updates.</li> * <li>To minimize the amount of state that we need to keep for on-going aggregations, * `mapGroupsWithState` and `dropDuplicates` operators.</li> * </ul> * The current watermark is computed by looking at the `MAX(eventTime)` seen across * all of the partitions in the query minus a user specified `delayThreshold`. Due to the cost * of coordinating this value across partitions, the actual watermark used is only guaranteed * to be at least `delayThreshold` behind the actual event time. In some cases we may still * process records that arrive more than `delayThreshold` late. * * @param eventTime the name of the column that contains the event time of the row. * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest * record that has been processed in the form of an interval * (e.g. \"1 minute\" or \"5 hours\"). NOTE: This should not be negative. * * @group streaming * @since 2.1.0 */ // We only accept an existing column name, not a derived column here as a watermark that is // defined on a derived column cannot referenced elsewhere in the plan. def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan { val parsedDelay = IntervalUtils.fromIntervalString(delayThreshold) require(!IntervalUtils.isNegative(parsedDelay), s\"delay threshold ($delayThreshold) should not be negative.\") EliminateEventTimeWatermark( EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)) } /** * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated, * and all cells will be aligned right. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * * @group action * @since 1.6.0 */ def show(numRows: Int): Unit = show(numRows, truncate = true) /** * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters * will be truncated, and all cells will be aligned right. * * @group action * @since 1.6.0 */ def show(): Unit = show(20) /** * Displays the top 20 rows of Dataset in a tabular form. * * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0 */ def show(truncate: Boolean): Unit = show(20, truncate) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * @param numRows Number of rows to show * @param truncate Whether truncate long strings. If true, strings more than 20 characters will * be truncated and all cells will be aligned right * * @group action * @since 1.6.0 */ // scalastyle:off println def show(numRows: Int, truncate: Boolean): Unit = if (truncate) { println(showString(numRows, truncate = 20)) } else { println(showString(numRows, truncate = 0)) } /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @group action * @since 1.6.0 */ def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false) /** * Displays the Dataset in a tabular form. For example: * {{{ * year month AVG('Adj Close) MAX('Adj Close) * 1980 12 0.503218 0.595103 * 1981 01 0.523289 0.570307 * 1982 02 0.436504 0.475256 * 1983 03 0.410516 0.442194 * 1984 04 0.450090 0.483521 * }}} * * If `vertical` enabled, this command prints output rows vertically (one line per column value)? * * {{{ * -RECORD 0------------------- * year | 1980 * month | 12 * AVG('Adj Close) | 0.503218 * AVG('Adj Close) | 0.595103 * -RECORD 1------------------- * year | 1981 * month | 01 * AVG('Adj Close) | 0.523289 * AVG('Adj Close) | 0.570307 * -RECORD 2------------------- * year | 1982 * month | 02 * AVG('Adj Close) | 0.436504 * AVG('Adj Close) | 0.475256 * -RECORD 3------------------- * year | 1983 * month | 03 * AVG('Adj Close) | 0.410516 * AVG('Adj Close) | 0.442194 * -RECORD 4------------------- * year | 1984 * month | 04 * AVG('Adj Close) | 0.450090 * AVG('Adj Close) | 0.483521 * }}} * * @param numRows Number of rows to show * @param truncate If set to more than 0, truncates strings to `truncate` characters and * all cells will be aligned right. * @param vertical If set to true, prints output rows vertically (one line per column value). * @group action * @since 2.3.0 */ // scalastyle:off println def show(numRows: Int, truncate: Int, vertical: Boolean): Unit = println(showString(numRows, truncate, vertical)) // scalastyle:on println /** * Returns a [[DataFrameNaFunctions]] for working with missing data. * {{{ * // Dropping rows containing any null values. * ds.na.drop() * }}} * * @group untypedrel * @since 1.6.0 */ def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF()) /** * Returns a [[DataFrameStatFunctions]] for working statistic functions support. * {{{ * // Finding frequent items in column with name 'a'. * ds.stat.freqItems(Seq(\"a\")) * }}} * * @group untypedrel * @since 1.6.0 */ def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF()) /** * Join with another `DataFrame`. * * Behaves as an INNER JOIN and requires a subsequent join predicate. * * @param right Right side of the join operation. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Inner, None, JoinHint.NONE) } /** * Inner equi-join with another `DataFrame` using the given column. * * Different from other join functions, the join column will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the column \"user_id\" * df1.join(df2, \"user_id\") * }}} * * @param right Right side of the join operation. * @param usingColumn Name of the column to join on. This column must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumn: String): DataFrame = { join(right, Seq(usingColumn)) } /** * Inner equi-join with another `DataFrame` using the given columns. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * {{{ * // Joining df1 and df2 using the columns \"user_id\" and \"user_name\" * df1.join(df2, Seq(\"user_id\", \"user_name\")) * }}} * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = { join(right, usingColumns, \"inner\") } /** * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate * is specified as an inner join. If you would explicitly like to perform a cross join use the * `crossJoin` method. * * Different from other join functions, the join columns will only appear once in the output, * i.e. similar to SQL's `JOIN USING` syntax. * * @param right Right side of the join operation. * @param usingColumns Names of the columns to join on. This columns must exist on both sides. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @note If you perform a self-join using this function without aliasing the input * `DataFrame`s, you will NOT be able to reference any columns after the join, since * there is no way to disambiguate which side of the join you would like to reference. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = { // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right // by creating a new instance for one of the branch. val joined = sparkSession.sessionState.executePlan( Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None, JoinHint.NONE)) .analyzed.asInstanceOf[Join] withPlan { Join( joined.left, joined.right, UsingJoin(JoinType(joinType), usingColumns), None, JoinHint.NONE) } } /** * Inner join with another `DataFrame`, using the given join expression. * * {{{ * // The following two are equivalent: * df1.join(df2, $\"df1Key\" === $\"df2Key\") * df1.join(df2).where($\"df1Key\" === $\"df2Key\") * }}} * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\") /** * find the trivially true predicates and automatically resolves them to both sides. */ private def resolveSelfJoinCondition(plan: Join): Join = { val resolver = sparkSession.sessionState.analyzer.resolver val cond = plan.condition.map { _.transform { case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualTo( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference) if a.sameRef(b) => catalyst.expressions.EqualNullSafe( plan.left.resolveQuoted(a.name, resolver) .getOrElse(throw resolveException(a.name, plan.left.schema.fieldNames)), plan.right.resolveQuoted(b.name, resolver) .getOrElse(throw resolveException(b.name, plan.right.schema.fieldNames))) }} plan.copy(condition = cond) } /** * find the trivially true predicates and automatically resolves them to both sides. */ private def resolveSelfJoinCondition( right: Dataset[_], joinExprs: Option[Column], joinType: String): Join = { // Note that in this function, we introduce a hack in the case of self-join to automatically // resolve ambiguous join conditions into ones that might make sense [SPARK-6231]. // Consider this case: df.join(df, df(\"key\") === df(\"key\")) // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a // cartesian join. However, most likely users expect to perform a self join using \"key\". // With that assumption, this hack turns the trivially true condition into equality on join // keys that are resolved to both sides. // Trigger analysis so in the case of self-join, the analyzer will clone the plan. // After the cloning, left and right side will have distinct expression ids. val plan = withPlan( Join(logicalPlan, right.logicalPlan, JoinType(joinType), joinExprs.map(_.expr), JoinHint.NONE)) .queryExecution.analyzed.asInstanceOf[Join] // If auto self join alias is disabled, return the plan. if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) { return plan } // If left/right have no output set intersection, return the plan. val lanalyzed = this.queryExecution.analyzed val ranalyzed = right.queryExecution.analyzed if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) { return plan } // Otherwise, find the trivially true predicates and automatically resolves them to both sides. // By the time we get here, since we have already run analysis, all attributes should've been // resolved and become AttributeReference. resolveSelfJoinCondition(plan) } /** * Join with another `DataFrame`, using the given join expression. The following performs * a full outer join between `df1` and `df2`. * * {{{ * // Scala: * import org.apache.spark.sql.functions._ * df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") * * // Java: * import static org.apache.spark.sql.functions.*; * df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\"); * }}} * * @param right Right side of the join. * @param joinExprs Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`, `full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`, * `semi`, `leftsemi`, `left_semi`, `anti`, `leftanti`, left_anti`. * * @group untypedrel * @since 2.0.0 */ def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = { withPlan { resolveSelfJoinCondition(right, Some(joinExprs), joinType) } } /** * Explicit cartesian join with another `DataFrame`. * * @param right Right side of the join operation. * * @note Cartesian joins are very expensive without an extra filter that can be pushed down. * * @group untypedrel * @since 2.1.0 */ def crossJoin(right: Dataset[_]): DataFrame = withPlan { Join(logicalPlan, right.logicalPlan, joinType = Cross, None, JoinHint.NONE) } /** * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to * true. * * This is similar to the relation `join` function with one important difference in the * result schema. Since `joinWith` preserves objects present on either side of the join, the * result schema is similarly nested into a tuple under the column names `_1` and `_2`. * * This type of join can be useful both for preserving type-safety with the original object * types as well as working with relational data where either side of the join has column * names in common. * * @param other Right side of the join. * @param condition Join expression. * @param joinType Type of join to perform. Default `inner`. Must be one of: * `inner`, `cross`, `outer`, `full`, `fullouter`,`full_outer`, `left`, * `leftouter`, `left_outer`, `right`, `rightouter`, `right_outer`. * * @group typedrel * @since 1.6.0 */ def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = { // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved, // etc. var joined = sparkSession.sessionState.executePlan( Join( this.logicalPlan, other.logicalPlan, JoinType(joinType), Some(condition.expr), JoinHint.NONE)).analyzed.asInstanceOf[Join] if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) { throw QueryCompilationErrors.invalidJoinTypeInJoinWithError(joined.joinType) } // If auto self join alias is enable if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) { joined = resolveSelfJoinCondition(joined) } implicit val tuple2Encoder: Encoder[(T, U)] = ExpressionEncoder.tuple(this.exprEnc, other.exprEnc) val leftResultExpr = { if (!this.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.left.output.length == 1) Alias(joined.left.output.head, \"_1\")() } else { Alias(CreateStruct(joined.left.output), \"_1\")() } } val rightResultExpr = { if (!other.exprEnc.isSerializedAsStructForTopLevel) { assert(joined.right.output.length == 1) Alias(joined.right.output.head, \"_2\")() } else { Alias(CreateStruct(joined.right.output), \"_2\")() } } if (joined.joinType.isInstanceOf[InnerLike]) { // For inner joins, we can directly perform the join and then can project the join // results into structs. This ensures that data remains flat during shuffles / // exchanges (unlike the outer join path, which nests the data before shuffling). withTypedPlan(Project(Seq(leftResultExpr, rightResultExpr), joined)) } else { // outer joins // For both join sides, combine all outputs into a single column and alias it with \"_1 // or \"_2\", to match the schema for the encoder of the join result. // Note that we do this before joining them, to enable the join operator to return null // for one side, in cases like outer-join. val left = Project(leftResultExpr :: Nil, joined.left) val right = Project(rightResultExpr :: Nil, joined.right) // Rewrites the join condition to make the attribute point to correct column/field, // after we combine the outputs of each join side. val conditionExpr = joined.condition.get transformUp { case a: Attribute if joined.left.outputSet.contains(a) => if (!this.exprEnc.isSerializedAsStructForTopLevel) { left.output.head } else { val index = joined.left.output.indexWhere(_.exprId == a.exprId) GetStructField(left.output.head, index) } case a: Attribute if joined.right.outputSet.contains(a) => if (!other.exprEnc.isSerializedAsStructForTopLevel) { right.output.head } else { val index = joined.right.output.indexWhere(_.exprId == a.exprId) GetStructField(right.output.head, index) } } withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr), JoinHint.NONE)) } } /** * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair * where `condition` evaluates to true. * * @param other Right side of the join. * @param condition Join expression. * * @group typedrel * @since 1.6.0 */ def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = { joinWith(other, condition, \"inner\") } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, usingColumns: Seq[String], joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joinExprs = usingColumns.map { column => EqualTo(resolve(column), other.resolve(column)) }.reduceOption(And).map(Column.apply).orNull joinAsOf(other, leftAsOf, rightAsOf, joinExprs, joinType, tolerance, allowExactMatches, direction) } // TODO(SPARK-22947): Fix the DataFrame API. private[sql] def joinAsOf( other: Dataset[_], leftAsOf: Column, rightAsOf: Column, joinExprs: Column, joinType: String, tolerance: Column, allowExactMatches: Boolean, direction: String): DataFrame = { val joined = resolveSelfJoinCondition(other, Option(joinExprs), joinType) val leftAsOfExpr = leftAsOf.expr.transformUp { case a: AttributeReference if logicalPlan.outputSet.contains(a) => val index = logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.left.output(index) } val rightAsOfExpr = rightAsOf.expr.transformUp { case a: AttributeReference if other.logicalPlan.outputSet.contains(a) => val index = other.logicalPlan.output.indexWhere(_.exprId == a.exprId) joined.right.output(index) } withPlan { AsOfJoin( joined.left, joined.right, leftAsOfExpr, rightAsOfExpr, joined.condition, joined.joinType, Option(tolerance).map(_.expr), allowExactMatches, AsOfJoinDirection(direction) ) } } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = { sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset with each partition sorted by the given expressions. * * This is the same operation as \"SORT BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sortWithinPartitions(sortExprs: Column*): Dataset[T] = { sortInternal(global = false, sortExprs) } /** * Returns a new Dataset sorted by the specified column, all in ascending order. * {{{ * // The following 3 are equivalent * ds.sort(\"sortcol\") * ds.sort($\"sortcol\") * ds.sort($\"sortcol\".asc) * }}} * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortCol: String, sortCols: String*): Dataset[T] = { sort((sortCol +: sortCols).map(Column(_)) : _*) } /** * Returns a new Dataset sorted by the given expressions. For example: * {{{ * ds.sort($\"col1\", $\"col2\".desc) * }}} * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def sort(sortExprs: Column*): Dataset[T] = { sortInternal(global = true, sortExprs) } /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*) /** * Returns a new Dataset sorted by the given expressions. * This is an alias of the `sort` function. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*) /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0 */ def apply(colName: String): Column = col(colName) /** * Specifies some hint on the current Dataset. As an example, the following code specifies * that one of the plan can be broadcasted: * * {{{ * df1.join(df2.hint(\"broadcast\")) * }}} * * @group basic * @since 2.2.0 */ @scala.annotation.varargs def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan { UnresolvedHint(name, parameters, logicalPlan) } /** * Selects column based on the column name and returns it as a [[Column]]. * * @note The column name can also reference to a nested column like `a.b`. * * @group untypedrel * @since 2.0.0 */ def col(colName: String): Column = colName match { case \"*\" => Column(ResolvedStar(queryExecution.analyzed.output)) case _ => if (sqlContext.conf.supportQuotedRegexColumnName) { colRegex(colName) } else { Column(addDataFrameIdToCol(resolve(colName))) } } // Attach the dataset id and column position to the column reference, so that we can detect // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`. // This must be called before we return a `Column` that contains `AttributeReference`. // Note that, the metadata added here are only available in the analyzer, as the analyzer rule // `DetectAmbiguousSelfJoin` will remove it. private def addDataFrameIdToCol(expr: NamedExpression): NamedExpression = { val newExpr = expr transform { case a: AttributeReference if sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) => val metadata = new MetadataBuilder() .withMetadata(a.metadata) .putLong(Dataset.DATASET_ID_KEY, id) .putLong(Dataset.COL_POS_KEY, logicalPlan.output.indexWhere(a.semanticEquals)) .build() a.withMetadata(metadata) } newExpr.asInstanceOf[NamedExpression] } /** * Selects column based on the column name specified as a regex and returns it as [[Column]]. * @group untypedrel * @since 2.3.0 */ def colRegex(colName: String): Column = { val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis colName match { case ParserUtils.escapedIdentifier(columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, None, caseSensitive)) case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) => Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive)) case _ => Column(addDataFrameIdToCol(resolve(colName))) } } /** * Returns a new Dataset with an alias set. * * @group typedrel * @since 1.6.0 */ def as(alias: String): Dataset[T] = withTypedPlan { SubqueryAlias(alias, logicalPlan) } /** * (Scala-specific) Returns a new Dataset with an alias set. * * @group typedrel * @since 2.0.0 */ def as(alias: Symbol): Dataset[T] = as(alias.name) /** * Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: String): Dataset[T] = as(alias) /** * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`. * * @group typedrel * @since 2.0.0 */ def alias(alias: Symbol): Dataset[T] = as(alias) /** * Selects a set of column based expressions. * {{{ * ds.select($\"colA\", $\"colB\" + 1) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def select(cols: Column*): DataFrame = withPlan { val untypedCols = cols.map { case typedCol: TypedColumn[_, _] => // Checks if a `TypedColumn` has been inserted with // specific input type and schema by `withInputType`. val needInputType = typedCol.expr.exists { case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty => true case _ => false } if (!needInputType) { typedCol } else { throw QueryCompilationErrors.cannotPassTypedColumnInUntypedSelectError(typedCol.toString) } case other => other } Project(untypedCols.map(_.named), logicalPlan) } /** * Selects a set of columns. This is a variant of `select` that can only select * existing columns using column names (i.e. cannot construct expressions). * * {{{ * // The following two are equivalent: * ds.select(\"colA\", \"colB\") * ds.select($\"colA\", $\"colB\") * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*) /** * Selects a set of SQL expressions. This is a variant of `select` that accepts * SQL expressions. * * {{{ * // The following are equivalent: * ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\") * ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\")) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def selectExpr(exprs: String*): DataFrame = { select(exprs.map { expr => Column(sparkSession.sessionState.sqlParser.parseExpression(expr)) }: _*) } /** * Returns a new Dataset by computing the given [[Column]] expression for each element. * * {{{ * val ds = Seq(1, 2, 3).toDS() * val newDS = ds.select(expr(\"value + 1\").as[Int]) * }}} * * @group typedrel * @since 1.6.0 */ def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = { implicit val encoder = c1.encoder val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan) if (!encoder.isSerializedAsStructForTopLevel) { new Dataset[U1](sparkSession, project, encoder) } else { // Flattens inner fields of U1 new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1) } } /** * Internal helper function for building typed selects that return tuples. For simplicity and * code reuse, we do this without the help of the type system and then use helper functions * that cast appropriately for the user facing interface. */ protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = { val encoders = columns.map(_.encoder) val namedColumns = columns.map(_.withInputType(exprEnc, logicalPlan.output).named) val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan)) new Dataset(execution, ExpressionEncoder.tuple(encoders)) } /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] = selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] = selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3, U4]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] = selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]] /** * Returns a new Dataset by computing the given [[Column]] expressions for each element. * * @group typedrel * @since 1.6.0 */ def select[U1, U2, U3, U4, U5]( c1: TypedColumn[T, U1], c2: TypedColumn[T, U2], c3: TypedColumn[T, U3], c4: TypedColumn[T, U4], c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] = selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]] /** * Filters rows using the given condition. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0 */ def filter(condition: Column): Dataset[T] = withTypedPlan { Filter(condition.expr, logicalPlan) } /** * Filters rows using the given SQL expression. * {{{ * peopleDs.filter(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0 */ def filter(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Filters rows using the given condition. This is an alias for `filter`. * {{{ * // The following are equivalent: * peopleDs.filter($\"age\" > 15) * peopleDs.where($\"age\" > 15) * }}} * * @group typedrel * @since 1.6.0 */ def where(condition: Column): Dataset[T] = filter(condition) /** * Filters rows using the given SQL expression. * {{{ * peopleDs.where(\"age > 15\") * }}} * * @group typedrel * @since 1.6.0 */ def where(conditionExpr: String): Dataset[T] = { filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr))) } /** * Groups the Dataset using the specified columns, so we can run aggregation on them. See * [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy($\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def groupBy(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType) } /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup($\"department\", $\"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube($\"department\", $\"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(cols: Column*): RelationalGroupedDataset = { RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType) } /** * Groups the Dataset using the specified columns, so that we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of groupBy that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns grouped by department. * ds.groupBy(\"department\").avg() * * // Compute the max age and average salary, grouped by department and gender. * ds.groupBy($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def groupBy(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType) } /** * (Scala-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0 */ def reduce(func: (T, T) => T): T = withNewRDDExecutionId { rdd.reduce(func) } /** * (Java-specific) * Reduces the elements of this Dataset using the specified binary function. The given `func` * must be commutative and associative or the result may be non-deterministic. * * @group action * @since 1.6.0 */ def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _)) /** * (Scala-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0 */ def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = { val withGroupingKey = AppendColumns(func, logicalPlan) val executed = sparkSession.sessionState.executePlan(withGroupingKey) new KeyValueGroupedDataset( encoderFor[K], encoderFor[T], executed, logicalPlan.output, withGroupingKey.newColumns) } /** * (Java-specific) * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`. * * @group typedrel * @since 2.0.0 */ def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] = groupByKey(func.call(_))(encoder) /** * Create a multi-dimensional rollup for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of rollup that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns rolled up by department and group. * ds.rollup(\"department\", \"group\").avg() * * // Compute the max age and average salary, rolled up by department and gender. * ds.rollup($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def rollup(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType) } /** * Create a multi-dimensional cube for the current Dataset using the specified columns, * so we can run aggregation on them. * See [[RelationalGroupedDataset]] for all the available aggregate functions. * * This is a variant of cube that can only group by existing columns using column names * (i.e. cannot construct expressions). * * {{{ * // Compute the average for all numeric columns cubed by department and group. * ds.cube(\"department\", \"group\").avg() * * // Compute the max age and average salary, cubed by department and gender. * ds.cube($\"department\", $\"gender\").agg(Map( * \"salary\" -> \"avg\", * \"age\" -> \"max\" * )) * }}} * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def cube(col1: String, cols: String*): RelationalGroupedDataset = { val colNames: Seq[String] = col1 +: cols RelationalGroupedDataset( toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\") * }}} * * @group untypedrel * @since 2.0.0 */ def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = { groupBy().agg(aggExpr, aggExprs : _*) } /** * (Scala-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0 */ def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs) /** * (Java-specific) Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\")) * }}} * * @group untypedrel * @since 2.0.0 */ def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs) /** * Aggregates on the entire Dataset without groups. * {{{ * // ds.agg(...) is a shorthand for ds.groupBy().agg(...) * ds.agg(max($\"age\"), avg($\"salary\")) * ds.groupBy().agg(max($\"age\"), avg($\"salary\")) * }}} * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*) /** * Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset * that returns the same result as the input, with the following guarantees: * <ul> * <li>It will compute the defined aggregates (metrics) on all the data that is flowing through * the Dataset at that point.</li> * <li>It will report the value of the defined aggregate columns as soon as we reach a completion * point. A completion point is either the end of a query (batch mode) or the end of a streaming * epoch. The value of the aggregates only reflects the data processed since the previous * completion point.</li> * </ul> * Please note that continuous execution is currently not supported. * * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that * contain references to the input Dataset's columns must always be wrapped in an aggregate * function. * * A user can observe these metrics by either adding * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session. * * {{{ * // Monitor the metrics using a listener. * spark.streams.addListener(new StreamingQueryListener() { * override def onQueryStarted(event: QueryStartedEvent): Unit = {} * override def onQueryProgress(event: QueryProgressEvent): Unit = { * event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row => * // Trigger if the number of errors exceeds 5 percent * val num_rows = row.getAs[Long](\"rc\") * val num_error_rows = row.getAs[Long](\"erc\") * val ratio = num_error_rows.toDouble / num_rows * if (ratio > 0.05) { * // Trigger alert * } * } * } * override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {} * }) * // Observe row count (rc) and error row count (erc) in the streaming Dataset * val observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\")) * observed_ds.writeStream.format(\"...\").start() * }}} * * @group typedrel * @since 3.0.0 */ @varargs def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan { CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan) } /** * Observe (named) metrics through an `org.apache.spark.sql.Observation` instance. * This is equivalent to calling `observe(String, Column, Column*)` but does not require * adding `org.apache.spark.sql.util.QueryExecutionListener` to the spark session. * This method does not support streaming datasets. * * A user can retrieve the metrics by accessing `org.apache.spark.sql.Observation.get`. * * {{{ * // Observe row count (rows) and highest id (maxid) in the Dataset while writing it * val observation = Observation(\"my_metrics\") * val observed_ds = ds.observe(observation, count(lit(1)).as(\"rows\"), max($\"id\").as(\"maxid\")) * observed_ds.write.parquet(\"ds.parquet\") * val metrics = observation.get * }}} * * @throws IllegalArgumentException If this is a streaming Dataset (this.isStreaming == true) * * @group typedrel * @since 3.3.0 */ @varargs def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = { observation.on(this, expr, exprs: _*) } /** * Returns a new Dataset by taking the first `n` rows. The difference between this function * and `head` is that `head` is an action and returns an array (by triggering query execution) * while `limit` returns a new Dataset. * * @group typedrel * @since 2.0.0 */ def limit(n: Int): Dataset[T] = withTypedPlan { Limit(Literal(n), logicalPlan) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.union(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 4| 5| 6| * // +----+----+----+ * }}} * * Notice that the column positions in the schema aren't necessarily matched with the * fields in the strongly typed objects in a Dataset. This function resolves columns * by their positions in the schema, not the fields in the strongly typed objects. Use * [[unionByName]] to resolve columns by field name in the typed objects. * * @group typedrel * @since 2.0.0 */ def union(other: Dataset[T]): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan, other.logicalPlan)) } /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * This is an alias for `union`. * * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does * deduplication of elements), use this function followed by a [[distinct]]. * * Also as standard in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.0.0 */ def unionAll(other: Dataset[T]): Dataset[T] = union(other) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set * union (that does deduplication of elements), use this function followed by a [[distinct]]. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position): * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col2\", \"col0\") * df1.unionByName(df2).show * * // output: * // +----+----+----+ * // |col0|col1|col2| * // +----+----+----+ * // | 1| 2| 3| * // | 6| 4| 5| * // +----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. Nested columns in map types * are not currently supported. * * @group typedrel * @since 2.3.0 */ def unionByName(other: Dataset[T]): Dataset[T] = unionByName(other, false) /** * Returns a new Dataset containing union of rows in this Dataset and another Dataset. * * The difference between this function and [[union]] is that this function * resolves columns by name (not by position). * * When the parameter `allowMissingColumns` is `true`, the set of column names * in this and other `Dataset` can differ; missing columns will be filled with null. * Further, the missing columns of this `Dataset` will be added at the end * in the schema of the union result: * * {{{ * val df1 = Seq((1, 2, 3)).toDF(\"col0\", \"col1\", \"col2\") * val df2 = Seq((4, 5, 6)).toDF(\"col1\", \"col0\", \"col3\") * df1.unionByName(df2, true).show * * // output: \"col3\" is missing at left df1 and added at the end of schema. * // +----+----+----+----+ * // |col0|col1|col2|col3| * // +----+----+----+----+ * // | 1| 2| 3|null| * // | 5| 4|null| 6| * // +----+----+----+----+ * * df2.unionByName(df1, true).show * * // output: \"col2\" is missing at left df2 and added at the end of schema. * // +----+----+----+----+ * // |col1|col0|col3|col2| * // +----+----+----+----+ * // | 4| 5| 6|null| * // | 2| 1|null| 3| * // +----+----+----+----+ * }}} * * Note that this supports nested columns in struct and array types. With `allowMissingColumns`, * missing nested columns of struct columns with the same name will also be filled with null * values and added to the end of struct. Nested columns in map types are not currently * supported. * * @group typedrel * @since 3.1.0 */ def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator { // This breaks caching, but it's usually ok because it addresses a very specific use case: // using union to union many files or partitions. CombineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns)) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset. * This is equivalent to `INTERSECT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 1.6.0 */ def intersect(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows only in both this Dataset and another Dataset while * preserving the duplicates. * This is equivalent to `INTERSECT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard * in SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0 */ def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator { Intersect(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset. * This is equivalent to `EXCEPT DISTINCT` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0 */ def except(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = false) } /** * Returns a new Dataset containing rows in this Dataset but not in another Dataset while * preserving the duplicates. * This is equivalent to `EXCEPT ALL` in SQL. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in * SQL, this function resolves columns by position (not by name). * * @group typedrel * @since 2.4.0 */ def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator { Except(logicalPlan, other.logicalPlan, isAll = true) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a user-supplied seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0 */ def sample(fraction: Double, seed: Long): Dataset[T] = { sample(withReplacement = false, fraction = fraction, seed = seed) } /** * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement), * using a random seed. * * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 2.3.0 */ def sample(fraction: Double): Dataset[T] = { sample(withReplacement = false, fraction = fraction) } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * @param seed Seed for sampling. * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = { withTypedPlan { Sample(0.0, fraction, withReplacement, seed, logicalPlan) } } /** * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed. * * @param withReplacement Sample with replacement or not. * @param fraction Fraction of rows to generate, range [0.0, 1.0]. * * @note This is NOT guaranteed to provide exactly the fraction of the total count * of the given [[Dataset]]. * * @group typedrel * @since 1.6.0 */ def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = { sample(withReplacement, fraction, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * For Java API, use [[randomSplitAsList]]. * * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its // constituent partitions each time a split is materialized which could result in // overlapping splits. To prevent this, we explicitly sort each input partition to make the // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out // from the sort order. val sortOrder = logicalPlan.output .filter(attr => RowOrdering.isOrderable(attr.dataType)) .map(SortOrder(_, Ascending)) val plan = if (sortOrder.nonEmpty) { Sort(sortOrder, global = false, logicalPlan) } else { // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism cache() logicalPlan } val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => new Dataset[T]( sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder) }.toArray } /** * Returns a Java list that contains randomly split Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. * * @group typedrel * @since 2.0.0 */ def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = { val values = randomSplit(weights, seed) java.util.Arrays.asList(values : _*) } /** * Randomly splits this Dataset with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @group typedrel * @since 2.0.0 */ def randomSplit(weights: Array[Double]): Array[Dataset[T]] = { randomSplit(weights, Utils.random.nextLong) } /** * Randomly splits this Dataset with the provided weights. Provided for the Python Api. * * @param weights weights for splits, will be normalized if they don't sum to 1. * @param seed Seed for sampling. */ private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = { randomSplit(weights.toArray, seed) } /** * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of * the input row are implicitly joined with each row that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count * the number of books that contain a given word: * * {{{ * case class Book(title: String, words: String) * val ds: Dataset[Book] * * val allWords = ds.select($\"title\", explode(split($\"words\", \" \")).as(\"word\")) * * val bookCountPerWord = allWords.groupBy(\"word\").agg(count_distinct(\"title\")) * }}} * * Using `flatMap()` this can similarly be exploded as: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0 */ @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = { val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema) val rowFunction = f.andThen(_.map(convert(_).asInstanceOf[InternalRow])) val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr)) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All * columns of the input row are implicitly joined with each value that is output by the function. * * Given that this is deprecated, as an alternative, you can explode columns either using * `functions.explode()`: * * {{{ * ds.select(explode(split($\"words\", \" \")).as(\"word\")) * }}} * * or `flatMap()`: * * {{{ * ds.flatMap(_.words.split(\" \")) * }}} * * @group untypedrel * @since 2.0.0 */ @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\") def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B]) : DataFrame = { val dataType = ScalaReflection.schemaFor[B].dataType val attributes = AttributeReference(outputColumn, dataType)() :: Nil // TODO handle the metadata? val elementSchema = attributes.toStructType def rowFunction(row: Row): TraversableOnce[InternalRow] = { val convert = CatalystTypeConverters.createToCatalystConverter(dataType) f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o))) } val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil) withPlan { Generate(generator, unrequiredChildIndex = Nil, outer = false, qualifier = None, generatorOutput = Nil, logicalPlan) } } /** * Returns a new Dataset by adding a column or replacing the existing column that has * the same name. * * `column`'s expression must only refer to attributes supplied by this Dataset. It is an * error to add a column that refers to some other Dataset. * * @note this method introduces a projection internally. Therefore, calling it multiple times, * for instance, via loops in order to add multiple columns can generate big plans which * can cause performance issues and even `StackOverflowException`. To avoid this, * use `select` with the multiple columns at once. * * @group untypedrel * @since 2.0.0 */ def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col)) /** * (Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attributes * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0 */ def withColumns(colsMap: Map[String, Column]): DataFrame = { val (colNames, newCols) = colsMap.toSeq.unzip withColumns(colNames, newCols) } /** * (Java-specific) Returns a new Dataset by adding columns or replacing the existing columns * that has the same names. * * `colsMap` is a map of column name and column, the column must only refer to attribute * supplied by this Dataset. It is an error to add columns that refers to some other Dataset. * * @group untypedrel * @since 3.3.0 */ def withColumns(colsMap: java.util.Map[String, Column]): DataFrame = withColumns( colsMap.asScala.toMap ) /** * Returns a new Dataset by adding columns or replacing the existing columns that has * the same names. */ private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = { require(colNames.size == cols.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of columns: ${cols.size}\") SchemaUtils.checkColumnNameDuplication( colNames, \"in given column names\", sparkSession.sessionState.conf.caseSensitiveAnalysis) val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val columnSeq = colNames.zip(cols) val replacedAndExistingColumns = output.map { field => columnSeq.find { case (colName, _) => resolver(field.name, colName) } match { case Some((colName: String, col: Column)) => col.as(colName) case _ => Column(field) } } val newColumns = columnSeq.filter { case (colName, col) => !output.exists(f => resolver(f.name, colName)) }.map { case (colName, col) => col.as(colName) } select(replacedAndExistingColumns ++ newColumns : _*) } /** * Returns a new Dataset by adding columns with metadata. */ private[spark] def withColumns( colNames: Seq[String], cols: Seq[Column], metadata: Seq[Metadata]): DataFrame = { require(colNames.size == metadata.size, s\"The size of column names: ${colNames.size} isn't equal to \" + s\"the size of metadata elements: ${metadata.size}\") val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) => col.as(colName, metadata) } withColumns(colNames, newCols) } /** * Returns a new Dataset by adding a column with metadata. */ private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = withColumns(Seq(colName), Seq(col), Seq(metadata)) /** * Returns a new Dataset with a column renamed. * This is a no-op if schema doesn't contain existingName. * * @group untypedrel * @since 2.0.0 */ def withColumnRenamed(existingName: String, newName: String): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val output = queryExecution.analyzed.output val shouldRename = output.exists(f => resolver(f.name, existingName)) if (shouldRename) { val columns = output.map { col => if (resolver(col.name, existingName)) { Column(col).as(newName) } else { Column(col) } } select(columns : _*) } else { toDF() } } /** * Returns a new Dataset by updating an existing column with metadata. * * @group untypedrel * @since 3.3.0 */ def withMetadata(columnName: String, metadata: Metadata): DataFrame = { withColumn(columnName, col(columnName), metadata) } /** * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain * column name. * * This method can only be used to drop top level columns. the colName string is treated * literally without further interpretation. * * @group untypedrel * @since 2.0.0 */ def drop(colName: String): DataFrame = { drop(Seq(colName) : _*) } /** * Returns a new Dataset with columns dropped. * This is a no-op if schema doesn't contain column name(s). * * This method can only be used to drop top level columns. the colName string is treated literally * without further interpretation. * * @group untypedrel * @since 2.0.0 */ @scala.annotation.varargs def drop(colNames: String*): DataFrame = { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output val remainingCols = allColumns.filter { attribute => colNames.forall(n => !resolver(attribute.name, n)) }.map(attribute => Column(attribute)) if (remainingCols.size == allColumns.size) { toDF() } else { this.select(remainingCols: _*) } } /** * Returns a new Dataset with a column dropped. * This version of drop accepts a [[Column]] rather than a name. * This is a no-op if the Dataset doesn't have a column * with an equivalent expression. * * @group untypedrel * @since 2.0.0 */ def drop(col: Column): DataFrame = { val expression = col match { case Column(u: UnresolvedAttribute) => queryExecution.analyzed.resolveQuoted( u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u) case Column(expr: Expression) => expr } val attrs = this.logicalPlan.output val colsAfterDrop = attrs.filter { attr => !attr.semanticEquals(expression) }.map(attr => Column(attr)) select(colsAfterDrop : _*) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `distinct`. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns) /** * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan { val resolver = sparkSession.sessionState.analyzer.resolver val allColumns = queryExecution.analyzed.output // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue // (the Streaming's state store depends on the `groupCols` order). val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) => // It is possibly there are more than one columns with the same name, // so we call filter instead of find. val cols = allColumns.filter(col => resolver(col.name, colName)) if (cols.isEmpty) { throw QueryCompilationErrors.cannotResolveColumnNameAmongAttributesError( colName, schema.fieldNames.mkString(\", \")) } cols } Deduplicate(groupCols, logicalPlan) } /** * Returns a new Dataset with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq) /** * Returns a new [[Dataset]] with duplicate rows removed, considering only * the subset of columns. * * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it * will keep all data across triggers as intermediate state to drop duplicates rows. You can use * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit * the state. In addition, too late data older than watermark will be dropped to avoid any * possibility of duplicates. * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def dropDuplicates(col1: String, cols: String*): Dataset[T] = { val colNames: Seq[String] = col1 +: cols dropDuplicates(colNames) } /** * Computes basic statistics for numeric and string columns, including count, mean, stddev, min, * and max. If no columns are given, this function computes statistics for all numerical or * string columns. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.describe(\"age\", \"height\").show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // max 92.0 192.0 * }}} * * Use [[summary]] for expanded statistics and control over which statistics to compute. * * @param cols Columns to compute statistics on. * * @group action * @since 1.6.0 */ @scala.annotation.varargs def describe(cols: String*): DataFrame = { val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*) selected.summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\") } /** * Computes specified statistics for numeric and string columns. Available statistics are: * <ul> * <li>count</li> * <li>mean</li> * <li>stddev</li> * <li>min</li> * <li>max</li> * <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li> * <li>count_distinct</li> * <li>approx_count_distinct</li> * </ul> * * If no statistics are given, this function computes count, mean, stddev, min, * approximate quartiles (percentiles at 25%, 50%, and 75%), and max. * * This function is meant for exploratory data analysis, as we make no guarantee about the * backward compatibility of the schema of the resulting Dataset. If you want to * programmatically compute summary statistics, use the `agg` function instead. * * {{{ * ds.summary().show() * * // output: * // summary age height * // count 10.0 10.0 * // mean 53.3 178.05 * // stddev 11.6 15.7 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 50% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * {{{ * ds.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show() * * // output: * // summary age height * // count 10.0 10.0 * // min 18.0 163.0 * // 25% 24.0 176.0 * // 75% 32.0 180.0 * // max 92.0 192.0 * }}} * * To do a summary for specific columns first select them: * * {{{ * ds.select(\"age\", \"height\").summary().show() * }}} * * Specify statistics to output custom summaries: * * {{{ * ds.summary(\"count\", \"count_distinct\").show() * }}} * * The distinct count isn't included by default. * * You can also run approximate distinct counts which are faster: * * {{{ * ds.summary(\"count\", \"approx_count_distinct\").show() * }}} * * See also [[describe]] for basic statistics. * * @param statistics Statistics from above list to be computed. * * @group action * @since 2.3.0 */ @scala.annotation.varargs def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq) /** * Returns the first `n` rows. * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @group action * @since 1.6.0 */ def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) /** * Returns the first row. * @group action * @since 1.6.0 */ def head(): T = head(1).head /** * Returns the first row. Alias for head(). * @group action * @since 1.6.0 */ def first(): T = head() /** * Concise syntax for chaining custom transformations. * {{{ * def featurize(ds: Dataset[T]): Dataset[U] = ... * * ds * .transform(featurize) * .transform(...) * }}} * * @group typedrel * @since 1.6.0 */ def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this) /** * (Scala-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0 */ def filter(func: T => Boolean): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Java-specific) * Returns a new Dataset that only contains elements where `func` returns `true`. * * @group typedrel * @since 1.6.0 */ def filter(func: FilterFunction[T]): Dataset[T] = { withTypedPlan(TypedFilter(func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0 */ def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan { MapElements[T, U](func, logicalPlan) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `func` to each element. * * @group typedrel * @since 1.6.0 */ def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { implicit val uEnc = encoder withTypedPlan(MapElements[T, U](func, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset that contains the result of applying `func` to each partition. * * @group typedrel * @since 1.6.0 */ def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = { new Dataset[U]( sparkSession, MapPartitions[T, U](func, logicalPlan), implicitly[Encoder[U]]) } /** * (Java-specific) * Returns a new Dataset that contains the result of applying `f` to each partition. * * @group typedrel * @since 1.6.0 */ def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala mapPartitions(func)(encoder) } /** * Returns a new `DataFrame` that contains the result of applying a serialized R function * `func` to each partition. */ private[sql] def mapPartitionsInR( func: Array[Byte], packageNames: Array[Byte], broadcastVars: Array[Broadcast[Object]], schema: StructType): DataFrame = { val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]] Dataset.ofRows( sparkSession, MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan)) } /** * Applies a Scalar iterator Pandas UDF to each partition. The user-defined function * defines a transformation: `iter(pandas.DataFrame)` -> `iter(pandas.DataFrame)`. * Each partition is each iterator consisting of DataFrames as batches. * * This function uses Apache Arrow as serialization format between Java executors and Python * workers. */ private[sql] def mapInPandas(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, MapInPandas( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * Applies a function to each partition in Arrow format. The user-defined function * defines a transformation: `iter(pyarrow.RecordBatch)` -> `iter(pyarrow.RecordBatch)`. * Each partition is each iterator consisting of `pyarrow.RecordBatch`s as batches. */ private[sql] def pythonMapInArrow(func: PythonUDF): DataFrame = { Dataset.ofRows( sparkSession, PythonMapInArrow( func, func.dataType.asInstanceOf[StructType].toAttributes, logicalPlan)) } /** * (Scala-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0 */ def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] = mapPartitions(_.flatMap(func)) /** * (Java-specific) * Returns a new Dataset by first applying a function to all elements of this Dataset, * and then flattening the results. * * @group typedrel * @since 1.6.0 */ def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = { val func: (T) => Iterator[U] = x => f.call(x).asScala flatMap(func)(encoder) } /** * Applies a function `f` to all rows. * * @group action * @since 1.6.0 */ def foreach(f: T => Unit): Unit = withNewRDDExecutionId { rdd.foreach(f) } /** * (Java-specific) * Runs `func` on each element of this Dataset. * * @group action * @since 1.6.0 */ def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_)) /** * Applies a function `f` to each partition of this Dataset. * * @group action * @since 1.6.0 */ def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId { rdd.foreachPartition(f) } /** * (Java-specific) * Runs `func` on each partition of this Dataset. * * @group action * @since 1.6.0 */ def foreachPartition(func: ForeachPartitionFunction[T]): Unit = { foreachPartition((it: Iterator[T]) => func.call(it.asJava)) } /** * Returns the first `n` rows in the Dataset. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def take(n: Int): Array[T] = head(n) /** * Returns the last `n` rows in the Dataset. * * Running tail requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 3.0.0 */ def tail(n: Int): Array[T] = withAction( \"tail\", withTypedPlan(Tail(Literal(n), logicalPlan)).queryExecution)(collectFromPlan) /** * Returns the first `n` rows in the Dataset as a list. * * Running take requires moving data into the application's driver process, and doing so with * a very large `n` can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*) /** * Returns an array that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * For Java API, use [[collectAsList]]. * * @group action * @since 1.6.0 */ def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan) /** * Returns a Java list that contains all rows in this Dataset. * * Running collect requires moving all the data into the application's driver process, and * doing so on a very large dataset can crash the driver process with OutOfMemoryError. * * @group action * @since 1.6.0 */ def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan => val values = collectFromPlan(plan) java.util.Arrays.asList(values : _*) } /** * Returns an iterator that contains all rows in this Dataset. * * The iterator will consume as much memory as the largest partition in this Dataset. * * @note this results in multiple Spark jobs, and if the input Dataset is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input Dataset should be cached first. * * @group action * @since 2.0.0 */ def toLocalIterator(): java.util.Iterator[T] = { withAction(\"toLocalIterator\", queryExecution) { plan => val fromRow = resolvedEnc.createDeserializer() plan.executeToIterator().map(fromRow).asJava } } /** * Returns the number of rows in the Dataset. * @group action * @since 1.6.0 */ def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan => plan.executeCollect().head.getLong(0) } /** * Returns a new Dataset that has exactly `numPartitions` partitions. * * @group typedrel * @since 1.6.0 */ def repartition(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = true, logicalPlan) } private def repartitionByExpression( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments. // However, we don't want to complicate the semantics of this API method. // Instead, let's give users a friendly error message, pointing them to the new method. val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder]) if (sortOrders.nonEmpty) throw new IllegalArgumentException( s\"\"\"Invalid partitionExprs specified: $sortOrders |For range partitioning use repartitionByRange(...) instead. \"\"\".stripMargin) withTypedPlan { RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByExpression(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is hash partitioned. * * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL). * * @group typedrel * @since 2.0.0 */ @scala.annotation.varargs def repartition(partitionExprs: Column*): Dataset[T] = { repartitionByExpression(None, partitionExprs) } private def repartitionByRange( numPartitions: Option[Int], partitionExprs: Seq[Column]): Dataset[T] = { require(partitionExprs.nonEmpty, \"At least one partition-by expression must be specified.\") val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) }) withTypedPlan { RepartitionByExpression(sortOrder, logicalPlan, numPartitions) } } /** * Returns a new Dataset partitioned by the given partitioning expressions into * `numPartitions`. The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0 */ @scala.annotation.varargs def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = { repartitionByRange(Some(numPartitions), partitionExprs) } /** * Returns a new Dataset partitioned by the given partitioning expressions, using * `spark.sql.shuffle.partitions` as number of partitions. * The resulting Dataset is range partitioned. * * At least one partition-by expression must be specified. * When no explicit sort order is specified, \"ascending nulls first\" is assumed. * Note, the rows are not sorted in each partition of the resulting Dataset. * * Note that due to performance reasons this method uses sampling to estimate the ranges. * Hence, the output may not be consistent, since sampling can return different values. * The sample size can be controlled by the config * `spark.sql.execution.rangeExchange.sampleSizePerPartition`. * * @group typedrel * @since 2.3.0 */ @scala.annotation.varargs def repartitionByRange(partitionExprs: Column*): Dataset[T] = { repartitionByRange(None, partitionExprs) } /** * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions * are requested. If a larger number of partitions is requested, it will stay at the current * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can call repartition. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @group typedrel * @since 1.6.0 */ def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan { Repartition(numPartitions, shuffle = false, logicalPlan) } /** * Returns a new Dataset that contains only the unique rows from this Dataset. * This is an alias for `dropDuplicates`. * * Note that for a streaming [[Dataset]], this method returns distinct rows only once * regardless of the output mode, which the behavior may not be same with `DISTINCT` in SQL * against streaming [[Dataset]]. * * @note Equality checking is performed directly on the encoded representation of the data * and thus is not affected by a custom `equals` function defined on `T`. * * @group typedrel * @since 2.0.0 */ def distinct(): Dataset[T] = dropDuplicates() /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */ def persist(): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this) this } /** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */ def cache(): this.type = persist() /** * Persist this Dataset with the given storage level. * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, * `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, * `MEMORY_AND_DISK_2`, etc. * * @group basic * @since 1.6.0 */ def persist(newLevel: StorageLevel): this.type = { sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this } /** * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted. * * @group basic * @since 2.1.0 */ def storageLevel: StorageLevel = { sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData => cachedData.cachedRepresentation.cacheBuilder.storageLevel }.getOrElse(StorageLevel.NONE) } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @param blocking Whether to block until all blocks are deleted. * * @group basic * @since 1.6.0 */ def unpersist(blocking: Boolean): this.type = { sparkSession.sharedState.cacheManager.uncacheQuery( sparkSession, logicalPlan, cascade = false, blocking) this } /** * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. * This will not un-persist any cached data that is built upon this Dataset. * * @group basic * @since 1.6.0 */ def unpersist(): this.type = unpersist(blocking = false) // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`. @transient private lazy val rddQueryExecution: QueryExecution = { val deserialized = CatalystSerde.deserialize[T](logicalPlan) sparkSession.sessionState.executePlan(deserialized) } /** * Represents the content of the Dataset as an `RDD` of `T`. * * @group basic * @since 1.6.0 */ lazy val rdd: RDD[T] = { val objectType = exprEnc.deserializer.dataType rddQueryExecution.toRdd.mapPartitions { rows => rows.map(_.get(0, objectType).asInstanceOf[T]) } } /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0 */ def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD() /** * Returns the content of the Dataset as a `JavaRDD` of `T`s. * @group basic * @since 1.6.0 */ def javaRDD: JavaRDD[T] = toJavaRDD /** * Registers this Dataset as a temporary table using the given name. The lifetime of this * temporary table is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 1.6.0 */ @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\") def registerTempTable(tableName: String): Unit = { createOrReplaceTempView(tableName) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that * created it, i.e. it will be automatically dropped when the session terminates. It's not * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.0.0 */ @throws[AnalysisException] def createTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = false) } /** * Creates a local temporary view using the given name. The lifetime of this * temporary view is tied to the [[SparkSession]] that was used to create this Dataset. * * @group basic * @since 2.0.0 */ def createOrReplaceTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = false) } /** * Creates a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @throws AnalysisException if the view name is invalid or already exists * * @group basic * @since 2.1.0 */ @throws[AnalysisException] def createGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = false, global = true) } /** * Creates or replaces a global temporary view using the given name. The lifetime of this * temporary view is tied to this Spark application. * * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, * i.e. it will be automatically dropped when the application terminates. It's tied to a system * preserved database `global_temp`, and we must use the qualified name to refer a global temp * view, e.g. `SELECT * FROM global_temp.view1`. * * @group basic * @since 2.2.0 */ def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan { createTempViewCommand(viewName, replace = true, global = true) } private def createTempViewCommand( viewName: String, replace: Boolean, global: Boolean): CreateViewCommand = { val viewType = if (global) GlobalTempView else LocalTempView val tableIdentifier = try { sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName) } catch { case _: ParseException => throw QueryCompilationErrors.invalidViewNameError(viewName) } CreateViewCommand( name = tableIdentifier, userSpecifiedColumns = Nil, comment = None, properties = Map.empty, originalText = None, plan = logicalPlan, allowExisting = false, replace = replace, viewType = viewType, isAnalyzed = true) } /** * Interface for saving the content of the non-streaming Dataset out into external storage. * * @group basic * @since 1.6.0 */ def write: DataFrameWriter[T] = { if (isStreaming) { logicalPlan.failAnalysis( \"'write' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriter[T](this) } /** * Create a write configuration builder for v2 sources. * * This builder is used to configure and execute write operations. For example, to append to an * existing table, run: * * {{{ * df.writeTo(\"catalog.db.table\").append() * }}} * * This can also be used to create or replace existing tables: * * {{{ * df.writeTo(\"catalog.db.table\").partitionedBy($\"col\").createOrReplace() * }}} * * @group basic * @since 3.0.0 */ def writeTo(table: String): DataFrameWriterV2[T] = { // TODO: streaming could be adapted to use this interface if (isStreaming) { logicalPlan.failAnalysis( \"'writeTo' can not be called on streaming Dataset/DataFrame\") } new DataFrameWriterV2[T](table, this) } /** * Interface for saving the content of the streaming Dataset out into external storage. * * @group basic * @since 2.0.0 */ def writeStream: DataStreamWriter[T] = { if (!isStreaming) { logicalPlan.failAnalysis( \"'writeStream' can be called only on streaming Dataset/DataFrame\") } new DataStreamWriter[T](this) } /** * Returns the content of the Dataset as a Dataset of JSON strings. * @since 2.0.0 */ def toJSON: Dataset[String] = { val rowSchema = this.schema val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone mapPartitions { iter => val writer = new CharArrayWriter() // create the Generator without separator inserted between 2 records val gen = new JacksonGenerator(rowSchema, writer, new JSONOptions(Map.empty[String, String], sessionLocalTimeZone)) new Iterator[String] { private val toRow = exprEnc.createSerializer() override def hasNext: Boolean = iter.hasNext override def next(): String = { gen.write(toRow(iter.next())) gen.flush() val json = writer.toString if (hasNext) { writer.reset() } else { gen.close() } json } } } (Encoders.STRING) } /** * Returns a best-effort snapshot of the files that compose this Dataset. This method simply * asks each constituent BaseRelation for its respective files and takes the union of all results. * Depending on the source relations, this may not find all input files. Duplicates are removed. * * @group basic * @since 2.0.0 */ def inputFiles: Array[String] = { val files: Seq[String] = queryExecution.optimizedPlan.collect { case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) => fsBasedRelation.inputFiles case fr: FileRelation => fr.inputFiles case r: HiveTableRelation => r.tableMeta.storage.locationUri.map(_.toString).toArray case DataSourceV2ScanRelation(DataSourceV2Relation(table: FileTable, _, _, _, _), _, _, _) => table.fileIndex.inputFiles }.flatten files.toSet.toArray } /** * Returns `true` when the logical query plans inside both [[Dataset]]s are equal and * therefore return same results. * * @note The equality comparison here is simplified by tolerating the cosmetic differences * such as attribute names. * @note This API can compare both [[Dataset]]s very fast but can still return `false` on * the [[Dataset]] that return the same results, for instance, from different plans. Such * false negative semantic can be useful when caching as an example. * @since 3.1.0 */ @DeveloperApi def sameSemantics(other: Dataset[T]): Boolean = { queryExecution.analyzed.sameResult(other.queryExecution.analyzed) } /** * Returns a `hashCode` of the logical query plan against this [[Dataset]]. * * @note Unlike the standard `hashCode`, the hash is calculated against the query plan * simplified by tolerating the cosmetic differences such as attribute names. * @since 3.1.0 */ @DeveloperApi def semanticHash(): Int = { queryExecution.analyzed.semanticHash() } //////////////////////////////////////////////////////////////////////////// // For Python API //////////////////////////////////////////////////////////////////////////// /** * It adds a new long column with the name `name` that increases one by one. * This is for 'distributed-sequence' default index in pandas API on Spark. */ private[sql] def withSequenceColumn(name: String) = { Dataset.ofRows( sparkSession, AttachDistributedSequence( AttributeReference(name, LongType, nullable = false)(), logicalPlan)) } /** * Converts a JavaRDD to a PythonRDD. */ private[sql] def javaToPython: JavaRDD[Array[Byte]] = { val structType = schema // capture it for closure val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType)) EvaluatePython.javaToPython(rdd) } private[sql] def collectToPython(): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"collectToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeCollect().iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def tailToPython(n: Int): Array[Any] = { EvaluatePython.registerPicklers() withAction(\"tailToPython\", queryExecution) { plan => val toJava: (Any) => Any = EvaluatePython.toJava(_, schema) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( plan.executeTail(n).iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-DataFrame\") } } private[sql] def getRowsToPython( _numRows: Int, truncate: Int): Array[Any] = { EvaluatePython.registerPicklers() val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) val rows = getRows(numRows, truncate).map(_.toArray).toArray val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType))) val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler( rows.iterator.map(toJava)) PythonRDD.serveIterator(iter, \"serve-GetRows\") } /** * Collect a Dataset as Arrow batches and serve stream to SparkR. It sends * arrow batches in an ordered manner with buffering. This is inevitable * due to missing R API that reads batches from socket directly. See ARROW-4512. * Eventually, this code should be deduplicated by `collectAsArrowToPython`. */ private[sql] def collectAsArrowToR(): Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone RRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToR\", queryExecution) { plan => val buffer = new ByteArrayOutputStream() val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, buffer, timeZoneId) val arrowBatchRdd = toArrowBatchRdd(plan) val numPartitions = arrowBatchRdd.partitions.length // Store collection results for worst case of 1 to N-1 partitions val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1)) var lastIndex = -1 // index of last partition written // Handler to eagerly write partitions to Python in order def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = { // If result is from next partition in order if (index - 1 == lastIndex) { batchWriter.writeBatches(arrowBatches.iterator) lastIndex += 1 // Write stored partitions that come next in order while (lastIndex < results.length && results(lastIndex) != null) { batchWriter.writeBatches(results(lastIndex).iterator) results(lastIndex) = null lastIndex += 1 } // After last batch, end the stream if (lastIndex == results.length) { batchWriter.end() val batches = buffer.toByteArray out.writeInt(batches.length) out.write(batches) } } else { // Store partitions received out of order results(index - 1) = arrowBatches } } sparkSession.sparkContext.runJob( arrowBatchRdd, (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray, 0 until numPartitions, handlePartitionBatches) } } } /** * Collect a Dataset as Arrow batches and serve stream to PySpark. It sends * arrow batches in an un-ordered manner without buffering, and then batch order * information at the end. The batches should be reordered at Python side. */ private[sql] def collectAsArrowToPython: Array[Any] = { val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone PythonRDD.serveToStream(\"serve-Arrow\") { outputStream => withAction(\"collectAsArrowToPython\", queryExecution) { plan => val out = new DataOutputStream(outputStream) val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId) // Batches ordered by (index of partition, batch index in that partition) tuple val batchOrder = ArrayBuffer.empty[(Int, Int)] // Handler to eagerly write batches to Python as they arrive, un-ordered val handlePartitionBatches = (index: Int, arrowBatches: Array[Array[Byte]]) => if (arrowBatches.nonEmpty) { // Write all batches (can be more than 1) in the partition, store the batch order tuple batchWriter.writeBatches(arrowBatches.iterator) arrowBatches.indices.foreach { partitionBatchIndex => batchOrder.append((index, partitionBatchIndex)) } } Utils.tryWithSafeFinally { val arrowBatchRdd = toArrowBatchRdd(plan) sparkSession.sparkContext.runJob( arrowBatchRdd, (it: Iterator[Array[Byte]]) => it.toArray, handlePartitionBatches) } { // After processing all partitions, end the batch stream batchWriter.end() // Write batch order indices out.writeInt(batchOrder.length) // Sort by (index of partition, batch index in that partition) tuple to get the // overall_batch_index from 0 to N-1 batches, which can be used to put the // transferred batches in the correct order batchOrder.zipWithIndex.sortBy(_._1).foreach { case (_, overallBatchIndex) => out.writeInt(overallBatchIndex) } } } } } private[sql] def toPythonIterator(prefetchPartitions: Boolean = false): Array[Any] = { withNewExecutionId { PythonRDD.toLocalIteratorAndServe(javaToPython.rdd, prefetchPartitions) } } //////////////////////////////////////////////////////////////////////////// // Private Helpers //////////////////////////////////////////////////////////////////////////// /** * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with * an execution. */ private def withNewExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(queryExecution)(body) } /** * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect * them with an execution. Before performing the action, the metrics of the executed plan will be * reset. */ private def withNewRDDExecutionId[U](body: => U): U = { SQLExecution.withNewExecutionId(rddQueryExecution) { rddQueryExecution.executedPlan.resetMetrics() body } } /** * Wrap a Dataset action to track the QueryExecution and time cost, then report to the * user-registered callback functions, and also to convert asserts/NPE to * the internal error exception. */ private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = { SQLExecution.withNewExecutionId(qe, Some(name)) { QueryExecution.withInternalError(s\"\"\"The \"$name\" action failed.\"\"\") { qe.executedPlan.resetMetrics() action(qe.executedPlan) } } } /** * Collect all elements from a spark plan. */ private def collectFromPlan(plan: SparkPlan): Array[T] = { val fromRow = resolvedEnc.createDeserializer() plan.executeCollect().map(fromRow) } private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = { val sortOrder: Seq[SortOrder] = sortExprs.map { col => col.expr match { case expr: SortOrder => expr case expr: Expression => SortOrder(expr, Ascending) } } withTypedPlan { Sort(sortOrder, global = global, logicalPlan) } } /** A convenient function to wrap a logical plan and produce a DataFrame. */ @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = { Dataset.ofRows(sparkSession, logicalPlan) } /** A convenient function to wrap a logical plan and produce a Dataset. */ @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { Dataset(sparkSession, logicalPlan) } /** A convenient function to wrap a set based logical plan and produce a Dataset. */ @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = { if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) { // Set operators widen types (change the schema), so we cannot reuse the row encoder. Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]] } else { Dataset(sparkSession, logicalPlan) } } /** Convert to an RDD of serialized ArrowRecordBatches. */ private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = { val schemaCaptured = this.schema val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone plan.execute().mapPartitionsInternal { iter => val context = TaskContext.get() ArrowConverters.toBatchIterator( iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context) } } // This is only used in tests, for now. private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = { toArrowBatchRdd(queryExecution.executedPlan) } }",
            "## CLASS: org/apache/spark/sql/SparkSession# (implementation)\n@Stable class SparkSession private( @transient val sparkContext: SparkContext, @transient private val existingSharedState: Option[SharedState], @transient private val parentSessionState: Option[SessionState], @transient private[sql] val extensions: SparkSessionExtensions, @transient private[sql] val initialSessionOptions: Map[String, String]) extends Serializable with Closeable with Logging { self => // The call site where this SparkSession was constructed. private val creationSite: CallSite = Utils.getCallSite() /** * Constructor used in Pyspark. Contains explicit application of Spark Session Extensions * which otherwise only occurs during getOrCreate. We cannot add this to the default constructor * since that would cause every new session to reinvoke Spark Session Extensions on the currently * running extensions. */ private[sql] def this( sc: SparkContext, initialSessionOptions: java.util.HashMap[String, String]) = { this(sc, None, None, SparkSession.applyExtensions( sc.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty), new SparkSessionExtensions), initialSessionOptions.asScala.toMap) } private[sql] def this(sc: SparkContext) = this(sc, new java.util.HashMap[String, String]()) private[sql] val sessionUUID: String = UUID.randomUUID.toString sparkContext.assertNotStopped() // If there is no active SparkSession, uses the default SQL conf. Otherwise, use the session's. SQLConf.setSQLConfGetter(() => { SparkSession.getActiveSession.filterNot(_.sparkContext.isStopped).map(_.sessionState.conf) .getOrElse(SQLConf.getFallbackConf) }) /** * The version of Spark on which this application is running. * * @since 2.0.0 */ def version: String = SPARK_VERSION /* ----------------------- * | Session-related state | * ----------------------- */ /** * State shared across sessions, including the `SparkContext`, cached data, listener, * and a catalog that interacts with external systems. * * This is internal to Spark and there is no guarantee on interface stability. * * @since 2.2.0 */ @Unstable @transient lazy val sharedState: SharedState = { existingSharedState.getOrElse(new SharedState(sparkContext, initialSessionOptions)) } /** * State isolated across sessions, including SQL configurations, temporary tables, registered * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]]. * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent. * * This is internal to Spark and there is no guarantee on interface stability. * * @since 2.2.0 */ @Unstable @transient lazy val sessionState: SessionState = { parentSessionState .map(_.clone(this)) .getOrElse { val state = SparkSession.instantiateSessionState( SparkSession.sessionStateClassName(sharedState.conf), self) state } } /** * A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility. * * @since 2.0.0 */ @transient val sqlContext: SQLContext = new SQLContext(this) /** * Runtime configuration interface for Spark. * * This is the interface through which the user can get and set all Spark and Hadoop * configurations that are relevant to Spark SQL. When getting the value of a config, * this defaults to the value set in the underlying `SparkContext`, if any. * * @since 2.0.0 */ @transient lazy val conf: RuntimeConfig = new RuntimeConfig(sessionState.conf) /** * An interface to register custom [[org.apache.spark.sql.util.QueryExecutionListener]]s * that listen for execution metrics. * * @since 2.0.0 */ def listenerManager: ExecutionListenerManager = sessionState.listenerManager /** * :: Experimental :: * A collection of methods that are considered experimental, but can be used to hook into * the query planner for advanced functionality. * * @since 2.0.0 */ @Experimental @Unstable def experimental: ExperimentalMethods = sessionState.experimentalMethods /** * A collection of methods for registering user-defined functions (UDF). * * The following example registers a Scala closure as UDF: * {{{ * sparkSession.udf.register(\"myUDF\", (arg1: Int, arg2: String) => arg2 + arg1) * }}} * * The following example registers a UDF in Java: * {{{ * sparkSession.udf().register(\"myUDF\", * (Integer arg1, String arg2) -> arg2 + arg1, * DataTypes.StringType); * }}} * * @note The user-defined functions must be deterministic. Due to optimization, * duplicate invocations may be eliminated or the function may even be invoked more times than * it is present in the query. * * @since 2.0.0 */ def udf: UDFRegistration = sessionState.udfRegistration /** * Returns a `StreamingQueryManager` that allows managing all the * `StreamingQuery`s active on `this`. * * @since 2.0.0 */ @Unstable def streams: StreamingQueryManager = sessionState.streamingQueryManager /** * Start a new session with isolated SQL configurations, temporary tables, registered * functions are isolated, but sharing the underlying `SparkContext` and cached data. * * @note Other than the `SparkContext`, all shared state is initialized lazily. * This method will force the initialization of the shared state to ensure that parent * and child sessions are set up with the same shared state. If the underlying catalog * implementation is Hive, this will initialize the metastore, which may take some time. * * @since 2.0.0 */ def newSession(): SparkSession = { new SparkSession( sparkContext, Some(sharedState), parentSessionState = None, extensions, initialSessionOptions) } /** * Create an identical copy of this `SparkSession`, sharing the underlying `SparkContext` * and shared state. All the state of this session (i.e. SQL configurations, temporary tables, * registered functions) is copied over, and the cloned session is set up with the same shared * state as this session. The cloned session is independent of this session, that is, any * non-global change in either session is not reflected in the other. * * @note Other than the `SparkContext`, all shared state is initialized lazily. * This method will force the initialization of the shared state to ensure that parent * and child sessions are set up with the same shared state. If the underlying catalog * implementation is Hive, this will initialize the metastore, which may take some time. */ private[sql] def cloneSession(): SparkSession = { val result = new SparkSession( sparkContext, Some(sharedState), Some(sessionState), extensions, Map.empty) result.sessionState // force copy of SessionState result } /* --------------------------------- * | Methods for creating DataFrames | * --------------------------------- */ /** * Returns a `DataFrame` with no rows or columns. * * @since 2.0.0 */ @transient lazy val emptyDataFrame: DataFrame = Dataset.ofRows(self, LocalRelation()) /** * Creates a new [[Dataset]] of type T containing zero elements. * * @return 2.0.0 */ def emptyDataset[T: Encoder]: Dataset[T] = { val encoder = implicitly[Encoder[T]] new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder) } /** * Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples). * * @since 2.0.0 */ def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = withActive { val encoder = Encoders.product[A] Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder)) } /** * Creates a `DataFrame` from a local Seq of Product. * * @since 2.0.0 */ def createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame = withActive { val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType] val attributeSeq = schema.toAttributes Dataset.ofRows(self, LocalRelation.fromProduct(attributeSeq, data)) } /** * :: DeveloperApi :: * Creates a `DataFrame` from an `RDD` containing [[Row]]s using the given schema. * It is important to make sure that the structure of every [[Row]] of the provided RDD matches * the provided schema. Otherwise, there will be runtime exception. * Example: * {{{ * import org.apache.spark.sql._ * import org.apache.spark.sql.types._ * val sparkSession = new org.apache.spark.sql.SparkSession(sc) * * val schema = * StructType( * StructField(\"name\", StringType, false) :: * StructField(\"age\", IntegerType, true) :: Nil) * * val people = * sc.textFile(\"examples/src/main/resources/people.txt\").map( * _.split(\",\")).map(p => Row(p(0), p(1).trim.toInt)) * val dataFrame = sparkSession.createDataFrame(people, schema) * dataFrame.printSchema * // root * // |-- name: string (nullable = false) * // |-- age: integer (nullable = true) * * dataFrame.createOrReplaceTempView(\"people\") * sparkSession.sql(\"select name from people\").collect.foreach(println) * }}} * * @since 2.0.0 */ @DeveloperApi def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame = withActive { val replaced = CharVarcharUtils.failIfHasCharVarchar(schema).asInstanceOf[StructType] // TODO: use MutableProjection when rowRDD is another DataFrame and the applied // schema differs from the existing schema on any field data type. val encoder = RowEncoder(replaced) val toRow = encoder.createSerializer() val catalystRows = rowRDD.map(toRow) internalCreateDataFrame(catalystRows.setName(rowRDD.name), schema) } /** * :: DeveloperApi :: * Creates a `DataFrame` from a `JavaRDD` containing [[Row]]s using the given schema. * It is important to make sure that the structure of every [[Row]] of the provided RDD matches * the provided schema. Otherwise, there will be runtime exception. * * @since 2.0.0 */ @DeveloperApi def createDataFrame(rowRDD: JavaRDD[Row], schema: StructType): DataFrame = { val replaced = CharVarcharUtils.failIfHasCharVarchar(schema).asInstanceOf[StructType] createDataFrame(rowRDD.rdd, replaced) } /** * :: DeveloperApi :: * Creates a `DataFrame` from a `java.util.List` containing [[Row]]s using the given schema. * It is important to make sure that the structure of every [[Row]] of the provided List matches * the provided schema. Otherwise, there will be runtime exception. * * @since 2.0.0 */ @DeveloperApi def createDataFrame(rows: java.util.List[Row], schema: StructType): DataFrame = withActive { val replaced = CharVarcharUtils.failIfHasCharVarchar(schema).asInstanceOf[StructType] Dataset.ofRows(self, LocalRelation.fromExternalRows(replaced.toAttributes, rows.asScala.toSeq)) } /** * Applies a schema to an RDD of Java Beans. * * WARNING: Since there is no guaranteed ordering for fields in a Java Bean, * SELECT * queries will return the columns in an undefined order. * * @since 2.0.0 */ def createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame = withActive { val attributeSeq: Seq[AttributeReference] = getSchema(beanClass) val className = beanClass.getName val rowRdd = rdd.mapPartitions { iter => // BeanInfo is not serializable so we must rediscover it remotely for each partition. SQLContext.beansToRows(iter, Utils.classForName(className), attributeSeq) } Dataset.ofRows(self, LogicalRDD(attributeSeq, rowRdd.setName(rdd.name))(self)) } /** * Applies a schema to an RDD of Java Beans. * * WARNING: Since there is no guaranteed ordering for fields in a Java Bean, * SELECT * queries will return the columns in an undefined order. * * @since 2.0.0 */ def createDataFrame(rdd: JavaRDD[_], beanClass: Class[_]): DataFrame = { createDataFrame(rdd.rdd, beanClass) } /** * Applies a schema to a List of Java Beans. * * WARNING: Since there is no guaranteed ordering for fields in a Java Bean, * SELECT * queries will return the columns in an undefined order. * @since 1.6.0 */ def createDataFrame(data: java.util.List[_], beanClass: Class[_]): DataFrame = withActive { val attrSeq = getSchema(beanClass) val rows = SQLContext.beansToRows(data.asScala.iterator, beanClass, attrSeq) Dataset.ofRows(self, LocalRelation(attrSeq, rows.toSeq)) } /** * Convert a `BaseRelation` created for external data sources into a `DataFrame`. * * @since 2.0.0 */ def baseRelationToDataFrame(baseRelation: BaseRelation): DataFrame = { Dataset.ofRows(self, LogicalRelation(baseRelation)) } /* ------------------------------- * | Methods for creating DataSets | * ------------------------------- */ /** * Creates a [[Dataset]] from a local Seq of data of a given type. This method requires an * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation) * that is generally created automatically through implicits from a `SparkSession`, or can be * created explicitly by calling static methods on [[Encoders]]. * * == Example == * * {{{ * * import spark.implicits._ * case class Person(name: String, age: Long) * val data = Seq(Person(\"Michael\", 29), Person(\"Andy\", 30), Person(\"Justin\", 19)) * val ds = spark.createDataset(data) * * ds.show() * // +-------+---+ * // | name|age| * // +-------+---+ * // |Michael| 29| * // | Andy| 30| * // | Justin| 19| * // +-------+---+ * }}} * * @since 2.0.0 */ def createDataset[T : Encoder](data: Seq[T]): Dataset[T] = { val enc = encoderFor[T] val toRow = enc.createSerializer() val attributes = enc.schema.toAttributes val encoded = data.map(d => toRow(d).copy()) val plan = new LocalRelation(attributes, encoded) Dataset[T](self, plan) } /** * Creates a [[Dataset]] from an RDD of a given type. This method requires an * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation) * that is generally created automatically through implicits from a `SparkSession`, or can be * created explicitly by calling static methods on [[Encoders]]. * * @since 2.0.0 */ def createDataset[T : Encoder](data: RDD[T]): Dataset[T] = { Dataset[T](self, ExternalRDD(data, self)) } /** * Creates a [[Dataset]] from a `java.util.List` of a given type. This method requires an * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation) * that is generally created automatically through implicits from a `SparkSession`, or can be * created explicitly by calling static methods on [[Encoders]]. * * == Java Example == * * {{{ * List<String> data = Arrays.asList(\"hello\", \"world\"); * Dataset<String> ds = spark.createDataset(data, Encoders.STRING()); * }}} * * @since 2.0.0 */ def createDataset[T : Encoder](data: java.util.List[T]): Dataset[T] = { createDataset(data.asScala.toSeq) } /** * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements * in a range from 0 to `end` (exclusive) with step value 1. * * @since 2.0.0 */ def range(end: Long): Dataset[java.lang.Long] = range(0, end) /** * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements * in a range from `start` to `end` (exclusive) with step value 1. * * @since 2.0.0 */ def range(start: Long, end: Long): Dataset[java.lang.Long] = { range(start, end, step = 1, numPartitions = leafNodeDefaultParallelism) } /** * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements * in a range from `start` to `end` (exclusive) with a step value. * * @since 2.0.0 */ def range(start: Long, end: Long, step: Long): Dataset[java.lang.Long] = { range(start, end, step, numPartitions = leafNodeDefaultParallelism) } /** * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements * in a range from `start` to `end` (exclusive) with a step value, with partition number * specified. * * @since 2.0.0 */ def range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long] = { new Dataset(self, Range(start, end, step, numPartitions), Encoders.LONG) } /** * Creates a `DataFrame` from an `RDD[InternalRow]`. */ private[sql] def internalCreateDataFrame( catalystRows: RDD[InternalRow], schema: StructType, isStreaming: Boolean = false): DataFrame = { // TODO: use MutableProjection when rowRDD is another DataFrame and the applied // schema differs from the existing schema on any field data type. val logicalPlan = LogicalRDD( schema.toAttributes, catalystRows, isStreaming = isStreaming)(self) Dataset.ofRows(self, logicalPlan) } /* ------------------------- * | Catalog-related methods | * ------------------------- */ /** * Interface through which the user may create, drop, alter or query underlying * databases, tables, functions etc. * * @since 2.0.0 */ @transient lazy val catalog: Catalog = new CatalogImpl(self) /** * Returns the specified table/view as a `DataFrame`. If it's a table, it must support batch * reading and the returned DataFrame is the batch scan query plan of this table. If it's a view, * the returned DataFrame is simply the query plan of the view, which can either be a batch or * streaming query plan. * * @param tableName is either a qualified or unqualified name that designates a table or view. * If a database is specified, it identifies the table/view from the database. * Otherwise, it first attempts to find a temporary view with the given name * and then match the table/view from the current database. * Note that, the global temporary view database is also valid here. * @since 2.0.0 */ def table(tableName: String): DataFrame = { read.table(tableName) } private[sql] def table(tableIdent: TableIdentifier): DataFrame = { Dataset.ofRows(self, UnresolvedRelation(tableIdent)) } /* ----------------- * | Everything else | * ----------------- */ /** * Executes a SQL query using Spark, returning the result as a `DataFrame`. * This API eagerly runs DDL/DML commands, but not for SELECT queries. * * @since 2.0.0 */ def sql(sqlText: String): DataFrame = withActive { val tracker = new QueryPlanningTracker val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) { sessionState.sqlParser.parsePlan(sqlText) } Dataset.ofRows(self, plan, tracker) } /** * Execute an arbitrary string command inside an external execution engine rather than Spark. * This could be useful when user wants to execute some commands out of Spark. For * example, executing custom DDL/DML command for JDBC, creating index for ElasticSearch, * creating cores for Solr and so on. * * The command will be eagerly executed after this method is called and the returned * DataFrame will contain the output of the command(if any). * * @param runner The class name of the runner that implements `ExternalCommandRunner`. * @param command The target command to be executed * @param options The options for the runner. * * @since 3.0.0 */ @Unstable def executeCommand(runner: String, command: String, options: Map[String, String]): DataFrame = { DataSource.lookupDataSource(runner, sessionState.conf) match { case source if classOf[ExternalCommandRunner].isAssignableFrom(source) => Dataset.ofRows(self, ExternalCommandExecutor( source.newInstance().asInstanceOf[ExternalCommandRunner], command, options)) case _ => throw QueryCompilationErrors.commandExecutionInRunnerUnsupportedError(runner) } } /** * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a * `DataFrame`. * {{{ * sparkSession.read.parquet(\"/path/to/file.parquet\") * sparkSession.read.schema(schema).json(\"/path/to/file.json\") * }}} * * @since 2.0.0 */ def read: DataFrameReader = new DataFrameReader(self) /** * Returns a `DataStreamReader` that can be used to read streaming data in as a `DataFrame`. * {{{ * sparkSession.readStream.parquet(\"/path/to/directory/of/parquet/files\") * sparkSession.readStream.schema(schema).json(\"/path/to/directory/of/json/files\") * }}} * * @since 2.0.0 */ def readStream: DataStreamReader = new DataStreamReader(self) /** * Executes some code block and prints to stdout the time taken to execute the block. This is * available in Scala only and is used primarily for interactive testing and debugging. * * @since 2.1.0 */ def time[T](f: => T): T = { val start = System.nanoTime() val ret = f val end = System.nanoTime() // scalastyle:off println println(s\"Time taken: ${NANOSECONDS.toMillis(end - start)} ms\") // scalastyle:on println ret } // scalastyle:off // Disable style checker so \"implicits\" object can start with lowercase i /** * (Scala-specific) Implicit methods available in Scala for converting * common Scala objects into `DataFrame`s. * * {{{ * val sparkSession = SparkSession.builder.getOrCreate() * import sparkSession.implicits._ * }}} * * @since 2.0.0 */ object implicits extends SQLImplicits with Serializable { protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext } // scalastyle:on /** * Stop the underlying `SparkContext`. * * @since 2.0.0 */ def stop(): Unit = { sparkContext.stop() } /** * Synonym for `stop()`. * * @since 2.1.0 */ override def close(): Unit = stop() /** * Parses the data type in our internal string representation. The data type string should * have the same format as the one generated by `toString` in scala. * It is only used by PySpark. */ protected[sql] def parseDataType(dataTypeString: String): DataType = { DataType.fromJson(dataTypeString) } /** * Apply a schema defined by the schemaString to an RDD. It is only used by PySpark. */ private[sql] def applySchemaToPythonRDD( rdd: RDD[Array[Any]], schemaString: String): DataFrame = { val schema = DataType.fromJson(schemaString).asInstanceOf[StructType] applySchemaToPythonRDD(rdd, schema) } /** * Apply `schema` to an RDD. * * @note Used by PySpark only */ private[sql] def applySchemaToPythonRDD( rdd: RDD[Array[Any]], schema: StructType): DataFrame = { val rowRdd = rdd.mapPartitions { iter => val fromJava = python.EvaluatePython.makeFromJava(schema) iter.map(r => fromJava(r).asInstanceOf[InternalRow]) } internalCreateDataFrame(rowRdd, schema) } /** * Returns a Catalyst Schema for the given java bean class. */ private def getSchema(beanClass: Class[_]): Seq[AttributeReference] = { val (dataType, _) = JavaTypeInference.inferDataType(beanClass) dataType.asInstanceOf[StructType].fields.map { f => AttributeReference(f.name, f.dataType, f.nullable)() } } /** * Execute a block of code with the this session set as the active session, and restore the * previous session on completion. */ private[sql] def withActive[T](block: => T): T = { // Use the active session thread local directly to make sure we get the session that is actually // set and not the default session. This to prevent that we promote the default session to the // active session once we are done. val old = SparkSession.activeThreadSession.get() SparkSession.setActiveSession(this) try block finally { SparkSession.setActiveSession(old) } } private[sql] def leafNodeDefaultParallelism: Int = { conf.get(SQLConf.LEAF_NODE_DEFAULT_PARALLELISM).getOrElse(sparkContext.defaultParallelism) } } @Stable object SparkSession extends Logging { /** * Builder for [[SparkSession]]. */ @Stable class Builder extends Logging { private[this] val options = new scala.collection.mutable.HashMap[String, String] private[this] val extensions = new SparkSessionExtensions private[this] var userSuppliedContext: Option[SparkContext] = None private[spark] def sparkContext(sparkContext: SparkContext): Builder = synchronized { userSuppliedContext = Option(sparkContext) this } /** * Sets a name for the application, which will be shown in the Spark web UI. * If no application name is set, a randomly generated name will be used. * * @since 2.0.0 */ def appName(name: String): Builder = config(\"spark.app.name\", name) /** * Sets a config option. Options set using this method are automatically propagated to * both `SparkConf` and SparkSession's own configuration. * * @since 2.0.0 */ def config(key: String, value: String): Builder = synchronized { options += key -> value this } /** * Sets a config option. Options set using this method are automatically propagated to * both `SparkConf` and SparkSession's own configuration. * * @since 2.0.0 */ def config(key: String, value: Long): Builder = synchronized { options += key -> value.toString this } /** * Sets a config option. Options set using this method are automatically propagated to * both `SparkConf` and SparkSession's own configuration. * * @since 2.0.0 */ def config(key: String, value: Double): Builder = synchronized { options += key -> value.toString this } /** * Sets a config option. Options set using this method are automatically propagated to * both `SparkConf` and SparkSession's own configuration. * * @since 2.0.0 */ def config(key: String, value: Boolean): Builder = synchronized { options += key -> value.toString this } /** * Sets a list of config options based on the given `SparkConf`. * * @since 2.0.0 */ def config(conf: SparkConf): Builder = synchronized { conf.getAll.foreach { case (k, v) => options += k -> v } this } /** * Sets the Spark master URL to connect to, such as \"local\" to run locally, \"local[4]\" to * run locally with 4 cores, or \"spark://master:7077\" to run on a Spark standalone cluster. * * @since 2.0.0 */ def master(master: String): Builder = config(\"spark.master\", master) /** * Enables Hive support, including connectivity to a persistent Hive metastore, support for * Hive serdes, and Hive user-defined functions. * * @since 2.0.0 */ def enableHiveSupport(): Builder = synchronized { if (hiveClassesArePresent) { config(CATALOG_IMPLEMENTATION.key, \"hive\") } else { throw new IllegalArgumentException( \"Unable to instantiate SparkSession with Hive support because \" + \"Hive classes are not found.\") } } /** * Inject extensions into the [[SparkSession]]. This allows a user to add Analyzer rules, * Optimizer rules, Planning Strategies or a customized parser. * * @since 2.2.0 */ def withExtensions(f: SparkSessionExtensions => Unit): Builder = synchronized { f(extensions) this } /** * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new * one based on the options set in this builder. * * This method first checks whether there is a valid thread-local SparkSession, * and if yes, return that one. It then checks whether there is a valid global * default SparkSession, and if yes, return that one. If no valid global default * SparkSession exists, the method creates a new SparkSession and assigns the * newly created SparkSession as the global default. * * In case an existing SparkSession is returned, the non-static config options specified in * this builder will be applied to the existing SparkSession. * * @since 2.0.0 */ def getOrCreate(): SparkSession = synchronized { val sparkConf = new SparkConf() options.foreach { case (k, v) => sparkConf.set(k, v) } if (!sparkConf.get(EXECUTOR_ALLOW_SPARK_CONTEXT)) { assertOnDriver() } // Get the session from current thread's active session. var session = activeThreadSession.get() if ((session ne null) && !session.sparkContext.isStopped) { applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava)) return session } // Global synchronization so we will only set the default session once. SparkSession.synchronized { // If the current thread does not have an active session, get it from the global session. session = defaultSession.get() if ((session ne null) && !session.sparkContext.isStopped) { applyModifiableSettings(session, new java.util.HashMap[String, String](options.asJava)) return session } // No active nor global default session. Create a new one. val sparkContext = userSuppliedContext.getOrElse { // set a random app name if not given. if (!sparkConf.contains(\"spark.app.name\")) { sparkConf.setAppName(java.util.UUID.randomUUID().toString) } SparkContext.getOrCreate(sparkConf) // Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions. } loadExtensions(extensions) applyExtensions( sparkContext.getConf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS).getOrElse(Seq.empty), extensions) session = new SparkSession(sparkContext, None, None, extensions, options.toMap) setDefaultSession(session) setActiveSession(session) registerContextListener(sparkContext) } return session } } /** * Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]]. * * @since 2.0.0 */ def builder(): Builder = new Builder /** * Changes the SparkSession that will be returned in this thread and its children when * SparkSession.getOrCreate() is called. This can be used to ensure that a given thread receives * a SparkSession with an isolated session, instead of the global (first created) context. * * @since 2.0.0 */ def setActiveSession(session: SparkSession): Unit = { activeThreadSession.set(session) } /** * Clears the active SparkSession for current thread. Subsequent calls to getOrCreate will * return the first created context instead of a thread-local override. * * @since 2.0.0 */ def clearActiveSession(): Unit = { activeThreadSession.remove() } /** * Sets the default SparkSession that is returned by the builder. * * @since 2.0.0 */ def setDefaultSession(session: SparkSession): Unit = { defaultSession.set(session) } /** * Clears the default SparkSession that is returned by the builder. * * @since 2.0.0 */ def clearDefaultSession(): Unit = { defaultSession.set(null) } /** * Returns the active SparkSession for the current thread, returned by the builder. * * @note Return None, when calling this function on executors * * @since 2.2.0 */ def getActiveSession: Option[SparkSession] = { if (Utils.isInRunningSparkTask) { // Return None when running on executors. None } else { Option(activeThreadSession.get) } } /** * Returns the default SparkSession that is returned by the builder. * * @note Return None, when calling this function on executors * * @since 2.2.0 */ def getDefaultSession: Option[SparkSession] = { if (Utils.isInRunningSparkTask) { // Return None when running on executors. None } else { Option(defaultSession.get) } } /** * Returns the currently active SparkSession, otherwise the default one. If there is no default * SparkSession, throws an exception. * * @since 2.4.0 */ def active: SparkSession = { getActiveSession.getOrElse(getDefaultSession.getOrElse( throw new IllegalStateException(\"No active or default Spark session found\"))) } /** * Apply modifiable settings to an existing [[SparkSession]]. This method are used * both in Scala and Python, so put this under [[SparkSession]] object. */ private[sql] def applyModifiableSettings( session: SparkSession, options: java.util.HashMap[String, String]): Unit = { // Lazy val to avoid an unnecessary session state initialization lazy val conf = session.sessionState.conf val dedupOptions = if (options.isEmpty) Map.empty[String, String] else ( options.asScala.toSet -- conf.getAllConfs.toSet).toMap val (staticConfs, otherConfs) = dedupOptions.partition(kv => SQLConf.isStaticConfigKey(kv._1)) otherConfs.foreach { case (k, v) => conf.setConfString(k, v) } // Note that other runtime SQL options, for example, for other third-party datasource // can be marked as an ignored configuration here. val maybeIgnoredConfs = otherConfs.filterNot { case (k, _) => conf.isModifiable(k) } if (staticConfs.nonEmpty || maybeIgnoredConfs.nonEmpty) { logWarning( \"Using an existing Spark session; only runtime SQL configurations will take effect.\") } if (staticConfs.nonEmpty) { logDebug(\"Ignored static SQL configurations:\\n \" + conf.redactOptions(staticConfs).toSeq.map { case (k, v) => s\"$k=$v\" }.mkString(\"\\n \")) } if (maybeIgnoredConfs.nonEmpty) { // Only print out non-static and non-runtime SQL configurations. // Note that this might show core configurations or source specific // options defined in the third-party datasource. logDebug(\"Configurations that might not take effect:\\n \" + conf.redactOptions( maybeIgnoredConfs).toSeq.map { case (k, v) => s\"$k=$v\" }.mkString(\"\\n \")) } } /** * Returns a cloned SparkSession with all specified configurations disabled, or * the original SparkSession if all configurations are already disabled. */ private[sql] def getOrCloneSessionWithConfigsOff( session: SparkSession, configurations: Seq[ConfigEntry[Boolean]]): SparkSession = { val configsEnabled = configurations.filter(session.sessionState.conf.getConf(_)) if (configsEnabled.isEmpty) { session } else { val newSession = session.cloneSession() configsEnabled.foreach(conf => { newSession.sessionState.conf.setConf(conf, false) }) newSession } } //////////////////////////////////////////////////////////////////////////////////////// // Private methods from now on //////////////////////////////////////////////////////////////////////////////////////// private val listenerRegistered: AtomicBoolean = new AtomicBoolean(false) /** Register the AppEnd listener onto the Context */ private def registerContextListener(sparkContext: SparkContext): Unit = { if (!listenerRegistered.get()) { sparkContext.addSparkListener(new SparkListener { override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = { defaultSession.set(null) listenerRegistered.set(false) } }) listenerRegistered.set(true) } } /** The active SparkSession for the current thread. */ private val activeThreadSession = new InheritableThreadLocal[SparkSession] /** Reference to the root SparkSession. */ private val defaultSession = new AtomicReference[SparkSession] private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME = \"org.apache.spark.sql.hive.HiveSessionStateBuilder\" private def sessionStateClassName(conf: SparkConf): String = { conf.get(CATALOG_IMPLEMENTATION) match { case \"hive\" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME case \"in-memory\" => classOf[SessionStateBuilder].getCanonicalName } } private def assertOnDriver(): Unit = { if (TaskContext.get != null) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkSession should only be created and accessed on the driver.\") } } /** * Helper method to create an instance of `SessionState` based on `className` from conf. * The result is either `SessionState` or a Hive based `SessionState`. */ private def instantiateSessionState( className: String, sparkSession: SparkSession): SessionState = { try { // invoke new [Hive]SessionStateBuilder( // SparkSession, // Option[SessionState]) val clazz = Utils.classForName(className) val ctor = clazz.getConstructors.head ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build() } catch { case NonFatal(e) => throw new IllegalArgumentException(s\"Error while instantiating '$className':\", e) } } /** * @return true if Hive classes can be loaded, otherwise false. */ private[spark] def hiveClassesArePresent: Boolean = { try { Utils.classForName(HIVE_SESSION_STATE_BUILDER_CLASS_NAME) Utils.classForName(\"org.apache.hadoop.hive.conf.HiveConf\") true } catch { case _: ClassNotFoundException | _: NoClassDefFoundError => false } } private[spark] def cleanupAnyExistingSession(): Unit = { val session = getActiveSession.orElse(getDefaultSession) if (session.isDefined) { logWarning( s\"\"\"An existing Spark session exists as the active or default session. |This probably means another suite leaked it. Attempting to stop it before continuing. |This existing Spark session was created at: | |${session.get.creationSite.longForm} | \"\"\".stripMargin) session.get.stop() SparkSession.clearActiveSession() SparkSession.clearDefaultSession() } } /** * Initialize extensions for given extension classnames. The classes will be applied to the * extensions passed into this function. */ private def applyExtensions( extensionConfClassNames: Seq[String], extensions: SparkSessionExtensions): SparkSessionExtensions = { extensionConfClassNames.foreach { extensionConfClassName => try { val extensionConfClass = Utils.classForName(extensionConfClassName) val extensionConf = extensionConfClass.getConstructor().newInstance() .asInstanceOf[SparkSessionExtensions => Unit] extensionConf(extensions) } catch { // Ignore the error if we cannot find the class or when the class has the wrong type. case e@(_: ClassCastException | _: ClassNotFoundException | _: NoClassDefFoundError) => logWarning(s\"Cannot use $extensionConfClassName to configure session extensions.\", e) } } extensions } /** * Load extensions from [[ServiceLoader]] and use them */ private def loadExtensions(extensions: SparkSessionExtensions): Unit = { val loader = ServiceLoader.load(classOf[SparkSessionExtensionsProvider], Utils.getContextOrSparkClassLoader) val loadedExts = loader.iterator() while (loadedExts.hasNext) { try { val ext = loadedExts.next() ext(extensions) } catch { case e: Throwable => logWarning(\"Failed to load session extension\", e) } } } }",
            "## CLASS: org/apache/spark/rdd/RDD# (implementation)\n*/ abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent */ def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition. */ @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }` */ protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it. */ protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences. */ protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned. */ @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD. */ def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext). */ val id: Int = sc.newRddId() /** A friendly name for this RDD */ @transient var name: String = _ /** Assign a name to this RDD */ def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one */ private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception. */ def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD. */ def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set. */ def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver). */ private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed */ private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not. */ final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing. */ final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not. */ final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD. */ @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed. */ final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD. */ final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned. */ private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing. */ private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached. */ private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body. */ private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD. */ def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */ def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate. */ def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle. */ def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable. */ def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition. */ val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]]. */ def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array */ def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement. */ private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function. */ def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. */ def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD */ def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD */ def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array. */ def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. */ def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process. */ def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process. */ def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD */ def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive. */ private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning. */ private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive. */ private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other). */ def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition. */ def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD. */ def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD. */ def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first. */ def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`. */ def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us. */ def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator. */ def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]] */ def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions */ def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions */ def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2) */ def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor */ def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD. */ def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds */ def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map. */ def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds */ def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped. */ def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. */ def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file. */ def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file. */ def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`. */ def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD. */ def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements */ def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD * */ def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD * */ def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition. */ def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements. */ def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements. */ def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects. */ def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`. */ def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition */ private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation. */ def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used. */ def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. */ def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing. */ private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing. */ private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized. */ private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally. */ def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned */ @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents. */ def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a> */ @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD. */ @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified */ @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`). */ @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations. */ @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD */ protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T] */ protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on. */ def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility. */ private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility. */ private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs. */ private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions. */ private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example. */ protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging. */ def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier. */ private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level. */ // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`. */ object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE. */ private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
            "## CLASS: org/apache/spark/api/java/JavaPairRDD# (implementation)\nclass JavaPairRDD[K, V](val rdd: RDD[(K, V)]) (implicit val kClassTag: ClassTag[K], implicit val vClassTag: ClassTag[V]) extends AbstractJavaRDDLike[(K, V), JavaPairRDD[K, V]] { override def wrapRDD(rdd: RDD[(K, V)]): JavaPairRDD[K, V] = JavaPairRDD.fromRDD(rdd) override val classTag: ClassTag[(K, V)] = rdd.elementClassTag import JavaPairRDD._ // Common RDD functions /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def cache(): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.cache()) /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. Can only be called once on each RDD. */ def persist(newLevel: StorageLevel): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.persist(newLevel)) /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * This method blocks until all blocks are deleted. */ def unpersist(): JavaPairRDD[K, V] = wrapRDD(rdd.unpersist()) /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted. */ def unpersist(blocking: Boolean): JavaPairRDD[K, V] = wrapRDD(rdd.unpersist(blocking)) // Transformations (return a new RDD) /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.distinct()) /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(numPartitions: Int): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.distinct(numPartitions)) /** * Return a new RDD containing only the elements that satisfy a predicate. */ def filter(f: JFunction[(K, V), java.lang.Boolean]): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.filter(x => f.call(x).booleanValue())) /** * Return a new RDD that is reduced into `numPartitions` partitions. */ def coalesce(numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.coalesce(numPartitions)) /** * Return a new RDD that is reduced into `numPartitions` partitions. */ def coalesce(numPartitions: Int, shuffle: Boolean): JavaPairRDD[K, V] = fromRDD(rdd.coalesce(numPartitions, shuffle)) /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle. */ def repartition(numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.repartition(numPartitions)) /** * Return a sampled subset of this RDD. */ def sample(withReplacement: Boolean, fraction: Double): JavaPairRDD[K, V] = sample(withReplacement, fraction, Utils.random.nextLong) /** * Return a sampled subset of this RDD. */ def sample(withReplacement: Boolean, fraction: Double, seed: Long): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.sample(withReplacement, fraction, seed)) /** * Return a subset of this RDD sampled by key (via stratified sampling). * * Create a sample of this RDD using variable sampling rates for different keys as specified by * `fractions`, a key to sampling rate map, via simple random sampling with one pass over the * RDD, to produce a sample of size that's approximately equal to the sum of * math.ceil(numItems * samplingRate) over all key values. */ def sampleByKey(withReplacement: Boolean, fractions: java.util.Map[K, jl.Double], seed: Long): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.sampleByKey( withReplacement, fractions.asScala.mapValues(_.toDouble).toMap, // map to Scala Double; toMap to serialize seed)) /** * Return a subset of this RDD sampled by key (via stratified sampling). * * Create a sample of this RDD using variable sampling rates for different keys as specified by * `fractions`, a key to sampling rate map, via simple random sampling with one pass over the * RDD, to produce a sample of size that's approximately equal to the sum of * math.ceil(numItems * samplingRate) over all key values. * * Use Utils.random.nextLong as the default seed for the random number generator. */ def sampleByKey(withReplacement: Boolean, fractions: java.util.Map[K, jl.Double]): JavaPairRDD[K, V] = sampleByKey(withReplacement, fractions, Utils.random.nextLong) /** * Return a subset of this RDD sampled by key (via stratified sampling) containing exactly * math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). * * This method differs from `sampleByKey` in that we make additional passes over the RDD to * create a sample size that's exactly equal to the sum of math.ceil(numItems * samplingRate) * over all key values with a 99.99% confidence. When sampling without replacement, we need one * additional pass over the RDD to guarantee sample size; when sampling with replacement, we need * two additional passes. */ def sampleByKeyExact(withReplacement: Boolean, fractions: java.util.Map[K, jl.Double], seed: Long): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.sampleByKeyExact( withReplacement, fractions.asScala.mapValues(_.toDouble).toMap, // map to Scala Double; toMap to serialize seed)) /** * Return a subset of this RDD sampled by key (via stratified sampling) containing exactly * math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). * * This method differs from `sampleByKey` in that we make additional passes over the RDD to * create a sample size that's exactly equal to the sum of math.ceil(numItems * samplingRate) * over all key values with a 99.99% confidence. When sampling without replacement, we need one * additional pass over the RDD to guarantee sample size; when sampling with replacement, we need * two additional passes. * * Use Utils.random.nextLong as the default seed for the random number generator. */ def sampleByKeyExact( withReplacement: Boolean, fractions: java.util.Map[K, jl.Double]): JavaPairRDD[K, V] = sampleByKeyExact(withReplacement, fractions, Utils.random.nextLong) /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def union(other: JavaPairRDD[K, V]): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.union(other.rdd)) /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. */ def intersection(other: JavaPairRDD[K, V]): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.intersection(other.rdd)) // first() has to be overridden here so that the generated method has the signature // 'public scala.Tuple2 first()'; if the trait's definition is used, // then the method has the signature 'public java.lang.Object first()', // causing NoSuchMethodErrors at runtime. override def first(): (K, V) = rdd.first() // Pair RDD functions /** * Generic function to combine the elements for each key using a custom set of aggregation * functions. Turns a JavaPairRDD[(K, V)] into a result of type JavaPairRDD[(K, C)], for a * \"combined type\" C. * * Users provide three functions: * * - `createCombiner`, which turns a V into a C (e.g., creates a one-element list) * - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list) * - `mergeCombiners`, to combine two C's into a single one. * * In addition, users can control the partitioning of the output RDD, the serializer that is use * for the shuffle, and whether to perform map-side aggregation (if a mapper can produce multiple * items with the same key). * * @note V and C can be different -- for example, one might group an RDD of type (Int, Int) into * an RDD of type (Int, List[Int]). */ def combineByKey[C](createCombiner: JFunction[V, C], mergeValue: JFunction2[C, V, C], mergeCombiners: JFunction2[C, C, C], partitioner: Partitioner, mapSideCombine: Boolean, serializer: Serializer): JavaPairRDD[K, C] = { implicit val ctag: ClassTag[C] = fakeClassTag fromRDD(rdd.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine, serializer )) } /** * Generic function to combine the elements for each key using a custom set of aggregation * functions. Turns a JavaPairRDD[(K, V)] into a result of type JavaPairRDD[(K, C)], for a * \"combined type\" C. * * Users provide three functions: * * - `createCombiner`, which turns a V into a C (e.g., creates a one-element list) * - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list) * - `mergeCombiners`, to combine two C's into a single one. * * In addition, users can control the partitioning of the output RDD. This method automatically * uses map-side aggregation in shuffling the RDD. * * @note V and C can be different -- for example, one might group an RDD of type (Int, Int) into * an RDD of type (Int, List[Int]). */ def combineByKey[C](createCombiner: JFunction[V, C], mergeValue: JFunction2[C, V, C], mergeCombiners: JFunction2[C, C, C], partitioner: Partitioner): JavaPairRDD[K, C] = { combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner, true, null) } /** * Simplified version of combineByKey that hash-partitions the output RDD and uses map-side * aggregation. */ def combineByKey[C](createCombiner: JFunction[V, C], mergeValue: JFunction2[C, V, C], mergeCombiners: JFunction2[C, C, C], numPartitions: Int): JavaPairRDD[K, C] = combineByKey(createCombiner, mergeValue, mergeCombiners, new HashPartitioner(numPartitions)) /** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a \"combiner\" in MapReduce. */ def reduceByKey(partitioner: Partitioner, func: JFunction2[V, V, V]): JavaPairRDD[K, V] = fromRDD(rdd.reduceByKey(partitioner, func)) /** * Merge the values for each key using an associative and commutative reduce function, but return * the result immediately to the master as a Map. This will also perform the merging locally on * each mapper before sending results to a reducer, similarly to a \"combiner\" in MapReduce. */ def reduceByKeyLocally(func: JFunction2[V, V, V]): java.util.Map[K, V] = mapAsSerializableJavaMap(rdd.reduceByKeyLocally(func)) /** Count the number of elements for each key, and return the result to the master as a Map. */ def countByKey(): java.util.Map[K, jl.Long] = mapAsSerializableJavaMap(rdd.countByKey()).asInstanceOf[java.util.Map[K, jl.Long]] /** * Approximate version of countByKey that can return a partial result if it does * not finish within a timeout. */ def countByKeyApprox(timeout: Long): PartialResult[java.util.Map[K, BoundedDouble]] = rdd.countByKeyApprox(timeout).map(mapAsSerializableJavaMap) /** * Approximate version of countByKey that can return a partial result if it does * not finish within a timeout. */ def countByKeyApprox(timeout: Long, confidence: Double = 0.95) : PartialResult[java.util.Map[K, BoundedDouble]] = rdd.countByKeyApprox(timeout, confidence).map(mapAsSerializableJavaMap) /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * This function can return a different result type, U, than the type of the values in this RDD, * V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, * as in scala.TraversableOnce. The former operation is used for merging values within a * partition, and the latter is used for merging values between partitions. To avoid memory * allocation, both of these functions are allowed to modify and return their first argument * instead of creating a new U. */ def aggregateByKey[U](zeroValue: U, partitioner: Partitioner, seqFunc: JFunction2[U, V, U], combFunc: JFunction2[U, U, U]): JavaPairRDD[K, U] = { implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.aggregateByKey(zeroValue, partitioner)(seqFunc, combFunc)) } /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * This function can return a different result type, U, than the type of the values in this RDD, * V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, * as in scala.TraversableOnce. The former operation is used for merging values within a * partition, and the latter is used for merging values between partitions. To avoid memory * allocation, both of these functions are allowed to modify and return their first argument * instead of creating a new U. */ def aggregateByKey[U](zeroValue: U, numPartitions: Int, seqFunc: JFunction2[U, V, U], combFunc: JFunction2[U, U, U]): JavaPairRDD[K, U] = { implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.aggregateByKey(zeroValue, numPartitions)(seqFunc, combFunc)) } /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * This function can return a different result type, U, than the type of the values in this RDD, * V. Thus, we need one operation for merging a V into a U and one operation for merging two U's. * The former operation is used for merging values within a partition, and the latter is used for * merging values between partitions. To avoid memory allocation, both of these functions are * allowed to modify and return their first argument instead of creating a new U. */ def aggregateByKey[U](zeroValue: U, seqFunc: JFunction2[U, V, U], combFunc: JFunction2[U, U, U]): JavaPairRDD[K, U] = { implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.aggregateByKey(zeroValue)(seqFunc, combFunc)) } /** * Merge the values for each key using an associative function and a neutral \"zero value\" which * may be added to the result an arbitrary number of times, and must not change the result * (e.g ., Nil for list concatenation, 0 for addition, or 1 for multiplication.). */ def foldByKey(zeroValue: V, partitioner: Partitioner, func: JFunction2[V, V, V]) : JavaPairRDD[K, V] = fromRDD(rdd.foldByKey(zeroValue, partitioner)(func)) /** * Merge the values for each key using an associative function and a neutral \"zero value\" which * may be added to the result an arbitrary number of times, and must not change the result * (e.g ., Nil for list concatenation, 0 for addition, or 1 for multiplication.). */ def foldByKey(zeroValue: V, numPartitions: Int, func: JFunction2[V, V, V]): JavaPairRDD[K, V] = fromRDD(rdd.foldByKey(zeroValue, numPartitions)(func)) /** * Merge the values for each key using an associative function and a neutral \"zero value\" * which may be added to the result an arbitrary number of times, and must not change the result * (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). */ def foldByKey(zeroValue: V, func: JFunction2[V, V, V]): JavaPairRDD[K, V] = fromRDD(rdd.foldByKey(zeroValue)(func)) /** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a \"combiner\" in MapReduce. Output will be hash-partitioned with numPartitions partitions. */ def reduceByKey(func: JFunction2[V, V, V], numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.reduceByKey(func, numPartitions)) /** * Group the values for each key in the RDD into a single sequence. Allows controlling the * partitioning of the resulting key-value pair RDD by passing a Partitioner. * * @note If you are grouping in order to perform an aggregation (such as a sum or average) over * each key, using `JavaPairRDD.reduceByKey` or `JavaPairRDD.combineByKey` * will provide much better performance. */ def groupByKey(partitioner: Partitioner): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey(partitioner))) /** * Group the values for each key in the RDD into a single sequence. Hash-partitions the * resulting RDD with into `numPartitions` partitions. * * @note If you are grouping in order to perform an aggregation (such as a sum or average) over * each key, using `JavaPairRDD.reduceByKey` or `JavaPairRDD.combineByKey` * will provide much better performance. */ def groupByKey(numPartitions: Int): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey(numPartitions))) /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us. */ def subtract(other: JavaPairRDD[K, V]): JavaPairRDD[K, V] = fromRDD(rdd.subtract(other)) /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: JavaPairRDD[K, V], numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.subtract(other, numPartitions)) /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: JavaPairRDD[K, V], p: Partitioner): JavaPairRDD[K, V] = fromRDD(rdd.subtract(other, p)) /** * Return an RDD with the pairs from `this` whose keys are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us. */ def subtractByKey[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, V] = { implicit val ctag: ClassTag[W] = fakeClassTag fromRDD(rdd.subtractByKey(other)) } /** * Return an RDD with the pairs from `this` whose keys are not in `other`. */ def subtractByKey[W](other: JavaPairRDD[K, W], numPartitions: Int): JavaPairRDD[K, V] = { implicit val ctag: ClassTag[W] = fakeClassTag fromRDD(rdd.subtractByKey(other, numPartitions)) } /** * Return an RDD with the pairs from `this` whose keys are not in `other`. */ def subtractByKey[W](other: JavaPairRDD[K, W], p: Partitioner): JavaPairRDD[K, V] = { implicit val ctag: ClassTag[W] = fakeClassTag fromRDD(rdd.subtractByKey(other, p)) } /** * Return a copy of the RDD partitioned using the specified partitioner. */ def partitionBy(partitioner: Partitioner): JavaPairRDD[K, V] = fromRDD(rdd.partitionBy(partitioner)) /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD. */ def join[W](other: JavaPairRDD[K, W], partitioner: Partitioner): JavaPairRDD[K, (V, W)] = fromRDD(rdd.join(other, partitioner)) /** * Perform a left outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (v, Some(w))) for w in `other`, or the * pair (k, (v, None)) if no elements in `other` have key k. Uses the given Partitioner to * partition the output RDD. */ def leftOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner) : JavaPairRDD[K, (V, Optional[W])] = { val joinResult = rdd.leftOuterJoin(other, partitioner) fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}) } /** * Perform a right outer join of `this` and `other`. For each element (k, w) in `other`, the * resulting RDD will either contain all pairs (k, (Some(v), w)) for v in `this`, or the * pair (k, (None, w)) if no elements in `this` have key k. Uses the given Partitioner to * partition the output RDD. */ def rightOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner) : JavaPairRDD[K, (Optional[V], W)] = { val joinResult = rdd.rightOuterJoin(other, partitioner) fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}) } /** * Perform a full outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in `other`, or * the pair (k, (Some(v), None)) if no elements in `other` have key k. Similarly, for each * element (k, w) in `other`, the resulting RDD will either contain all pairs * (k, (Some(v), Some(w))) for v in `this`, or the pair (k, (None, Some(w))) if no elements * in `this` have key k. Uses the given Partitioner to partition the output RDD. */ def fullOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner) : JavaPairRDD[K, (Optional[V], Optional[W])] = { val joinResult = rdd.fullOuterJoin(other, partitioner) fromRDD(joinResult.mapValues{ case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w)) }) } /** * Simplified version of combineByKey that hash-partitions the resulting RDD using the existing * partitioner/parallelism level and using map-side aggregation. */ def combineByKey[C](createCombiner: JFunction[V, C], mergeValue: JFunction2[C, V, C], mergeCombiners: JFunction2[C, C, C]): JavaPairRDD[K, C] = { implicit val ctag: ClassTag[C] = fakeClassTag fromRDD(combineByKey(createCombiner, mergeValue, mergeCombiners, defaultPartitioner(rdd))) } /** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a \"combiner\" in MapReduce. Output will be hash-partitioned with the existing partitioner/ * parallelism level. */ def reduceByKey(func: JFunction2[V, V, V]): JavaPairRDD[K, V] = { fromRDD(reduceByKey(defaultPartitioner(rdd), func)) } /** * Group the values for each key in the RDD into a single sequence. Hash-partitions the * resulting RDD with the existing partitioner/parallelism level. * * @note If you are grouping in order to perform an aggregation (such as a sum or average) over * each key, using `JavaPairRDD.reduceByKey` or `JavaPairRDD.combineByKey` * will provide much better performance. */ def groupByKey(): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey())) /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and * (k, v2) is in `other`. Performs a hash join across the cluster. */ def join[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (V, W)] = fromRDD(rdd.join(other)) /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and * (k, v2) is in `other`. Performs a hash join across the cluster. */ def join[W](other: JavaPairRDD[K, W], numPartitions: Int): JavaPairRDD[K, (V, W)] = fromRDD(rdd.join(other, numPartitions)) /** * Perform a left outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (v, Some(w))) for w in `other`, or the * pair (k, (v, None)) if no elements in `other` have key k. Hash-partitions the output * using the existing partitioner/parallelism level. */ def leftOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (V, Optional[W])] = { val joinResult = rdd.leftOuterJoin(other) fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}) } /** * Perform a left outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (v, Some(w))) for w in `other`, or the * pair (k, (v, None)) if no elements in `other` have key k. Hash-partitions the output * into `numPartitions` partitions. */ def leftOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int) : JavaPairRDD[K, (V, Optional[W])] = { val joinResult = rdd.leftOuterJoin(other, numPartitions) fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}) } /** * Perform a right outer join of `this` and `other`. For each element (k, w) in `other`, the * resulting RDD will either contain all pairs (k, (Some(v), w)) for v in `this`, or the * pair (k, (None, w)) if no elements in `this` have key k. Hash-partitions the resulting * RDD using the existing partitioner/parallelism level. */ def rightOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (Optional[V], W)] = { val joinResult = rdd.rightOuterJoin(other) fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}) } /** * Perform a right outer join of `this` and `other`. For each element (k, w) in `other`, the * resulting RDD will either contain all pairs (k, (Some(v), w)) for v in `this`, or the * pair (k, (None, w)) if no elements in `this` have key k. Hash-partitions the resulting * RDD into the given number of partitions. */ def rightOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int) : JavaPairRDD[K, (Optional[V], W)] = { val joinResult = rdd.rightOuterJoin(other, numPartitions) fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}) } /** * Perform a full outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in `other`, or * the pair (k, (Some(v), None)) if no elements in `other` have key k. Similarly, for each * element (k, w) in `other`, the resulting RDD will either contain all pairs * (k, (Some(v), Some(w))) for v in `this`, or the pair (k, (None, Some(w))) if no elements * in `this` have key k. Hash-partitions the resulting RDD using the existing partitioner/ * parallelism level. */ def fullOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (Optional[V], Optional[W])] = { val joinResult = rdd.fullOuterJoin(other) fromRDD(joinResult.mapValues{ case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w)) }) } /** * Perform a full outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in `other`, or * the pair (k, (Some(v), None)) if no elements in `other` have key k. Similarly, for each * element (k, w) in `other`, the resulting RDD will either contain all pairs * (k, (Some(v), Some(w))) for v in `this`, or the pair (k, (None, Some(w))) if no elements * in `this` have key k. Hash-partitions the resulting RDD into the given number of partitions. */ def fullOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int) : JavaPairRDD[K, (Optional[V], Optional[W])] = { val joinResult = rdd.fullOuterJoin(other, numPartitions) fromRDD(joinResult.mapValues{ case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w)) }) } /** * Return the key-value pairs in this RDD to the master as a Map. * * @note this method should only be used if the resulting data is expected to be small, as * all the data is loaded into the driver's memory. */ def collectAsMap(): java.util.Map[K, V] = mapAsSerializableJavaMap(rdd.collectAsMap()) /** * Pass each value in the key-value pair RDD through a map function without changing the keys; * this also retains the original RDD's partitioning. */ def mapValues[U](f: JFunction[V, U]): JavaPairRDD[K, U] = { implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.mapValues(f)) } /** * Pass each value in the key-value pair RDD through a flatMap function without changing the * keys; this also retains the original RDD's partitioning. */ def flatMapValues[U](f: FlatMapFunction[V, U]): JavaPairRDD[K, U] = { def fn: (V) => Iterator[U] = (x: V) => f.call(x).asScala implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.flatMapValues(fn)) } /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. */ def cogroup[W](other: JavaPairRDD[K, W], partitioner: Partitioner) : JavaPairRDD[K, (JIterable[V], JIterable[W])] = fromRDD(cogroupResultToJava(rdd.cogroup(other, partitioner))) /** * For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a * tuple with the list of values for that key in `this`, `other1` and `other2`. */ def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], partitioner: Partitioner): JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2])] = fromRDD(cogroupResult2ToJava(rdd.cogroup(other1, other2, partitioner))) /** * For each key k in `this` or `other1` or `other2` or `other3`, * return a resulting RDD that contains a tuple with the list of values * for that key in `this`, `other1`, `other2` and `other3`. */ def cogroup[W1, W2, W3](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], other3: JavaPairRDD[K, W3], partitioner: Partitioner) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3])] = fromRDD(cogroupResult3ToJava(rdd.cogroup(other1, other2, other3, partitioner))) /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. */ def cogroup[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (JIterable[V], JIterable[W])] = fromRDD(cogroupResultToJava(rdd.cogroup(other))) /** * For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a * tuple with the list of values for that key in `this`, `other1` and `other2`. */ def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2]) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2])] = fromRDD(cogroupResult2ToJava(rdd.cogroup(other1, other2))) /** * For each key k in `this` or `other1` or `other2` or `other3`, * return a resulting RDD that contains a tuple with the list of values * for that key in `this`, `other1`, `other2` and `other3`. */ def cogroup[W1, W2, W3](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], other3: JavaPairRDD[K, W3]) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3])] = fromRDD(cogroupResult3ToJava(rdd.cogroup(other1, other2, other3))) /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. */ def cogroup[W](other: JavaPairRDD[K, W], numPartitions: Int) : JavaPairRDD[K, (JIterable[V], JIterable[W])] = fromRDD(cogroupResultToJava(rdd.cogroup(other, numPartitions))) /** * For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a * tuple with the list of values for that key in `this`, `other1` and `other2`. */ def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], numPartitions: Int) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2])] = fromRDD(cogroupResult2ToJava(rdd.cogroup(other1, other2, numPartitions))) /** * For each key k in `this` or `other1` or `other2` or `other3`, * return a resulting RDD that contains a tuple with the list of values * for that key in `this`, `other1`, `other2` and `other3`. */ def cogroup[W1, W2, W3](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], other3: JavaPairRDD[K, W3], numPartitions: Int) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3])] = fromRDD(cogroupResult3ToJava(rdd.cogroup(other1, other2, other3, numPartitions))) /** Alias for cogroup. */ def groupWith[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (JIterable[V], JIterable[W])] = fromRDD(cogroupResultToJava(rdd.groupWith(other))) /** Alias for cogroup. */ def groupWith[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2]) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2])] = fromRDD(cogroupResult2ToJava(rdd.groupWith(other1, other2))) /** Alias for cogroup. */ def groupWith[W1, W2, W3](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], other3: JavaPairRDD[K, W3]) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3])] = fromRDD(cogroupResult3ToJava(rdd.groupWith(other1, other2, other3))) /** * Return the list of values in the RDD for key `key`. This operation is done efficiently if the * RDD has a known partitioner by only searching the partition that the key maps to. */ def lookup(key: K): JList[V] = rdd.lookup(key).asJava /** Output the RDD to any Hadoop-supported file system. */ def saveAsHadoopFile[F <: OutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F], conf: JobConf): Unit = { rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, conf) } /** Output the RDD to any Hadoop-supported file system. */ def saveAsHadoopFile[F <: OutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F]): Unit = { rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass) } /** Output the RDD to any Hadoop-supported file system, compressing with the supplied codec. */ def saveAsHadoopFile[F <: OutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F], codec: Class[_ <: CompressionCodec]): Unit = { rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, codec) } /** Output the RDD to any Hadoop-supported file system. */ def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F], conf: Configuration): Unit = { rdd.saveAsNewAPIHadoopFile(path, keyClass, valueClass, outputFormatClass, conf) } /** * Output the RDD to any Hadoop-supported storage system, using * a Configuration object for that storage system. */ def saveAsNewAPIHadoopDataset(conf: Configuration): Unit = { rdd.saveAsNewAPIHadoopDataset(conf) } /** Output the RDD to any Hadoop-supported file system. */ def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F]): Unit = { rdd.saveAsNewAPIHadoopFile(path, keyClass, valueClass, outputFormatClass) } /** * Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for * that storage system. The JobConf should set an OutputFormat and any output paths required * (e.g. a table name to write to) in the same way as it would be configured for a Hadoop * MapReduce job. */ def saveAsHadoopDataset(conf: JobConf): Unit = { rdd.saveAsHadoopDataset(conf) } /** * Repartition the RDD according to the given partitioner and, within each resulting partition, * sort records by their keys. * * This is more efficient than calling `repartition` and then sorting within each partition * because it can push the sorting down into the shuffle machinery. */ def repartitionAndSortWithinPartitions(partitioner: Partitioner): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] repartitionAndSortWithinPartitions(partitioner, comp) } /** * Repartition the RDD according to the given partitioner and, within each resulting partition, * sort records by their keys. * * This is more efficient than calling `repartition` and then sorting within each partition * because it can push the sorting down into the shuffle machinery. */ def repartitionAndSortWithinPartitions(partitioner: Partitioner, comp: Comparator[K]) : JavaPairRDD[K, V] = { implicit val ordering = comp // Allow implicit conversion of Comparator to Ordering. fromRDD( new OrderedRDDFunctions[K, V, (K, V)](rdd).repartitionAndSortWithinPartitions(partitioner)) } /** * Sort the RDD by key, so that each partition contains a sorted range of the elements in * ascending order. Calling `collect` or `save` on the resulting RDD will return or output an * ordered list of records (in the `save` case, they will be written to multiple `part-X` files * in the filesystem, in order of the keys). */ def sortByKey(): JavaPairRDD[K, V] = sortByKey(true) /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(ascending: Boolean): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] sortByKey(comp, ascending) } /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(ascending: Boolean, numPartitions: Int): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] sortByKey(comp, ascending, numPartitions) } /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(comp: Comparator[K]): JavaPairRDD[K, V] = sortByKey(comp, true) /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(comp: Comparator[K], ascending: Boolean): JavaPairRDD[K, V] = { implicit val ordering = comp // Allow implicit conversion of Comparator to Ordering. fromRDD(new OrderedRDDFunctions[K, V, (K, V)](rdd).sortByKey(ascending)) } /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(comp: Comparator[K], ascending: Boolean, numPartitions: Int): JavaPairRDD[K, V] = { implicit val ordering = comp // Allow implicit conversion of Comparator to Ordering. fromRDD(new OrderedRDDFunctions[K, V, (K, V)](rdd).sortByKey(ascending, numPartitions)) } /** * Return a RDD containing only the elements in the inclusive range `lower` to `upper`. * If the RDD has been partitioned using a `RangePartitioner`, then this operation can be * performed efficiently by only scanning the partitions that might contain matching elements. * Otherwise, a standard `filter` is applied to all partitions. * * @since 3.1.0 */ @Since(\"3.1.0\") def filterByRange(lower: K, upper: K): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] filterByRange(comp, lower, upper) } /** * Return a RDD containing only the elements in the inclusive range `lower` to `upper`. * If the RDD has been partitioned using a `RangePartitioner`, then this operation can be * performed efficiently by only scanning the partitions that might contain matching elements. * Otherwise, a standard `filter` is applied to all partitions. * * @since 3.1.0 */ @Since(\"3.1.0\") def filterByRange(comp: Comparator[K], lower: K, upper: K): JavaPairRDD[K, V] = { implicit val ordering = comp // Allow implicit conversion of Comparator to Ordering. fromRDD(new OrderedRDDFunctions[K, V, (K, V)](rdd).filterByRange(lower, upper)) } /** * Return an RDD with the keys of each tuple. */ def keys(): JavaRDD[K] = JavaRDD.fromRDD[K](rdd.map(_._1)) /** * Return an RDD with the values of each tuple. */ def values(): JavaRDD[V] = JavaRDD.fromRDD[V](rdd.map(_._2)) /** * Return approximate number of distinct values for each key in this RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. * @param partitioner partitioner of the resulting RDD. */ def countApproxDistinctByKey(relativeSD: Double, partitioner: Partitioner) : JavaPairRDD[K, jl.Long] = { fromRDD(rdd.countApproxDistinctByKey(relativeSD, partitioner)). asInstanceOf[JavaPairRDD[K, jl.Long]] } /** * Return approximate number of distinct values for each key in this RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. * @param numPartitions number of partitions of the resulting RDD. */ def countApproxDistinctByKey(relativeSD: Double, numPartitions: Int): JavaPairRDD[K, jl.Long] = { fromRDD(rdd.countApproxDistinctByKey(relativeSD, numPartitions)). asInstanceOf[JavaPairRDD[K, jl.Long]] } /** * Return approximate number of distinct values for each key in this RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. */ def countApproxDistinctByKey(relativeSD: Double): JavaPairRDD[K, jl.Long] = { fromRDD(rdd.countApproxDistinctByKey(relativeSD)).asInstanceOf[JavaPairRDD[K, jl.Long]] } /** Assign a name to this RDD */ def setName(name: String): JavaPairRDD[K, V] = { rdd.setName(name) this } } object JavaPairRDD { private[spark] def groupByResultToJava[K: ClassTag, T](rdd: RDD[(K, Iterable[T])]): RDD[(K, JIterable[T])] = { rddToPairRDDFunctions(rdd).mapValues(_.asJava) } private[spark] def cogroupResultToJava[K: ClassTag, V, W]( rdd: RDD[(K, (Iterable[V], Iterable[W]))]): RDD[(K, (JIterable[V], JIterable[W]))] = { rddToPairRDDFunctions(rdd).mapValues(x => (x._1.asJava, x._2.asJava)) } private[spark] def cogroupResult2ToJava[K: ClassTag, V, W1, W2]( rdd: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]) : RDD[(K, (JIterable[V], JIterable[W1], JIterable[W2]))] = { rddToPairRDDFunctions(rdd).mapValues(x => (x._1.asJava, x._2.asJava, x._3.asJava)) } private[spark] def cogroupResult3ToJava[K: ClassTag, V, W1, W2, W3]( rdd: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2], Iterable[W3]))]) : RDD[(K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3]))] = { rddToPairRDDFunctions(rdd).mapValues(x => (x._1.asJava, x._2.asJava, x._3.asJava, x._4.asJava)) } def fromRDD[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)]): JavaPairRDD[K, V] = { new JavaPairRDD[K, V](rdd) } implicit def toRDD[K, V](rdd: JavaPairRDD[K, V]): RDD[(K, V)] = rdd.rdd private[spark] implicit def toScalaFunction2[T1, T2, R](fun: JFunction2[T1, T2, R]): Function2[T1, T2, R] = { (x: T1, x1: T2) => fun.call(x, x1) } private[spark] implicit def toScalaFunction[T, R](fun: JFunction[T, R]): T => R = x => fun.call(x) private[spark] implicit def pairFunToScalaFun[A, B, C](x: PairFunction[A, B, C]): A => (B, C) = y => x.call(y) /** Convert a JavaRDD of key-value pairs to JavaPairRDD. */ def fromJavaRDD[K, V](rdd: JavaRDD[(K, V)]): JavaPairRDD[K, V] = { implicit val ctagK: ClassTag[K] = fakeClassTag implicit val ctagV: ClassTag[V] = fakeClassTag new JavaPairRDD[K, V](rdd.rdd) } }",
            "## CLASS: org/apache/spark/SparkConf# (implementation)\n*/ class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging with Serializable { import SparkConf._ /** Create a SparkConf that loads defaults from system properties and the classpath */ def this() = this(true) private val settings = new ConcurrentHashMap[String, String]() @transient private lazy val reader: ConfigReader = { val _reader = new ConfigReader(new SparkConfigProvider(settings)) _reader.bindEnv((key: String) => Option(getenv(key))) _reader } if (loadDefaults) { loadFromSystemProperties(false) } private[spark] def loadFromSystemProperties(silent: Boolean): SparkConf = { // Load any spark.* system properties for ((key, value) <- Utils.getSystemProperties if key.startsWith(\"spark.\")) { set(key, value, silent) } this } /** Set a configuration variable. */ def set(key: String, value: String): SparkConf = { set(key, value, false) } private[spark] def set(key: String, value: String, silent: Boolean): SparkConf = { if (key == null) { throw new NullPointerException(\"null key\") } if (value == null) { throw new NullPointerException(\"null value for \" + key) } if (!silent) { logDeprecationWarning(key) } settings.put(key, value) this } private[spark] def set[T](entry: ConfigEntry[T], value: T): SparkConf = { set(entry.key, entry.stringConverter(value)) this } private[spark] def set[T](entry: OptionalConfigEntry[T], value: T): SparkConf = { set(entry.key, entry.rawStringConverter(value)) this } /** * The master URL to connect to, such as \"local\" to run locally with one thread, \"local[4]\" to * run locally with 4 cores, or \"spark://master:7077\" to run on a Spark standalone cluster. */ def setMaster(master: String): SparkConf = { set(\"spark.master\", master) } /** Set a name for your application. Shown in the Spark web UI. */ def setAppName(name: String): SparkConf = { set(\"spark.app.name\", name) } /** Set JAR files to distribute to the cluster. */ def setJars(jars: Seq[String]): SparkConf = { for (jar <- jars if (jar == null)) logWarning(\"null jar passed to SparkContext constructor\") set(JARS, jars.filter(_ != null)) } /** Set JAR files to distribute to the cluster. (Java-friendly version.) */ def setJars(jars: Array[String]): SparkConf = { setJars(jars.toSeq) } /** * Set an environment variable to be used when launching executors for this application. * These variables are stored as properties of the form spark.executorEnv.VAR_NAME * (for example spark.executorEnv.PATH) but this method makes them easier to set. */ def setExecutorEnv(variable: String, value: String): SparkConf = { set(\"spark.executorEnv.\" + variable, value) } /** * Set multiple environment variables to be used when launching executors. * These variables are stored as properties of the form spark.executorEnv.VAR_NAME * (for example spark.executorEnv.PATH) but this method makes them easier to set. */ def setExecutorEnv(variables: Seq[(String, String)]): SparkConf = { for ((k, v) <- variables) { setExecutorEnv(k, v) } this } /** * Set multiple environment variables to be used when launching executors. * (Java-friendly version.) */ def setExecutorEnv(variables: Array[(String, String)]): SparkConf = { setExecutorEnv(variables.toSeq) } /** * Set the location where Spark is installed on worker nodes. */ def setSparkHome(home: String): SparkConf = { set(\"spark.home\", home) } /** Set multiple parameters together */ def setAll(settings: Iterable[(String, String)]): SparkConf = { settings.foreach { case (k, v) => set(k, v) } this } /** Set a parameter if it isn't already configured */ def setIfMissing(key: String, value: String): SparkConf = { if (settings.putIfAbsent(key, value) == null) { logDeprecationWarning(key) } this } private[spark] def setIfMissing[T](entry: ConfigEntry[T], value: T): SparkConf = { if (settings.putIfAbsent(entry.key, entry.stringConverter(value)) == null) { logDeprecationWarning(entry.key) } this } private[spark] def setIfMissing[T](entry: OptionalConfigEntry[T], value: T): SparkConf = { if (settings.putIfAbsent(entry.key, entry.rawStringConverter(value)) == null) { logDeprecationWarning(entry.key) } this } /** * Use Kryo serialization and register the given set of classes with Kryo. * If called multiple times, this will append the classes from all calls together. */ def registerKryoClasses(classes: Array[Class[_]]): SparkConf = { val allClassNames = new LinkedHashSet[String]() allClassNames ++= get(KRYO_CLASSES_TO_REGISTER).map(_.trim) .filter(!_.isEmpty) allClassNames ++= classes.map(_.getName) set(KRYO_CLASSES_TO_REGISTER, allClassNames.toSeq) set(SERIALIZER, classOf[KryoSerializer].getName) this } private final val avroNamespace = \"avro.schema.\" /** * Use Kryo serialization and register the given set of Avro schemas so that the generic * record serializer can decrease network IO */ def registerAvroSchemas(schemas: Schema*): SparkConf = { for (schema <- schemas) { set(avroNamespace + SchemaNormalization.parsingFingerprint64(schema), schema.toString) } this } /** Gets all the avro schemas in the configuration used in the generic Avro record serializer */ def getAvroSchema: Map[Long, String] = { getAll.filter { case (k, v) => k.startsWith(avroNamespace) } .map { case (k, v) => (k.substring(avroNamespace.length).toLong, v) } .toMap } /** Remove a parameter from the configuration */ def remove(key: String): SparkConf = { settings.remove(key) this } private[spark] def remove(entry: ConfigEntry[_]): SparkConf = { remove(entry.key) } /** Get a parameter; throws a NoSuchElementException if it's not set */ def get(key: String): String = { getOption(key).getOrElse(throw new NoSuchElementException(key)) } /** Get a parameter, falling back to a default if not set */ def get(key: String, defaultValue: String): String = { getOption(key).getOrElse(defaultValue) } /** * Retrieves the value of a pre-defined configuration entry. * * - This is an internal Spark API. * - The return type if defined by the configuration entry. * - This will throw an exception is the config is not optional and the value is not set. */ private[spark] def get[T](entry: ConfigEntry[T]): T = { entry.readFrom(reader) } /** * Get a time parameter as seconds; throws a NoSuchElementException if it's not set. If no * suffix is provided then seconds are assumed. * @throws java.util.NoSuchElementException If the time parameter is not set * @throws NumberFormatException If the value cannot be interpreted as seconds */ def getTimeAsSeconds(key: String): Long = catchIllegalValue(key) { Utils.timeStringAsSeconds(get(key)) } /** * Get a time parameter as seconds, falling back to a default if not set. If no * suffix is provided then seconds are assumed. * @throws NumberFormatException If the value cannot be interpreted as seconds */ def getTimeAsSeconds(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.timeStringAsSeconds(get(key, defaultValue)) } /** * Get a time parameter as milliseconds; throws a NoSuchElementException if it's not set. If no * suffix is provided then milliseconds are assumed. * @throws java.util.NoSuchElementException If the time parameter is not set * @throws NumberFormatException If the value cannot be interpreted as milliseconds */ def getTimeAsMs(key: String): Long = catchIllegalValue(key) { Utils.timeStringAsMs(get(key)) } /** * Get a time parameter as milliseconds, falling back to a default if not set. If no * suffix is provided then milliseconds are assumed. * @throws NumberFormatException If the value cannot be interpreted as milliseconds */ def getTimeAsMs(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.timeStringAsMs(get(key, defaultValue)) } /** * Get a size parameter as bytes; throws a NoSuchElementException if it's not set. If no * suffix is provided then bytes are assumed. * @throws java.util.NoSuchElementException If the size parameter is not set * @throws NumberFormatException If the value cannot be interpreted as bytes */ def getSizeAsBytes(key: String): Long = catchIllegalValue(key) { Utils.byteStringAsBytes(get(key)) } /** * Get a size parameter as bytes, falling back to a default if not set. If no * suffix is provided then bytes are assumed. * @throws NumberFormatException If the value cannot be interpreted as bytes */ def getSizeAsBytes(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.byteStringAsBytes(get(key, defaultValue)) } /** * Get a size parameter as bytes, falling back to a default if not set. * @throws NumberFormatException If the value cannot be interpreted as bytes */ def getSizeAsBytes(key: String, defaultValue: Long): Long = catchIllegalValue(key) { Utils.byteStringAsBytes(get(key, defaultValue + \"B\")) } /** * Get a size parameter as Kibibytes; throws a NoSuchElementException if it's not set. If no * suffix is provided then Kibibytes are assumed. * @throws java.util.NoSuchElementException If the size parameter is not set * @throws NumberFormatException If the value cannot be interpreted as Kibibytes */ def getSizeAsKb(key: String): Long = catchIllegalValue(key) { Utils.byteStringAsKb(get(key)) } /** * Get a size parameter as Kibibytes, falling back to a default if not set. If no * suffix is provided then Kibibytes are assumed. * @throws NumberFormatException If the value cannot be interpreted as Kibibytes */ def getSizeAsKb(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.byteStringAsKb(get(key, defaultValue)) } /** * Get a size parameter as Mebibytes; throws a NoSuchElementException if it's not set. If no * suffix is provided then Mebibytes are assumed. * @throws java.util.NoSuchElementException If the size parameter is not set * @throws NumberFormatException If the value cannot be interpreted as Mebibytes */ def getSizeAsMb(key: String): Long = catchIllegalValue(key) { Utils.byteStringAsMb(get(key)) } /** * Get a size parameter as Mebibytes, falling back to a default if not set. If no * suffix is provided then Mebibytes are assumed. * @throws NumberFormatException If the value cannot be interpreted as Mebibytes */ def getSizeAsMb(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.byteStringAsMb(get(key, defaultValue)) } /** * Get a size parameter as Gibibytes; throws a NoSuchElementException if it's not set. If no * suffix is provided then Gibibytes are assumed. * @throws java.util.NoSuchElementException If the size parameter is not set * @throws NumberFormatException If the value cannot be interpreted as Gibibytes */ def getSizeAsGb(key: String): Long = catchIllegalValue(key) { Utils.byteStringAsGb(get(key)) } /** * Get a size parameter as Gibibytes, falling back to a default if not set. If no * suffix is provided then Gibibytes are assumed. * @throws NumberFormatException If the value cannot be interpreted as Gibibytes */ def getSizeAsGb(key: String, defaultValue: String): Long = catchIllegalValue(key) { Utils.byteStringAsGb(get(key, defaultValue)) } /** Get a parameter as an Option */ def getOption(key: String): Option[String] = { Option(settings.get(key)).orElse(getDeprecatedConfig(key, settings)) } /** Get an optional value, applying variable substitution. */ private[spark] def getWithSubstitution(key: String): Option[String] = { getOption(key).map(reader.substitute) } /** Get all parameters as a list of pairs */ def getAll: Array[(String, String)] = { settings.entrySet().asScala.map(x => (x.getKey, x.getValue)).toArray } /** * Get all parameters that start with `prefix` */ def getAllWithPrefix(prefix: String): Array[(String, String)] = { getAll.filter { case (k, v) => k.startsWith(prefix) } .map { case (k, v) => (k.substring(prefix.length), v) } } /** * Get a parameter as an integer, falling back to a default if not set * @throws NumberFormatException If the value cannot be interpreted as an integer */ def getInt(key: String, defaultValue: Int): Int = catchIllegalValue(key) { getOption(key).map(_.toInt).getOrElse(defaultValue) } /** * Get a parameter as a long, falling back to a default if not set * @throws NumberFormatException If the value cannot be interpreted as a long */ def getLong(key: String, defaultValue: Long): Long = catchIllegalValue(key) { getOption(key).map(_.toLong).getOrElse(defaultValue) } /** * Get a parameter as a double, falling back to a default if not ste * @throws NumberFormatException If the value cannot be interpreted as a double */ def getDouble(key: String, defaultValue: Double): Double = catchIllegalValue(key) { getOption(key).map(_.toDouble).getOrElse(defaultValue) } /** * Get a parameter as a boolean, falling back to a default if not set * @throws IllegalArgumentException If the value cannot be interpreted as a boolean */ def getBoolean(key: String, defaultValue: Boolean): Boolean = catchIllegalValue(key) { getOption(key).map(_.toBoolean).getOrElse(defaultValue) } /** Get all executor environment variables set on this SparkConf */ def getExecutorEnv: Seq[(String, String)] = { getAllWithPrefix(\"spark.executorEnv.\") } /** * Returns the Spark application id, valid in the Driver after TaskScheduler registration and * from the start in the Executor. */ def getAppId: String = get(\"spark.app.id\") /** Does the configuration contain a given parameter? */ def contains(key: String): Boolean = { settings.containsKey(key) || configsWithAlternatives.get(key).toSeq.flatten.exists { alt => contains(alt.key) } } private[spark] def contains(entry: ConfigEntry[_]): Boolean = contains(entry.key) /** Copy this object */ override def clone: SparkConf = { val cloned = new SparkConf(false) settings.entrySet().asScala.foreach { e => cloned.set(e.getKey(), e.getValue(), true) } cloned } /** * By using this instead of System.getenv(), environment variables can be mocked * in unit tests. */ private[spark] def getenv(name: String): String = System.getenv(name) /** * Wrapper method for get() methods which require some specific value format. This catches * any [[NumberFormatException]] or [[IllegalArgumentException]] and re-raises it with the * incorrectly configured key in the exception message. */ private def catchIllegalValue[T](key: String)(getValue: => T): T = { try { getValue } catch { case e: NumberFormatException => // NumberFormatException doesn't have a constructor that takes a cause for some reason. throw new NumberFormatException(s\"Illegal value for config key $key: ${e.getMessage}\") .initCause(e) case e: IllegalArgumentException => throw new IllegalArgumentException(s\"Illegal value for config key $key: ${e.getMessage}\", e) } } /** * Checks for illegal or deprecated config settings. Throws an exception for the former. Not * idempotent - may mutate this conf object to convert deprecated settings to supported ones. */ private[spark] def validateSettings(): Unit = { if (contains(\"spark.local.dir\")) { val msg = \"Note that spark.local.dir will be overridden by the value set by \" + \"the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS\" + \" in YARN).\" logWarning(msg) } val executorOptsKey = EXECUTOR_JAVA_OPTIONS.key // Used by Yarn in 1.1 and before sys.props.get(\"spark.driver.libraryPath\").foreach { value => val warning = s\"\"\" |spark.driver.libraryPath was detected (set to '$value'). |This is deprecated in Spark 1.2+. | |Please instead use: ${DRIVER_LIBRARY_PATH.key} \"\"\".stripMargin logWarning(warning) } // Validate spark.executor.extraJavaOptions getOption(executorOptsKey).foreach { javaOpts => if (javaOpts.contains(\"-Dspark\")) { val msg = s\"$executorOptsKey is not allowed to set Spark options (was '$javaOpts'). \" + \"Set them directly on a SparkConf or in a properties file when using ./bin/spark-submit.\" throw new Exception(msg) } if (javaOpts.contains(\"-Xmx\")) { val msg = s\"$executorOptsKey is not allowed to specify max heap memory settings \" + s\"(was '$javaOpts'). Use spark.executor.memory instead.\" throw new Exception(msg) } } // Validate memory fractions for (key <- Seq(MEMORY_FRACTION.key, MEMORY_STORAGE_FRACTION.key)) { val value = getDouble(key, 0.5) if (value > 1 || value < 0) { throw new IllegalArgumentException(s\"$key should be between 0 and 1 (was '$value').\") } } if (contains(SUBMIT_DEPLOY_MODE)) { get(SUBMIT_DEPLOY_MODE) match { case \"cluster\" | \"client\" => case e => throw new SparkException(s\"${SUBMIT_DEPLOY_MODE.key} can only be \" + \"\\\"cluster\\\" or \\\"client\\\".\") } } if (contains(CORES_MAX) && contains(EXECUTOR_CORES)) { val totalCores = getInt(CORES_MAX.key, 1) val executorCores = get(EXECUTOR_CORES) val leftCores = totalCores % executorCores if (leftCores != 0) { logWarning(s\"Total executor cores: ${totalCores} is not \" + s\"divisible by cores per executor: ${executorCores}, \" + s\"the left cores: ${leftCores} will not be allocated\") } } val encryptionEnabled = get(NETWORK_CRYPTO_ENABLED) || get(SASL_ENCRYPTION_ENABLED) require(!encryptionEnabled || get(NETWORK_AUTH_ENABLED), s\"${NETWORK_AUTH_ENABLED.key} must be enabled when enabling encryption.\") val executorTimeoutThresholdMs = get(NETWORK_TIMEOUT) * 1000 val executorHeartbeatIntervalMs = get(EXECUTOR_HEARTBEAT_INTERVAL) val networkTimeout = NETWORK_TIMEOUT.key // If spark.executor.heartbeatInterval bigger than spark.network.timeout, // it will almost always cause ExecutorLostFailure. See SPARK-22754. require(executorTimeoutThresholdMs > executorHeartbeatIntervalMs, \"The value of \" + s\"${networkTimeout}=${executorTimeoutThresholdMs}ms must be greater than the value of \" + s\"${EXECUTOR_HEARTBEAT_INTERVAL.key}=${executorHeartbeatIntervalMs}ms.\") } /** * Return a string listing all keys and values, one per line. This is useful to print the * configuration out for debugging. */ def toDebugString: String = { Utils.redact(this, getAll).sorted.map { case (k, v) => k + \"=\" + v }.mkString(\"\\n\") } } private[spark] object SparkConf extends Logging { /** * Maps deprecated config keys to information about the deprecation. * * The extra information is logged as a warning when the config is present in the user's * configuration. */ private val deprecatedConfigs: Map[String, DeprecatedConfig] = { val configs = Seq( DeprecatedConfig(\"spark.cache.class\", \"0.8\", \"The spark.cache.class property is no longer being used! Specify storage levels using \" + \"the RDD.persist() method instead.\"), DeprecatedConfig(\"spark.yarn.user.classpath.first\", \"1.3\", \"Please use spark.{driver,executor}.userClassPathFirst instead.\"), DeprecatedConfig(\"spark.kryoserializer.buffer.mb\", \"1.4\", \"Please use spark.kryoserializer.buffer instead. The default value for \" + \"spark.kryoserializer.buffer.mb was previously specified as '0.064'. Fractional values \" + \"are no longer accepted. To specify the equivalent now, one may use '64k'.\"), DeprecatedConfig(\"spark.rpc\", \"2.0\", \"Not used anymore.\"), DeprecatedConfig(\"spark.scheduler.executorTaskBlacklistTime\", \"2.1.0\", \"Please use the new excludedOnFailure options, spark.excludeOnFailure.*\"), DeprecatedConfig(\"spark.yarn.am.port\", \"2.0.0\", \"Not used anymore\"), DeprecatedConfig(\"spark.executor.port\", \"2.0.0\", \"Not used anymore\"), DeprecatedConfig(\"spark.shuffle.service.index.cache.entries\", \"2.3.0\", \"Not used anymore. Please use spark.shuffle.service.index.cache.size\"), DeprecatedConfig(\"spark.yarn.credentials.file.retention.count\", \"2.4.0\", \"Not used anymore.\"), DeprecatedConfig(\"spark.yarn.credentials.file.retention.days\", \"2.4.0\", \"Not used anymore.\"), DeprecatedConfig(\"spark.yarn.services\", \"3.0.0\", \"Feature no longer available.\"), DeprecatedConfig(\"spark.executor.plugins\", \"3.0.0\", \"Feature replaced with new plugin API. See Monitoring documentation.\"), DeprecatedConfig(\"spark.blacklist.enabled\", \"3.1.0\", \"Please use spark.excludeOnFailure.enabled\"), DeprecatedConfig(\"spark.blacklist.task.maxTaskAttemptsPerExecutor\", \"3.1.0\", \"Please use spark.excludeOnFailure.task.maxTaskAttemptsPerExecutor\"), DeprecatedConfig(\"spark.blacklist.task.maxTaskAttemptsPerNode\", \"3.1.0\", \"Please use spark.excludeOnFailure.task.maxTaskAttemptsPerNode\"), DeprecatedConfig(\"spark.blacklist.application.maxFailedTasksPerExecutor\", \"3.1.0\", \"Please use spark.excludeOnFailure.application.maxFailedTasksPerExecutor\"), DeprecatedConfig(\"spark.blacklist.stage.maxFailedTasksPerExecutor\", \"3.1.0\", \"Please use spark.excludeOnFailure.stage.maxFailedTasksPerExecutor\"), DeprecatedConfig(\"spark.blacklist.application.maxFailedExecutorsPerNode\", \"3.1.0\", \"Please use spark.excludeOnFailure.application.maxFailedExecutorsPerNode\"), DeprecatedConfig(\"spark.blacklist.stage.maxFailedExecutorsPerNode\", \"3.1.0\", \"Please use spark.excludeOnFailure.stage.maxFailedExecutorsPerNode\"), DeprecatedConfig(\"spark.blacklist.timeout\", \"3.1.0\", \"Please use spark.excludeOnFailure.timeout\"), DeprecatedConfig(\"spark.blacklist.application.fetchFailure.enabled\", \"3.1.0\", \"Please use spark.excludeOnFailure.application.fetchFailure.enabled\"), DeprecatedConfig(\"spark.scheduler.blacklist.unschedulableTaskSetTimeout\", \"3.1.0\", \"Please use spark.scheduler.excludeOnFailure.unschedulableTaskSetTimeout\"), DeprecatedConfig(\"spark.blacklist.killBlacklistedExecutors\", \"3.1.0\", \"Please use spark.excludeOnFailure.killExcludedExecutors\"), DeprecatedConfig(\"spark.yarn.blacklist.executor.launch.blacklisting.enabled\", \"3.1.0\", \"Please use spark.yarn.executor.launch.excludeOnFailure.enabled\") ) Map(configs.map { cfg => (cfg.key -> cfg) } : _*) } /** * Maps a current config key to alternate keys that were used in previous version of Spark. * * The alternates are used in the order defined in this map. If deprecated configs are * present in the user's configuration, a warning is logged. * * TODO: consolidate it with `ConfigBuilder.withAlternative`. */ private val configsWithAlternatives = Map[String, Seq[AlternateConfig]]( EXECUTOR_USER_CLASS_PATH_FIRST.key -> Seq( AlternateConfig(\"spark.files.userClassPathFirst\", \"1.3\")), UPDATE_INTERVAL_S.key -> Seq( AlternateConfig(\"spark.history.fs.update.interval.seconds\", \"1.4\"), AlternateConfig(\"spark.history.fs.updateInterval\", \"1.3\"), AlternateConfig(\"spark.history.updateInterval\", \"1.3\")), CLEANER_INTERVAL_S.key -> Seq( AlternateConfig(\"spark.history.fs.cleaner.interval.seconds\", \"1.4\")), MAX_LOG_AGE_S.key -> Seq( AlternateConfig(\"spark.history.fs.cleaner.maxAge.seconds\", \"1.4\")), \"spark.yarn.am.waitTime\" -> Seq( AlternateConfig(\"spark.yarn.applicationMaster.waitTries\", \"1.3\", // Translate old value to a duration, with 10s wait time per try. translation = s => s\"${s.toLong * 10}s\")), REDUCER_MAX_SIZE_IN_FLIGHT.key -> Seq( AlternateConfig(\"spark.reducer.maxMbInFlight\", \"1.4\")), KRYO_SERIALIZER_BUFFER_SIZE.key -> Seq( AlternateConfig(\"spark.kryoserializer.buffer.mb\", \"1.4\", translation = s => s\"${(s.toDouble * 1000).toInt}k\")), KRYO_SERIALIZER_MAX_BUFFER_SIZE.key -> Seq( AlternateConfig(\"spark.kryoserializer.buffer.max.mb\", \"1.4\")), SHUFFLE_FILE_BUFFER_SIZE.key -> Seq( AlternateConfig(\"spark.shuffle.file.buffer.kb\", \"1.4\")), EXECUTOR_LOGS_ROLLING_MAX_SIZE.key -> Seq( AlternateConfig(\"spark.executor.logs.rolling.size.maxBytes\", \"1.4\")), IO_COMPRESSION_SNAPPY_BLOCKSIZE.key -> Seq( AlternateConfig(\"spark.io.compression.snappy.block.size\", \"1.4\")), IO_COMPRESSION_LZ4_BLOCKSIZE.key -> Seq( AlternateConfig(\"spark.io.compression.lz4.block.size\", \"1.4\")), RPC_NUM_RETRIES.key -> Seq( AlternateConfig(\"spark.akka.num.retries\", \"1.4\")), RPC_RETRY_WAIT.key -> Seq( AlternateConfig(\"spark.akka.retry.wait\", \"1.4\")), RPC_ASK_TIMEOUT.key -> Seq( AlternateConfig(\"spark.akka.askTimeout\", \"1.4\")), RPC_LOOKUP_TIMEOUT.key -> Seq( AlternateConfig(\"spark.akka.lookupTimeout\", \"1.4\")), \"spark.streaming.fileStream.minRememberDuration\" -> Seq( AlternateConfig(\"spark.streaming.minRememberDuration\", \"1.5\")), \"spark.yarn.max.executor.failures\" -> Seq( AlternateConfig(\"spark.yarn.max.worker.failures\", \"1.5\")), MEMORY_OFFHEAP_ENABLED.key -> Seq( AlternateConfig(\"spark.unsafe.offHeap\", \"1.6\")), RPC_MESSAGE_MAX_SIZE.key -> Seq( AlternateConfig(\"spark.akka.frameSize\", \"1.6\")), \"spark.yarn.jars\" -> Seq( AlternateConfig(\"spark.yarn.jar\", \"2.0\")), MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM.key -> Seq( AlternateConfig(\"spark.reducer.maxReqSizeShuffleToMem\", \"2.3\"), AlternateConfig(\"spark.maxRemoteBlockSizeFetchToMem\", \"3.0\")), LISTENER_BUS_EVENT_QUEUE_CAPACITY.key -> Seq( AlternateConfig(\"spark.scheduler.listenerbus.eventqueue.size\", \"2.3\")), DRIVER_MEMORY_OVERHEAD.key -> Seq( AlternateConfig(\"spark.yarn.driver.memoryOverhead\", \"2.3\")), EXECUTOR_MEMORY_OVERHEAD.key -> Seq( AlternateConfig(\"spark.yarn.executor.memoryOverhead\", \"2.3\")), KEYTAB.key -> Seq( AlternateConfig(\"spark.yarn.keytab\", \"3.0\")), PRINCIPAL.key -> Seq( AlternateConfig(\"spark.yarn.principal\", \"3.0\")), KERBEROS_RELOGIN_PERIOD.key -> Seq( AlternateConfig(\"spark.yarn.kerberos.relogin.period\", \"3.0\")), KERBEROS_FILESYSTEMS_TO_ACCESS.key -> Seq( AlternateConfig(\"spark.yarn.access.namenodes\", \"2.2\"), AlternateConfig(\"spark.yarn.access.hadoopFileSystems\", \"3.0\")), \"spark.kafka.consumer.cache.capacity\" -> Seq( AlternateConfig(\"spark.sql.kafkaConsumerCache.capacity\", \"3.0\")) ) /** * A view of `configsWithAlternatives` that makes it more efficient to look up deprecated * config keys. * * Maps the deprecated config name to a 2-tuple (new config name, alternate config info). */ private val allAlternatives: Map[String, (String, AlternateConfig)] = { configsWithAlternatives.keys.flatMap { key => configsWithAlternatives(key).map { cfg => (cfg.key -> (key -> cfg)) } }.toMap } /** * Return whether the given config should be passed to an executor on start-up. * * Certain authentication configs are required from the executor when it connects to * the scheduler, while the rest of the spark configs can be inherited from the driver later. */ def isExecutorStartupConf(name: String): Boolean = { (name.startsWith(\"spark.auth\") && name != SecurityManager.SPARK_AUTH_SECRET_CONF) || name.startsWith(\"spark.rpc\") || name.startsWith(\"spark.network\") || isSparkPortConf(name) } /** * Return true if the given config matches either `spark.*.port` or `spark.port.*`. */ def isSparkPortConf(name: String): Boolean = { (name.startsWith(\"spark.\") && name.endsWith(\".port\")) || name.startsWith(\"spark.port.\") } /** * Looks for available deprecated keys for the given config option, and return the first * value available. */ def getDeprecatedConfig(key: String, conf: JMap[String, String]): Option[String] = { configsWithAlternatives.get(key).flatMap { alts => alts.collectFirst { case alt if conf.containsKey(alt.key) => val value = conf.get(alt.key) if (alt.translation != null) alt.translation(value) else value } } } /** * Logs a warning message if the given config key is deprecated. */ def logDeprecationWarning(key: String): Unit = { deprecatedConfigs.get(key).foreach { cfg => logWarning( s\"The configuration key '$key' has been deprecated as of Spark ${cfg.version} and \" + s\"may be removed in the future. ${cfg.deprecationMessage}\") return } allAlternatives.get(key).foreach { case (newKey, cfg) => logWarning( s\"The configuration key '$key' has been deprecated as of Spark ${cfg.version} and \" + s\"may be removed in the future. Please use the new key '$newKey' instead.\") return } if (key.startsWith(\"spark.akka\") || key.startsWith(\"spark.ssl.akka\")) { logWarning( s\"The configuration key $key is not supported anymore \" + s\"because Spark doesn't use Akka since 2.0\") } } /** * Holds information about keys that have been deprecated and do not have a replacement. * * @param key The deprecated key. * @param version Version of Spark where key was deprecated. * @param deprecationMessage Message to include in the deprecation warning. */ private case class DeprecatedConfig( key: String, version: String, deprecationMessage: String) /** * Information about an alternate configuration key that has been deprecated. * * @param key The deprecated config key. * @param version The Spark version in which the key was deprecated. * @param translation A translation function for converting old config values into new ones. */ private case class AlternateConfig( key: String, version: String, translation: String => String = null) }",
            "## METHOD: org/apache/spark/SparkContext#getExecutorThreadDump(). (implementation)\n* dump message back to the driver. */ private[spark] def getExecutorThreadDump(executorId: String): Option[Array[ThreadStackTrace]] = { try { if (executorId == SparkContext.DRIVER_IDENTIFIER) { Some(Utils.getThreadDump()) } else { env.blockManager.master.getExecutorEndpointRef(executorId) match { case Some(endpointRef) => Some(endpointRef.askSync[Array[ThreadStackTrace]](TriggerThreadDump)) case None => logWarning(s\"Executor $executorId might already have stopped and \" + \"can not request thread dump from it.\") None } } } catch { case e: Exception => logError(s\"Exception getting thread dump from executor $executorId\", e) None } } private[spark] def getLocalProperties: Properties = localProperties.get() private[spark] def setLocalProperties(props: Properties): Unit = { localProperties.set(props) } /** * Set a local property that affects jobs submitted from this thread, such as the Spark fair * scheduler pool. User-defined properties may also be set here. These properties are propagated * through to worker tasks and can be accessed there via * [[org.apache.spark.TaskContext#getLocalProperty]]. * * These properties are inherited by child threads spawned from this thread. This * may have unexpected consequences when working with thread pools. The standard java * implementation of thread pools have worker threads spawn other worker threads. * As a result, local properties may propagate unpredictably. */ def setLocalProperty(key: String, value: String): Unit = { if (value == null) { localProperties.get.remove(key) } else { localProperties.get.setProperty(key, value) } } /** * Get a local property set in this thread, or null if it is missing. See * `org.apache.spark.SparkContext.setLocalProperty`. */ def getLocalProperty(key: String): String = Option(localProperties.get).map(_.getProperty(key)).orNull /** Set a human readable description of the current job. */ def setJobDescription(value: String): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, value) } /** * Assigns a group ID to all the jobs started by this thread until the group ID is set to a * different value or cleared. * * Often, a unit of execution in an application consists of multiple Spark actions or jobs. * Application programmers can use this method to group all those jobs together and give a * group description. Once set, the Spark web UI will associate such jobs with this group. * * The application can also use `org.apache.spark.SparkContext.cancelJobGroup` to cancel all * running jobs in this group. For example, * {{{ * // In the main thread: * sc.setJobGroup(\"some_job_to_cancel\", \"some job description\") * sc.parallelize(1 to 10000, 2).map { i => Thread.sleep(10); i }.count() * * // In a separate thread: * sc.cancelJobGroup(\"some_job_to_cancel\") * }}} * * @param interruptOnCancel If true, then job cancellation will result in `Thread.interrupt()` * being called on the job's executor threads. This is useful to help ensure that the tasks * are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS * may respond to Thread.interrupt() by marking nodes as dead. */ def setJobGroup(groupId: String, description: String, interruptOnCancel: Boolean = false): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId) // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup // APIs to also take advantage of this property (e.g., internal job failures or canceling from // JobProgressTab UI) on a per-job basis. setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString) } /** Clear the current thread's job group ID and its description. */ def clearJobGroup(): Unit = { setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, null) setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, null) setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, null) } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * @note Return statements are NOT allowed in the given body. */ private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](this)(body) // Methods for creating RDDs /** Distribute a local Scala collection to form an RDD. * * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call * to parallelize and before the first action on the RDD, the resultant RDD will reflect the * modified collection. Pass a copy of the argument to avoid this. * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection */ def parallelize[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { assertNotStopped() new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]()) } /** * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by * `step` every element. * * @note if we need to cache this RDD, we should make sure each partition does not exceed limit. * * @param start the start value. * @param end the end value. * @param step the incremental step * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed range */ def range( start: Long, end: Long, step: Long = 1, numSlices: Int = defaultParallelism): RDD[Long] = withScope { assertNotStopped() // when step is 0, range will run infinitely require(step != 0, \"step cannot be 0\") val numElements: BigInt = { val safeStart = BigInt(start) val safeEnd = BigInt(end) if ((safeEnd - safeStart) % step == 0 || (safeEnd > safeStart) != (step > 0)) { (safeEnd - safeStart) / step } else { // the remainder has the same sign with range, could add 1 more (safeEnd - safeStart) / step + 1 } } parallelize(0 until numSlices, numSlices).mapPartitionsWithIndex { (i, _) => val partitionStart = (i * numElements) / numSlices * step + start val partitionEnd = (((i + 1) * numElements) / numSlices) * step + start def getSafeMargin(bi: BigInt): Long = if (bi.isValidLong) { bi.toLong } else if (bi > 0) { Long.MaxValue } else { Long.MinValue } val safePartitionStart = getSafeMargin(partitionStart) val safePartitionEnd = getSafeMargin(partitionEnd) new Iterator[Long] { private[this] var number: Long = safePartitionStart private[this] var overflow: Boolean = false override def hasNext = if (!overflow) { if (step > 0) { number < safePartitionEnd } else { number > safePartitionEnd } } else false override def next() = { val ret = number number += step if (number < ret ^ step < 0) { // we have Long.MaxValue + Long.MaxValue < Long.MaxValue // and Long.MinValue + Long.MinValue > Long.MinValue, so iff the step causes a step // back, we are pretty sure that we have an overflow. overflow = true } ret } } } } /** Distribute a local Scala collection to form an RDD. * * This method is identical to `parallelize`. * @param seq Scala collection to distribute * @param numSlices number of partitions to divide the collection into * @return RDD representing distributed collection */ def makeRDD[T: ClassTag]( seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices) } /** * Distribute a local Scala collection to form an RDD, with one or more * location preferences (hostnames of Spark nodes) for each object. * Create a new partition for each collection item. * @param seq list of tuples of data and location preferences (hostnames of Spark nodes) * @return RDD representing data partitioned according to location preferences */ def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope { assertNotStopped() val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs) } /** * Read a text file from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI, and return it as an RDD of Strings. * The text files must be encoded as UTF-8. * * @param path path to the text file on a supported file system * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of lines of the text file */ def textFile( path: String, minPartitions: Int = defaultMinPartitions): RDD[String] = withScope { assertNotStopped() hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair => pair._2.toString).setName(path) } /** * Read a directory of text files from HDFS, a local file system (available on all nodes), or any * Hadoop-supported file system URI. Each file is read as a single record and returned in a * key-value pair, where the key is the path of each file, the value is the content of each file. * The text files must be encoded as UTF-8. * * <p> For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do `val rdd = sparkContext.wholeTextFile(\"hdfs://a-hdfs-path\")`, * * <p> then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred, large file is also allowable, but may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and the corresponding file content */ def wholeTextFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, String)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that wholeTextFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new WholeTextFileRDD( this, classOf[WholeTextFileInputFormat], classOf[Text], classOf[Text], updateConf, minPartitions).map(record => (record._1.toString, record._2.toString)).setName(path) } /** * Get an RDD for a Hadoop-readable dataset as PortableDataStream for each file * (useful for binary data) * * For example, if you have the following files: * {{{ * hdfs://a-hdfs-path/part-00000 * hdfs://a-hdfs-path/part-00001 * ... * hdfs://a-hdfs-path/part-nnnnn * }}} * * Do * `val rdd = sparkContext.binaryFiles(\"hdfs://a-hdfs-path\")`, * * then `rdd` contains * {{{ * (a-hdfs-path/part-00000, its content) * (a-hdfs-path/part-00001, its content) * ... * (a-hdfs-path/part-nnnnn, its content) * }}} * * @note Small files are preferred; very large files may cause bad performance. * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files * in a directory rather than `.../path/` or `.../path` * @note Partitioning is determined by data locality. This may result in too few partitions * by default. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param minPartitions A suggestion value of the minimal splitting number for input data. * @return RDD representing tuples of file path and corresponding file content */ def binaryFiles( path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, PortableDataStream)] = withScope { assertNotStopped() val job = NewHadoopJob.getInstance(hadoopConfiguration) // Use setInputPaths so that binaryFiles aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updateConf = job.getConfiguration new BinaryFileRDD( this, classOf[StreamInputFormat], classOf[String], classOf[PortableDataStream], updateConf, minPartitions).setName(path) } /** * Load data from a flat binary file, assuming the length of each record is constant. * * @note We ensure that the byte array for each record in the resulting RDD * has the provided record length. * * @param path Directory to the input data files, the path can be comma separated paths as the * list of inputs. * @param recordLength The length at which to split the records * @param conf Configuration for setting up the dataset. * * @return An RDD of data with values, represented as byte arrays */ def binaryRecords( path: String, recordLength: Int, conf: Configuration = hadoopConfiguration): RDD[Array[Byte]] = withScope { assertNotStopped() conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength) val br = newAPIHadoopFile[LongWritable, BytesWritable, FixedLengthBinaryInputFormat](path, classOf[FixedLengthBinaryInputFormat], classOf[LongWritable], classOf[BytesWritable], conf = conf) br.map { case (k, v) => val bytes = v.copyBytes() assert(bytes.length == recordLength, \"Byte array does not have correct length\") bytes } } /** * Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf given its InputFormat and other * necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable), * using the older MapReduce API (`org.apache.hadoop.mapred`). * * @param conf JobConf for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions Minimum number of Hadoop Splits to generate. * @return RDD of tuples of key and corresponding value * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. */ def hadoopRDD[K, V]( conf: JobConf, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf before broadcasting it. SparkHadoopUtil.get.addCredentials(conf) new HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minPartitions) } /** Get an RDD for a Hadoop file with an arbitrary InputFormat * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param inputFormatClass storage format of the data to be read * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value */ def hadoopFile[K, V]( path: String, inputFormatClass: Class[_ <: InputFormat[K, V]], keyClass: Class[K], valueClass: Class[V], minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // A Hadoop configuration can be about 10 KiB, which is pretty big, so broadcast it. val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration)) val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path) new HadoopRDD( this, confBroadcast, Some(setInputPathsFunc), inputFormatClass, keyClass, valueClass, minPartitions).setName(path) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minPartitions) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value */ def hadoopFile[K, V, F <: InputFormat[K, V]] (path: String, minPartitions: Int) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile(path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]], minPartitions) } /** * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys, * values and the InputFormat so that users don't need to pass them directly. Instead, callers * can just write, for example, * {{{ * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * }}} * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths as * a list of inputs * @return RDD of tuples of key and corresponding value */ def hadoopFile[K, V, F <: InputFormat[K, V]](path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { hadoopFile[K, V, F](path, defaultMinPartitions) } /** * Smarter version of `newApiHadoopFile` that uses class tags to figure out the classes of keys, * values and the `org.apache.hadoop.mapreduce.InputFormat` (new MapReduce API) so that user * don't need to pass them directly. Instead, callers can just write, for example: * ``` * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path) * ``` * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @return RDD of tuples of key and corresponding value */ def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]] (path: String) (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope { newAPIHadoopFile( path, fm.runtimeClass.asInstanceOf[Class[F]], km.runtimeClass.asInstanceOf[Class[K]], vm.runtimeClass.asInstanceOf[Class[V]]) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * @param conf Hadoop configuration * @return RDD of tuples of key and corresponding value */ def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]( path: String, fClass: Class[F], kClass: Class[K], vClass: Class[V], conf: Configuration = hadoopConfiguration): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(hadoopConfiguration) // The call to NewHadoopJob automatically adds security credentials to conf, // so we don't need to explicitly add them ourselves val job = NewHadoopJob.getInstance(conf) // Use setInputPaths so that newAPIHadoopFile aligns with hadoopFile/textFile in taking // comma separated files as input. (see SPARK-7155) NewFileInputFormat.setInputPaths(job, path) val updatedConf = job.getConfiguration new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path) } /** * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat * and extra configuration options to pass to the input format. * * @param conf Configuration for setting up the dataset. Note: This will be put into a Broadcast. * Therefore if you plan to reuse this conf to create multiple RDDs, you need to make * sure you won't modify the conf. A safe approach is always creating a new conf for * a new RDD. * @param fClass storage format of the data to be read * @param kClass `Class` of the key associated with the `fClass` parameter * @param vClass `Class` of the value associated with the `fClass` parameter * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. */ def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]( conf: Configuration = hadoopConfiguration, fClass: Class[F], kClass: Class[K], vClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() // This is a hack to enforce loading hdfs-site.xml. // See SPARK-11227 for details. FileSystem.getLocal(conf) // Add necessary security credentials to the JobConf. Required to access secure HDFS. val jconf = new JobConf(conf) SparkHadoopUtil.get.addCredentials(jconf) new NewHadoopRDD(this, fClass, kClass, vClass, jconf) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value */ def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V], minPartitions: Int ): RDD[(K, V)] = withScope { assertNotStopped() val inputFormatClass = classOf[SequenceFileInputFormat[K, V]] hadoopFile(path, inputFormatClass, keyClass, valueClass, minPartitions) } /** * Get an RDD for a Hadoop SequenceFile with given key and value types. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param keyClass `Class` of the key associated with `SequenceFileInputFormat` * @param valueClass `Class` of the value associated with `SequenceFileInputFormat` * @return RDD of tuples of key and corresponding value */ def sequenceFile[K, V]( path: String, keyClass: Class[K], valueClass: Class[V]): RDD[(K, V)] = withScope { assertNotStopped() sequenceFile(path, keyClass, valueClass, defaultMinPartitions) } /** * Version of sequenceFile() for types implicitly convertible to Writables through a * WritableConverter. For example, to access a SequenceFile where the keys are Text and the * values are IntWritable, you could simply write * {{{ * sparkContext.sequenceFile[String, Int](path, ...) * }}} * * WritableConverters are provided in a somewhat strange way (by an implicit function) to support * both subclasses of Writable and types for which we define a converter (e.g. Int to * IntWritable). The most natural thing would've been to have implicit objects for the * converters, but then we couldn't have an object for every subclass of Writable (you can't * have a parameterized singleton object). We use functions instead to create a new converter * for the appropriate type. In addition, we pass the converter a ClassTag of its type to * allow it to figure out the Writable class to use in the subclass case. * * @note Because Hadoop's RecordReader class re-uses the same Writable object for each * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle * operation will create many references to the same object. * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first * copy them using a `map` function. * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD of tuples of key and corresponding value */ def sequenceFile[K, V] (path: String, minPartitions: Int = defaultMinPartitions) (implicit km: ClassTag[K], vm: ClassTag[V], kcf: () => WritableConverter[K], vcf: () => WritableConverter[V]): RDD[(K, V)] = { withScope { assertNotStopped() val kc = clean(kcf)() val vc = clean(vcf)() val format = classOf[SequenceFileInputFormat[Writable, Writable]] val writables = hadoopFile(path, format, kc.writableClass(km).asInstanceOf[Class[Writable]], vc.writableClass(vm).asInstanceOf[Class[Writable]], minPartitions) writables.map { case (k, v) => (kc.convert(k), vc.convert(v)) } } } /** * Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and * BytesWritable values that contain a serialized partition. This is still an experimental * storage format and may not be supported exactly as is in future Spark releases. It will also * be pretty slow if you use the default serializer (Java serialization), * though the nice thing about it is that there's very little effort required to save arbitrary * objects. * * @param path directory to the input data files, the path can be comma separated paths * as a list of inputs * @param minPartitions suggested minimum number of partitions for the resulting RDD * @return RDD representing deserialized data from the file(s) */ def objectFile[T: ClassTag]( path: String, minPartitions: Int = defaultMinPartitions): RDD[T] = withScope { assertNotStopped() sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions) .flatMap(x => Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader)) } protected[spark] def checkpointFile[T: ClassTag](path: String): RDD[T] = withScope { new ReliableCheckpointRDD[T](this, path) } /** Build the union of a list of RDDs. */ def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope { val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty) val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet if (nonEmptyRdds.forall(_.partitioner.isDefined) && partitioners.size == 1) { new PartitionerAwareUnionRDD(this, nonEmptyRdds) } else { new UnionRDD(this, nonEmptyRdds) } } /** Build the union of a list of RDDs passed as variable-length arguments. */ def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope { union(Seq(first) ++ rest) } /** Get an RDD that has no partitions or elements. */ def emptyRDD[T: ClassTag]: RDD[T] = new EmptyRDD[T](this) // Methods for creating shared variables /** * Register the given accumulator. * * @note Accumulators must be registered before use, or it will throw exception. */ def register(acc: AccumulatorV2[_, _]): Unit = { acc.register(this) } /** * Register the given accumulator with given name. * * @note Accumulators must be registered before use, or it will throw exception. */ def register(acc: AccumulatorV2[_, _], name: String): Unit = { acc.register(this, name = Option(name)) } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`. */ def longAccumulator: LongAccumulator = { val acc = new LongAccumulator register(acc) acc } /** * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`. */ def longAccumulator(name: String): LongAccumulator = { val acc = new LongAccumulator register(acc, name) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`. */ def doubleAccumulator: DoubleAccumulator = { val acc = new DoubleAccumulator register(acc) acc } /** * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`. */ def doubleAccumulator(name: String): DoubleAccumulator = { val acc = new DoubleAccumulator register(acc, name) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list. */ def collectionAccumulator[T]: CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc) acc } /** * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates * inputs by adding them into the list. */ def collectionAccumulator[T](name: String): CollectionAccumulator[T] = { val acc = new CollectionAccumulator[T] register(acc, name) acc } /** * Broadcast a read-only variable to the cluster, returning a * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions. * The variable will be sent to each cluster only once. * * @param value value to broadcast to the Spark nodes * @return `Broadcast` object, a read-only variable cached on each machine */ def broadcast[T: ClassTag](value: T): Broadcast[T] = { assertNotStopped() require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass), \"Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\") val bc = env.broadcastManager.newBroadcast[T](value, isLocal) val callSite = getCallSite logInfo(\"Created broadcast \" + bc.id + \" from \" + callSite.shortForm) cleaner.foreach(_.registerBroadcastForCleanup(bc)) bc } /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * * @note A path can be added only once. Subsequent additions of the same path are ignored. */ def addFile(path: String): Unit = { addFile(path, false, false) } /** * Returns a list of file paths that are added to resources. */ def listFiles(): Seq[String] = addedFiles.keySet.toSeq /** * :: Experimental :: * Add an archive to be downloaded and unpacked with this Spark job on every node. * * If an archive is added during execution, it will not be available until the next TaskSet * starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(paths-to-files)` to find its download/unpacked location. * The given path should be one of .zip, .tar, .tar.gz, .tgz and .jar. * * @note A path can be added only once. Subsequent additions of the same path are ignored. * * @since 3.1.0 */ @Experimental def addArchive(path: String): Unit = { addFile(path, false, false, isArchive = true) } /** * :: Experimental :: * Returns a list of archive paths that are added to resources. * * @since 3.1.0 */ @Experimental def listArchives(): Seq[String] = addedArchives.keySet.toSeq /** * Add a file to be downloaded with this Spark job on every node. * * If a file is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs, * use `SparkFiles.get(fileName)` to find its download location. * @param recursive if true, a directory can be given in `path`. Currently directories are * only supported for Hadoop-supported filesystems. * * @note A path can be added only once. Subsequent additions of the same path are ignored. */ def addFile(path: String, recursive: Boolean): Unit = { addFile(path, recursive, false) } private def addFile( path: String, recursive: Boolean, addedOnSubmit: Boolean, isArchive: Boolean = false ): Unit = { val uri = Utils.resolveURI(path) val schemeCorrectedURI = uri.getScheme match { case null => new File(path).getCanonicalFile.toURI case \"local\" => logWarning(s\"File with 'local' scheme $path is not supported to add to file server, \" + s\"since it is already available on every node.\") return case _ => uri } val hadoopPath = new Path(schemeCorrectedURI) val scheme = schemeCorrectedURI.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) { val fs = hadoopPath.getFileSystem(hadoopConfiguration) val isDir = fs.getFileStatus(hadoopPath).isDirectory if (!isLocal && scheme == \"file\" && isDir) { throw new SparkException(s\"addFile does not support local directories when not running \" + \"local mode.\") } if (!recursive && isDir) { throw new SparkException(s\"Added file $hadoopPath is a directory and recursive is not \" + \"turned on.\") } } else { // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) } val key = if (!isLocal && scheme == \"file\") { env.rpcEnv.fileServer.addFile(new File(uri.getPath)) } else if (uri.getScheme == null) { schemeCorrectedURI.toString } else { uri.toString } val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis if (!isArchive && addedFiles.putIfAbsent(key, timestamp).isEmpty) { logInfo(s\"Added file $path at $key with timestamp $timestamp\") // Fetch the file locally so that closures which are run on the driver can still use the // SparkFiles API to access files. Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf, hadoopConfiguration, timestamp, useCache = false) postEnvironmentUpdate() } else if ( isArchive && addedArchives.putIfAbsent( UriBuilder.fromUri(new URI(key)).fragment(uri.getFragment).build().toString, timestamp).isEmpty) { logInfo(s\"Added archive $path at $key with timestamp $timestamp\") // If the scheme is file, use URI to simply copy instead of downloading. val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key) val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build() val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false) val dest = new File( SparkFiles.getRootDirectory(), if (uri.getFragment != null) uri.getFragment else source.getName) logInfo( s\"Unpacking an archive $path from ${source.getAbsolutePath} to ${dest.getAbsolutePath}\") Utils.deleteRecursively(dest) Utils.unpack(source, dest) postEnvironmentUpdate() } else { logWarning(s\"The path $path has been added already. Overwriting of added paths \" + \"is not supported in the current version.\") } } /** * :: DeveloperApi :: * Register a listener to receive up-calls from events that happen during execution. */ @DeveloperApi def addSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.addToSharedQueue(listener) } /** * :: DeveloperApi :: * Deregister the listener from Spark's listener bus. */ @DeveloperApi def removeSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.removeListener(listener) } private[spark] def getExecutorIds(): Seq[String] = { schedulerBackend match { case b: ExecutorAllocationClient => b.getExecutorIds() case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") Nil } } /** * Get the max number of tasks that can be concurrent launched based on the ResourceProfile * could be used, even if some of them are being used at the moment. * Note that please don't cache the value returned by this method, because the number can change * due to add/remove executors. * * @param rp ResourceProfile which to use to calculate max concurrent tasks. * @return The max number of tasks that can be concurrent launched currently. */ private[spark] def maxNumConcurrentTasks(rp: ResourceProfile): Int = { schedulerBackend.maxNumConcurrentTasks(rp) } /** * Update the cluster manager on our scheduling needs. Three bits of information are included * to help it make decisions. This applies to the default ResourceProfile. * @param numExecutors The total number of executors we'd like to have. The cluster manager * shouldn't kill any running executor to reach this number, but, * if all existing executors were to die, this is the number of executors * we'd want to be allocated. * @param localityAwareTasks The number of tasks in all active stages that have a locality * preferences. This includes running, pending, and completed tasks. * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages * that would like to like to run on that host. * This includes running, pending, and completed tasks. * @return whether the request is acknowledged by the cluster manager. */ @DeveloperApi def requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: immutable.Map[String, Int] ): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => // this is being applied to the default resource profile, would need to add api to support // others val defaultProfId = resourceProfileManager.defaultResourceProfile.id b.requestTotalExecutors(immutable.Map(defaultProfId-> numExecutors), immutable.Map(localityAwareTasks -> defaultProfId), immutable.Map(defaultProfId -> hostToLocalTaskCount)) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request an additional number of executors from the cluster manager. * @return whether the request is received. */ @DeveloperApi def requestExecutors(numAdditionalExecutors: Int): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.requestExecutors(numAdditionalExecutors) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executors. * * This is not supported when dynamic allocation is turned on. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executors it kills * through this method with new ones, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received. */ @DeveloperApi def killExecutors(executorIds: Seq[String]): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => require(executorAllocationManager.isEmpty, \"killExecutors() unsupported with Dynamic Allocation turned on\") b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executor. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executor it kills * through this method with a new one, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received. */ @DeveloperApi def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId)) /** * Request that the cluster manager kill the specified executor without adjusting the * application resource requirements. * * The effect is that a new executor will be launched in place of the one killed by * this request. This assumes the cluster manager will automatically and eventually * fulfill all missing application resource requests. * * @note The replace is by no means guaranteed; another application on the same cluster * can steal the window of opportunity and acquire this application's resources in the * mean time. * * @return whether the request is received. */ private[spark] def killAndReplaceExecutor(executorId: String): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** The version of Spark on which this application is running. */ def version: String = SPARK_VERSION /** * Return a map from the block manager to the max memory available for caching and the remaining * memory available for caching. */ def getExecutorMemoryStatus: Map[String, (Long, Long)] = { assertNotStopped() env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) => (blockManagerId.host + \":\" + blockManagerId.port, mem) } } /** * :: DeveloperApi :: * Return information about what RDDs are cached, if they are in mem or on disk, how much space * they take, etc. */ @DeveloperApi def getRDDStorageInfo: Array[RDDInfo] = { getRDDStorageInfo(_ => true) } private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = { assertNotStopped() val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray rddInfos.foreach { rddInfo => val rddId = rddInfo.id val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId)) rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0) rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L) rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L) } rddInfos.filter(_.isCached) } /** * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call. * * @note This does not necessarily mean the caching or computation was successful. */ def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap /** * :: DeveloperApi :: * Return pools for fair scheduler */ @DeveloperApi def getAllPools: Seq[Schedulable] = { assertNotStopped() // TODO(xiajunluan): We should take nested pools into account taskScheduler.rootPool.schedulableQueue.asScala.toSeq } /** * :: DeveloperApi :: * Return the pool associated with the given name, if one exists */ @DeveloperApi def getPoolForName(pool: String): Option[Schedulable] = { assertNotStopped() Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool)) } /** * Return current scheduling mode */ def getSchedulingMode: SchedulingMode.SchedulingMode = { assertNotStopped() taskScheduler.schedulingMode } /** * Gets the locality information associated with the partition in a particular rdd * @param rdd of interest * @param partition to be looked up for locality * @return list of preferred locations for the partition */ private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { dagScheduler.getPreferredLocs(rdd, partition) } /** * Register an RDD to be persisted in memory and/or disk storage */ private[spark] def persistRDD(rdd: RDD[_]): Unit = { persistentRdds(rdd.id) = rdd } /** * Unpersist an RDD from memory and/or disk storage */ private[spark] def unpersistRDD(rddId: Int, blocking: Boolean): Unit = { env.blockManager.master.removeRdd(rddId, blocking) persistentRdds.remove(rddId) listenerBus.post(SparkListenerUnpersistRDD(rddId)) } /** * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future. * * If a jar is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. * * @note A path can be added only once. Subsequent additions of the same path are ignored. */ def addJar(path: String): Unit = { addJar(path, false) } private def addJar(path: String, addedOnSubmit: Boolean): Unit = { def addLocalJarFile(file: File): Seq[String] = { try { if (!file.exists()) { throw new FileNotFoundException(s\"Jar ${file.getAbsolutePath} not found\") } if (file.isDirectory) { throw new IllegalArgumentException( s\"Directory ${file.getAbsoluteFile} is not allowed for addJar\") } Seq(env.rpcEnv.fileServer.addJar(file)) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } def checkRemoteJarFile(path: String): Seq[String] = { val hadoopPath = new Path(path) val scheme = hadoopPath.toUri.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme)) { try { val fs = hadoopPath.getFileSystem(hadoopConfiguration) if (!fs.exists(hadoopPath)) { throw new FileNotFoundException(s\"Jar ${path} not found\") } if (fs.getFileStatus(hadoopPath).isDirectory) { throw new IllegalArgumentException( s\"Directory ${path} is not allowed for addJar\") } Seq(path) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } else { Seq(path) } } if (path == null || path.isEmpty) { logWarning(\"null or empty path specified as parameter to addJar\") } else { val (keys, scheme) = if (path.contains(\"\\\\\") && Utils.isWindows) { // For local paths with backslashes on Windows, URI throws an exception (addLocalJarFile(new File(path)), \"local\") } else { val uri = Utils.resolveURI(path) // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) val uriScheme = uri.getScheme val jarPaths = uriScheme match { // A JAR file which exists only on the driver node case null => // SPARK-22585 path without schema is not url encoded addLocalJarFile(new File(uri.getPath)) // A JAR file which exists only on the driver node case \"file\" => addLocalJarFile(new File(uri.getPath)) // A JAR file which exists locally on every worker node case \"local\" => Seq(\"file:\" + uri.getPath) case \"ivy\" => // Since `new Path(path).toUri` will lose query information, // so here we use `URI.create(path)` DependencyUtils.resolveMavenDependencies(URI.create(path)) .flatMap(jar => addLocalJarFile(new File(jar))) case _ => checkRemoteJarFile(path) } (jarPaths, uriScheme) } if (keys.nonEmpty) { val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis val (added, existed) = keys.partition(addedJars.putIfAbsent(_, timestamp).isEmpty) if (added.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"Added $jarMessage $path at ${added.mkString(\",\")} with timestamp $timestamp\") postEnvironmentUpdate() } if (existed.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" + \" Overwriting of added jar is not supported in the current version.\") } } } } /** * Returns a list of jar files that are added to resources. */ def listJars(): Seq[String] = addedJars.keySet.toSeq /** * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark * may wait for some internal threads to finish. It's better to use this method to stop * SparkContext instead. */ private[spark] def stopInNewThread(): Unit = { new Thread(\"stop-spark-context\") { setDaemon(true) override def run(): Unit = { try { SparkContext.this.stop() } catch { case e: Throwable => logError(e.getMessage, e) throw e } } }.start() } /** * Shut down the SparkContext. */ def stop(): Unit = { if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop SparkContext within listener bus thread.\") } // Use the stopping variable to ensure no contention for the stop scenario. // Still track the stopped variable for use elsewhere in the code. if (!stopped.compareAndSet(false, true)) { logInfo(\"SparkContext already stopped.\") return } if (_shutdownHookRef != null) { ShutdownHookManager.removeShutdownHook(_shutdownHookRef) } if (listenerBus != null) { Utils.tryLogNonFatalError { postApplicationEnd() } } Utils.tryLogNonFatalError { _driverLogger.foreach(_.stop()) } Utils.tryLogNonFatalError { _ui.foreach(_.stop()) } Utils.tryLogNonFatalError { _cleaner.foreach(_.stop()) } Utils.tryLogNonFatalError { _executorAllocationManager.foreach(_.stop()) } if (_dagScheduler != null) { Utils.tryLogNonFatalError { _dagScheduler.stop() } _dagScheduler = null } if (_listenerBusStarted) { Utils.tryLogNonFatalError { listenerBus.stop() _listenerBusStarted = false } } if (env != null) { Utils.tryLogNonFatalError { env.metricsSystem.report() } } Utils.tryLogNonFatalError { _plugins.foreach(_.shutdown()) } FallbackStorage.cleanUp(_conf, _hadoopConfiguration) Utils.tryLogNonFatalError { _eventLogger.foreach(_.stop()) } if (_heartbeater != null) { Utils.tryLogNonFatalError { _heartbeater.stop() } _heartbeater = null } if (_shuffleDriverComponents != null) { Utils.tryLogNonFatalError { _shuffleDriverComponents.cleanupApplication() } } if (env != null && _heartbeatReceiver != null) { Utils.tryLogNonFatalError { env.rpcEnv.stop(_heartbeatReceiver) } } Utils.tryLogNonFatalError { _progressBar.foreach(_.stop()) } _taskScheduler = null // TODO: Cache.stop()? if (_env != null) { Utils.tryLogNonFatalError { _env.stop() } SparkEnv.set(null) } if (_statusStore != null) { _statusStore.close() } // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this // `SparkContext` is stopped. localProperties.remove() ResourceProfile.clearDefaultProfile() // Unset YARN mode system env variable, to allow switching between cluster types. SparkContext.clearActiveContext() logInfo(\"Successfully stopped SparkContext\") } /** * Get Spark's home location from either a value set through the constructor, * or the spark.home Java property, or the SPARK_HOME environment variable * (in that order of preference). If neither of these is set, return None. */ private[spark] def getSparkHome(): Option[String] = { conf.getOption(\"spark.home\").orElse(Option(System.getenv(\"SPARK_HOME\"))) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs. */ def setCallSite(shortCallSite: String): Unit = { setLocalProperty(CallSite.SHORT_FORM, shortCallSite) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs. */ private[spark] def setCallSite(callSite: CallSite): Unit = { setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm) setLocalProperty(CallSite.LONG_FORM, callSite.longForm) } /** * Clear the thread-local property for overriding the call sites * of actions and RDDs. */ def clearCallSite(): Unit = { setLocalProperty(CallSite.SHORT_FORM, null) setLocalProperty(CallSite.LONG_FORM, null) } /** * Capture the current user callsite and return a formatted version for printing. If the user * has overridden the call site using `setCallSite()`, this will return the user's version. */ private[spark] def getCallSite(): CallSite = { lazy val callSite = Utils.getCallSite() CallSite( Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm), Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm) ) } /** * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to */ def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * The function that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */ def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res) results } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */ def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions) } /** * Run a job on all partitions in an RDD and return the results in an array. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */ def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and return the results in an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */ def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to */ def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: (TaskContext, Iterator[T]) => U, resultHandler: (Int, U) => Unit): Unit = { runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to */ def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: Iterator[T] => U, resultHandler: (Int, U) => Unit): Unit = { val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter) runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler) } /** * :: DeveloperApi :: * Run a job that can return approximate results. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param evaluator `ApproximateEvaluator` to receive the partial results * @param timeout maximum time to wait for the job, in milliseconds * @return partial result (how partial depends on whether the job was finished before or * after timeout) */ @DeveloperApi def runApproximateJob[T, U, R]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, evaluator: ApproximateEvaluator[U, R], timeout: Long): PartialResult[R] = { assertNotStopped() val callSite = getCallSite logInfo(\"Starting job: \" + callSite.shortForm) val start = System.nanoTime val cleanedFunc = clean(func) val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout, localProperties.get) logInfo( \"Job finished: \" + callSite.shortForm + \", took \" + (System.nanoTime - start) / 1e9 + \" s\") result } /** * Submit a job for execution and return a FutureJob holding the result. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to * @param resultFunc function to be executed when the result is ready */ def submitJob[T, U, R]( rdd: RDD[T], processPartition: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit, resultFunc: => R): SimpleFutureAction[R] = { assertNotStopped() val cleanF = clean(processPartition) val callSite = getCallSite val waiter = dagScheduler.submitJob( rdd, (context: TaskContext, iter: Iterator[T]) => cleanF(iter), partitions, callSite, resultHandler, localProperties.get) new SimpleFutureAction(waiter, resultFunc) } /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future. */ private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } /** * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup` * for more information. */ def cancelJobGroup(groupId: String): Unit = { assertNotStopped() dagScheduler.cancelJobGroup(groupId) } /** Cancel all jobs that have been scheduled or are running. */ def cancelAllJobs(): Unit = { assertNotStopped() dagScheduler.cancelAllJobs() } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @param reason optional reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent */ def cancelJob(jobId: Int, reason: String): Unit = { dagScheduler.cancelJob(jobId, Option(reason)) } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent */ def cancelJob(jobId: Int): Unit = { dagScheduler.cancelJob(jobId, None) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @param reason reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent */ def cancelStage(stageId: Int, reason: String): Unit = { dagScheduler.cancelStage(stageId, Option(reason)) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent */ def cancelStage(stageId: Int): Unit = { dagScheduler.cancelStage(stageId, None) } /** * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI * or through SparkListener.onTaskStart. * * @param taskId the task ID to kill. This id uniquely identifies the task attempt. * @param interruptThread whether to interrupt the thread running the task. * @param reason the reason for killing the task, which should be a short string. If a task * is killed multiple times with different reasons, only one reason will be reported. * * @return Whether the task was successfully killed. */ def killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean = { dagScheduler.killTaskAttempt(taskId, interruptThread, reason) } /** * Clean a closure to make it ready to be serialized and sent to tasks * (removes unreferenced variables in $outer's, updates REPL variables) * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt> * if not. * * @param f the closure to clean * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not * serializable * @return the cleaned closure */ private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = { ClosureCleaner.clean(f, checkSerializable) f } /** * Set the directory under which RDDs are going to be checkpointed. * @param directory path to the directory where checkpoint files will be stored * (must be HDFS path if running in cluster) */ def setCheckpointDir(directory: String): Unit = { // If we are running on a cluster, log a warning if the directory is local. // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from // its own local file system, which is incorrect because the checkpoint files // are actually on the executor machines. if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) { logWarning(\"Spark is not running in local mode, therefore the checkpoint directory \" + s\"must not be on the local filesystem. Directory '$directory' \" + \"appears to be on the local filesystem.\") } checkpointDir = Option(directory).map { dir => val path = new Path(dir, UUID.randomUUID().toString) val fs = path.getFileSystem(hadoopConfiguration) fs.mkdirs(path) fs.getFileStatus(path).getPath.toString } } def getCheckpointDir: Option[String] = checkpointDir /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). */ def defaultParallelism: Int = { assertNotStopped() taskScheduler.defaultParallelism } /** * Default min number of partitions for Hadoop RDDs when not given by user * Notice that we use math.min so the \"defaultMinPartitions\" cannot be higher than 2. * The reasons for this are discussed in https://github.com/mesos/spark/pull/718 */ def defaultMinPartitions: Int = math.min(defaultParallelism, 2) private val nextShuffleId = new AtomicInteger(0) private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement() private val nextRddId = new AtomicInteger(0) /** Register a new RDD, returning its RDD ID */ private[spark] def newRddId(): Int = nextRddId.getAndIncrement() /** * Registers listeners specified in spark.extraListeners, then starts the listener bus. * This should be called after all internal listeners have been registered with the listener bus * (e.g. after the web UI and event logging listeners have been registered). */ private def setupAndStartListenerBus(): Unit = { try { conf.get(EXTRA_LISTENERS).foreach { classNames => val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf) listeners.foreach { listener => listenerBus.addToSharedQueue(listener) logInfo(s\"Registered listener ${listener.getClass().getName()}\") } } } catch { case e: Exception => try { stop() } finally { throw new SparkException(s\"Exception when registering SparkListener\", e) } } listenerBus.start(this, _env.metricsSystem) _listenerBusStarted = true } /** Post the application start event */ private def postApplicationStart(): Unit = { // Note: this code assumes that the task scheduler has been initialized and has contacted // the cluster manager to get an application ID (in case the cluster manager provides one). listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId), startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls, schedulerBackend.getDriverAttributes)) _driverLogger.foreach(_.startSync(_hadoopConfiguration)) } /** Post the application end event */ private def postApplicationEnd(): Unit = { listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis)) } /** Post the environment update event once the task scheduler is ready */ private def postEnvironmentUpdate(): Unit = { if (taskScheduler != null) { val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val addedArchivePaths = addedArchives.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, hadoopConfiguration, schedulingMode, addedJarPaths, addedFilePaths, addedArchivePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) } } /** Reports heartbeat metrics for the driver. */ private def reportHeartBeat(executorMetricsSource: Option[ExecutorMetricsSource]): Unit = { val currentMetrics = ExecutorMetrics.getCurrentMetrics(env.memoryManager) executorMetricsSource.foreach(_.updateMetricsSnapshot(currentMetrics)) val driverUpdates = new HashMap[(Int, Int), ExecutorMetrics] // In the driver, we do not track per-stage metrics, so use a dummy stage for the key driverUpdates.put(EventLoggingListener.DRIVER_STAGE_KEY, new ExecutorMetrics(currentMetrics)) val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0) listenerBus.post(SparkListenerExecutorMetricsUpdate(\"driver\", accumUpdates, driverUpdates)) } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having finished construction. // NOTE: this must be placed at the end of the SparkContext constructor. SparkContext.setActiveContext(this) } /** * The SparkContext object contains a number of implicit conversions and parameters for use with * various Spark features. */ object SparkContext extends Logging { private val VALID_LOG_LEVELS = Set(\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\") /** * Lock that guards access to global variables that track SparkContext construction. */ private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object() /** * The active, fully-constructed SparkContext. If no SparkContext is active, then this is `null`. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`. */ private val activeContext: AtomicReference[SparkContext] = new AtomicReference[SparkContext](null) /** * Points to a partially-constructed SparkContext if another thread is in the SparkContext * constructor, or `None` if no SparkContext is being constructed. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`. */ private var contextBeingConstructed: Option[SparkContext] = None /** * Called to ensure that no other SparkContext is running in this JVM. * * Throws an exception if a running context is detected and logs a warning if another thread is * constructing a SparkContext. This warning is necessary because the current locking scheme * prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception. */ private def assertNoOtherContextIsRunning(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()).filter(_ ne sc).foreach { ctx => val errMsg = \"Only one SparkContext should be running in this JVM (see SPARK-2243).\" + s\"The currently running SparkContext was created at:\\n${ctx.creationSite.longForm}\" throw new SparkException(errMsg) } contextBeingConstructed.filter(_ ne sc).foreach { otherContext => // Since otherContext might point to a partially-constructed context, guard against // its creationSite field being null: val otherContextCreationSite = Option(otherContext.creationSite).map(_.longForm).getOrElse(\"unknown location\") val warnMsg = \"Another SparkContext is being constructed (or threw an exception in its\" + \" constructor). This may indicate an error, since only one SparkContext should be\" + \" running in this JVM (see SPARK-2243).\" + s\" The other SparkContext was created at:\\n$otherContextCreationSite\" logWarning(warnMsg) } } } /** * Called to ensure that SparkContext is created or accessed only on the Driver. * * Throws an exception if a SparkContext is about to be created in executors. */ private def assertOnDriver(): Unit = { if (Utils.isInRunningSparkTask) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkContext should only be created and accessed on the driver.\") } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * @param config `SparkConfig` that will be used for initialisation of the `SparkContext` * @return current `SparkContext` (or a new one if it wasn't created before the function call) */ def getOrCreate(config: SparkConf): SparkContext = { // Synchronize to ensure that multiple create requests don't trigger an exception // from assertNoOtherContextIsRunning within setActiveContext SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext(config)) } else { if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") } } activeContext.get() } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * This method allows not passing a SparkConf (useful if just retrieving). * * @return current `SparkContext` (or a new one if wasn't created before the function call) */ def getOrCreate(): SparkContext = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext()) } activeContext.get() } } /** Return the current active [[SparkContext]] if any. */ private[spark] def getActive: Option[SparkContext] = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()) } } /** * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is * running. Throws an exception if a running context is detected and logs a warning if another * thread is constructing a SparkContext. This warning is necessary because the current locking * scheme prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception. */ private[spark] def markPartiallyConstructed(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = Some(sc) } } /** * Called at the end of the SparkContext constructor to ensure that no other SparkContext has * raced with this constructor and started. */ private[spark] def setActiveContext(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = None activeContext.set(sc) } } /** * Clears the active SparkContext metadata. This is called by `SparkContext#stop()`. It's * also called in unit tests to prevent a flood of warnings from test suites that don't / can't * properly clean up their SparkContexts. */ private[spark] def clearActiveContext(): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { activeContext.set(null) } } private[spark] val SPARK_JOB_DESCRIPTION = \"spark.job.description\" private[spark] val SPARK_JOB_GROUP_ID = \"spark.jobGroup.id\" private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = \"spark.job.interruptOnCancel\" private[spark] val SPARK_SCHEDULER_POOL = \"spark.scheduler.pool\" private[spark] val RDD_SCOPE_KEY = \"spark.rdd.scope\" private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = \"spark.rdd.scope.noOverride\" /** * Executor id for the driver. In earlier versions of Spark, this was `<driver>`, but this was * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see * SPARK-6716 for more details). */ private[spark] val DRIVER_IDENTIFIER = \"driver\" private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Iterable[T]) : ArrayWritable = { def anyToWritable[U <: Writable](u: U): Writable = u new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]], arr.map(x => anyToWritable(x)).toArray) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to SparkContext. * * @param cls class that should be inside of the jar * @return jar that contains the Class, `None` if not found */ def jarOfClass(cls: Class[_]): Option[String] = { val uri = cls.getResource(\"/\" + cls.getName.replace('.', '/') + \".class\") if (uri != null) { val uriStr = uri.toString if (uriStr.startsWith(\"jar:file:\")) { // URI will be of the form \"jar:file:/path/foo.jar!/package/cls.class\", // so pull out the /path/foo.jar Some(uriStr.substring(\"jar:file:\".length, uriStr.indexOf('!'))) } else { None } } else { None } } /** * Find the JAR that contains the class of a particular object, to make it easy for users * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in * your driver program. * * @param obj reference to an instance which class should be inside of the jar * @return jar that contains the class of the instance, `None` if not found */ def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass) /** * Creates a modified version of a SparkConf with the parameters that can be passed separately * to SparkContext, to make it easier to write SparkContext's constructors. This ignores * parameters that are passed as the default value of null, instead of throwing an exception * like SparkConf would. */ private[spark] def updatedConf( conf: SparkConf, master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()): SparkConf = { val res = conf.clone() res.setMaster(master) res.setAppName(appName) if (sparkHome != null) { res.setSparkHome(sparkHome) } if (jars != null && !jars.isEmpty) { res.setJars(jars) } res.setExecutorEnv(environment.toSeq) res } /** * The number of cores available to the driver to use for tasks such as I/O with Netty */ private[spark] def numDriverCores(master: String): Int = { numDriverCores(master, null) } /** * The number of cores available to the driver to use for tasks such as I/O with Netty */ private[spark] def numDriverCores(master: String, conf: SparkConf): Int = { def convertToInt(threads: String): Int = { if (threads == \"*\") Runtime.getRuntime.availableProcessors() else threads.toInt } master match { case \"local\" => 1 case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads) case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads) case \"yarn\" | SparkMasterRegex.KUBERNETES_REGEX(_) => if (conf != null && conf.get(SUBMIT_DEPLOY_MODE) == \"cluster\") { conf.getInt(DRIVER_CORES.key, 0) } else { 0 } case _ => 0 // Either driver is not being used, or its core count will be interpolated later } } /** * Create a task scheduler based on a given master URL. * Return a 2-tuple of the scheduler backend and the task scheduler. */ private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = { import SparkMasterRegex._ // When running locally, don't try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 // Ensure that default executor's resources satisfies one or more tasks requirement. // This function is for cluster managers that don't set the executor cores config, for // others its checked in ResourceProfile. def checkResourcesPerTask(executorCores: Int): Unit = { val taskCores = sc.conf.get(CPUS_PER_TASK) if (!sc.conf.get(SKIP_VALIDATE_CORES_TESTING)) { validateTaskCpusLargeEnough(sc.conf, executorCores, taskCores) } val defaultProf = sc.resourceProfileManager.defaultResourceProfile ResourceUtils.warnOnWastedResources(defaultProf, sc.conf, Some(executorCores)) } master match { case \"local\" => checkResourcesPerTask(1) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_REGEX(threads) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads. val threadCount = if (threads == \"*\") localCpuCount else threads.toInt if (threadCount <= 0) { throw new SparkException(s\"Asked to run locally with $threadCount threads\") } checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_FAILURES_REGEX(threads, maxFailures) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*, M] means the number of cores on the computer with M failures // local[N, M] means exactly N threads with M failures val threadCount = if (threads == \"*\") localCpuCount else threads.toInt checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case SPARK_REGEX(sparkUrl) => val scheduler = new TaskSchedulerImpl(sc) val masterUrls = sparkUrl.split(\",\").map(\"spark://\" + _) val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) (backend, scheduler) case LOCAL_CLUSTER_REGEX(numWorkers, coresPerWorker, memoryPerWorker) => checkResourcesPerTask(coresPerWorker.toInt) // Check to make sure memory requested <= memoryPerWorker. Otherwise Spark will just hang. val memoryPerWorkerInt = memoryPerWorker.toInt if (sc.executorMemory > memoryPerWorkerInt) { throw new SparkException( \"Asked to launch cluster with %d MiB/worker but requested %d MiB/executor\".format( memoryPerWorkerInt, sc.executorMemory)) } // For host local mode setting the default of SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED // to false because this mode is intended to be used for testing and in this case all the // executors are running on the same host. So if host local reading was enabled here then // testing of the remote fetching would be secondary as setting this config explicitly to // false would be required in most of the unit test (despite the fact that remote fetching // is much more frequent in production). sc.conf.setIfMissing(SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED, false) val scheduler = new TaskSchedulerImpl(sc) val localCluster = LocalSparkCluster( numWorkers.toInt, coresPerWorker.toInt, memoryPerWorkerInt, sc.conf) val masterUrls = localCluster.start() val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => { localCluster.stop() } (backend, scheduler) case masterUrl => val cm = getClusterManager(masterUrl) match { case Some(clusterMgr) => clusterMgr case None => throw new SparkException(\"Could not parse Master URL: '\" + master + \"'\") } try { val scheduler = cm.createTaskScheduler(sc, masterUrl) val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) cm.initialize(scheduler, backend) (backend, scheduler) } catch { case se: SparkException => throw se case NonFatal(e) => throw new SparkException(\"External scheduler cannot be instantiated\", e) } } } private def getClusterManager(url: String): Option[ExternalClusterManager] = { val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size > 1) { throw new SparkException( s\"Multiple external cluster managers registered for the url $url: $serviceLoaders\") } serviceLoaders.headOption } /** * This is a helper function to complete the missing S3A magic committer configurations * based on a single conf: `spark.hadoop.fs.s3a.bucket.<bucket>.committer.magic.enabled` */ private def fillMissingMagicCommitterConfsIfNeeded(conf: SparkConf): Unit = { val magicCommitterConfs = conf .getAllWithPrefix(\"spark.hadoop.fs.s3a.bucket.\") .filter(_._1.endsWith(\".committer.magic.enabled\")) .filter(_._2.equalsIgnoreCase(\"true\")) if (magicCommitterConfs.nonEmpty) { // Try to enable S3 magic committer if missing conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\") if (conf.get(\"spark.hadoop.fs.s3a.committer.magic.enabled\").equals(\"true\")) { conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") conf.setIfMissing(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\") conf.setIfMissing(\"spark.sql.parquet.output.committer.class\", \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\") conf.setIfMissing(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\") } } } /** * SPARK-36796: This is a helper function to supplement `--add-opens` options to * `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`. */ private def supplementJavaModuleOptions(conf: SparkConf): Unit = { def supplement(key: OptionalConfigEntry[String]): Unit = { val v = conf.get(key) match { case Some(opts) => s\"${JavaModuleOptions.defaultModuleOptions()} $opts\" case None => JavaModuleOptions.defaultModuleOptions() } conf.set(key.key, v) } supplement(DRIVER_JAVA_OPTIONS) supplement(EXECUTOR_JAVA_OPTIONS) } } /** * A collection of regexes for extracting information from the master string. */ private object SparkMasterRegex { // Regular expression used for local[N] and local[*] master formats val LOCAL_N_REGEX = \"\"\"local\\[([0-9]+|\\*)\\]\"\"\".r // Regular expression for local[N, maxRetries], used in tests with failing tasks val LOCAL_N_FAILURES_REGEX = \"\"\"local\\[([0-9]+|\\*)\\s*,\\s*([0-9]+)\\]\"\"\".r // Regular expression for simulating a Spark cluster of [N, cores, memory] locally val LOCAL_CLUSTER_REGEX = \"\"\"local-cluster\\[\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*]\"\"\".r // Regular expression for connecting to Spark deploy clusters val SPARK_REGEX = \"\"\"spark://(.*)\"\"\".r // Regular expression for connecting to kubernetes clusters val KUBERNETES_REGEX = \"\"\"k8s://(.*)\"\"\".r } /** * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`). */ private[spark] class WritableConverter[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: Writable => T) extends Serializable object WritableConverter { // Helper objects for converting common types to Writable private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T) : WritableConverter[T] = { val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]] new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W])) } // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. // The following implicit declarations have been added on top of the very similar ones // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta // expansion of zero-arg methods and thus won't match a no-arg method where it expects // an implicit that is a function of no args. implicit val intWritableConverterFn: () => WritableConverter[Int] = () => simpleWritableConverter[Int, IntWritable](_.get) implicit val longWritableConverterFn: () => WritableConverter[Long] = () => simpleWritableConverter[Long, LongWritable](_.get) implicit val doubleWritableConverterFn: () => WritableConverter[Double] = () => simpleWritableConverter[Double, DoubleWritable](_.get) implicit val floatWritableConverterFn: () => WritableConverter[Float] = () => simpleWritableConverter[Float, FloatWritable](_.get) implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] = () => simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = { () => simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit val stringWritableConverterFn: () => WritableConverter[String] = () => simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] = () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) // These implicits remain included for backwards-compatibility. They fulfill the // same role as those above. implicit def intWritableConverter(): WritableConverter[Int] = simpleWritableConverter[Int, IntWritable](_.get) implicit def longWritableConverter(): WritableConverter[Long] = simpleWritableConverter[Long, LongWritable](_.get) implicit def doubleWritableConverter(): WritableConverter[Double] = simpleWritableConverter[Double, DoubleWritable](_.get) implicit def floatWritableConverter(): WritableConverter[Float] = simpleWritableConverter[Float, FloatWritable](_.get) implicit def booleanWritableConverter(): WritableConverter[Boolean] = simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = { simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit def stringWritableConverter(): WritableConverter[String] = simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] = new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) } /** * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The `Writable` class will be used in `SequenceFileRDDFunctions`. */ private[spark] class WritableFactory[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: T => Writable) extends Serializable object WritableFactory { private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W) : WritableFactory[T] = { val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]] new WritableFactory[T](_ => writableClass, convert) } implicit def intWritableFactory: WritableFactory[Int] = simpleWritableFactory(new IntWritable(_)) implicit def longWritableFactory: WritableFactory[Long] = simpleWritableFactory(new LongWritable(_)) implicit def floatWritableFactory: WritableFactory[Float] = simpleWritableFactory(new FloatWritable(_)) implicit def doubleWritableFactory: WritableFactory[Double] = simpleWritableFactory(new DoubleWritable(_)) implicit def booleanWritableFactory: WritableFactory[Boolean] = simpleWritableFactory(new BooleanWritable(_)) implicit def bytesWritableFactory: WritableFactory[Array[Byte]] = simpleWritableFactory(new BytesWritable(_)) implicit def stringWritableFactory: WritableFactory[String] = simpleWritableFactory(new Text(_)) implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] = simpleWritableFactory(w => w) }"
          ],
          "tokens": 1060,
          "more_info": "Wants to make specific node query before giving full answer and proposing that query in answer. Needed `return full answer in .md file`."
        },
        "without_mcp": {
          "answer": "# How the project manages external data connectivity and writing\n\nThis repository is Apache Spark 3.3.0. Spark manages external data connectivity and writing primarily through Spark SQL Data Sources with the DataFrame/Dataset API, plus Hadoop-compatible filesystems and optional connectors.\n\n## Core APIs\n- DataFrameReader (spark.read / SparkSession.read)\n  - load data from external sources via: format(), options, schema(), load(path) or dedicated helpers (csv/json/parquet/orc/text, etc.).\n- DataFrameWriter (df.write)\n  - write data to external systems via: format(), options, mode(), save(path) or dedicated helpers (csv/json/parquet/orc/text, etc.).\n- SQL\n  - CREATE TABLE USING, INSERT INTO/OVERWRITE, and SELECT ... FROM file-based sources directly using the datasource V1/V2 implementations.\n\n## Supported source/sink types (high level)\n- File-based formats (through Hadoop FS APIs):\n  - Parquet, ORC, JSON, CSV, Text, Avro, Whole binary files.\n  - Works with any Hadoop-compatible filesystem (local FS, HDFS, S3, Azure Blob/ADLS, GCS, etc.) configured via Hadoop/Spark settings.\n- JDBC: relational databases via the built-in JDBC data source.\n- Hive tables: via Hive support (metastore) when enabled.\n- Structured Streaming sources/sinks: file sources/sinks, plus ecosystem connectors (e.g., Kafka) when added.\n\nSee documentation under docs/sql-data-sources*.md in this repo for details and options.\n\n## Connecting to files and object stores\nSpark uses Hadoops FileSystem abstraction. Paths like hdfs://, s3a://, wasbs:///abfs://, gs:// are resolved by corresponding Hadoop filesystem implementations.\n- Configure credentials/endpoints using Hadoop core-site.xml, hdfs-site.xml, and Spark configs (spark.hadoop.*), or environment variables per provider.\n- Generic file source options (docs/sql-data-sources-generic-options.md) include:\n  - ignoreCorruptFiles, ignoreMissingFiles\n  - pathGlobFilter\n  - recursiveFileLookup\n  - compression (codec specific to formats, e.g., gzip, snappy, zstd)\n\nExamples:\n- Read Parquet: spark.read.parquet(\"s3a://bucket/path/\")\n- Write ORC: df.write.mode(\"overwrite\").orc(\"abfs://container@account.dfs.core.windows.net/path\")\n\n## JDBC connectivity\nUse format(\"jdbc\") with options or the jdbc helper.\nKey options:\n- url: JDBC URL (e.g., jdbc:postgresql://host:5432/db)\n- dbtable: table or query (use (select ...) t alias for subqueries)\n- driver: JDBC driver class\n- user, password: authentication\n- partitionColumn, lowerBound, upperBound, numPartitions: parallel reads\n\nExamples:\n- Read: spark.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", \"public.sales\").option(\"user\", user).option(\"password\", pwd).load()\n- Write: df.write.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", \"public.sales_out\").option(\"user\", user).option(\"password\", pwd).mode(\"append\").save()\n\n## Hive tables\nWhen Hive support is enabled, Spark can read/write Hive-managed and external tables:\n- CREATE TABLE USING or CREATE TABLE ... STORED AS ...\n- saveAsTable, insertInto\n- Interacts with the Hive metastore; storage formats typically Parquet/ORC.\n\n## Writing controls\n- Save modes (docs/sql-data-sources-load-save-functions.md#save-modes):\n  - errorIfExists (default), append, overwrite, ignore.\n- Partitioning, bucketing, sorting:\n  - df.write.partitionBy(\"col1\", ...)\n  - df.write.bucketBy(n, \"col\").sortBy(\"col2\") (saveAsTable required for bucketing)\n  - Affects directory layout and query performance.\n- Overwrite behavior for partitions:\n  - spark.sql.sources.partitionOverwriteMode = dynamic | static\n  - Dynamic overwrite replaces only matched partitions.\n\n## Format-specific options\nEach format has documented options:\n- Parquet: mergeSchema, compression, datetimeRebaseModeInWrite, etc. (docs/sql-data-sources-parquet.md)\n- ORC: compression, etc. (docs/sql-data-sources-orc.md)\n- JSON: multiline, encoding, dateFormat, etc. (docs/sql-data-sources-json.md)\n- CSV: header, inferSchema, sep, quote, escape, nullValue, etc. (docs/sql-data-sources-csv.md)\n- Avro: various read/write options (docs/sql-data-sources-avro.md)\n\n## Structured Streaming\n- Read: spark.readStream.format(\"parquet\"|\"json\"|\"csv\"|...).option(...).load(path)\n- Write: df.writeStream.format(\"parquet\"|\"console\"|\"memory\"|...).option(\"checkpointLocation\", path).start(outputPath)\n- Many external streaming systems (e.g., Kafka) are available via separate connectors; they are configured with format(\"kafka\") and options like kafka.bootstrap.servers, subscribe, etc., when the connector is present.\n\n## Security and credentials\n- Filesystems: authenticate via underlying Hadoop/Cloud provider mechanisms (config files, tokens, instance profiles/managed identities, environment variables). Configure in Spark with spark.hadoop.* or provider-specific settings.\n- JDBC: supply user/password or token-based mechanisms via driver-specific properties. Avoid embedding secrets in code; use secure config/secret management.\n\n## Programmatic patterns\n- Reader:\n  - spark.read.format(fmt).options(opts).schema(schemaOpt).load(pathOpt)\n  - Convenience: spark.read.parquet/json/csv/orc/text\n- Writer:\n  - df.write.format(fmt).options(opts).mode(mode).save(path)\n  - Convenience: df.write.parquet/json/csv/orc/text\n  - Tables: df.write.saveAsTable(name), insertInto(name)\n\n## Troubleshooting\n- See docs/sql-data-sources-troubleshooting.md for common issues:\n  - Schema inference pitfalls, corrupt/missing files behavior, and performance tips.\n  - Use spark.sql.shuffle.partitions, repartition/coalesce, and predicate pushdown where applicable.\n\n## References inside this repo\n- docs/sql-data-sources.md (overview)\n- docs/sql-data-sources-load-save-functions.md (API, modes, tables, partitioning)\n- docs/sql-data-sources-generic-options.md (common options)\n- docs/sql-data-sources-parquet.md, -orc.md, -json.md, -csv.md, -text.md, -avro.md\n- docs/sql-data-sources-hive-tables.md, -jdbc.md\n- structured-streaming-programming-guide.md (streaming specifics)\n\nIn summary, Spark manages external connectivity via its unified Data Source API, leveraging Hadoop-compatible filesystems for file/object stores and built-in JDBC/Hive integrations, with rich per-format options, writer controls (modes, partitioning/bucketing), and streaming read/write APIs.",
          "answer_time": 58,
          "tokens": 1457
        }
      },
       "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 2,
          "reasoning": "Answer A provides a comprehensive, well-structured explanation of Spark's data connectivity and writing mechanisms with detailed coverage of DataFrameWriter, DataStreamWriter, connection management, and execution flow. It includes practical code examples and architectural insights. Answer B acknowledges upfront that it's limited to visible code snippets and provides incomplete information, explicitly stating that key components like DataFrameReader/Writer and actual connector implementations are missing from their analysis. While Answer B is honest about its limitations, it fails to adequately address the user's question about how the project manages external data connectivity."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 3,
          "reasoning": "Answer A provides a more comprehensive and well-structured explanation of Spark's data connectivity and writing mechanisms. It covers both batch and streaming writing with detailed technical explanations, code examples, and architectural insights including execution flow and special features. Answer B, while accurate, is incomplete (cuts off mid-sentence) and reads more like documentation references rather than a cohesive explanation. Answer A demonstrates deeper technical understanding with specific implementation details like V1/V2 data sources, save modes, and connection management."
        }
      }
    },
    {
      "id": "Q008",
      "question": "How does the project manage memory allocation?",
      "category": "general",
      "parameters": {
        "kinds": [
          "CLASS",
          "METHOD",
          "INTERFACE",
          "TRAIT"
        ],
        "keywords": [
          "memory",
          "alloc",
          "allocation",
          "manager",
          "allocator",
          "pool"
        ],
        "top_nodes": 8,
        "max_neighbors": 5
      },
      "ground_truth_contexts": [
        "import org.apache.spark.internal.config._ import org.apache.spark.network.util.JavaUtils import org.apache.spark.util.{ThreadUtils, Utils} private[spark] class DriverLogger(conf: SparkConf) extends Logging { private val UPLOAD_CHUNK_SIZE = 1024 * 1024 private val UPLOAD_INTERVAL_IN_SECS = 5 private val DEFAULT_LAYOUT = \"%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex\" private val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort) private val localLogFile: String = FileUtils.getFile( Utils.getLocalDir(conf), DriverLogger.DRIVER_LOG_DIR, DriverLogger.DRIVER_LOG_FILE).getAbsolutePath() private var writer: Option[DfsAsyncWriter] = None addLogAppender() private def addLogAppender(): Unit = { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val layout = if (conf.contains(DRIVER_LOG_LAYOUT)) { PatternLayout.newBuilder().withPattern(conf.get(DRIVER_LOG_LAYOUT).get).build() } else { PatternLayout.newBuilder().withPattern(DEFAULT_LAYOUT).build() } val config = logger.getContext.getConfiguration() def log4jFileAppender() = { // SPARK-37853: We can't use the chained API invocation mode because // `AbstractFilterable.Builder.asBuilder()` method will return `Any` in Scala. val builder: Log4jFileAppender.Builder[_] = Log4jFileAppender.newBuilder() builder.withAppend(false) builder.withBufferedIo(false) builder.setConfiguration(config) builder.withFileName(localLogFile) builder.setIgnoreExceptions(false) builder.setLayout(layout) builder.setName(DriverLogger.APPENDER_NAME) builder.build() } val fa = log4jFileAppender() logger.addAppender(fa) fa.start() logInfo(s\"Added a local log appender at: $localLogFile\") } def startSync(hadoopConf: Configuration): Unit = { try { // Setup a writer which moves the local file to hdfs continuously val appId = Utils.sanitizeDirName(conf.getAppId) writer = Some(new DfsAsyncWriter(appId, hadoopConf)) } catch { case e: Exception => logError(s\"Could not persist driver logs to dfs\", e) } } def stop(): Unit = { try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
        "try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
        "*/ private[spark] case class ErrorInfo(message: Seq[String], sqlState: Option[String]) { // For compatibility with multi-line error messages @JsonIgnore val messageFormat: String = message.mkString(\"\\n\") } /** * Companion object used by instances of [[SparkThrowable]] to access error class information and * construct error messages. */ private[spark] object SparkThrowableHelper { val errorClassesUrl: URL = Utils.getSparkClassLoader.getResource(\"error/error-classes.json\") val errorClassToInfoMap: SortedMap[String, ErrorInfo] = { val mapper: JsonMapper = JsonMapper.builder() .addModule(DefaultScalaModule) .build() mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {}) } def getMessage( errorClass: String, messageParameters: Array[String], queryContext: String = \"\"): String = { val errorInfo = errorClassToInfoMap.getOrElse(errorClass, throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\")) val displayQueryContext = if (queryContext.isEmpty) { \"\" } else { s\"\\n$queryContext\" } String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"), messageParameters: _*) + displayQueryContext } def getSqlState(errorClass: String): String = { Option(errorClass).flatMap(errorClassToInfoMap.get).flatMap(_.sqlState).orNull } def isInternalError(errorClass: String): Boolean = { errorClass == \"INTERNAL_ERROR\" } }",
        "private[streaming] case class ReceiverErrorInfo( lastErrorMessage: String = \"\", lastError: String = \"\", lastErrorTime: Long = -1L) /** * Class having information about a receiver. * * @param receiverId the unique receiver id * @param state the current Receiver state * @param scheduledLocations the scheduled locations provided by ReceiverSchedulingPolicy * @param runningExecutor the running executor if the receiver is active * @param name the receiver name * @param endpoint the receiver endpoint. It can be used to send messages to the receiver * @param errorInfo the receiver error information if it fails */ private[streaming] case class ReceiverTrackingInfo( receiverId: Int, state: ReceiverState, scheduledLocations: Option[Seq[TaskLocation]], runningExecutor: Option[ExecutorCacheTaskLocation], name: Option[String] = None, endpoint: Option[RpcEndpointRef] = None, errorInfo: Option[ReceiverErrorInfo] = None) { def toReceiverInfo: ReceiverInfo = ReceiverInfo( receiverId, name.getOrElse(\"\"), state == ReceiverState.ACTIVE, location = runningExecutor.map(_.host).getOrElse(\"\"), executorId = runningExecutor.map(_.executorId).getOrElse(\"\"), lastErrorMessage = errorInfo.map(_.lastErrorMessage).getOrElse(\"\"), lastError = errorInfo.map(_.lastError).getOrElse(\"\"), lastErrorTime = errorInfo.map(_.lastErrorTime).getOrElse(-1L) ) }",
        ".build() mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {}) } def getMessage( errorClass: String, messageParameters: Array[String], queryContext: String = \"\"): String = { val errorInfo = errorClassToInfoMap.getOrElse(errorClass, throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\")) val displayQueryContext = if (queryContext.isEmpty) { \"\" } else { s\"\\n$queryContext\" } String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"), messageParameters: _*) + displayQueryContext } def getSqlState(errorClass: String): String = { Option(errorClass).flatMap(errorClassToInfoMap.get).flatMap(_.sqlState).orNull } def isInternalError(errorClass: String): Boolean = { errorClass == \"INTERNAL_ERROR\" } }"
      ],
      "key_entities": [
        "org/apache/spark/util/logging/DriverLogger#",
        "org/apache/spark/util/logging/DriverLogger#DfsAsyncWriter#",
        "org/apache/spark/ErrorInfo#",
        "org/apache/spark/streaming/scheduler/ReceiverErrorInfo#",
        "local2:core/src/main/scala/org/apache/spark/ErrorInfo.scala"
      ],
      "claude_stats": {
        "answer": "Based on the code analysis, here's how this Apache Spark project manages memory allocation:\n\n## Memory Management Architecture\n\nThe project uses a **dual-pool memory management system** with separate pools for execution and storage:\n\n### 1. **ExecutionMemoryPool**\nManages memory for computational tasks with the following characteristics:\n\n- **Per-Task Tracking**: Uses `memoryForTask` HashMap to track memory consumption by `taskAttemptId`\n- **Fair Allocation**: Implements a fairness algorithm where each task can acquire up to `1/N` of the pool (where N = number of active tasks)\n- **Blocking with Minimum Guarantee**: Tasks are guaranteed at least `1/(2*N)` of the pool before blocking. If a task can't get this minimum, it waits via `lock.wait()` until memory is freed\n- **Dynamic Pool Growth**: Supports callback `maybeGrowPool()` to potentially expand the pool by evicting cached blocks from storage\n- **Memory Modes**: Operates in either `ON_HEAP` or `OFF_HEAP` mode\n\n### 2. **StorageMemoryPool**\nManages memory for cached data blocks:\n\n- **Block-Based Allocation**: Acquires memory for specific `BlockId` instances\n- **Eviction Support**: Can evict existing blocks via `memoryStore.evictBlocksToFreeSpace()` when space is needed\n- **Shrinkage**: Supports `freeSpaceToShrinkPool()` to release memory back to execution pool\n- **Single Counter**: Tracks total usage with `_memoryUsed` variable\n\n### 3. **Chunked Buffer System**\nHandles large data efficiently:\n\n- **ChunkedByteBuffer**: Splits large data into multiple `ByteBuffer` chunks to avoid JVM array size limits\n- **Memory-Mapped Alternative**: Explicitly avoids memory-mapping files to prevent lifecycle conflicts (SPARK-25422)\n- **Direct vs Heap Buffers**: Supports both heap-allocated and off-heap direct buffers based on storage level\n- **Chunk Size Control**: Uses configurable `BUFFER_WRITE_CHUNK_SIZE` to limit temporary direct buffer growth and prevent memory leaks\n\n### 4. **Block Storage Strategy**\nThe `BlockManager` coordinates memory allocation across storage tiers:\n\n- **Memory-First**: Attempts memory storage first, falls back to disk if needed\n- **Serialized vs Deserialized**: Can store blocks as Java objects (deserialized) or serialized bytes based on `StorageLevel`\n- **Replication**: Supports multi-replica storage for fault tolerance\n- **Cleanup**: Implements reference-based cleanup for temporary files via `RemoteBlockDownloadFileManager`\n\n### 5. **Key Allocation Patterns**\n\n**Acquiring Memory**:\n```\n1. Check if enough free memory exists\n2. If not, try to grow pool by evicting storage blocks\n3. Calculate per-task limits (max = poolSize/N, min = poolSize/2N)\n4. Grant as much as available within limits\n5. Block if can't meet minimum guarantee\n```\n\n**Releasing Memory**:\n```\n1. Decrement task's memory counter\n2. Remove task entry if memory reaches zero\n3. Call lock.notifyAll() to wake waiting tasks\n```\n\nThis design balances fairness, prevents starvation, and efficiently handles both small and large data blocks while supporting multiple memory modes and storage tiers.",
        "prompt_time": 16.167,
        "answer_time": 50,
        "context_tokens": 25449,
        "tokens": 795,
        "used_context": [
          "## CLASS: org/apache/spark/memory/ExecutionMemoryPool# (implementation)\n*/ private[memory] class ExecutionMemoryPool( lock: Object, memoryMode: MemoryMode ) extends MemoryPool(lock) with Logging { private[this] val poolName: String = memoryMode match { case MemoryMode.ON_HEAP => \"on-heap execution\" case MemoryMode.OFF_HEAP => \"off-heap execution\" } /** * Map from taskAttemptId -> memory consumption in bytes */ @GuardedBy(\"lock\") private val memoryForTask = new mutable.HashMap[Long, Long]() override def memoryUsed: Long = lock.synchronized { memoryForTask.values.sum } /** * Returns the memory consumption, in bytes, for the given task. */ def getMemoryUsageForTask(taskAttemptId: Long): Long = lock.synchronized { memoryForTask.getOrElse(taskAttemptId, 0L) } /** * Try to acquire up to `numBytes` of memory for the given task and return the number of bytes * obtained, or 0 if none can be allocated. * * This call may block until there is enough free memory in some situations, to make sure each * task has a chance to ramp up to at least 1 / 2N of the total memory pool (where N is the # of * active tasks) before it is forced to spill. This can happen if the number of tasks increase * but an older task had a lot of memory already. * * @param numBytes number of bytes to acquire * @param taskAttemptId the task attempt acquiring memory * @param maybeGrowPool a callback that potentially grows the size of this pool. It takes in * one parameter (Long) that represents the desired amount of memory by * which this pool should be expanded. * @param computeMaxPoolSize a callback that returns the maximum allowable size of this pool * at this given moment. This is not a field because the max pool * size is variable in certain cases. For instance, in unified * memory management, the execution pool can be expanded by evicting * cached blocks, thereby shrinking the storage pool. * * @return the number of bytes granted to the task. */ private[memory] def acquireMemory( numBytes: Long, taskAttemptId: Long, maybeGrowPool: Long => Unit = (additionalSpaceNeeded: Long) => (), computeMaxPoolSize: () => Long = () => poolSize): Long = lock.synchronized { assert(numBytes > 0, s\"invalid number of bytes requested: $numBytes\") // TODO: clean up this clunky method signature // Add this task to the taskMemory map just so we can keep an accurate count of the number // of active tasks, to let other tasks ramp down their memory in calls to `acquireMemory` if (!memoryForTask.contains(taskAttemptId)) { memoryForTask(taskAttemptId) = 0L // This will later cause waiting tasks to wake up and check numTasks again lock.notifyAll() } // Keep looping until we're either sure that we don't want to grant this request (because this // task would have more than 1 / numActiveTasks of the memory) or we have enough free // memory to give it (we always let each task get at least 1 / (2 * numActiveTasks)). // TODO: simplify this to limit each task to its own slot while (true) { val numActiveTasks = memoryForTask.keys.size val curMem = memoryForTask(taskAttemptId) // In every iteration of this loop, we should first try to reclaim any borrowed execution // space from storage. This is necessary because of the potential race condition where new // storage blocks may steal the free execution memory that this task was waiting for. maybeGrowPool(numBytes - memoryFree) // Maximum size the pool would have after potentially growing the pool. // This is used to compute the upper bound of how much memory each task can occupy. This // must take into account potential free memory as well as the amount this pool currently // occupies. Otherwise, we may run into SPARK-12155 where, in unified memory management, // we did not take into account space that could have been freed by evicting cached blocks. val maxPoolSize = computeMaxPoolSize() val maxMemoryPerTask = maxPoolSize / numActiveTasks val minMemoryPerTask = poolSize / (2 * numActiveTasks) // How much we can grant this task; keep its share within 0 <= X <= 1 / numActiveTasks val maxToGrant = math.min(numBytes, math.max(0, maxMemoryPerTask - curMem)) // Only give it as much memory as is free, which might be none if it reached 1 / numTasks val toGrant = math.min(maxToGrant, memoryFree) // We want to let each task get at least 1 / (2 * numActiveTasks) before blocking; // if we can't give it this much now, wait for other tasks to free up memory // (this happens if older tasks allocated lots of memory before N grew) if (toGrant < numBytes && curMem + toGrant < minMemoryPerTask) { logInfo(s\"TID $taskAttemptId waiting for at least 1/2N of $poolName pool to be free\") lock.wait() } else { memoryForTask(taskAttemptId) += toGrant return toGrant } } 0L // Never reached } /** * Release `numBytes` of memory acquired by the given task. */ def releaseMemory(numBytes: Long, taskAttemptId: Long): Unit = lock.synchronized { val curMem = memoryForTask.getOrElse(taskAttemptId, 0L) val memoryToFree = if (curMem < numBytes) { logWarning( s\"Internal error: release called on $numBytes bytes but task only has $curMem bytes \" + s\"of memory from the $poolName pool\") curMem } else { numBytes } if (memoryForTask.contains(taskAttemptId)) { memoryForTask(taskAttemptId) -= memoryToFree if (memoryForTask(taskAttemptId) <= 0) { memoryForTask.remove(taskAttemptId) } } lock.notifyAll() // Notify waiters in acquireMemory() that memory has been freed } /** * Release all memory for the given task and mark it as inactive (e.g. when a task ends). * @return the number of bytes freed. */ def releaseAllMemoryForTask(taskAttemptId: Long): Long = lock.synchronized { val numBytesToFree = getMemoryUsageForTask(taskAttemptId) releaseMemory(numBytesToFree, taskAttemptId) numBytesToFree } }",
          "## CLASS: org/apache/spark/memory/StorageMemoryPool# (implementation)\n*/ private[memory] class StorageMemoryPool( lock: Object, memoryMode: MemoryMode ) extends MemoryPool(lock) with Logging { private[this] val poolName: String = memoryMode match { case MemoryMode.ON_HEAP => \"on-heap storage\" case MemoryMode.OFF_HEAP => \"off-heap storage\" } @GuardedBy(\"lock\") private[this] var _memoryUsed: Long = 0L override def memoryUsed: Long = lock.synchronized { _memoryUsed } private var _memoryStore: MemoryStore = _ def memoryStore: MemoryStore = { if (_memoryStore == null) { throw new IllegalStateException(\"memory store not initialized yet\") } _memoryStore } /** * Set the [[MemoryStore]] used by this manager to evict cached blocks. * This must be set after construction due to initialization ordering constraints. */ final def setMemoryStore(store: MemoryStore): Unit = { _memoryStore = store } /** * Acquire N bytes of memory to cache the given block, evicting existing ones if necessary. * * @return whether all N bytes were successfully granted. */ def acquireMemory(blockId: BlockId, numBytes: Long): Boolean = lock.synchronized { val numBytesToFree = math.max(0, numBytes - memoryFree) acquireMemory(blockId, numBytes, numBytesToFree) } /** * Acquire N bytes of storage memory for the given block, evicting existing ones if necessary. * * @param blockId the ID of the block we are acquiring storage memory for * @param numBytesToAcquire the size of this block * @param numBytesToFree the amount of space to be freed through evicting blocks * @return whether all N bytes were successfully granted. */ def acquireMemory( blockId: BlockId, numBytesToAcquire: Long, numBytesToFree: Long): Boolean = lock.synchronized { assert(numBytesToAcquire >= 0) assert(numBytesToFree >= 0) assert(memoryUsed <= poolSize) if (numBytesToFree > 0) { memoryStore.evictBlocksToFreeSpace(Some(blockId), numBytesToFree, memoryMode) } // NOTE: If the memory store evicts blocks, then those evictions will synchronously call // back into this StorageMemoryPool in order to free memory. Therefore, these variables // should have been updated. val enoughMemory = numBytesToAcquire <= memoryFree if (enoughMemory) { _memoryUsed += numBytesToAcquire } enoughMemory } def releaseMemory(size: Long): Unit = lock.synchronized { if (size > _memoryUsed) { logWarning(s\"Attempted to release $size bytes of storage \" + s\"memory when we only have ${_memoryUsed} bytes\") _memoryUsed = 0 } else { _memoryUsed -= size } } def releaseAllMemory(): Unit = lock.synchronized { _memoryUsed = 0 } /** * Free space to shrink the size of this storage memory pool by `spaceToFree` bytes. * Note: this method doesn't actually reduce the pool size but relies on the caller to do so. * * @return number of bytes to be removed from the pool's capacity. */ def freeSpaceToShrinkPool(spaceToFree: Long): Long = lock.synchronized { val spaceFreedByReleasingUnusedMemory = math.min(spaceToFree, memoryFree) val remainingSpaceToFree = spaceToFree - spaceFreedByReleasingUnusedMemory if (remainingSpaceToFree > 0) { // If reclaiming free memory did not adequately shrink the pool, begin evicting blocks: val spaceFreedByEviction = memoryStore.evictBlocksToFreeSpace(None, remainingSpaceToFree, memoryMode) // When a block is released, BlockManager.dropFromMemory() calls releaseMemory(), so we do // not need to decrement _memoryUsed here. However, we do need to decrement the pool size. spaceFreedByReleasingUnusedMemory + spaceFreedByEviction } else { spaceFreedByReleasingUnusedMemory } } }",
          "## CLASS: org/apache/spark/util/io/ChunkedByteBuffer# (implementation)\n*/ private[spark] class ChunkedByteBuffer(var chunks: Array[ByteBuffer]) { require(chunks != null, \"chunks must not be null\") require(chunks.forall(_.position() == 0), \"chunks' positions must be 0\") // Chunk size in bytes private val bufferWriteChunkSize = Option(SparkEnv.get).map(_.conf.get(config.BUFFER_WRITE_CHUNK_SIZE)) .getOrElse(config.BUFFER_WRITE_CHUNK_SIZE.defaultValue.get).toInt private[this] var disposed: Boolean = false /** * This size of this buffer, in bytes. */ val size: Long = chunks.map(_.limit().asInstanceOf[Long]).sum def this(byteBuffer: ByteBuffer) = { this(Array(byteBuffer)) } /** * Write this buffer to a channel. */ def writeFully(channel: WritableByteChannel): Unit = { for (bytes <- getChunks()) { val originalLimit = bytes.limit() while (bytes.hasRemaining) { // If `bytes` is an on-heap ByteBuffer, the Java NIO API will copy it to a temporary direct // ByteBuffer when writing it out. This temporary direct ByteBuffer is cached per thread. // Its size has no limit and can keep growing if it sees a larger input ByteBuffer. This may // cause significant native memory leak, if a large direct ByteBuffer is allocated and // cached, as it's never released until thread exits. Here we write the `bytes` with // fixed-size slices to limit the size of the cached direct ByteBuffer. // Please refer to http://www.evanjones.ca/java-bytebuffer-leak.html for more details. val ioSize = Math.min(bytes.remaining(), bufferWriteChunkSize) bytes.limit(bytes.position() + ioSize) channel.write(bytes) bytes.limit(originalLimit) } } } /** * Wrap this in a custom \"FileRegion\" which allows us to transfer over 2 GB. */ def toNetty: ChunkedByteBufferFileRegion = { new ChunkedByteBufferFileRegion(this, bufferWriteChunkSize) } /** * Copy this buffer into a new byte array. * * @throws UnsupportedOperationException if this buffer's size exceeds the maximum array size. */ def toArray: Array[Byte] = { if (size >= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH) { throw new UnsupportedOperationException( s\"cannot call toArray because buffer size ($size bytes) exceeds maximum array size\") } val byteChannel = new ByteArrayWritableChannel(size.toInt) writeFully(byteChannel) byteChannel.close() byteChannel.getData } /** * Convert this buffer to a ByteBuffer. If this buffer is backed by a single chunk, its underlying * data will not be copied. Instead, it will be duplicated. If this buffer is backed by multiple * chunks, the data underlying this buffer will be copied into a new byte buffer. As a result, it * is suggested to use this method only if the caller does not need to manage the memory * underlying this buffer. * * @throws UnsupportedOperationException if this buffer's size exceeds the max ByteBuffer size. */ def toByteBuffer: ByteBuffer = { if (chunks.length == 1) { chunks.head.duplicate() } else { ByteBuffer.wrap(toArray) } } /** * Creates an input stream to read data from this ChunkedByteBuffer. * * @param dispose if true, [[dispose()]] will be called at the end of the stream * in order to close any memory-mapped files which back this buffer. */ def toInputStream(dispose: Boolean = false): InputStream = { new ChunkedByteBufferInputStream(this, dispose) } /** * Get duplicates of the ByteBuffers backing this ChunkedByteBuffer. */ def getChunks(): Array[ByteBuffer] = { chunks.map(_.duplicate()) } /** * Make a copy of this ChunkedByteBuffer, copying all of the backing data into new buffers. * The new buffer will share no resources with the original buffer. * * @param allocator a method for allocating byte buffers */ def copy(allocator: Int => ByteBuffer): ChunkedByteBuffer = { val copiedChunks = getChunks().map { chunk => val newChunk = allocator(chunk.limit()) newChunk.put(chunk) newChunk.flip() newChunk } new ChunkedByteBuffer(copiedChunks) } /** * Attempt to clean up any ByteBuffer in this ChunkedByteBuffer which is direct or memory-mapped. * See [[StorageUtils.dispose]] for more information. */ def dispose(): Unit = { if (!disposed) { chunks.foreach(StorageUtils.dispose) disposed = true } } } private[spark] object ChunkedByteBuffer { def fromManagedBuffer(data: ManagedBuffer): ChunkedByteBuffer = { data match { case f: FileSegmentManagedBuffer => fromFile(f.getFile, f.getOffset, f.getLength) case e: EncryptedManagedBuffer => e.blockData.toChunkedByteBuffer(ByteBuffer.allocate _) case other => new ChunkedByteBuffer(other.nioByteBuffer()) } } def fromFile(file: File): ChunkedByteBuffer = { fromFile(file, 0, file.length()) } private def fromFile( file: File, offset: Long, length: Long): ChunkedByteBuffer = { // We do *not* memory map the file, because we may end up putting this into the memory store, // and spark currently is not expecting memory-mapped buffers in the memory store, it conflicts // with other parts that manage the lifecycle of buffers and dispose them. See SPARK-25422. val is = new FileInputStream(file) ByteStreams.skipFully(is, offset) val in = new LimitedInputStream(is, length) val chunkSize = math.min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH, length).toInt val out = new ChunkedByteBufferOutputStream(chunkSize, ByteBuffer.allocate _) Utils.tryWithSafeFinally { IOUtils.copy(in, out) } { in.close() out.close() } out.toChunkedByteBuffer } } /** * Reads data from a ChunkedByteBuffer. * * @param dispose if true, `ChunkedByteBuffer.dispose()` will be called at the end of the stream * in order to close any memory-mapped files which back the buffer. */ private[spark] class ChunkedByteBufferInputStream( var chunkedByteBuffer: ChunkedByteBuffer, dispose: Boolean) extends InputStream { // Filter out empty chunks since `read()` assumes all chunks are non-empty. private[this] var chunks = chunkedByteBuffer.getChunks().filter(_.hasRemaining).iterator private[this] var currentChunk: ByteBuffer = { if (chunks.hasNext) { chunks.next() } else { null } } override def read(): Int = { if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) { currentChunk = chunks.next() } if (currentChunk != null && currentChunk.hasRemaining) { UnsignedBytes.toInt(currentChunk.get()) } else { close() -1 } } override def read(dest: Array[Byte], offset: Int, length: Int): Int = { if (currentChunk != null && !currentChunk.hasRemaining && chunks.hasNext) { currentChunk = chunks.next() } if (currentChunk != null && currentChunk.hasRemaining) { val amountToGet = math.min(currentChunk.remaining(), length) currentChunk.get(dest, offset, amountToGet) amountToGet } else { close() -1 } } override def skip(bytes: Long): Long = { if (currentChunk != null) { val amountToSkip = math.min(bytes, currentChunk.remaining).toInt currentChunk.position(currentChunk.position() + amountToSkip) if (currentChunk.remaining() == 0) { if (chunks.hasNext) { currentChunk = chunks.next() } else { close() } } amountToSkip } else { 0L } } override def close(): Unit = { if (chunkedByteBuffer != null && dispose) { chunkedByteBuffer.dispose() } chunkedByteBuffer = null chunks = null currentChunk = null } }",
          "## CLASS: org/apache/spark/storage/ByteBufferBlockData# (implementation)\n} private[spark] class ByteBufferBlockData( val buffer: ChunkedByteBuffer, val shouldDispose: Boolean) extends BlockData { override def toInputStream(): InputStream = buffer.toInputStream(dispose = false) override def toNetty(): Object = buffer.toNetty override def toChunkedByteBuffer(allocator: Int => ByteBuffer): ChunkedByteBuffer = { buffer.copy(allocator) } override def toByteBuffer(): ByteBuffer = buffer.toByteBuffer override def size: Long = buffer.size override def dispose(): Unit = { if (shouldDispose) { buffer.dispose() } } } private[spark] class HostLocalDirManager( futureExecutionContext: ExecutionContext, cacheSize: Int, blockStoreClient: BlockStoreClient) extends Logging { private val executorIdToLocalDirsCache = CacheBuilder .newBuilder() .maximumSize(cacheSize) .build[String, Array[String]]() private[spark] def getCachedHostLocalDirs: Map[String, Array[String]] = executorIdToLocalDirsCache.synchronized { executorIdToLocalDirsCache.asMap().asScala.toMap } private[spark] def getCachedHostLocalDirsFor(executorId: String): Option[Array[String]] = executorIdToLocalDirsCache.synchronized { Option(executorIdToLocalDirsCache.getIfPresent(executorId)) } private[spark] def getHostLocalDirs( host: String, port: Int, executorIds: Array[String])( callback: Try[Map[String, Array[String]]] => Unit): Unit = { val hostLocalDirsCompletable = new CompletableFuture[java.util.Map[String, Array[String]]] blockStoreClient.getHostLocalDirs( host, port, executorIds, hostLocalDirsCompletable) hostLocalDirsCompletable.whenComplete { (hostLocalDirs, throwable) => if (hostLocalDirs != null) { callback(Success(hostLocalDirs.asScala.toMap)) executorIdToLocalDirsCache.synchronized { executorIdToLocalDirsCache.putAll(hostLocalDirs) } } else { callback(Failure(throwable)) } } } } /** * Manager running on every node (driver and executors) which provides interfaces for putting and * retrieving blocks both locally and remotely into various stores (memory, disk, and off-heap). * * Note that [[initialize()]] must be called before the BlockManager is usable. */ private[spark] class BlockManager( val executorId: String, rpcEnv: RpcEnv, val master: BlockManagerMaster, val serializerManager: SerializerManager, val conf: SparkConf, memoryManager: MemoryManager, mapOutputTracker: MapOutputTracker, shuffleManager: ShuffleManager, val blockTransferService: BlockTransferService, securityManager: SecurityManager, externalBlockStoreClient: Option[ExternalBlockStoreClient]) extends BlockDataManager with BlockEvictionHandler with Logging { // same as `conf.get(config.SHUFFLE_SERVICE_ENABLED)` private[spark] val externalShuffleServiceEnabled: Boolean = externalBlockStoreClient.isDefined private val isDriver = executorId == SparkContext.DRIVER_IDENTIFIER private val remoteReadNioBufferConversion = conf.get(Network.NETWORK_REMOTE_READ_NIO_BUFFER_CONVERSION) private[spark] val subDirsPerLocalDir = conf.get(config.DISKSTORE_SUB_DIRECTORIES) val diskBlockManager = { // Only perform cleanup if an external service is not serving our shuffle files. val deleteFilesOnStop = !externalShuffleServiceEnabled || isDriver new DiskBlockManager(conf, deleteFilesOnStop = deleteFilesOnStop, isDriver = isDriver) } // Visible for testing private[storage] val blockInfoManager = new BlockInfoManager private val futureExecutionContext = ExecutionContext.fromExecutorService( ThreadUtils.newDaemonCachedThreadPool(\"block-manager-future\", 128)) // Actual storage of where blocks are kept private[spark] val memoryStore = new MemoryStore(conf, blockInfoManager, serializerManager, memoryManager, this) private[spark] val diskStore = new DiskStore(conf, diskBlockManager, securityManager) memoryManager.setMemoryStore(memoryStore) // Note: depending on the memory manager, `maxMemory` may actually vary over time. // However, since we use this only for reporting and logging, what we actually want here is // the absolute maximum value that `maxMemory` can ever possibly reach. We may need // to revisit whether reporting this value as the \"max\" is intuitive to the user. private val maxOnHeapMemory = memoryManager.maxOnHeapStorageMemory private val maxOffHeapMemory = memoryManager.maxOffHeapStorageMemory private[spark] val externalShuffleServicePort = StorageUtils.externalShuffleServicePort(conf) var blockManagerId: BlockManagerId = _ // Address of the server that serves this executor's shuffle files. This is either an external // service, or just our own Executor's BlockManager. private[spark] var shuffleServerId: BlockManagerId = _ // Client to read other executors' blocks. This is either an external service, or just the // standard BlockTransferService to directly connect to other Executors. private[spark] val blockStoreClient = externalBlockStoreClient.getOrElse(blockTransferService) // Max number of failures before this block manager refreshes the block locations from the driver private val maxFailuresBeforeLocationRefresh = conf.get(config.BLOCK_FAILURES_BEFORE_LOCATION_REFRESH) private val storageEndpoint = rpcEnv.setupEndpoint( \"BlockManagerEndpoint\" + BlockManager.ID_GENERATOR.next, new BlockManagerStorageEndpoint(rpcEnv, this, mapOutputTracker)) // Pending re-registration action being executed asynchronously or null if none is pending. // Accesses should synchronize on asyncReregisterLock. private var asyncReregisterTask: Future[Unit] = null private val asyncReregisterLock = new Object // Field related to peer block managers that are necessary for block replication @volatile private var cachedPeers: Seq[BlockManagerId] = _ private val peerFetchLock = new Object private var lastPeerFetchTimeNs = 0L private var blockReplicationPolicy: BlockReplicationPolicy = _ // visible for test // This is volatile since if it's defined we should not accept remote blocks. @volatile private[spark] var decommissioner: Option[BlockManagerDecommissioner] = None // A DownloadFileManager used to track all the files of remote blocks which are above the // specified memory threshold. Files will be deleted automatically based on weak reference. // Exposed for test private[storage] val remoteBlockTempFileManager = new BlockManager.RemoteBlockDownloadFileManager( this, securityManager.getIOEncryptionKey()) private val maxRemoteBlockToMem = conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM) var hostLocalDirManager: Option[HostLocalDirManager] = None @inline final private def isDecommissioning() = { decommissioner.isDefined } @inline final private def checkShouldStore(blockId: BlockId) = { // Don't reject broadcast blocks since they may be stored during task exec and // don't need to be migrated. if (isDecommissioning() && !blockId.isBroadcast) { throw SparkCoreErrors.cannotSaveBlockOnDecommissionedExecutorError(blockId) } } // This is a lazy val so someone can migrating RDDs even if they don't have a MigratableResolver // for shuffles. Used in BlockManagerDecommissioner & block puts. private[storage] lazy val migratableResolver: MigratableResolver = { shuffleManager.shuffleBlockResolver.asInstanceOf[MigratableResolver] } override def getLocalDiskDirs: Array[String] = diskBlockManager.localDirsString /** * Diagnose the possible cause of the shuffle data corruption by verifying the shuffle checksums * * @param blockId The blockId of the corrupted shuffle block * @param checksumByReader The checksum value of the corrupted block * @param algorithm The cheksum algorithm that is used when calculating the checksum value */ override def diagnoseShuffleBlockCorruption( blockId: BlockId, checksumByReader: Long, algorithm: String): Cause = { assert(blockId.isInstanceOf[ShuffleBlockId], s\"Corruption diagnosis only supports shuffle block yet, but got $blockId\") val shuffleBlock = blockId.asInstanceOf[ShuffleBlockId] val resolver = shuffleManager.shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver] val checksumFile = resolver.getChecksumFile(shuffleBlock.shuffleId, shuffleBlock.mapId, algorithm) val reduceId = shuffleBlock.reduceId ShuffleChecksumHelper.diagnoseCorruption( algorithm, checksumFile, reduceId, resolver.getBlockData(shuffleBlock), checksumByReader) } /** * Abstraction for storing blocks from bytes, whether they start in memory or on disk. * * @param blockSize the decrypted size of the block */ private[spark] abstract class BlockStoreUpdater[T]( blockSize: Long, blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], tellMaster: Boolean, keepReadLock: Boolean) { /** * Reads the block content into the memory. If the update of the block store is based on a * temporary file this could lead to loading the whole file into a ChunkedByteBuffer. */ protected def readToByteBuffer(): ChunkedByteBuffer protected def blockData(): BlockData protected def saveToDiskStore(): Unit private def saveDeserializedValuesToMemoryStore(inputStream: InputStream): Boolean = { try { val values = serializerManager.dataDeserializeStream(blockId, inputStream)(classTag) memoryStore.putIteratorAsValues(blockId, values, level.memoryMode, classTag) match { case Right(_) => true case Left(iter) => // If putting deserialized values in memory failed, we will put the bytes directly // to disk, so we don't need this iterator and can close it to free resources // earlier. iter.close() false } } catch { case ex: KryoException if ex.getCause.isInstanceOf[IOException] => // We need to have detailed log message to catch environmental problems easily. // Further details: https://issues.apache.org/jira/browse/SPARK-37710 processKryoException(ex, blockId) throw ex } finally { IOUtils.closeQuietly(inputStream) } } private def saveSerializedValuesToMemoryStore(bytes: ChunkedByteBuffer): Boolean = { val memoryMode = level.memoryMode memoryStore.putBytes(blockId, blockSize, memoryMode, () => { if (memoryMode == MemoryMode.OFF_HEAP && bytes.chunks.exists(!_.isDirect)) { bytes.copy(Platform.allocateDirectBuffer) } else { bytes } }) } /** * Put the given data according to the given level in one of the block stores, replicating * the values if necessary. * * If the block already exists, this method will not overwrite it. * * If keepReadLock is true, this method will hold the read lock when it returns (even if the * block already exists). If false, this method will hold no locks when it returns. * * @return true if the block was already present or if the put succeeded, false otherwise. */ def save(): Boolean = { doPut(blockId, level, classTag, tellMaster, keepReadLock) { info => val startTimeNs = System.nanoTime() // Since we're storing bytes, initiate the replication before storing them locally. // This is faster as data is already serialized and ready to send. val replicationFuture = if (level.replication > 1) { Future { // This is a blocking action and should run in futureExecutionContext which is a cached // thread pool. replicate(blockId, blockData(), level, classTag) }(futureExecutionContext) } else { null } if (level.useMemory) { // Put it in memory first, even if it also has useDisk set to true; // We will drop it to disk later if the memory store can't hold it. val putSucceeded = if (level.deserialized) { saveDeserializedValuesToMemoryStore(blockData().toInputStream()) } else { saveSerializedValuesToMemoryStore(readToByteBuffer()) } if (!putSucceeded && level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") saveToDiskStore() } } else if (level.useDisk) { saveToDiskStore() } val putBlockStatus = getCurrentBlockStatus(blockId, info) val blockWasSuccessfullyStored = putBlockStatus.storageLevel.isValid if (blockWasSuccessfullyStored) { // Now that the block is in either the memory or disk store, // tell the master about it. info.size = blockSize if (tellMaster && info.tellMaster) { reportBlockStatus(blockId, putBlockStatus) } addUpdatedBlockStatusToTaskMetrics(blockId, putBlockStatus) } logDebug(s\"Put block ${blockId} locally took ${Utils.getUsedTimeNs(startTimeNs)}\") if (level.replication > 1) { // Wait for asynchronous replication to finish try { ThreadUtils.awaitReady(replicationFuture, Duration.Inf) } catch { case NonFatal(t) => throw SparkCoreErrors.waitingForReplicationToFinishError(t) } } if (blockWasSuccessfullyStored) { None } else { Some(blockSize) } }.isEmpty } } /** * Helper for storing a block from bytes already in memory. * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. */ private case class ByteBufferBlockStoreUpdater[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], bytes: ChunkedByteBuffer, tellMaster: Boolean = true, keepReadLock: Boolean = false) extends BlockStoreUpdater[T](bytes.size, blockId, level, classTag, tellMaster, keepReadLock) { override def readToByteBuffer(): ChunkedByteBuffer = bytes /** * The ByteBufferBlockData wrapper is not disposed of to avoid releasing buffers that are * owned by the caller. */ override def blockData(): BlockData = new ByteBufferBlockData(bytes, false) override def saveToDiskStore(): Unit = diskStore.putBytes(blockId, bytes) } /** * Helper for storing a block based from bytes already in a local temp file. */ private[spark] case class TempFileBasedBlockStoreUpdater[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], tmpFile: File, blockSize: Long, tellMaster: Boolean = true, keepReadLock: Boolean = false) extends BlockStoreUpdater[T](blockSize, blockId, level, classTag, tellMaster, keepReadLock) { override def readToByteBuffer(): ChunkedByteBuffer = { val allocator = level.memoryMode match { case MemoryMode.ON_HEAP => ByteBuffer.allocate _ case MemoryMode.OFF_HEAP => Platform.allocateDirectBuffer _ } blockData().toChunkedByteBuffer(allocator) } override def blockData(): BlockData = diskStore.getBytes(tmpFile, blockSize) override def saveToDiskStore(): Unit = diskStore.moveFileToBlock(tmpFile, blockSize, blockId) override def save(): Boolean = { val res = super.save() tmpFile.delete() res } } /** * Initializes the BlockManager with the given appId. This is not performed in the constructor as * the appId may not be known at BlockManager instantiation time (in particular for the driver, * where it is only learned after registration with the TaskScheduler). * * This method initializes the BlockTransferService and BlockStoreClient, registers with the * BlockManagerMaster, starts the BlockManagerWorker endpoint, and registers with a local shuffle * service if configured. */ def initialize(appId: String): Unit = { blockTransferService.init(this) externalBlockStoreClient.foreach { blockStoreClient => blockStoreClient.init(appId) } blockReplicationPolicy = { val priorityClass = conf.get(config.STORAGE_REPLICATION_POLICY) val clazz = Utils.classForName(priorityClass) val ret = clazz.getConstructor().newInstance().asInstanceOf[BlockReplicationPolicy] logInfo(s\"Using $priorityClass for block replication policy\") ret } val id = BlockManagerId(executorId, blockTransferService.hostName, blockTransferService.port, None) val idFromMaster = master.registerBlockManager( id, diskBlockManager.localDirsString, maxOnHeapMemory, maxOffHeapMemory, storageEndpoint) blockManagerId = if (idFromMaster != null) idFromMaster else id shuffleServerId = if (externalShuffleServiceEnabled) { logInfo(s\"external shuffle service port = $externalShuffleServicePort\") BlockManagerId(executorId, blockTransferService.hostName, externalShuffleServicePort) } else { blockManagerId } // Register Executors' configuration with the local shuffle service, if one should exist. if (externalShuffleServiceEnabled && !blockManagerId.isDriver) { registerWithExternalShuffleServer() } hostLocalDirManager = { if ((conf.get(config.SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED) && !conf.get(config.SHUFFLE_USE_OLD_FETCH_PROTOCOL)) || Utils.isPushBasedShuffleEnabled(conf, isDriver)) { Some(new HostLocalDirManager( futureExecutionContext, conf.get(config.STORAGE_LOCAL_DISK_BY_EXECUTORS_CACHE_SIZE), blockStoreClient)) } else { None } } logInfo(s\"Initialized BlockManager: $blockManagerId\") } def shuffleMetricsSource: Source = { import BlockManager._ if (externalShuffleServiceEnabled) { new ShuffleMetricsSource(\"ExternalShuffle\", blockStoreClient.shuffleMetrics()) } else { new ShuffleMetricsSource(\"NettyBlockTransfer\", blockStoreClient.shuffleMetrics()) } } private def registerWithExternalShuffleServer(): Unit = { logInfo(\"Registering executor with local external shuffle service.\") val shuffleManagerMeta = if (Utils.isPushBasedShuffleEnabled(conf, isDriver = isDriver, checkSerializer = false)) { s\"${shuffleManager.getClass.getName}:\" + s\"${diskBlockManager.getMergeDirectoryAndAttemptIDJsonString()}}}\" } else { shuffleManager.getClass.getName } val shuffleConfig = new ExecutorShuffleInfo( diskBlockManager.localDirsString, diskBlockManager.subDirsPerLocalDir, shuffleManagerMeta) val MAX_ATTEMPTS = conf.get(config.SHUFFLE_REGISTRATION_MAX_ATTEMPTS) val SLEEP_TIME_SECS = 5 for (i <- 1 to MAX_ATTEMPTS) { try { // Synchronous and will throw an exception if we cannot connect. blockStoreClient.asInstanceOf[ExternalBlockStoreClient].registerWithShuffleServer( shuffleServerId.host, shuffleServerId.port, shuffleServerId.executorId, shuffleConfig) return } catch { case e: Exception if i < MAX_ATTEMPTS => logError(s\"Failed to connect to external shuffle server, will retry ${MAX_ATTEMPTS - i}\" + s\" more times after waiting $SLEEP_TIME_SECS seconds...\", e) Thread.sleep(SLEEP_TIME_SECS * 1000L) case NonFatal(e) => throw SparkCoreErrors.unableToRegisterWithExternalShuffleServerError(e) } } } /** * Report all blocks to the BlockManager again. This may be necessary if we are dropped * by the BlockManager and come back or if we become capable of recovering blocks on disk after * an executor crash. * * This function deliberately fails silently if the master returns false (indicating that * the storage endpoint needs to re-register). The error condition will be detected again by the * next heart beat attempt or new block registration and another try to re-register all blocks * will be made then. */ private def reportAllBlocks(): Unit = { logInfo(s\"Reporting ${blockInfoManager.size} blocks to the master.\") for ((blockId, info) <- blockInfoManager.entries) { val status = getCurrentBlockStatus(blockId, info) if (info.tellMaster && !tryToReportBlockStatus(blockId, status)) { logError(s\"Failed to report $blockId to master; giving up.\") return } } } /** * Re-register with the master and report all blocks to it. This will be called by the heart beat * thread if our heartbeat to the block manager indicates that we were not registered. * * Note that this method must be called without any BlockInfo locks held. */ def reregister(): Unit = { // TODO: We might need to rate limit re-registering. logInfo(s\"BlockManager $blockManagerId re-registering with master\") master.registerBlockManager(blockManagerId, diskBlockManager.localDirsString, maxOnHeapMemory, maxOffHeapMemory, storageEndpoint) reportAllBlocks() } /** * Re-register with the master sometime soon. */ private def asyncReregister(): Unit = { asyncReregisterLock.synchronized { if (asyncReregisterTask == null) { asyncReregisterTask = Future[Unit] { // This is a blocking action and should run in futureExecutionContext which is a cached // thread pool reregister() asyncReregisterLock.synchronized { asyncReregisterTask = null } }(futureExecutionContext) } } } /** * For testing. Wait for any pending asynchronous re-registration; otherwise, do nothing. */ def waitForAsyncReregister(): Unit = { val task = asyncReregisterTask if (task != null) { try { ThreadUtils.awaitReady(task, Duration.Inf) } catch { case NonFatal(t) => throw SparkCoreErrors.waitingForAsyncReregistrationError(t) } } } override def getHostLocalShuffleData( blockId: BlockId, dirs: Array[String]): ManagedBuffer = { shuffleManager.shuffleBlockResolver.getBlockData(blockId, Some(dirs)) } /** * Interface to get local block data. Throws an exception if the block cannot be found or * cannot be read successfully. */ override def getLocalBlockData(blockId: BlockId): ManagedBuffer = { if (blockId.isShuffle) { logDebug(s\"Getting local shuffle block ${blockId}\") try { shuffleManager.shuffleBlockResolver.getBlockData(blockId) } catch { case e: IOException => if (conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined) { FallbackStorage.read(conf, blockId) } else { throw e } } } else { getLocalBytes(blockId) match { case Some(blockData) => new BlockManagerManagedBuffer(blockInfoManager, blockId, blockData, true) case None => // If this block manager receives a request for a block that it doesn't have then it's // likely that the master has outdated block statuses for this block. Therefore, we send // an RPC so that this block is marked as being unavailable from this block manager. reportBlockStatus(blockId, BlockStatus.empty) throw SparkCoreErrors.blockNotFoundError(blockId) } } } /** * Put the block locally, using the given storage level. * * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. */ override def putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean = { putBytes(blockId, new ChunkedByteBuffer(data.nioByteBuffer()), level)(classTag) } override def putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID = { checkShouldStore(blockId) if (blockId.isShuffle) { logDebug(s\"Putting shuffle block ${blockId}\") try { return migratableResolver.putShuffleBlockAsStream(blockId, serializerManager) } catch { case e: ClassCastException => throw SparkCoreErrors.unexpectedShuffleBlockWithUnsupportedResolverError(shuffleManager, blockId) } } logDebug(s\"Putting regular block ${blockId}\") // All other blocks val (_, tmpFile) = diskBlockManager.createTempLocalBlock() val channel = new CountingWritableChannel( Channels.newChannel(serializerManager.wrapForEncryption(new FileOutputStream(tmpFile)))) logTrace(s\"Streaming block $blockId to tmp file $tmpFile\") new StreamCallbackWithID { override def getID: String = blockId.name override def onData(streamId: String, buf: ByteBuffer): Unit = { while (buf.hasRemaining) { channel.write(buf) } } override def onComplete(streamId: String): Unit = { logTrace(s\"Done receiving block $blockId, now putting into local blockManager\") // Note this is all happening inside the netty thread as soon as it reads the end of the // stream. channel.close() val blockSize = channel.getCount val blockStored = TempFileBasedBlockStoreUpdater( blockId, level, classTag, tmpFile, blockSize).save() if (!blockStored) { throw SparkCoreErrors.failToStoreBlockOnBlockManagerError(blockManagerId, blockId) } } override def onFailure(streamId: String, cause: Throwable): Unit = { // the framework handles the connection itself, we just need to do local cleanup channel.close() tmpFile.delete() } } } /** * Get the local merged shuffle block data for the given block ID as multiple chunks. * A merged shuffle file is divided into multiple chunks according to the index file. * Instead of reading the entire file as a single block, we split it into smaller chunks * which will be memory efficient when performing certain operations. */ def getLocalMergedBlockData( blockId: ShuffleMergedBlockId, dirs: Array[String]): Seq[ManagedBuffer] = { shuffleManager.shuffleBlockResolver.getMergedBlockData(blockId, Some(dirs)) } /** * Get the local merged shuffle block meta data for the given block ID. */ def getLocalMergedBlockMeta( blockId: ShuffleMergedBlockId, dirs: Array[String]): MergedBlockMeta = { shuffleManager.shuffleBlockResolver.getMergedBlockMeta(blockId, Some(dirs)) } /** * Get the BlockStatus for the block identified by the given ID, if it exists. * NOTE: This is mainly for testing. */ def getStatus(blockId: BlockId): Option[BlockStatus] = { blockInfoManager.get(blockId).map { info => val memSize = if (memoryStore.contains(blockId)) memoryStore.getSize(blockId) else 0L val diskSize = if (diskStore.contains(blockId)) diskStore.getSize(blockId) else 0L BlockStatus(info.level, memSize = memSize, diskSize = diskSize) } } /** * Get the ids of existing blocks that match the given filter. Note that this will * query the blocks stored in the disk block manager (that the block manager * may not know of). */ def getMatchingBlockIds(filter: BlockId => Boolean): Seq[BlockId] = { // The `toArray` is necessary here in order to force the list to be materialized so that we // don't try to serialize a lazy iterator when responding to client requests. (blockInfoManager.entries.map(_._1) ++ diskBlockManager.getAllBlocks()) .filter(filter) .toArray .toSeq } /** * Tell the master about the current storage status of a block. This will send a block update * message reflecting the current status, *not* the desired storage level in its block info. * For example, a block with MEMORY_AND_DISK set might have fallen out to be only on disk. * * droppedMemorySize exists to account for when the block is dropped from memory to disk (so * it is still valid). This ensures that update in master will compensate for the increase in * memory on the storage endpoint. */ private[spark] def reportBlockStatus( blockId: BlockId, status: BlockStatus, droppedMemorySize: Long = 0L): Unit = { val needReregister = !tryToReportBlockStatus(blockId, status, droppedMemorySize) if (needReregister) { logInfo(s\"Got told to re-register updating block $blockId\") // Re-registering will report our new block for free. asyncReregister() } logDebug(s\"Told master about block $blockId\") } /** * Actually send a UpdateBlockInfo message. Returns the master's response, * which will be true if the block was successfully recorded and false if * the storage endpoint needs to re-register. */ private def tryToReportBlockStatus( blockId: BlockId, status: BlockStatus, droppedMemorySize: Long = 0L): Boolean = { val storageLevel = status.storageLevel val inMemSize = Math.max(status.memSize, droppedMemorySize) val onDiskSize = status.diskSize master.updateBlockInfo(blockManagerId, blockId, storageLevel, inMemSize, onDiskSize) } /** * Return the updated storage status of the block with the given ID. More specifically, if * the block is dropped from memory and possibly added to disk, return the new storage level * and the updated in-memory and on-disk sizes. */ private def getCurrentBlockStatus(blockId: BlockId, info: BlockInfo): BlockStatus = { info.synchronized { info.level match { case null => BlockStatus.empty case level => val inMem = level.useMemory && memoryStore.contains(blockId) val onDisk = level.useDisk && diskStore.contains(blockId) val deserialized = if (inMem) level.deserialized else false val replication = if (inMem || onDisk) level.replication else 1 val storageLevel = StorageLevel( useDisk = onDisk, useMemory = inMem, useOffHeap = level.useOffHeap, deserialized = deserialized, replication = replication) val memSize = if (inMem) memoryStore.getSize(blockId) else 0L val diskSize = if (onDisk) diskStore.getSize(blockId) else 0L BlockStatus(storageLevel, memSize, diskSize) } } } /** * Get locations of an array of blocks. */ private def getLocationBlockIds(blockIds: Array[BlockId]): Array[Seq[BlockManagerId]] = { val startTimeNs = System.nanoTime() val locations = master.getLocations(blockIds).toArray logDebug(s\"Got multiple block location in ${Utils.getUsedTimeNs(startTimeNs)}\") locations } /** * Cleanup code run in response to a failed local read. * Must be called while holding a read lock on the block. */ private def handleLocalReadFailure(blockId: BlockId): Nothing = { releaseLock(blockId) // Remove the missing block so that its unavailability is reported to the driver removeBlock(blockId) throw SparkCoreErrors.readLockedBlockNotFoundError(blockId) } /** * Get block from local block manager as an iterator of Java objects. */ def getLocalValues(blockId: BlockId): Option[BlockResult] = { logDebug(s\"Getting local block $blockId\") blockInfoManager.lockForReading(blockId) match { case None => logDebug(s\"Block $blockId was not found\") None case Some(info) => val level = info.level logDebug(s\"Level for block $blockId is $level\") val taskContext = Option(TaskContext.get()) if (level.useMemory && memoryStore.contains(blockId)) { val iter: Iterator[Any] = if (level.deserialized) { memoryStore.getValues(blockId).get } else { serializerManager.dataDeserializeStream( blockId, memoryStore.getBytes(blockId).get.toInputStream())(info.classTag) } // We need to capture the current taskId in case the iterator completion is triggered // from a different thread which does not have TaskContext set; see SPARK-18406 for // discussion. val ci = CompletionIterator[Any, Iterator[Any]](iter, { releaseLock(blockId, taskContext) }) Some(new BlockResult(ci, DataReadMethod.Memory, info.size)) } else if (level.useDisk && diskStore.contains(blockId)) { try { val diskData = diskStore.getBytes(blockId) val iterToReturn: Iterator[Any] = { if (level.deserialized) { val diskValues = serializerManager.dataDeserializeStream( blockId, diskData.toInputStream())(info.classTag) maybeCacheDiskValuesInMemory(info, blockId, level, diskValues) } else { val stream = maybeCacheDiskBytesInMemory(info, blockId, level, diskData) .map { _.toInputStream(dispose = false) } .getOrElse { diskData.toInputStream() } serializerManager.dataDeserializeStream(blockId, stream)(info.classTag) } } val ci = CompletionIterator[Any, Iterator[Any]](iterToReturn, { releaseLockAndDispose(blockId, diskData, taskContext) }) Some(new BlockResult(ci, DataReadMethod.Disk, info.size)) } catch { case ex: KryoException if ex.getCause.isInstanceOf[IOException] => // We need to have detailed log message to catch environmental problems easily. // Further details: https://issues.apache.org/jira/browse/SPARK-37710 processKryoException(ex, blockId) throw ex } } else { handleLocalReadFailure(blockId) } } } private def processKryoException(ex: KryoException, blockId: BlockId): Unit = { var message = \"%s. %s - blockId: %s\".format(ex.getMessage, blockManagerId.toString, blockId) val file = diskBlockManager.getFile(blockId) if (file.exists()) { message = \"%s - blockDiskPath: %s\".format(message, file.getAbsolutePath) } logInfo(message) } /** * Get block from the local block manager as serialized bytes. */ def getLocalBytes(blockId: BlockId): Option[BlockData] = { logDebug(s\"Getting local block $blockId as bytes\") assert(!blockId.isShuffle, s\"Unexpected ShuffleBlockId $blockId\") blockInfoManager.lockForReading(blockId).map { info => doGetLocalBytes(blockId, info) } } /** * Get block from the local block manager as serialized bytes. * * Must be called while holding a read lock on the block. * Releases the read lock upon exception; keeps the read lock upon successful return. */ private def doGetLocalBytes(blockId: BlockId, info: BlockInfo): BlockData = { val level = info.level logDebug(s\"Level for block $blockId is $level\") // In order, try to read the serialized bytes from memory, then from disk, then fall back to // serializing in-memory objects, and, finally, throw an exception if the block does not exist. if (level.deserialized) { // Try to avoid expensive serialization by reading a pre-serialized copy from disk: if (level.useDisk && diskStore.contains(blockId)) { // Note: we purposely do not try to put the block back into memory here. Since this branch // handles deserialized blocks, this block may only be cached in memory as objects, not // serialized bytes. Because the caller only requested bytes, it doesn't make sense to // cache the block's deserialized objects since that caching may not have a payoff. diskStore.getBytes(blockId) } else if (level.useMemory && memoryStore.contains(blockId)) { // The block was not found on disk, so serialize an in-memory copy: new ByteBufferBlockData(serializerManager.dataSerializeWithExplicitClassTag( blockId, memoryStore.getValues(blockId).get, info.classTag), true) } else { handleLocalReadFailure(blockId) } } else { // storage level is serialized if (level.useMemory && memoryStore.contains(blockId)) { new ByteBufferBlockData(memoryStore.getBytes(blockId).get, false) } else if (level.useDisk && diskStore.contains(blockId)) { val diskData = diskStore.getBytes(blockId) maybeCacheDiskBytesInMemory(info, blockId, level, diskData) .map(new ByteBufferBlockData(_, false)) .getOrElse(diskData) } else { handleLocalReadFailure(blockId) } } } /** * Get block from remote block managers. * * This does not acquire a lock on this block in this JVM. */ private[spark] def getRemoteValues[T: ClassTag](blockId: BlockId): Option[BlockResult] = { val ct = implicitly[ClassTag[T]] getRemoteBlock(blockId, (data: ManagedBuffer) => { val values = serializerManager.dataDeserializeStream(blockId, data.createInputStream())(ct) new BlockResult(values, DataReadMethod.Network, data.size) }) } /** * Get the remote block and transform it to the provided data type. * * If the block is persisted to the disk and stored at an executor running on the same host then * first it is tried to be accessed using the local directories of the other executor directly. * If the file is successfully identified then tried to be transformed by the provided * transformation function which expected to open the file. If there is any exception during this * transformation then block access falls back to fetching it from the remote executor via the * network. * * @param blockId identifies the block to get * @param bufferTransformer this transformer expected to open the file if the block is backed by a * file by this it is guaranteed the whole content can be loaded * @tparam T result type */ private[spark] def getRemoteBlock[T]( blockId: BlockId, bufferTransformer: ManagedBuffer => T): Option[T] = { logDebug(s\"Getting remote block $blockId\") require(blockId != null, \"BlockId is null\") // Because all the remote blocks are registered in driver, it is not necessary to ask // all the storage endpoints to get block status. val locationsAndStatusOption = master.getLocationsAndStatus(blockId, blockManagerId.host) if (locationsAndStatusOption.isEmpty) { logDebug(s\"Block $blockId is unknown by block manager master\") None } else { val locationsAndStatus = locationsAndStatusOption.get val blockSize = locationsAndStatus.status.diskSize.max(locationsAndStatus.status.memSize) locationsAndStatus.localDirs.flatMap { localDirs => val blockDataOption = readDiskBlockFromSameHostExecutor(blockId, localDirs, locationsAndStatus.status.diskSize) val res = blockDataOption.flatMap { blockData => try { Some(bufferTransformer(blockData)) } catch { case NonFatal(e) => logDebug(\"Block from the same host executor cannot be opened: \", e) None } } logInfo(s\"Read $blockId from the disk of a same host executor is \" + (if (res.isDefined) \"successful.\" else \"failed.\")) res }.orElse { fetchRemoteManagedBuffer(blockId, blockSize, locationsAndStatus).map(bufferTransformer) } } } private def preferExecutors(locations: Seq[BlockManagerId]): Seq[BlockManagerId] = { val (executors, shuffleServers) = locations.partition(_.port != externalShuffleServicePort) executors ++ shuffleServers } /** * Return a list of locations for the given block, prioritizing the local machine since * multiple block managers can share the same host, followed by hosts on the same rack. * * Within each of the above listed groups (same host, same rack and others) executors are * preferred over the external shuffle service. */ private[spark] def sortLocations(locations: Seq[BlockManagerId]): Seq[BlockManagerId] = { val locs = Random.shuffle(locations) val (preferredLocs, otherLocs) = locs.partition(_.host == blockManagerId.host) val orderedParts = blockManagerId.topologyInfo match { case None => Seq(preferredLocs, otherLocs) case Some(_) => val (sameRackLocs, differentRackLocs) = otherLocs.partition { loc => blockManagerId.topologyInfo == loc.topologyInfo } Seq(preferredLocs, sameRackLocs, differentRackLocs) } orderedParts.map(preferExecutors).reduce(_ ++ _) } /** * Fetch the block from remote block managers as a ManagedBuffer. */ private def fetchRemoteManagedBuffer( blockId: BlockId, blockSize: Long, locationsAndStatus: BlockManagerMessages.BlockLocationsAndStatus): Option[ManagedBuffer] = { // If the block size is above the threshold, we should pass our FileManger to // BlockTransferService, which will leverage it to spill the block; if not, then passed-in // null value means the block will be persisted in memory. val tempFileManager = if (blockSize > maxRemoteBlockToMem) { remoteBlockTempFileManager } else { null } var runningFailureCount = 0 var totalFailureCount = 0 val locations = sortLocations(locationsAndStatus.locations) val maxFetchFailures = locations.size var locationIterator = locations.iterator while (locationIterator.hasNext) { val loc = locationIterator.next() logDebug(s\"Getting remote block $blockId from $loc\") val data = try { val buf = blockTransferService.fetchBlockSync(loc.host, loc.port, loc.executorId, blockId.toString, tempFileManager) if (blockSize > 0 && buf.size() == 0) { throw new IllegalStateException(\"Empty buffer received for non empty block \" + s\"when fetching remote block $blockId from $loc\") } buf } catch { case NonFatal(e) => runningFailureCount += 1 totalFailureCount += 1 if (totalFailureCount >= maxFetchFailures) { // Give up trying anymore locations. Either we've tried all of the original locations, // or we've refreshed the list of locations from the master, and have still // hit failures after trying locations from the refreshed list. logWarning(s\"Failed to fetch remote block $blockId \" + s\"from [${locations.mkString(\", \")}] after $totalFailureCount fetch failures. \" + s\"Most recent failure cause:\", e) return None } logWarning(s\"Failed to fetch remote block $blockId \" + s\"from $loc (failed attempt $runningFailureCount)\", e) // If there is a large number of executors then locations list can contain a // large number of stale entries causing a large number of retries that may // take a significant amount of time. To get rid of these stale entries // we refresh the block locations after a certain number of fetch failures if (runningFailureCount >= maxFailuresBeforeLocationRefresh) { locationIterator = sortLocations(master.getLocations(blockId)).iterator logDebug(s\"Refreshed locations from the driver \" + s\"after ${runningFailureCount} fetch failures.\") runningFailureCount = 0 } // This location failed, so we retry fetch from a different one by returning null here null } if (data != null) { // If the ManagedBuffer is a BlockManagerManagedBuffer, the disposal of the // byte buffers backing it may need to be handled after reading the bytes. // In this case, since we just fetched the bytes remotely, we do not have // a BlockManagerManagedBuffer. The assert here is to ensure that this holds // true (or the disposal is handled). assert(!data.isInstanceOf[BlockManagerManagedBuffer]) return Some(data) } logDebug(s\"The value of block $blockId is null\") } logDebug(s\"Block $blockId not found\") None } /** * Reads the block from the local directories of another executor which runs on the same host. */ private[spark] def readDiskBlockFromSameHostExecutor( blockId: BlockId, localDirs: Array[String], blockSize: Long): Option[ManagedBuffer] = { val file = new File(ExecutorDiskUtils.getFilePath(localDirs, subDirsPerLocalDir, blockId.name)) if (file.exists()) { val managedBuffer = securityManager.getIOEncryptionKey() match { case Some(key) => // Encrypted blocks cannot be memory mapped; return a special object that does decryption // and provides InputStream / FileRegion implementations for reading the data. new EncryptedManagedBuffer( new EncryptedBlockData(file, blockSize, conf, key)) case _ => val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\") new FileSegmentManagedBuffer(transportConf, file, 0, file.length) } Some(managedBuffer) } else { None } } /** * Get block from remote block managers as serialized bytes. */ def getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer] = { getRemoteBlock(blockId, (data: ManagedBuffer) => { // SPARK-24307 undocumented \"escape-hatch\" in case there are any issues in converting to // ChunkedByteBuffer, to go back to old code-path. Can be removed post Spark 2.4 if // new path is stable. if (remoteReadNioBufferConversion) { new ChunkedByteBuffer(data.nioByteBuffer()) } else { ChunkedByteBuffer.fromManagedBuffer(data) } }) } /** * Get a block from the block manager (either local or remote). * * This acquires a read lock on the block if the block was stored locally and does not acquire * any locks if the block was fetched from a remote block manager. The read lock will * automatically be freed once the result's `data` iterator is fully consumed. */ def get[T: ClassTag](blockId: BlockId): Option[BlockResult] = { val local = getLocalValues(blockId) if (local.isDefined) { logInfo(s\"Found block $blockId locally\") return local } val remote = getRemoteValues[T](blockId) if (remote.isDefined) { logInfo(s\"Found block $blockId remotely\") return remote } None } /** * Downgrades an exclusive write lock to a shared read lock. */ def downgradeLock(blockId: BlockId): Unit = { blockInfoManager.downgradeLock(blockId) } /** * Release a lock on the given block with explicit TaskContext. * The param `taskContext` should be passed in case we can't get the correct TaskContext, * for example, the input iterator of a cached RDD iterates to the end in a child * thread. */ def releaseLock(blockId: BlockId, taskContext: Option[TaskContext] = None): Unit = { val taskAttemptId = taskContext.map(_.taskAttemptId()) // SPARK-27666. When a task completes, Spark automatically releases all the blocks locked // by this task. We should not release any locks for a task that is already completed. if (taskContext.isDefined && taskContext.get.isCompleted) { logWarning(s\"Task ${taskAttemptId.get} already completed, not releasing lock for $blockId\") } else { blockInfoManager.unlock(blockId, taskAttemptId) } } /** * Registers a task with the BlockManager in order to initialize per-task bookkeeping structures. */ def registerTask(taskAttemptId: Long): Unit = { blockInfoManager.registerTask(taskAttemptId) } /** * Release all locks for the given task. * * @return the blocks whose locks were released. */ def releaseAllLocksForTask(taskAttemptId: Long): Seq[BlockId] = { blockInfoManager.releaseAllLocksForTask(taskAttemptId) } /** * Retrieve the given block if it exists, otherwise call the provided `makeIterator` method * to compute the block, persist it, and return its values. * * @return either a BlockResult if the block was successfully cached, or an iterator if the block * could not be cached. */ def getOrElseUpdate[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], makeIterator: () => Iterator[T]): Either[BlockResult, Iterator[T]] = { // Attempt to read the block from local or remote storage. If it's present, then we don't need // to go through the local-get-or-put path. get[T](blockId)(classTag) match { case Some(block) => return Left(block) case _ => // Need to compute the block. } // Initially we hold no locks on this block. doPutIterator(blockId, makeIterator, level, classTag, keepReadLock = true) match { case None => // doPut() didn't hand work back to us, so the block already existed or was successfully // stored. Therefore, we now hold a read lock on the block. val blockResult = getLocalValues(blockId).getOrElse { // Since we held a read lock between the doPut() and get() calls, the block should not // have been evicted, so get() not returning the block indicates some internal error. releaseLock(blockId) throw SparkCoreErrors.failToGetBlockWithLockError(blockId) } // We already hold a read lock on the block from the doPut() call and getLocalValues() // acquires the lock again, so we need to call releaseLock() here so that the net number // of lock acquisitions is 1 (since the caller will only call release() once). releaseLock(blockId) Left(blockResult) case Some(iter) => // The put failed, likely because the data was too large to fit in memory and could not be // dropped to disk. Therefore, we need to pass the input iterator back to the caller so // that they can decide what to do with the values (e.g. process them without caching). Right(iter) } } /** * @return true if the block was stored or false if an error occurred. */ def putIterator[T: ClassTag]( blockId: BlockId, values: Iterator[T], level: StorageLevel, tellMaster: Boolean = true): Boolean = { require(values != null, \"Values is null\") doPutIterator(blockId, () => values, level, implicitly[ClassTag[T]], tellMaster) match { case None => true case Some(iter) => // Caller doesn't care about the iterator values, so we can close the iterator here // to free resources earlier iter.close() false } } /** * A short circuited method to get a block writer that can write data directly to disk. * The Block will be appended to the File specified by filename. Callers should handle error * cases. */ def getDiskWriter( blockId: BlockId, file: File, serializerInstance: SerializerInstance, bufferSize: Int, writeMetrics: ShuffleWriteMetricsReporter): DiskBlockObjectWriter = { val syncWrites = conf.get(config.SHUFFLE_SYNC) new DiskBlockObjectWriter(file, serializerManager, serializerInstance, bufferSize, syncWrites, writeMetrics, blockId) } /** * Put a new block of serialized bytes to the block manager. * * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. * * @return true if the block was stored or false if an error occurred. */ def putBytes[T: ClassTag]( blockId: BlockId, bytes: ChunkedByteBuffer, level: StorageLevel, tellMaster: Boolean = true): Boolean = { require(bytes != null, \"Bytes is null\") val blockStoreUpdater = ByteBufferBlockStoreUpdater(blockId, level, implicitly[ClassTag[T]], bytes, tellMaster) blockStoreUpdater.save() } /** * Helper method used to abstract common code from [[BlockStoreUpdater.save()]] * and [[doPutIterator()]]. * * @param putBody a function which attempts the actual put() and returns None on success * or Some on failure. */ private def doPut[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_], tellMaster: Boolean, keepReadLock: Boolean)(putBody: BlockInfo => Option[T]): Option[T] = { require(blockId != null, \"BlockId is null\") require(level != null && level.isValid, \"StorageLevel is null or invalid\") checkShouldStore(blockId) val putBlockInfo = { val newInfo = new BlockInfo(level, classTag, tellMaster) if (blockInfoManager.lockNewBlockForWriting(blockId, newInfo)) { newInfo } else { logWarning(s\"Block $blockId already exists on this machine; not re-adding it\") if (!keepReadLock) { // lockNewBlockForWriting returned a read lock on the existing block, so we must free it: releaseLock(blockId) } return None } } val startTimeNs = System.nanoTime() var exceptionWasThrown: Boolean = true val result: Option[T] = try { val res = putBody(putBlockInfo) exceptionWasThrown = false if (res.isEmpty) { // the block was successfully stored if (keepReadLock) { blockInfoManager.downgradeLock(blockId) } else { blockInfoManager.unlock(blockId) } } else { removeBlockInternal(blockId, tellMaster = false) logWarning(s\"Putting block $blockId failed\") } res } catch { // Since removeBlockInternal may throw exception, // we should print exception first to show root cause. case NonFatal(e) => logWarning(s\"Putting block $blockId failed due to exception $e.\") throw e } finally { // This cleanup is performed in a finally block rather than a `catch` to avoid having to // catch and properly re-throw InterruptedException. if (exceptionWasThrown) { // If an exception was thrown then it's possible that the code in `putBody` has already // notified the master about the availability of this block, so we need to send an update // to remove this block location. removeBlockInternal(blockId, tellMaster = tellMaster) // The `putBody` code may have also added a new block status to TaskMetrics, so we need // to cancel that out by overwriting it with an empty block status. We only do this if // the finally block was entered via an exception because doing this unconditionally would // cause us to send empty block statuses for every block that failed to be cached due to // a memory shortage (which is an expected failure, unlike an uncaught exception). addUpdatedBlockStatusToTaskMetrics(blockId, BlockStatus.empty) } } val usedTimeMs = Utils.getUsedTimeNs(startTimeNs) if (level.replication > 1) { logDebug(s\"Putting block ${blockId} with replication took $usedTimeMs\") } else { logDebug(s\"Putting block ${blockId} without replication took ${usedTimeMs}\") } result } /** * Put the given block according to the given level in one of the block stores, replicating * the values if necessary. * * If the block already exists, this method will not overwrite it. * * @param keepReadLock if true, this method will hold the read lock when it returns (even if the * block already exists). If false, this method will hold no locks when it * returns. * @return None if the block was already present or if the put succeeded, or Some(iterator) * if the put failed. */ private def doPutIterator[T]( blockId: BlockId, iterator: () => Iterator[T], level: StorageLevel, classTag: ClassTag[T], tellMaster: Boolean = true, keepReadLock: Boolean = false): Option[PartiallyUnrolledIterator[T]] = { doPut(blockId, level, classTag, tellMaster = tellMaster, keepReadLock = keepReadLock) { info => val startTimeNs = System.nanoTime() var iteratorFromFailedMemoryStorePut: Option[PartiallyUnrolledIterator[T]] = None // Size of the block in bytes var size = 0L if (level.useMemory) { // Put it in memory first, even if it also has useDisk set to true; // We will drop it to disk later if the memory store can't hold it. if (level.deserialized) { memoryStore.putIteratorAsValues(blockId, iterator(), level.memoryMode, classTag) match { case Right(s) => size = s case Left(iter) => // Not enough space to unroll this block; drop to disk if applicable if (level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream(blockId, out, iter)(classTag) } size = diskStore.getSize(blockId) } else { iteratorFromFailedMemoryStorePut = Some(iter) } } } else { // !level.deserialized memoryStore.putIteratorAsBytes(blockId, iterator(), classTag, level.memoryMode) match { case Right(s) => size = s case Left(partiallySerializedValues) => // Not enough space to unroll this block; drop to disk if applicable if (level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) partiallySerializedValues.finishWritingToStream(out) } size = diskStore.getSize(blockId) } else { iteratorFromFailedMemoryStorePut = Some(partiallySerializedValues.valuesIterator) } } } } else if (level.useDisk) { diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream(blockId, out, iterator())(classTag) } size = diskStore.getSize(blockId) } val putBlockStatus = getCurrentBlockStatus(blockId, info) val blockWasSuccessfullyStored = putBlockStatus.storageLevel.isValid if (blockWasSuccessfullyStored) { // Now that the block is in either the memory or disk store, tell the master about it. info.size = size if (tellMaster && info.tellMaster) { reportBlockStatus(blockId, putBlockStatus) } addUpdatedBlockStatusToTaskMetrics(blockId, putBlockStatus) logDebug(s\"Put block $blockId locally took ${Utils.getUsedTimeNs(startTimeNs)}\") if (level.replication > 1) { val remoteStartTimeNs = System.nanoTime() val bytesToReplicate = doGetLocalBytes(blockId, info) // [SPARK-16550] Erase the typed classTag when using default serialization, since // NettyBlockRpcServer crashes when deserializing repl-defined classes. // TODO(ekl) remove this once the classloader issue on the remote end is fixed. val remoteClassTag = if (!serializerManager.canUseKryo(classTag)) { scala.reflect.classTag[Any] } else { classTag } try { replicate(blockId, bytesToReplicate, level, remoteClassTag) } finally { bytesToReplicate.dispose() } logDebug(s\"Put block $blockId remotely took ${Utils.getUsedTimeNs(remoteStartTimeNs)}\") } } assert(blockWasSuccessfullyStored == iteratorFromFailedMemoryStorePut.isEmpty) iteratorFromFailedMemoryStorePut } } /** * Attempts to cache spilled bytes read from disk into the MemoryStore in order to speed up * subsequent reads. This method requires the caller to hold a read lock on the block. * * @return a copy of the bytes from the memory store if the put succeeded, otherwise None. * If this returns bytes from the memory store then the original disk store bytes will * automatically be disposed and the caller should not continue to use them. Otherwise, * if this returns None then the original disk store bytes will be unaffected. */ private def maybeCacheDiskBytesInMemory( blockInfo: BlockInfo, blockId: BlockId, level: StorageLevel, diskData: BlockData): Option[ChunkedByteBuffer] = { require(!level.deserialized) if (level.useMemory) { // Synchronize on blockInfo to guard against a race condition where two readers both try to // put values read from disk into the MemoryStore. blockInfo.synchronized { if (memoryStore.contains(blockId)) { diskData.dispose() Some(memoryStore.getBytes(blockId).get) } else { val allocator = level.memoryMode match { case MemoryMode.ON_HEAP => ByteBuffer.allocate _ case MemoryMode.OFF_HEAP => Platform.allocateDirectBuffer _ } val putSucceeded = memoryStore.putBytes(blockId, diskData.size, level.memoryMode, () => { // https://issues.apache.org/jira/browse/SPARK-6076 // If the file size is bigger than the free memory, OOM will happen. So if we // cannot put it into MemoryStore, copyForMemory should not be created. That's why // this action is put into a `() => ChunkedByteBuffer` and created lazily. diskData.toChunkedByteBuffer(allocator) }) if (putSucceeded) { diskData.dispose() Some(memoryStore.getBytes(blockId).get) } else { None } } } } else { None } } /** * Attempts to cache spilled values read from disk into the MemoryStore in order to speed up * subsequent reads. This method requires the caller to hold a read lock on the block. * * @return a copy of the iterator. The original iterator passed this method should no longer * be used after this method returns. */ private def maybeCacheDiskValuesInMemory[T]( blockInfo: BlockInfo, blockId: BlockId, level: StorageLevel, diskIterator: Iterator[T]): Iterator[T] = { require(level.deserialized) val classTag = blockInfo.classTag.asInstanceOf[ClassTag[T]] if (level.useMemory) { // Synchronize on blockInfo to guard against a race condition where two readers both try to // put values read from disk into the MemoryStore. blockInfo.synchronized { if (memoryStore.contains(blockId)) { // Note: if we had a means to discard the disk iterator, we would do that here. memoryStore.getValues(blockId).get } else { memoryStore.putIteratorAsValues(blockId, diskIterator, level.memoryMode, classTag) match { case Left(iter) => // The memory store put() failed, so it returned the iterator back to us: iter case Right(_) => // The put() succeeded, so we can read the values back: memoryStore.getValues(blockId).get } } }.asInstanceOf[Iterator[T]] } else { diskIterator } } /** * Get peer block managers in the system. */ private[storage] def getPeers(forceFetch: Boolean): Seq[BlockManagerId] = { peerFetchLock.synchronized { val cachedPeersTtl = conf.get(config.STORAGE_CACHED_PEERS_TTL) // milliseconds val diff = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - lastPeerFetchTimeNs) val timeout = diff > cachedPeersTtl if (cachedPeers == null || forceFetch || timeout) { cachedPeers = master.getPeers(blockManagerId).sortBy(_.hashCode) lastPeerFetchTimeNs = System.nanoTime() logDebug(\"Fetched peers from master: \" + cachedPeers.mkString(\"[\", \",\", \"]\")) } if (cachedPeers.isEmpty && conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined) { Seq(FallbackStorage.FALLBACK_BLOCK_MANAGER_ID) } else { cachedPeers } } } /** * Replicates a block to peer block managers based on existingReplicas and maxReplicas * * @param blockId blockId being replicate * @param existingReplicas existing block managers that have a replica * @param maxReplicas maximum replicas needed * @param maxReplicationFailures number of replication failures to tolerate before * giving up. * @return whether block was successfully replicated or not */ def replicateBlock( blockId: BlockId, existingReplicas: Set[BlockManagerId], maxReplicas: Int, maxReplicationFailures: Option[Int] = None): Boolean = { logInfo(s\"Using $blockManagerId to pro-actively replicate $blockId\") blockInfoManager.lockForReading(blockId).forall { info => val data = doGetLocalBytes(blockId, info) val storageLevel = StorageLevel( useDisk = info.level.useDisk, useMemory = info.level.useMemory, useOffHeap = info.level.useOffHeap, deserialized = info.level.deserialized, replication = maxReplicas) // we know we are called as a result of an executor removal or because the current executor // is getting decommissioned. so we refresh peer cache before trying replication, we won't // try to replicate to a missing executor/another decommissioning executor getPeers(forceFetch = true) try { replicate( blockId, data, storageLevel, info.classTag, existingReplicas, maxReplicationFailures) } finally { logDebug(s\"Releasing lock for $blockId\") releaseLockAndDispose(blockId, data) } } } /** * Replicate block to another node. Note that this is a blocking call that returns after * the block has been replicated. */ private def replicate( blockId: BlockId, data: BlockData, level: StorageLevel, classTag: ClassTag[_], existingReplicas: Set[BlockManagerId] = Set.empty, maxReplicationFailures: Option[Int] = None): Boolean = { val maxReplicationFailureCount = maxReplicationFailures.getOrElse( conf.get(config.STORAGE_MAX_REPLICATION_FAILURE)) val tLevel = StorageLevel( useDisk = level.useDisk, useMemory = level.useMemory, useOffHeap = level.useOffHeap, deserialized = level.deserialized, replication = 1) val numPeersToReplicateTo = level.replication - 1 val startTime = System.nanoTime val peersReplicatedTo = mutable.HashSet.empty ++ existingReplicas val peersFailedToReplicateTo = mutable.HashSet.empty[BlockManagerId] var numFailures = 0 val initialPeers = getPeers(false).filterNot(existingReplicas.contains) var peersForReplication = blockReplicationPolicy.prioritize( blockManagerId, initialPeers, peersReplicatedTo, blockId, numPeersToReplicateTo) while(numFailures <= maxReplicationFailureCount && !peersForReplication.isEmpty && peersReplicatedTo.size < numPeersToReplicateTo) { val peer = peersForReplication.head try { val onePeerStartTime = System.nanoTime logTrace(s\"Trying to replicate $blockId of ${data.size} bytes to $peer\") // This thread keeps a lock on the block, so we do not want the netty thread to unlock // block when it finishes sending the message. val buffer = new BlockManagerManagedBuffer(blockInfoManager, blockId, data, false, unlockOnDeallocate = false) blockTransferService.uploadBlockSync( peer.host, peer.port, peer.executorId, blockId, buffer, tLevel, classTag) logTrace(s\"Replicated $blockId of ${data.size} bytes to $peer\" + s\" in ${(System.nanoTime - onePeerStartTime).toDouble / 1e6} ms\") peersForReplication = peersForReplication.tail peersReplicatedTo += peer } catch { // Rethrow interrupt exception case e: InterruptedException => throw e // Everything else we may retry case NonFatal(e) => logWarning(s\"Failed to replicate $blockId to $peer, failure #$numFailures\", e) peersFailedToReplicateTo += peer // we have a failed replication, so we get the list of peers again // we don't want peers we have already replicated to and the ones that // have failed previously val filteredPeers = getPeers(true).filter { p => !peersFailedToReplicateTo.contains(p) && !peersReplicatedTo.contains(p) } numFailures += 1 peersForReplication = blockReplicationPolicy.prioritize( blockManagerId, filteredPeers, peersReplicatedTo, blockId, numPeersToReplicateTo - peersReplicatedTo.size) } } logDebug(s\"Replicating $blockId of ${data.size} bytes to \" + s\"${peersReplicatedTo.size} peer(s) took ${(System.nanoTime - startTime) / 1e6} ms\") if (peersReplicatedTo.size < numPeersToReplicateTo) { logWarning(s\"Block $blockId replicated to only \" + s\"${peersReplicatedTo.size} peer(s) instead of $numPeersToReplicateTo peers\") return false } logDebug(s\"block $blockId replicated to ${peersReplicatedTo.mkString(\", \")}\") return true } /** * Read a block consisting of a single object. */ def getSingle[T: ClassTag](blockId: BlockId): Option[T] = { get[T](blockId).map(_.data.next().asInstanceOf[T]) } /** * Write a block consisting of a single object. * * @return true if the block was stored or false if the block was already stored or an * error occurred. */ def putSingle[T: ClassTag]( blockId: BlockId, value: T, level: StorageLevel, tellMaster: Boolean = true): Boolean = { putIterator(blockId, Iterator(value), level, tellMaster) } /** * Drop a block from memory, possibly putting it on disk if applicable. Called when the memory * store reaches its limit and needs to free up space. * * If `data` is not put on disk, it won't be created. * * The caller of this method must hold a write lock on the block before calling this method. * This method does not release the write lock. * * @return the block's new effective StorageLevel. */ private[storage] override def dropFromMemory[T: ClassTag]( blockId: BlockId, data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel = { logInfo(s\"Dropping block $blockId from memory\") val info = blockInfoManager.assertBlockIsLockedForWriting(blockId) var blockIsUpdated = false val level = info.level // Drop to disk, if storage level requires if (level.useDisk && !diskStore.contains(blockId)) { logInfo(s\"Writing block $blockId to disk\") data() match { case Left(elements) => diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream( blockId, out, elements.iterator)(info.classTag.asInstanceOf[ClassTag[T]]) } case Right(bytes) => diskStore.putBytes(blockId, bytes) } blockIsUpdated = true } // Actually drop from memory store val droppedMemorySize = if (memoryStore.contains(blockId)) memoryStore.getSize(blockId) else 0L val blockIsRemoved = memoryStore.remove(blockId) if (blockIsRemoved) { blockIsUpdated = true } else { logWarning(s\"Block $blockId could not be dropped from memory as it does not exist\") } val status = getCurrentBlockStatus(blockId, info) if (info.tellMaster) { reportBlockStatus(blockId, status, droppedMemorySize) } if (blockIsUpdated) { addUpdatedBlockStatusToTaskMetrics(blockId, status) } status.storageLevel } /** * Remove all blocks belonging to the given RDD. * * @return The number of blocks removed. */ def removeRdd(rddId: Int): Int = { // TODO: Avoid a linear scan by creating another mapping of RDD.id to blocks. logInfo(s\"Removing RDD $rddId\") val blocksToRemove = blockInfoManager.entries.flatMap(_._1.asRDDId).filter(_.rddId == rddId) blocksToRemove.foreach { blockId => removeBlock(blockId, tellMaster = false) } blocksToRemove.size } def decommissionBlockManager(): Unit = storageEndpoint.ask(DecommissionBlockManager) private[spark] def decommissionSelf(): Unit = synchronized { decommissioner match { case None => logInfo(\"Starting block manager decommissioning process...\") decommissioner = Some(new BlockManagerDecommissioner(conf, this)) decommissioner.foreach(_.start()) case Some(_) => logDebug(\"Block manager already in decommissioning state\") } } /** * Returns the last migration time and a boolean denoting if all the blocks have been migrated. * If there are any tasks running since that time the boolean may be incorrect. */ private[spark] def lastMigrationInfo(): (Long, Boolean) = { decommissioner.map(_.lastMigrationInfo()).getOrElse((0, false)) } private[storage] def getMigratableRDDBlocks(): Seq[ReplicateBlock] = master.getReplicateInfoForRDDBlocks(blockManagerId) /** * Remove all blocks belonging to the given broadcast. */ def removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int = { logDebug(s\"Removing broadcast $broadcastId\") val blocksToRemove = blockInfoManager.entries.map(_._1).collect { case bid @ BroadcastBlockId(`broadcastId`, _) => bid } blocksToRemove.foreach { blockId => removeBlock(blockId, tellMaster) } blocksToRemove.size } /** * Remove a block from both memory and disk. */ def removeBlock(blockId: BlockId, tellMaster: Boolean = true): Unit = { logDebug(s\"Removing block $blockId\") blockInfoManager.lockForWriting(blockId) match { case None => // The block has already been removed; do nothing. logWarning(s\"Asked to remove block $blockId, which does not exist\") case Some(info) => removeBlockInternal(blockId, tellMaster = tellMaster && info.tellMaster) addUpdatedBlockStatusToTaskMetrics(blockId, BlockStatus.empty) } } /** * Internal version of [[removeBlock()]] which assumes that the caller already holds a write * lock on the block. */ private def removeBlockInternal(blockId: BlockId, tellMaster: Boolean): Unit = { val blockStatus = if (tellMaster) { val blockInfo = blockInfoManager.assertBlockIsLockedForWriting(blockId) Some(getCurrentBlockStatus(blockId, blockInfo)) } else None // Removals are idempotent in disk store and memory store. At worst, we get a warning. val removedFromMemory = memoryStore.remove(blockId) val removedFromDisk = diskStore.remove(blockId) if (!removedFromMemory && !removedFromDisk) { logWarning(s\"Block $blockId could not be removed as it was not found on disk or in memory\") } blockInfoManager.removeBlock(blockId) if (tellMaster) { // Only update storage level from the captured block status before deleting, so that // memory size and disk size are being kept for calculating delta. reportBlockStatus(blockId, blockStatus.get.copy(storageLevel = StorageLevel.NONE)) } } private def addUpdatedBlockStatusToTaskMetrics(blockId: BlockId, status: BlockStatus): Unit = { if (conf.get(config.TASK_METRICS_TRACK_UPDATED_BLOCK_STATUSES)) { Option(TaskContext.get()).foreach { c => c.taskMetrics().incUpdatedBlockStatuses(blockId -> status) } } } def releaseLockAndDispose( blockId: BlockId, data: BlockData, taskContext: Option[TaskContext] = None): Unit = { releaseLock(blockId, taskContext) data.dispose() } def stop(): Unit = { decommissioner.foreach(_.stop()) blockTransferService.close() if (blockStoreClient ne blockTransferService) { // Closing should be idempotent, but maybe not for the NioBlockTransferService. blockStoreClient.close() } remoteBlockTempFileManager.stop() diskBlockManager.stop() rpcEnv.stop(storageEndpoint) blockInfoManager.clear() memoryStore.clear() futureExecutionContext.shutdownNow() logInfo(\"BlockManager stopped\") } } private[spark] object BlockManager { private val ID_GENERATOR = new IdGenerator def blockIdsToLocations( blockIds: Array[BlockId], env: SparkEnv, blockManagerMaster: BlockManagerMaster = null): Map[BlockId, Seq[String]] = { // blockManagerMaster != null is used in tests assert(env != null || blockManagerMaster != null) val blockLocations: Seq[Seq[BlockManagerId]] = if (blockManagerMaster == null) { env.blockManager.getLocationBlockIds(blockIds) } else { blockManagerMaster.getLocations(blockIds) } val blockManagers = new HashMap[BlockId, Seq[String]] for (i <- 0 until blockIds.length) { blockManagers(blockIds(i)) = blockLocations(i).map { loc => ExecutorCacheTaskLocation(loc.host, loc.executorId).toString } } blockManagers.toMap } private class ShuffleMetricsSource( override val sourceName: String, metricSet: MetricSet) extends Source { override val metricRegistry = new MetricRegistry metricRegistry.registerAll(metricSet) } class RemoteBlockDownloadFileManager( blockManager: BlockManager, encryptionKey: Option[Array[Byte]]) extends DownloadFileManager with Logging { private class ReferenceWithCleanup( file: DownloadFile, referenceQueue: JReferenceQueue[DownloadFile] ) extends WeakReference[DownloadFile](file, referenceQueue) { // we cannot use `file.delete()` here otherwise it won't be garbage-collected val filePath = file.path() def cleanUp(): Unit = { logDebug(s\"Clean up file $filePath\") if (!new File(filePath).delete()) { logDebug(s\"Fail to delete file $filePath\") } } } private val referenceQueue = new JReferenceQueue[DownloadFile] private val referenceBuffer = Collections.newSetFromMap[ReferenceWithCleanup]( new ConcurrentHashMap) private val POLL_TIMEOUT = 1000 @volatile private var stopped = false private val cleaningThread = new Thread() { override def run(): Unit = { keepCleaning() } } cleaningThread.setDaemon(true) cleaningThread.setName(\"RemoteBlock-temp-file-clean-thread\") cleaningThread.start() override def createTempFile(transportConf: TransportConf): DownloadFile = { val file = blockManager.diskBlockManager.createTempLocalBlock()._2 encryptionKey match { case Some(key) => // encryption is enabled, so when we read the decrypted data off the network, we need to // encrypt it when writing to disk. Note that the data may have been encrypted when it // was cached on disk on the remote side, but it was already decrypted by now (see // EncryptedBlockData). new EncryptedDownloadFile(file, key) case None => new SimpleDownloadFile(file, transportConf) } } override def registerTempFileToClean(file: DownloadFile): Boolean = { referenceBuffer.add(new ReferenceWithCleanup(file, referenceQueue)) } def stop(): Unit = { stopped = true cleaningThread.interrupt() cleaningThread.join() } private def keepCleaning(): Unit = { while (!stopped) { try { Option(referenceQueue.remove(POLL_TIMEOUT)) .map(_.asInstanceOf[ReferenceWithCleanup]) .foreach { ref => referenceBuffer.remove(ref) ref.cleanUp() } } catch { case _: InterruptedException => // no-op case NonFatal(e) => logError(\"Error in cleaning thread\", e) } } } } /** * A DownloadFile that encrypts data when it is written, and decrypts when it's read. */ private class EncryptedDownloadFile( file: File, key: Array[Byte]) extends DownloadFile { private val env = SparkEnv.get override def delete(): Boolean = file.delete() override def openForWriting(): DownloadFileWritableChannel = { new EncryptedDownloadWritableChannel() } override def path(): String = file.getAbsolutePath private class EncryptedDownloadWritableChannel extends DownloadFileWritableChannel { private val countingOutput: CountingWritableChannel = new CountingWritableChannel( Channels.newChannel(env.serializerManager.wrapForEncryption(new FileOutputStream(file)))) override def closeAndRead(): ManagedBuffer = { countingOutput.close() val size = countingOutput.getCount new EncryptedManagedBuffer(new EncryptedBlockData(file, size, env.conf, key)) } override def write(src: ByteBuffer): Int = countingOutput.write(src) override def isOpen: Boolean = countingOutput.isOpen() override def close(): Unit = countingOutput.close() } } }",
          "## CLASS: org/apache/spark/util/io/ChunkedByteBufferOutputStream# (implementation)\n*/ private[spark] class ChunkedByteBufferOutputStream( chunkSize: Int, allocator: Int => ByteBuffer) extends OutputStream { private[this] var toChunkedByteBufferWasCalled = false private val chunks = new ArrayBuffer[ByteBuffer] /** Index of the last chunk. Starting with -1 when the chunks array is empty. */ private[this] var lastChunkIndex = -1 /** * Next position to write in the last chunk. * * If this equals chunkSize, it means for next write we need to allocate a new chunk. * This can also never be 0. */ private[this] var position = chunkSize private[this] var _size = 0L private[this] var closed: Boolean = false def size: Long = _size override def close(): Unit = { if (!closed) { super.close() closed = true } } override def write(b: Int): Unit = { require(!closed, \"cannot write to a closed ChunkedByteBufferOutputStream\") allocateNewChunkIfNeeded() chunks(lastChunkIndex).put(b.toByte) position += 1 _size += 1 } override def write(bytes: Array[Byte], off: Int, len: Int): Unit = { require(!closed, \"cannot write to a closed ChunkedByteBufferOutputStream\") var written = 0 while (written < len) { allocateNewChunkIfNeeded() val thisBatch = math.min(chunkSize - position, len - written) chunks(lastChunkIndex).put(bytes, written + off, thisBatch) written += thisBatch position += thisBatch } _size += len } @inline private def allocateNewChunkIfNeeded(): Unit = { if (position == chunkSize) { chunks += allocator(chunkSize) lastChunkIndex += 1 position = 0 } } def toChunkedByteBuffer: ChunkedByteBuffer = { require(closed, \"cannot call toChunkedByteBuffer() unless close() has been called\") require(!toChunkedByteBufferWasCalled, \"toChunkedByteBuffer() can only be called once\") toChunkedByteBufferWasCalled = true if (lastChunkIndex == -1) { new ChunkedByteBuffer(Array.empty[ByteBuffer]) } else { // Copy the first n-1 chunks to the output, and then create an array that fits the last chunk. // An alternative would have been returning an array of ByteBuffers, with the last buffer // bounded to only the last chunk's position. However, given our use case in Spark (to put // the chunks in block manager), only limiting the view bound of the buffer would still // require the block manager to store the whole chunk. val ret = new Array[ByteBuffer](chunks.size) for (i <- 0 until chunks.size - 1) { ret(i) = chunks(i) ret(i).flip() } if (position == chunkSize) { ret(lastChunkIndex) = chunks(lastChunkIndex) ret(lastChunkIndex).flip() } else { ret(lastChunkIndex) = allocator(position) chunks(lastChunkIndex).flip() ret(lastChunkIndex).put(chunks(lastChunkIndex)) ret(lastChunkIndex).flip() StorageUtils.dispose(chunks(lastChunkIndex)) } new ChunkedByteBuffer(ret) } } }\n"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### Memory Allocation in This Project (from code analysis)\n\nTimestamp: 2026-01-03 11:15 (local)\n\nThis document summarizes how memory allocation is managed in the repository, based strictly on the visible code paths that wire memory management and storage components.\n\n---\n\n#### Responsibilities\n\n- MemoryManager initializes and accounts for four memory pools: on-heap and off-heap pools for both storage (StorageMemoryPool) and execution (ExecutionMemoryPool). Pool sizes are derived from SparkConf and core counts passed to the constructor. It exposes abstract getters (e.g., maxOnHeapStorageMemory) for reporting limits that can vary by implementation.\n- BlockManager wires the actual storage backends by creating MemoryStore and DiskStore, then passes MemoryStore into MemoryManager via setMemoryStore so evictions/spills can be coordinated. It also creates DiskBlockManager and manages whether shuffle files are cleaned up.\n- JavaPairRDD (as an example RDD facade) triggers use of storage memory via cache()/persist(newLevel) and frees memory/disk via unpersist(blocking?).\n\n---\n\n#### Data flow\n\n1) BlockManager constructor receives MemoryManager and creates: DiskBlockManager (with deleteFilesOnStop chosen by externalShuffleServiceEnabled and isDriver), BlockInfoManager, MemoryStore(conf, blockInfoManager, serializerManager, memoryManager, this), and DiskStore.\n2) BlockManager calls memoryManager.setMemoryStore(memoryStore), linking the manager to the in-memory store used for caching and bookkeeping.\n3) MemoryManager constructor allocates pools:\n   - Creates onHeapStorageMemoryPool, offHeapStorageMemoryPool, onHeapExecutionMemoryPool, offHeapExecutionMemoryPool (all @GuardedBy(\"this\")).\n   - onHeapStorageMemoryPool.incrementPoolSize(onHeapStorageMemory) and onHeapExecutionMemoryPool.incrementPoolSize(onHeapExecutionMemory) using constructor arguments.\n   - Reads maxOffHeapMemory = conf.get(MEMORY_OFFHEAP_SIZE).\n   - Computes offHeapStorageMemory = (maxOffHeapMemory * conf.get(MEMORY_STORAGE_FRACTION)).toLong.\n   - offHeapExecutionMemoryPool.incrementPoolSize(maxOffHeapMemory - offHeapStorageMemory) and offHeapStorageMemoryPool.incrementPoolSize(offHeapStorageMemory).\n4) RDD-facing APIs (example: JavaPairRDD.cache()/persist) delegate to underlying RDD to mark blocks for storage; later unpersist(blocking: Boolean) removes blocks from memory and disk.\n\n---\n\n#### Key methods and logic (visible in snippets)\n\n- MemoryManager:\n  - Constructor sets up four pools and sizes them via incrementPoolSize(...). Uses configuration keys MEMORY_OFFHEAP_SIZE and MEMORY_STORAGE_FRACTION to split off-heap between storage and execution. Requires onHeapExecutionMemory > 0.\n  - maxOnHeapStorageMemory and maxOffHeapStorageMemory are abstract; implementations report current storage capacity based on the chosen policy.\n- BlockManager:\n  - Computes externalShuffleServiceEnabled from the presence of externalBlockStoreClient.\n  - Chooses deleteFilesOnStop = !externalShuffleServiceEnabled || isDriver for DiskBlockManager.\n  - Constructs MemoryStore and DiskStore; calls memoryManager.setMemoryStore(memoryStore). Also holds BlockInfoManager for metadata and uses subDirsPerLocalDir and other confs.\n- JavaPairRDD:\n  - cache(): returns new JavaPairRDD(rdd.cache()).\n  - persist(newLevel): returns new JavaPairRDD(rdd.persist(newLevel)).\n  - unpersist(blocking?): delegates to rdd.unpersist(blocking) and wrapRDD.\n\n---\n\n#### Summary\n\nThe project partitions memory into execution vs storage and on-heap vs off-heap pools inside MemoryManager, sizes them from SparkConf, and connects them to caching infrastructure through BlockManager  MemoryStore. User APIs like cache/persist/unpersist trigger allocation and release via the underlying RDD and BlockManager/MemoryStore mechanisms.\n",
          "prompt_time": 18.844,
          "answer_time": 112,
          "context_tokens": 30137,
          "used_context": [
            "## CLASS: org/apache/spark/storage/BlockManager# (implementation)\n* Note that [[initialize()]] must be called before the BlockManager is usable. */ private[spark] class BlockManager( val executorId: String, rpcEnv: RpcEnv, val master: BlockManagerMaster, val serializerManager: SerializerManager, val conf: SparkConf, memoryManager: MemoryManager, mapOutputTracker: MapOutputTracker, shuffleManager: ShuffleManager, val blockTransferService: BlockTransferService, securityManager: SecurityManager, externalBlockStoreClient: Option[ExternalBlockStoreClient]) extends BlockDataManager with BlockEvictionHandler with Logging { // same as `conf.get(config.SHUFFLE_SERVICE_ENABLED)` private[spark] val externalShuffleServiceEnabled: Boolean = externalBlockStoreClient.isDefined private val isDriver = executorId == SparkContext.DRIVER_IDENTIFIER private val remoteReadNioBufferConversion = conf.get(Network.NETWORK_REMOTE_READ_NIO_BUFFER_CONVERSION) private[spark] val subDirsPerLocalDir = conf.get(config.DISKSTORE_SUB_DIRECTORIES) val diskBlockManager = { // Only perform cleanup if an external service is not serving our shuffle files. val deleteFilesOnStop = !externalShuffleServiceEnabled || isDriver new DiskBlockManager(conf, deleteFilesOnStop = deleteFilesOnStop, isDriver = isDriver) } // Visible for testing private[storage] val blockInfoManager = new BlockInfoManager private val futureExecutionContext = ExecutionContext.fromExecutorService( ThreadUtils.newDaemonCachedThreadPool(\"block-manager-future\", 128)) // Actual storage of where blocks are kept private[spark] val memoryStore = new MemoryStore(conf, blockInfoManager, serializerManager, memoryManager, this) private[spark] val diskStore = new DiskStore(conf, diskBlockManager, securityManager) memoryManager.setMemoryStore(memoryStore) // Note: depending on the memory manager, `maxMemory` may actually vary over time. // However, since we use this only for reporting and logging, what we actually want here is // the absolute maximum value that `maxMemory` can ever possibly reach. We may need // to revisit whether reporting this value as the \"max\" is intuitive to the user. private val maxOnHeapMemory = memoryManager.maxOnHeapStorageMemory private val maxOffHeapMemory = memoryManager.maxOffHeapStorageMemory private[spark] val externalShuffleServicePort = StorageUtils.externalShuffleServicePort(conf) var blockManagerId: BlockManagerId = _ // Address of the server that serves this executor's shuffle files. This is either an external // service, or just our own Executor's BlockManager. private[spark] var shuffleServerId: BlockManagerId = _ // Client to read other executors' blocks. This is either an external service, or just the // standard BlockTransferService to directly connect to other Executors. private[spark] val blockStoreClient = externalBlockStoreClient.getOrElse(blockTransferService) // Max number of failures before this block manager refreshes the block locations from the driver private val maxFailuresBeforeLocationRefresh = conf.get(config.BLOCK_FAILURES_BEFORE_LOCATION_REFRESH) private val storageEndpoint = rpcEnv.setupEndpoint( \"BlockManagerEndpoint\" + BlockManager.ID_GENERATOR.next, new BlockManagerStorageEndpoint(rpcEnv, this, mapOutputTracker)) // Pending re-registration action being executed asynchronously or null if none is pending. // Accesses should synchronize on asyncReregisterLock. private var asyncReregisterTask: Future[Unit] = null private val asyncReregisterLock = new Object // Field related to peer block managers that are necessary for block replication @volatile private var cachedPeers: Seq[BlockManagerId] = _ private val peerFetchLock = new Object private var lastPeerFetchTimeNs = 0L private var blockReplicationPolicy: BlockReplicationPolicy = _ // visible for test // This is volatile since if it's defined we should not accept remote blocks. @volatile private[spark] var decommissioner: Option[BlockManagerDecommissioner] = None // A DownloadFileManager used to track all the files of remote blocks which are above the // specified memory threshold. Files will be deleted automatically based on weak reference. // Exposed for test private[storage] val remoteBlockTempFileManager = new BlockManager.RemoteBlockDownloadFileManager( this, securityManager.getIOEncryptionKey()) private val maxRemoteBlockToMem = conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM) var hostLocalDirManager: Option[HostLocalDirManager] = None @inline final private def isDecommissioning() = { decommissioner.isDefined } @inline final private def checkShouldStore(blockId: BlockId) = { // Don't reject broadcast blocks since they may be stored during task exec and // don't need to be migrated. if (isDecommissioning() && !blockId.isBroadcast) { throw SparkCoreErrors.cannotSaveBlockOnDecommissionedExecutorError(blockId) } } // This is a lazy val so someone can migrating RDDs even if they don't have a MigratableResolver // for shuffles. Used in BlockManagerDecommissioner & block puts. private[storage] lazy val migratableResolver: MigratableResolver = { shuffleManager.shuffleBlockResolver.asInstanceOf[MigratableResolver] } override def getLocalDiskDirs: Array[String] = diskBlockManager.localDirsString /** * Diagnose the possible cause of the shuffle data corruption by verifying the shuffle checksums * * @param blockId The blockId of the corrupted shuffle block * @param checksumByReader The checksum value of the corrupted block * @param algorithm The cheksum algorithm that is used when calculating the checksum value */ override def diagnoseShuffleBlockCorruption( blockId: BlockId, checksumByReader: Long, algorithm: String): Cause = { assert(blockId.isInstanceOf[ShuffleBlockId], s\"Corruption diagnosis only supports shuffle block yet, but got $blockId\") val shuffleBlock = blockId.asInstanceOf[ShuffleBlockId] val resolver = shuffleManager.shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver] val checksumFile = resolver.getChecksumFile(shuffleBlock.shuffleId, shuffleBlock.mapId, algorithm) val reduceId = shuffleBlock.reduceId ShuffleChecksumHelper.diagnoseCorruption( algorithm, checksumFile, reduceId, resolver.getBlockData(shuffleBlock), checksumByReader) } /** * Abstraction for storing blocks from bytes, whether they start in memory or on disk. * * @param blockSize the decrypted size of the block */ private[spark] abstract class BlockStoreUpdater[T]( blockSize: Long, blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], tellMaster: Boolean, keepReadLock: Boolean) { /** * Reads the block content into the memory. If the update of the block store is based on a * temporary file this could lead to loading the whole file into a ChunkedByteBuffer. */ protected def readToByteBuffer(): ChunkedByteBuffer protected def blockData(): BlockData protected def saveToDiskStore(): Unit private def saveDeserializedValuesToMemoryStore(inputStream: InputStream): Boolean = { try { val values = serializerManager.dataDeserializeStream(blockId, inputStream)(classTag) memoryStore.putIteratorAsValues(blockId, values, level.memoryMode, classTag) match { case Right(_) => true case Left(iter) => // If putting deserialized values in memory failed, we will put the bytes directly // to disk, so we don't need this iterator and can close it to free resources // earlier. iter.close() false } } catch { case ex: KryoException if ex.getCause.isInstanceOf[IOException] => // We need to have detailed log message to catch environmental problems easily. // Further details: https://issues.apache.org/jira/browse/SPARK-37710 processKryoException(ex, blockId) throw ex } finally { IOUtils.closeQuietly(inputStream) } } private def saveSerializedValuesToMemoryStore(bytes: ChunkedByteBuffer): Boolean = { val memoryMode = level.memoryMode memoryStore.putBytes(blockId, blockSize, memoryMode, () => { if (memoryMode == MemoryMode.OFF_HEAP && bytes.chunks.exists(!_.isDirect)) { bytes.copy(Platform.allocateDirectBuffer) } else { bytes } }) } /** * Put the given data according to the given level in one of the block stores, replicating * the values if necessary. * * If the block already exists, this method will not overwrite it. * * If keepReadLock is true, this method will hold the read lock when it returns (even if the * block already exists). If false, this method will hold no locks when it returns. * * @return true if the block was already present or if the put succeeded, false otherwise. */ def save(): Boolean = { doPut(blockId, level, classTag, tellMaster, keepReadLock) { info => val startTimeNs = System.nanoTime() // Since we're storing bytes, initiate the replication before storing them locally. // This is faster as data is already serialized and ready to send. val replicationFuture = if (level.replication > 1) { Future { // This is a blocking action and should run in futureExecutionContext which is a cached // thread pool. replicate(blockId, blockData(), level, classTag) }(futureExecutionContext) } else { null } if (level.useMemory) { // Put it in memory first, even if it also has useDisk set to true; // We will drop it to disk later if the memory store can't hold it. val putSucceeded = if (level.deserialized) { saveDeserializedValuesToMemoryStore(blockData().toInputStream()) } else { saveSerializedValuesToMemoryStore(readToByteBuffer()) } if (!putSucceeded && level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") saveToDiskStore() } } else if (level.useDisk) { saveToDiskStore() } val putBlockStatus = getCurrentBlockStatus(blockId, info) val blockWasSuccessfullyStored = putBlockStatus.storageLevel.isValid if (blockWasSuccessfullyStored) { // Now that the block is in either the memory or disk store, // tell the master about it. info.size = blockSize if (tellMaster && info.tellMaster) { reportBlockStatus(blockId, putBlockStatus) } addUpdatedBlockStatusToTaskMetrics(blockId, putBlockStatus) } logDebug(s\"Put block ${blockId} locally took ${Utils.getUsedTimeNs(startTimeNs)}\") if (level.replication > 1) { // Wait for asynchronous replication to finish try { ThreadUtils.awaitReady(replicationFuture, Duration.Inf) } catch { case NonFatal(t) => throw SparkCoreErrors.waitingForReplicationToFinishError(t) } } if (blockWasSuccessfullyStored) { None } else { Some(blockSize) } }.isEmpty } } /** * Helper for storing a block from bytes already in memory. * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. */ private case class ByteBufferBlockStoreUpdater[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], bytes: ChunkedByteBuffer, tellMaster: Boolean = true, keepReadLock: Boolean = false) extends BlockStoreUpdater[T](bytes.size, blockId, level, classTag, tellMaster, keepReadLock) { override def readToByteBuffer(): ChunkedByteBuffer = bytes /** * The ByteBufferBlockData wrapper is not disposed of to avoid releasing buffers that are * owned by the caller. */ override def blockData(): BlockData = new ByteBufferBlockData(bytes, false) override def saveToDiskStore(): Unit = diskStore.putBytes(blockId, bytes) } /** * Helper for storing a block based from bytes already in a local temp file. */ private[spark] case class TempFileBasedBlockStoreUpdater[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], tmpFile: File, blockSize: Long, tellMaster: Boolean = true, keepReadLock: Boolean = false) extends BlockStoreUpdater[T](blockSize, blockId, level, classTag, tellMaster, keepReadLock) { override def readToByteBuffer(): ChunkedByteBuffer = { val allocator = level.memoryMode match { case MemoryMode.ON_HEAP => ByteBuffer.allocate _ case MemoryMode.OFF_HEAP => Platform.allocateDirectBuffer _ } blockData().toChunkedByteBuffer(allocator) } override def blockData(): BlockData = diskStore.getBytes(tmpFile, blockSize) override def saveToDiskStore(): Unit = diskStore.moveFileToBlock(tmpFile, blockSize, blockId) override def save(): Boolean = { val res = super.save() tmpFile.delete() res } } /** * Initializes the BlockManager with the given appId. This is not performed in the constructor as * the appId may not be known at BlockManager instantiation time (in particular for the driver, * where it is only learned after registration with the TaskScheduler). * * This method initializes the BlockTransferService and BlockStoreClient, registers with the * BlockManagerMaster, starts the BlockManagerWorker endpoint, and registers with a local shuffle * service if configured. */ def initialize(appId: String): Unit = { blockTransferService.init(this) externalBlockStoreClient.foreach { blockStoreClient => blockStoreClient.init(appId) } blockReplicationPolicy = { val priorityClass = conf.get(config.STORAGE_REPLICATION_POLICY) val clazz = Utils.classForName(priorityClass) val ret = clazz.getConstructor().newInstance().asInstanceOf[BlockReplicationPolicy] logInfo(s\"Using $priorityClass for block replication policy\") ret } val id = BlockManagerId(executorId, blockTransferService.hostName, blockTransferService.port, None) val idFromMaster = master.registerBlockManager( id, diskBlockManager.localDirsString, maxOnHeapMemory, maxOffHeapMemory, storageEndpoint) blockManagerId = if (idFromMaster != null) idFromMaster else id shuffleServerId = if (externalShuffleServiceEnabled) { logInfo(s\"external shuffle service port = $externalShuffleServicePort\") BlockManagerId(executorId, blockTransferService.hostName, externalShuffleServicePort) } else { blockManagerId } // Register Executors' configuration with the local shuffle service, if one should exist. if (externalShuffleServiceEnabled && !blockManagerId.isDriver) { registerWithExternalShuffleServer() } hostLocalDirManager = { if ((conf.get(config.SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED) && !conf.get(config.SHUFFLE_USE_OLD_FETCH_PROTOCOL)) || Utils.isPushBasedShuffleEnabled(conf, isDriver)) { Some(new HostLocalDirManager( futureExecutionContext, conf.get(config.STORAGE_LOCAL_DISK_BY_EXECUTORS_CACHE_SIZE), blockStoreClient)) } else { None } } logInfo(s\"Initialized BlockManager: $blockManagerId\") } def shuffleMetricsSource: Source = { import BlockManager._ if (externalShuffleServiceEnabled) { new ShuffleMetricsSource(\"ExternalShuffle\", blockStoreClient.shuffleMetrics()) } else { new ShuffleMetricsSource(\"NettyBlockTransfer\", blockStoreClient.shuffleMetrics()) } } private def registerWithExternalShuffleServer(): Unit = { logInfo(\"Registering executor with local external shuffle service.\") val shuffleManagerMeta = if (Utils.isPushBasedShuffleEnabled(conf, isDriver = isDriver, checkSerializer = false)) { s\"${shuffleManager.getClass.getName}:\" + s\"${diskBlockManager.getMergeDirectoryAndAttemptIDJsonString()}}}\" } else { shuffleManager.getClass.getName } val shuffleConfig = new ExecutorShuffleInfo( diskBlockManager.localDirsString, diskBlockManager.subDirsPerLocalDir, shuffleManagerMeta) val MAX_ATTEMPTS = conf.get(config.SHUFFLE_REGISTRATION_MAX_ATTEMPTS) val SLEEP_TIME_SECS = 5 for (i <- 1 to MAX_ATTEMPTS) { try { // Synchronous and will throw an exception if we cannot connect. blockStoreClient.asInstanceOf[ExternalBlockStoreClient].registerWithShuffleServer( shuffleServerId.host, shuffleServerId.port, shuffleServerId.executorId, shuffleConfig) return } catch { case e: Exception if i < MAX_ATTEMPTS => logError(s\"Failed to connect to external shuffle server, will retry ${MAX_ATTEMPTS - i}\" + s\" more times after waiting $SLEEP_TIME_SECS seconds...\", e) Thread.sleep(SLEEP_TIME_SECS * 1000L) case NonFatal(e) => throw SparkCoreErrors.unableToRegisterWithExternalShuffleServerError(e) } } } /** * Report all blocks to the BlockManager again. This may be necessary if we are dropped * by the BlockManager and come back or if we become capable of recovering blocks on disk after * an executor crash. * * This function deliberately fails silently if the master returns false (indicating that * the storage endpoint needs to re-register). The error condition will be detected again by the * next heart beat attempt or new block registration and another try to re-register all blocks * will be made then. */ private def reportAllBlocks(): Unit = { logInfo(s\"Reporting ${blockInfoManager.size} blocks to the master.\") for ((blockId, info) <- blockInfoManager.entries) { val status = getCurrentBlockStatus(blockId, info) if (info.tellMaster && !tryToReportBlockStatus(blockId, status)) { logError(s\"Failed to report $blockId to master; giving up.\") return } } } /** * Re-register with the master and report all blocks to it. This will be called by the heart beat * thread if our heartbeat to the block manager indicates that we were not registered. * * Note that this method must be called without any BlockInfo locks held. */ def reregister(): Unit = { // TODO: We might need to rate limit re-registering. logInfo(s\"BlockManager $blockManagerId re-registering with master\") master.registerBlockManager(blockManagerId, diskBlockManager.localDirsString, maxOnHeapMemory, maxOffHeapMemory, storageEndpoint) reportAllBlocks() } /** * Re-register with the master sometime soon. */ private def asyncReregister(): Unit = { asyncReregisterLock.synchronized { if (asyncReregisterTask == null) { asyncReregisterTask = Future[Unit] { // This is a blocking action and should run in futureExecutionContext which is a cached // thread pool reregister() asyncReregisterLock.synchronized { asyncReregisterTask = null } }(futureExecutionContext) } } } /** * For testing. Wait for any pending asynchronous re-registration; otherwise, do nothing. */ def waitForAsyncReregister(): Unit = { val task = asyncReregisterTask if (task != null) { try { ThreadUtils.awaitReady(task, Duration.Inf) } catch { case NonFatal(t) => throw SparkCoreErrors.waitingForAsyncReregistrationError(t) } } } override def getHostLocalShuffleData( blockId: BlockId, dirs: Array[String]): ManagedBuffer = { shuffleManager.shuffleBlockResolver.getBlockData(blockId, Some(dirs)) } /** * Interface to get local block data. Throws an exception if the block cannot be found or * cannot be read successfully. */ override def getLocalBlockData(blockId: BlockId): ManagedBuffer = { if (blockId.isShuffle) { logDebug(s\"Getting local shuffle block ${blockId}\") try { shuffleManager.shuffleBlockResolver.getBlockData(blockId) } catch { case e: IOException => if (conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined) { FallbackStorage.read(conf, blockId) } else { throw e } } } else { getLocalBytes(blockId) match { case Some(blockData) => new BlockManagerManagedBuffer(blockInfoManager, blockId, blockData, true) case None => // If this block manager receives a request for a block that it doesn't have then it's // likely that the master has outdated block statuses for this block. Therefore, we send // an RPC so that this block is marked as being unavailable from this block manager. reportBlockStatus(blockId, BlockStatus.empty) throw SparkCoreErrors.blockNotFoundError(blockId) } } } /** * Put the block locally, using the given storage level. * * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. */ override def putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean = { putBytes(blockId, new ChunkedByteBuffer(data.nioByteBuffer()), level)(classTag) } override def putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID = { checkShouldStore(blockId) if (blockId.isShuffle) { logDebug(s\"Putting shuffle block ${blockId}\") try { return migratableResolver.putShuffleBlockAsStream(blockId, serializerManager) } catch { case e: ClassCastException => throw SparkCoreErrors.unexpectedShuffleBlockWithUnsupportedResolverError(shuffleManager, blockId) } } logDebug(s\"Putting regular block ${blockId}\") // All other blocks val (_, tmpFile) = diskBlockManager.createTempLocalBlock() val channel = new CountingWritableChannel( Channels.newChannel(serializerManager.wrapForEncryption(new FileOutputStream(tmpFile)))) logTrace(s\"Streaming block $blockId to tmp file $tmpFile\") new StreamCallbackWithID { override def getID: String = blockId.name override def onData(streamId: String, buf: ByteBuffer): Unit = { while (buf.hasRemaining) { channel.write(buf) } } override def onComplete(streamId: String): Unit = { logTrace(s\"Done receiving block $blockId, now putting into local blockManager\") // Note this is all happening inside the netty thread as soon as it reads the end of the // stream. channel.close() val blockSize = channel.getCount val blockStored = TempFileBasedBlockStoreUpdater( blockId, level, classTag, tmpFile, blockSize).save() if (!blockStored) { throw SparkCoreErrors.failToStoreBlockOnBlockManagerError(blockManagerId, blockId) } } override def onFailure(streamId: String, cause: Throwable): Unit = { // the framework handles the connection itself, we just need to do local cleanup channel.close() tmpFile.delete() } } } /** * Get the local merged shuffle block data for the given block ID as multiple chunks. * A merged shuffle file is divided into multiple chunks according to the index file. * Instead of reading the entire file as a single block, we split it into smaller chunks * which will be memory efficient when performing certain operations. */ def getLocalMergedBlockData( blockId: ShuffleMergedBlockId, dirs: Array[String]): Seq[ManagedBuffer] = { shuffleManager.shuffleBlockResolver.getMergedBlockData(blockId, Some(dirs)) } /** * Get the local merged shuffle block meta data for the given block ID. */ def getLocalMergedBlockMeta( blockId: ShuffleMergedBlockId, dirs: Array[String]): MergedBlockMeta = { shuffleManager.shuffleBlockResolver.getMergedBlockMeta(blockId, Some(dirs)) } /** * Get the BlockStatus for the block identified by the given ID, if it exists. * NOTE: This is mainly for testing. */ def getStatus(blockId: BlockId): Option[BlockStatus] = { blockInfoManager.get(blockId).map { info => val memSize = if (memoryStore.contains(blockId)) memoryStore.getSize(blockId) else 0L val diskSize = if (diskStore.contains(blockId)) diskStore.getSize(blockId) else 0L BlockStatus(info.level, memSize = memSize, diskSize = diskSize) } } /** * Get the ids of existing blocks that match the given filter. Note that this will * query the blocks stored in the disk block manager (that the block manager * may not know of). */ def getMatchingBlockIds(filter: BlockId => Boolean): Seq[BlockId] = { // The `toArray` is necessary here in order to force the list to be materialized so that we // don't try to serialize a lazy iterator when responding to client requests. (blockInfoManager.entries.map(_._1) ++ diskBlockManager.getAllBlocks()) .filter(filter) .toArray .toSeq } /** * Tell the master about the current storage status of a block. This will send a block update * message reflecting the current status, *not* the desired storage level in its block info. * For example, a block with MEMORY_AND_DISK set might have fallen out to be only on disk. * * droppedMemorySize exists to account for when the block is dropped from memory to disk (so * it is still valid). This ensures that update in master will compensate for the increase in * memory on the storage endpoint. */ private[spark] def reportBlockStatus( blockId: BlockId, status: BlockStatus, droppedMemorySize: Long = 0L): Unit = { val needReregister = !tryToReportBlockStatus(blockId, status, droppedMemorySize) if (needReregister) { logInfo(s\"Got told to re-register updating block $blockId\") // Re-registering will report our new block for free. asyncReregister() } logDebug(s\"Told master about block $blockId\") } /** * Actually send a UpdateBlockInfo message. Returns the master's response, * which will be true if the block was successfully recorded and false if * the storage endpoint needs to re-register. */ private def tryToReportBlockStatus( blockId: BlockId, status: BlockStatus, droppedMemorySize: Long = 0L): Boolean = { val storageLevel = status.storageLevel val inMemSize = Math.max(status.memSize, droppedMemorySize) val onDiskSize = status.diskSize master.updateBlockInfo(blockManagerId, blockId, storageLevel, inMemSize, onDiskSize) } /** * Return the updated storage status of the block with the given ID. More specifically, if * the block is dropped from memory and possibly added to disk, return the new storage level * and the updated in-memory and on-disk sizes. */ private def getCurrentBlockStatus(blockId: BlockId, info: BlockInfo): BlockStatus = { info.synchronized { info.level match { case null => BlockStatus.empty case level => val inMem = level.useMemory && memoryStore.contains(blockId) val onDisk = level.useDisk && diskStore.contains(blockId) val deserialized = if (inMem) level.deserialized else false val replication = if (inMem || onDisk) level.replication else 1 val storageLevel = StorageLevel( useDisk = onDisk, useMemory = inMem, useOffHeap = level.useOffHeap, deserialized = deserialized, replication = replication) val memSize = if (inMem) memoryStore.getSize(blockId) else 0L val diskSize = if (onDisk) diskStore.getSize(blockId) else 0L BlockStatus(storageLevel, memSize, diskSize) } } } /** * Get locations of an array of blocks. */ private def getLocationBlockIds(blockIds: Array[BlockId]): Array[Seq[BlockManagerId]] = { val startTimeNs = System.nanoTime() val locations = master.getLocations(blockIds).toArray logDebug(s\"Got multiple block location in ${Utils.getUsedTimeNs(startTimeNs)}\") locations } /** * Cleanup code run in response to a failed local read. * Must be called while holding a read lock on the block. */ private def handleLocalReadFailure(blockId: BlockId): Nothing = { releaseLock(blockId) // Remove the missing block so that its unavailability is reported to the driver removeBlock(blockId) throw SparkCoreErrors.readLockedBlockNotFoundError(blockId) } /** * Get block from local block manager as an iterator of Java objects. */ def getLocalValues(blockId: BlockId): Option[BlockResult] = { logDebug(s\"Getting local block $blockId\") blockInfoManager.lockForReading(blockId) match { case None => logDebug(s\"Block $blockId was not found\") None case Some(info) => val level = info.level logDebug(s\"Level for block $blockId is $level\") val taskContext = Option(TaskContext.get()) if (level.useMemory && memoryStore.contains(blockId)) { val iter: Iterator[Any] = if (level.deserialized) { memoryStore.getValues(blockId).get } else { serializerManager.dataDeserializeStream( blockId, memoryStore.getBytes(blockId).get.toInputStream())(info.classTag) } // We need to capture the current taskId in case the iterator completion is triggered // from a different thread which does not have TaskContext set; see SPARK-18406 for // discussion. val ci = CompletionIterator[Any, Iterator[Any]](iter, { releaseLock(blockId, taskContext) }) Some(new BlockResult(ci, DataReadMethod.Memory, info.size)) } else if (level.useDisk && diskStore.contains(blockId)) { try { val diskData = diskStore.getBytes(blockId) val iterToReturn: Iterator[Any] = { if (level.deserialized) { val diskValues = serializerManager.dataDeserializeStream( blockId, diskData.toInputStream())(info.classTag) maybeCacheDiskValuesInMemory(info, blockId, level, diskValues) } else { val stream = maybeCacheDiskBytesInMemory(info, blockId, level, diskData) .map { _.toInputStream(dispose = false) } .getOrElse { diskData.toInputStream() } serializerManager.dataDeserializeStream(blockId, stream)(info.classTag) } } val ci = CompletionIterator[Any, Iterator[Any]](iterToReturn, { releaseLockAndDispose(blockId, diskData, taskContext) }) Some(new BlockResult(ci, DataReadMethod.Disk, info.size)) } catch { case ex: KryoException if ex.getCause.isInstanceOf[IOException] => // We need to have detailed log message to catch environmental problems easily. // Further details: https://issues.apache.org/jira/browse/SPARK-37710 processKryoException(ex, blockId) throw ex } } else { handleLocalReadFailure(blockId) } } } private def processKryoException(ex: KryoException, blockId: BlockId): Unit = { var message = \"%s. %s - blockId: %s\".format(ex.getMessage, blockManagerId.toString, blockId) val file = diskBlockManager.getFile(blockId) if (file.exists()) { message = \"%s - blockDiskPath: %s\".format(message, file.getAbsolutePath) } logInfo(message) } /** * Get block from the local block manager as serialized bytes. */ def getLocalBytes(blockId: BlockId): Option[BlockData] = { logDebug(s\"Getting local block $blockId as bytes\") assert(!blockId.isShuffle, s\"Unexpected ShuffleBlockId $blockId\") blockInfoManager.lockForReading(blockId).map { info => doGetLocalBytes(blockId, info) } } /** * Get block from the local block manager as serialized bytes. * * Must be called while holding a read lock on the block. * Releases the read lock upon exception; keeps the read lock upon successful return. */ private def doGetLocalBytes(blockId: BlockId, info: BlockInfo): BlockData = { val level = info.level logDebug(s\"Level for block $blockId is $level\") // In order, try to read the serialized bytes from memory, then from disk, then fall back to // serializing in-memory objects, and, finally, throw an exception if the block does not exist. if (level.deserialized) { // Try to avoid expensive serialization by reading a pre-serialized copy from disk: if (level.useDisk && diskStore.contains(blockId)) { // Note: we purposely do not try to put the block back into memory here. Since this branch // handles deserialized blocks, this block may only be cached in memory as objects, not // serialized bytes. Because the caller only requested bytes, it doesn't make sense to // cache the block's deserialized objects since that caching may not have a payoff. diskStore.getBytes(blockId) } else if (level.useMemory && memoryStore.contains(blockId)) { // The block was not found on disk, so serialize an in-memory copy: new ByteBufferBlockData(serializerManager.dataSerializeWithExplicitClassTag( blockId, memoryStore.getValues(blockId).get, info.classTag), true) } else { handleLocalReadFailure(blockId) } } else { // storage level is serialized if (level.useMemory && memoryStore.contains(blockId)) { new ByteBufferBlockData(memoryStore.getBytes(blockId).get, false) } else if (level.useDisk && diskStore.contains(blockId)) { val diskData = diskStore.getBytes(blockId) maybeCacheDiskBytesInMemory(info, blockId, level, diskData) .map(new ByteBufferBlockData(_, false)) .getOrElse(diskData) } else { handleLocalReadFailure(blockId) } } } /** * Get block from remote block managers. * * This does not acquire a lock on this block in this JVM. */ private[spark] def getRemoteValues[T: ClassTag](blockId: BlockId): Option[BlockResult] = { val ct = implicitly[ClassTag[T]] getRemoteBlock(blockId, (data: ManagedBuffer) => { val values = serializerManager.dataDeserializeStream(blockId, data.createInputStream())(ct) new BlockResult(values, DataReadMethod.Network, data.size) }) } /** * Get the remote block and transform it to the provided data type. * * If the block is persisted to the disk and stored at an executor running on the same host then * first it is tried to be accessed using the local directories of the other executor directly. * If the file is successfully identified then tried to be transformed by the provided * transformation function which expected to open the file. If there is any exception during this * transformation then block access falls back to fetching it from the remote executor via the * network. * * @param blockId identifies the block to get * @param bufferTransformer this transformer expected to open the file if the block is backed by a * file by this it is guaranteed the whole content can be loaded * @tparam T result type */ private[spark] def getRemoteBlock[T]( blockId: BlockId, bufferTransformer: ManagedBuffer => T): Option[T] = { logDebug(s\"Getting remote block $blockId\") require(blockId != null, \"BlockId is null\") // Because all the remote blocks are registered in driver, it is not necessary to ask // all the storage endpoints to get block status. val locationsAndStatusOption = master.getLocationsAndStatus(blockId, blockManagerId.host) if (locationsAndStatusOption.isEmpty) { logDebug(s\"Block $blockId is unknown by block manager master\") None } else { val locationsAndStatus = locationsAndStatusOption.get val blockSize = locationsAndStatus.status.diskSize.max(locationsAndStatus.status.memSize) locationsAndStatus.localDirs.flatMap { localDirs => val blockDataOption = readDiskBlockFromSameHostExecutor(blockId, localDirs, locationsAndStatus.status.diskSize) val res = blockDataOption.flatMap { blockData => try { Some(bufferTransformer(blockData)) } catch { case NonFatal(e) => logDebug(\"Block from the same host executor cannot be opened: \", e) None } } logInfo(s\"Read $blockId from the disk of a same host executor is \" + (if (res.isDefined) \"successful.\" else \"failed.\")) res }.orElse { fetchRemoteManagedBuffer(blockId, blockSize, locationsAndStatus).map(bufferTransformer) } } } private def preferExecutors(locations: Seq[BlockManagerId]): Seq[BlockManagerId] = { val (executors, shuffleServers) = locations.partition(_.port != externalShuffleServicePort) executors ++ shuffleServers } /** * Return a list of locations for the given block, prioritizing the local machine since * multiple block managers can share the same host, followed by hosts on the same rack. * * Within each of the above listed groups (same host, same rack and others) executors are * preferred over the external shuffle service. */ private[spark] def sortLocations(locations: Seq[BlockManagerId]): Seq[BlockManagerId] = { val locs = Random.shuffle(locations) val (preferredLocs, otherLocs) = locs.partition(_.host == blockManagerId.host) val orderedParts = blockManagerId.topologyInfo match { case None => Seq(preferredLocs, otherLocs) case Some(_) => val (sameRackLocs, differentRackLocs) = otherLocs.partition { loc => blockManagerId.topologyInfo == loc.topologyInfo } Seq(preferredLocs, sameRackLocs, differentRackLocs) } orderedParts.map(preferExecutors).reduce(_ ++ _) } /** * Fetch the block from remote block managers as a ManagedBuffer. */ private def fetchRemoteManagedBuffer( blockId: BlockId, blockSize: Long, locationsAndStatus: BlockManagerMessages.BlockLocationsAndStatus): Option[ManagedBuffer] = { // If the block size is above the threshold, we should pass our FileManger to // BlockTransferService, which will leverage it to spill the block; if not, then passed-in // null value means the block will be persisted in memory. val tempFileManager = if (blockSize > maxRemoteBlockToMem) { remoteBlockTempFileManager } else { null } var runningFailureCount = 0 var totalFailureCount = 0 val locations = sortLocations(locationsAndStatus.locations) val maxFetchFailures = locations.size var locationIterator = locations.iterator while (locationIterator.hasNext) { val loc = locationIterator.next() logDebug(s\"Getting remote block $blockId from $loc\") val data = try { val buf = blockTransferService.fetchBlockSync(loc.host, loc.port, loc.executorId, blockId.toString, tempFileManager) if (blockSize > 0 && buf.size() == 0) { throw new IllegalStateException(\"Empty buffer received for non empty block \" + s\"when fetching remote block $blockId from $loc\") } buf } catch { case NonFatal(e) => runningFailureCount += 1 totalFailureCount += 1 if (totalFailureCount >= maxFetchFailures) { // Give up trying anymore locations. Either we've tried all of the original locations, // or we've refreshed the list of locations from the master, and have still // hit failures after trying locations from the refreshed list. logWarning(s\"Failed to fetch remote block $blockId \" + s\"from [${locations.mkString(\", \")}] after $totalFailureCount fetch failures. \" + s\"Most recent failure cause:\", e) return None } logWarning(s\"Failed to fetch remote block $blockId \" + s\"from $loc (failed attempt $runningFailureCount)\", e) // If there is a large number of executors then locations list can contain a // large number of stale entries causing a large number of retries that may // take a significant amount of time. To get rid of these stale entries // we refresh the block locations after a certain number of fetch failures if (runningFailureCount >= maxFailuresBeforeLocationRefresh) { locationIterator = sortLocations(master.getLocations(blockId)).iterator logDebug(s\"Refreshed locations from the driver \" + s\"after ${runningFailureCount} fetch failures.\") runningFailureCount = 0 } // This location failed, so we retry fetch from a different one by returning null here null } if (data != null) { // If the ManagedBuffer is a BlockManagerManagedBuffer, the disposal of the // byte buffers backing it may need to be handled after reading the bytes. // In this case, since we just fetched the bytes remotely, we do not have // a BlockManagerManagedBuffer. The assert here is to ensure that this holds // true (or the disposal is handled). assert(!data.isInstanceOf[BlockManagerManagedBuffer]) return Some(data) } logDebug(s\"The value of block $blockId is null\") } logDebug(s\"Block $blockId not found\") None } /** * Reads the block from the local directories of another executor which runs on the same host. */ private[spark] def readDiskBlockFromSameHostExecutor( blockId: BlockId, localDirs: Array[String], blockSize: Long): Option[ManagedBuffer] = { val file = new File(ExecutorDiskUtils.getFilePath(localDirs, subDirsPerLocalDir, blockId.name)) if (file.exists()) { val managedBuffer = securityManager.getIOEncryptionKey() match { case Some(key) => // Encrypted blocks cannot be memory mapped; return a special object that does decryption // and provides InputStream / FileRegion implementations for reading the data. new EncryptedManagedBuffer( new EncryptedBlockData(file, blockSize, conf, key)) case _ => val transportConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\") new FileSegmentManagedBuffer(transportConf, file, 0, file.length) } Some(managedBuffer) } else { None } } /** * Get block from remote block managers as serialized bytes. */ def getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer] = { getRemoteBlock(blockId, (data: ManagedBuffer) => { // SPARK-24307 undocumented \"escape-hatch\" in case there are any issues in converting to // ChunkedByteBuffer, to go back to old code-path. Can be removed post Spark 2.4 if // new path is stable. if (remoteReadNioBufferConversion) { new ChunkedByteBuffer(data.nioByteBuffer()) } else { ChunkedByteBuffer.fromManagedBuffer(data) } }) } /** * Get a block from the block manager (either local or remote). * * This acquires a read lock on the block if the block was stored locally and does not acquire * any locks if the block was fetched from a remote block manager. The read lock will * automatically be freed once the result's `data` iterator is fully consumed. */ def get[T: ClassTag](blockId: BlockId): Option[BlockResult] = { val local = getLocalValues(blockId) if (local.isDefined) { logInfo(s\"Found block $blockId locally\") return local } val remote = getRemoteValues[T](blockId) if (remote.isDefined) { logInfo(s\"Found block $blockId remotely\") return remote } None } /** * Downgrades an exclusive write lock to a shared read lock. */ def downgradeLock(blockId: BlockId): Unit = { blockInfoManager.downgradeLock(blockId) } /** * Release a lock on the given block with explicit TaskContext. * The param `taskContext` should be passed in case we can't get the correct TaskContext, * for example, the input iterator of a cached RDD iterates to the end in a child * thread. */ def releaseLock(blockId: BlockId, taskContext: Option[TaskContext] = None): Unit = { val taskAttemptId = taskContext.map(_.taskAttemptId()) // SPARK-27666. When a task completes, Spark automatically releases all the blocks locked // by this task. We should not release any locks for a task that is already completed. if (taskContext.isDefined && taskContext.get.isCompleted) { logWarning(s\"Task ${taskAttemptId.get} already completed, not releasing lock for $blockId\") } else { blockInfoManager.unlock(blockId, taskAttemptId) } } /** * Registers a task with the BlockManager in order to initialize per-task bookkeeping structures. */ def registerTask(taskAttemptId: Long): Unit = { blockInfoManager.registerTask(taskAttemptId) } /** * Release all locks for the given task. * * @return the blocks whose locks were released. */ def releaseAllLocksForTask(taskAttemptId: Long): Seq[BlockId] = { blockInfoManager.releaseAllLocksForTask(taskAttemptId) } /** * Retrieve the given block if it exists, otherwise call the provided `makeIterator` method * to compute the block, persist it, and return its values. * * @return either a BlockResult if the block was successfully cached, or an iterator if the block * could not be cached. */ def getOrElseUpdate[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[T], makeIterator: () => Iterator[T]): Either[BlockResult, Iterator[T]] = { // Attempt to read the block from local or remote storage. If it's present, then we don't need // to go through the local-get-or-put path. get[T](blockId)(classTag) match { case Some(block) => return Left(block) case _ => // Need to compute the block. } // Initially we hold no locks on this block. doPutIterator(blockId, makeIterator, level, classTag, keepReadLock = true) match { case None => // doPut() didn't hand work back to us, so the block already existed or was successfully // stored. Therefore, we now hold a read lock on the block. val blockResult = getLocalValues(blockId).getOrElse { // Since we held a read lock between the doPut() and get() calls, the block should not // have been evicted, so get() not returning the block indicates some internal error. releaseLock(blockId) throw SparkCoreErrors.failToGetBlockWithLockError(blockId) } // We already hold a read lock on the block from the doPut() call and getLocalValues() // acquires the lock again, so we need to call releaseLock() here so that the net number // of lock acquisitions is 1 (since the caller will only call release() once). releaseLock(blockId) Left(blockResult) case Some(iter) => // The put failed, likely because the data was too large to fit in memory and could not be // dropped to disk. Therefore, we need to pass the input iterator back to the caller so // that they can decide what to do with the values (e.g. process them without caching). Right(iter) } } /** * @return true if the block was stored or false if an error occurred. */ def putIterator[T: ClassTag]( blockId: BlockId, values: Iterator[T], level: StorageLevel, tellMaster: Boolean = true): Boolean = { require(values != null, \"Values is null\") doPutIterator(blockId, () => values, level, implicitly[ClassTag[T]], tellMaster) match { case None => true case Some(iter) => // Caller doesn't care about the iterator values, so we can close the iterator here // to free resources earlier iter.close() false } } /** * A short circuited method to get a block writer that can write data directly to disk. * The Block will be appended to the File specified by filename. Callers should handle error * cases. */ def getDiskWriter( blockId: BlockId, file: File, serializerInstance: SerializerInstance, bufferSize: Int, writeMetrics: ShuffleWriteMetricsReporter): DiskBlockObjectWriter = { val syncWrites = conf.get(config.SHUFFLE_SYNC) new DiskBlockObjectWriter(file, serializerManager, serializerInstance, bufferSize, syncWrites, writeMetrics, blockId) } /** * Put a new block of serialized bytes to the block manager. * * '''Important!''' Callers must not mutate or release the data buffer underlying `bytes`. Doing * so may corrupt or change the data stored by the `BlockManager`. * * @return true if the block was stored or false if an error occurred. */ def putBytes[T: ClassTag]( blockId: BlockId, bytes: ChunkedByteBuffer, level: StorageLevel, tellMaster: Boolean = true): Boolean = { require(bytes != null, \"Bytes is null\") val blockStoreUpdater = ByteBufferBlockStoreUpdater(blockId, level, implicitly[ClassTag[T]], bytes, tellMaster) blockStoreUpdater.save() } /** * Helper method used to abstract common code from [[BlockStoreUpdater.save()]] * and [[doPutIterator()]]. * * @param putBody a function which attempts the actual put() and returns None on success * or Some on failure. */ private def doPut[T]( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_], tellMaster: Boolean, keepReadLock: Boolean)(putBody: BlockInfo => Option[T]): Option[T] = { require(blockId != null, \"BlockId is null\") require(level != null && level.isValid, \"StorageLevel is null or invalid\") checkShouldStore(blockId) val putBlockInfo = { val newInfo = new BlockInfo(level, classTag, tellMaster) if (blockInfoManager.lockNewBlockForWriting(blockId, newInfo)) { newInfo } else { logWarning(s\"Block $blockId already exists on this machine; not re-adding it\") if (!keepReadLock) { // lockNewBlockForWriting returned a read lock on the existing block, so we must free it: releaseLock(blockId) } return None } } val startTimeNs = System.nanoTime() var exceptionWasThrown: Boolean = true val result: Option[T] = try { val res = putBody(putBlockInfo) exceptionWasThrown = false if (res.isEmpty) { // the block was successfully stored if (keepReadLock) { blockInfoManager.downgradeLock(blockId) } else { blockInfoManager.unlock(blockId) } } else { removeBlockInternal(blockId, tellMaster = false) logWarning(s\"Putting block $blockId failed\") } res } catch { // Since removeBlockInternal may throw exception, // we should print exception first to show root cause. case NonFatal(e) => logWarning(s\"Putting block $blockId failed due to exception $e.\") throw e } finally { // This cleanup is performed in a finally block rather than a `catch` to avoid having to // catch and properly re-throw InterruptedException. if (exceptionWasThrown) { // If an exception was thrown then it's possible that the code in `putBody` has already // notified the master about the availability of this block, so we need to send an update // to remove this block location. removeBlockInternal(blockId, tellMaster = tellMaster) // The `putBody` code may have also added a new block status to TaskMetrics, so we need // to cancel that out by overwriting it with an empty block status. We only do this if // the finally block was entered via an exception because doing this unconditionally would // cause us to send empty block statuses for every block that failed to be cached due to // a memory shortage (which is an expected failure, unlike an uncaught exception). addUpdatedBlockStatusToTaskMetrics(blockId, BlockStatus.empty) } } val usedTimeMs = Utils.getUsedTimeNs(startTimeNs) if (level.replication > 1) { logDebug(s\"Putting block ${blockId} with replication took $usedTimeMs\") } else { logDebug(s\"Putting block ${blockId} without replication took ${usedTimeMs}\") } result } /** * Put the given block according to the given level in one of the block stores, replicating * the values if necessary. * * If the block already exists, this method will not overwrite it. * * @param keepReadLock if true, this method will hold the read lock when it returns (even if the * block already exists). If false, this method will hold no locks when it * returns. * @return None if the block was already present or if the put succeeded, or Some(iterator) * if the put failed. */ private def doPutIterator[T]( blockId: BlockId, iterator: () => Iterator[T], level: StorageLevel, classTag: ClassTag[T], tellMaster: Boolean = true, keepReadLock: Boolean = false): Option[PartiallyUnrolledIterator[T]] = { doPut(blockId, level, classTag, tellMaster = tellMaster, keepReadLock = keepReadLock) { info => val startTimeNs = System.nanoTime() var iteratorFromFailedMemoryStorePut: Option[PartiallyUnrolledIterator[T]] = None // Size of the block in bytes var size = 0L if (level.useMemory) { // Put it in memory first, even if it also has useDisk set to true; // We will drop it to disk later if the memory store can't hold it. if (level.deserialized) { memoryStore.putIteratorAsValues(blockId, iterator(), level.memoryMode, classTag) match { case Right(s) => size = s case Left(iter) => // Not enough space to unroll this block; drop to disk if applicable if (level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream(blockId, out, iter)(classTag) } size = diskStore.getSize(blockId) } else { iteratorFromFailedMemoryStorePut = Some(iter) } } } else { // !level.deserialized memoryStore.putIteratorAsBytes(blockId, iterator(), classTag, level.memoryMode) match { case Right(s) => size = s case Left(partiallySerializedValues) => // Not enough space to unroll this block; drop to disk if applicable if (level.useDisk) { logWarning(s\"Persisting block $blockId to disk instead.\") diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) partiallySerializedValues.finishWritingToStream(out) } size = diskStore.getSize(blockId) } else { iteratorFromFailedMemoryStorePut = Some(partiallySerializedValues.valuesIterator) } } } } else if (level.useDisk) { diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream(blockId, out, iterator())(classTag) } size = diskStore.getSize(blockId) } val putBlockStatus = getCurrentBlockStatus(blockId, info) val blockWasSuccessfullyStored = putBlockStatus.storageLevel.isValid if (blockWasSuccessfullyStored) { // Now that the block is in either the memory or disk store, tell the master about it. info.size = size if (tellMaster && info.tellMaster) { reportBlockStatus(blockId, putBlockStatus) } addUpdatedBlockStatusToTaskMetrics(blockId, putBlockStatus) logDebug(s\"Put block $blockId locally took ${Utils.getUsedTimeNs(startTimeNs)}\") if (level.replication > 1) { val remoteStartTimeNs = System.nanoTime() val bytesToReplicate = doGetLocalBytes(blockId, info) // [SPARK-16550] Erase the typed classTag when using default serialization, since // NettyBlockRpcServer crashes when deserializing repl-defined classes. // TODO(ekl) remove this once the classloader issue on the remote end is fixed. val remoteClassTag = if (!serializerManager.canUseKryo(classTag)) { scala.reflect.classTag[Any] } else { classTag } try { replicate(blockId, bytesToReplicate, level, remoteClassTag) } finally { bytesToReplicate.dispose() } logDebug(s\"Put block $blockId remotely took ${Utils.getUsedTimeNs(remoteStartTimeNs)}\") } } assert(blockWasSuccessfullyStored == iteratorFromFailedMemoryStorePut.isEmpty) iteratorFromFailedMemoryStorePut } } /** * Attempts to cache spilled bytes read from disk into the MemoryStore in order to speed up * subsequent reads. This method requires the caller to hold a read lock on the block. * * @return a copy of the bytes from the memory store if the put succeeded, otherwise None. * If this returns bytes from the memory store then the original disk store bytes will * automatically be disposed and the caller should not continue to use them. Otherwise, * if this returns None then the original disk store bytes will be unaffected. */ private def maybeCacheDiskBytesInMemory( blockInfo: BlockInfo, blockId: BlockId, level: StorageLevel, diskData: BlockData): Option[ChunkedByteBuffer] = { require(!level.deserialized) if (level.useMemory) { // Synchronize on blockInfo to guard against a race condition where two readers both try to // put values read from disk into the MemoryStore. blockInfo.synchronized { if (memoryStore.contains(blockId)) { diskData.dispose() Some(memoryStore.getBytes(blockId).get) } else { val allocator = level.memoryMode match { case MemoryMode.ON_HEAP => ByteBuffer.allocate _ case MemoryMode.OFF_HEAP => Platform.allocateDirectBuffer _ } val putSucceeded = memoryStore.putBytes(blockId, diskData.size, level.memoryMode, () => { // https://issues.apache.org/jira/browse/SPARK-6076 // If the file size is bigger than the free memory, OOM will happen. So if we // cannot put it into MemoryStore, copyForMemory should not be created. That's why // this action is put into a `() => ChunkedByteBuffer` and created lazily. diskData.toChunkedByteBuffer(allocator) }) if (putSucceeded) { diskData.dispose() Some(memoryStore.getBytes(blockId).get) } else { None } } } } else { None } } /** * Attempts to cache spilled values read from disk into the MemoryStore in order to speed up * subsequent reads. This method requires the caller to hold a read lock on the block. * * @return a copy of the iterator. The original iterator passed this method should no longer * be used after this method returns. */ private def maybeCacheDiskValuesInMemory[T]( blockInfo: BlockInfo, blockId: BlockId, level: StorageLevel, diskIterator: Iterator[T]): Iterator[T] = { require(level.deserialized) val classTag = blockInfo.classTag.asInstanceOf[ClassTag[T]] if (level.useMemory) { // Synchronize on blockInfo to guard against a race condition where two readers both try to // put values read from disk into the MemoryStore. blockInfo.synchronized { if (memoryStore.contains(blockId)) { // Note: if we had a means to discard the disk iterator, we would do that here. memoryStore.getValues(blockId).get } else { memoryStore.putIteratorAsValues(blockId, diskIterator, level.memoryMode, classTag) match { case Left(iter) => // The memory store put() failed, so it returned the iterator back to us: iter case Right(_) => // The put() succeeded, so we can read the values back: memoryStore.getValues(blockId).get } } }.asInstanceOf[Iterator[T]] } else { diskIterator } } /** * Get peer block managers in the system. */ private[storage] def getPeers(forceFetch: Boolean): Seq[BlockManagerId] = { peerFetchLock.synchronized { val cachedPeersTtl = conf.get(config.STORAGE_CACHED_PEERS_TTL) // milliseconds val diff = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - lastPeerFetchTimeNs) val timeout = diff > cachedPeersTtl if (cachedPeers == null || forceFetch || timeout) { cachedPeers = master.getPeers(blockManagerId).sortBy(_.hashCode) lastPeerFetchTimeNs = System.nanoTime() logDebug(\"Fetched peers from master: \" + cachedPeers.mkString(\"[\", \",\", \"]\")) } if (cachedPeers.isEmpty && conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined) { Seq(FallbackStorage.FALLBACK_BLOCK_MANAGER_ID) } else { cachedPeers } } } /** * Replicates a block to peer block managers based on existingReplicas and maxReplicas * * @param blockId blockId being replicate * @param existingReplicas existing block managers that have a replica * @param maxReplicas maximum replicas needed * @param maxReplicationFailures number of replication failures to tolerate before * giving up. * @return whether block was successfully replicated or not */ def replicateBlock( blockId: BlockId, existingReplicas: Set[BlockManagerId], maxReplicas: Int, maxReplicationFailures: Option[Int] = None): Boolean = { logInfo(s\"Using $blockManagerId to pro-actively replicate $blockId\") blockInfoManager.lockForReading(blockId).forall { info => val data = doGetLocalBytes(blockId, info) val storageLevel = StorageLevel( useDisk = info.level.useDisk, useMemory = info.level.useMemory, useOffHeap = info.level.useOffHeap, deserialized = info.level.deserialized, replication = maxReplicas) // we know we are called as a result of an executor removal or because the current executor // is getting decommissioned. so we refresh peer cache before trying replication, we won't // try to replicate to a missing executor/another decommissioning executor getPeers(forceFetch = true) try { replicate( blockId, data, storageLevel, info.classTag, existingReplicas, maxReplicationFailures) } finally { logDebug(s\"Releasing lock for $blockId\") releaseLockAndDispose(blockId, data) } } } /** * Replicate block to another node. Note that this is a blocking call that returns after * the block has been replicated. */ private def replicate( blockId: BlockId, data: BlockData, level: StorageLevel, classTag: ClassTag[_], existingReplicas: Set[BlockManagerId] = Set.empty, maxReplicationFailures: Option[Int] = None): Boolean = { val maxReplicationFailureCount = maxReplicationFailures.getOrElse( conf.get(config.STORAGE_MAX_REPLICATION_FAILURE)) val tLevel = StorageLevel( useDisk = level.useDisk, useMemory = level.useMemory, useOffHeap = level.useOffHeap, deserialized = level.deserialized, replication = 1) val numPeersToReplicateTo = level.replication - 1 val startTime = System.nanoTime val peersReplicatedTo = mutable.HashSet.empty ++ existingReplicas val peersFailedToReplicateTo = mutable.HashSet.empty[BlockManagerId] var numFailures = 0 val initialPeers = getPeers(false).filterNot(existingReplicas.contains) var peersForReplication = blockReplicationPolicy.prioritize( blockManagerId, initialPeers, peersReplicatedTo, blockId, numPeersToReplicateTo) while(numFailures <= maxReplicationFailureCount && !peersForReplication.isEmpty && peersReplicatedTo.size < numPeersToReplicateTo) { val peer = peersForReplication.head try { val onePeerStartTime = System.nanoTime logTrace(s\"Trying to replicate $blockId of ${data.size} bytes to $peer\") // This thread keeps a lock on the block, so we do not want the netty thread to unlock // block when it finishes sending the message. val buffer = new BlockManagerManagedBuffer(blockInfoManager, blockId, data, false, unlockOnDeallocate = false) blockTransferService.uploadBlockSync( peer.host, peer.port, peer.executorId, blockId, buffer, tLevel, classTag) logTrace(s\"Replicated $blockId of ${data.size} bytes to $peer\" + s\" in ${(System.nanoTime - onePeerStartTime).toDouble / 1e6} ms\") peersForReplication = peersForReplication.tail peersReplicatedTo += peer } catch { // Rethrow interrupt exception case e: InterruptedException => throw e // Everything else we may retry case NonFatal(e) => logWarning(s\"Failed to replicate $blockId to $peer, failure #$numFailures\", e) peersFailedToReplicateTo += peer // we have a failed replication, so we get the list of peers again // we don't want peers we have already replicated to and the ones that // have failed previously val filteredPeers = getPeers(true).filter { p => !peersFailedToReplicateTo.contains(p) && !peersReplicatedTo.contains(p) } numFailures += 1 peersForReplication = blockReplicationPolicy.prioritize( blockManagerId, filteredPeers, peersReplicatedTo, blockId, numPeersToReplicateTo - peersReplicatedTo.size) } } logDebug(s\"Replicating $blockId of ${data.size} bytes to \" + s\"${peersReplicatedTo.size} peer(s) took ${(System.nanoTime - startTime) / 1e6} ms\") if (peersReplicatedTo.size < numPeersToReplicateTo) { logWarning(s\"Block $blockId replicated to only \" + s\"${peersReplicatedTo.size} peer(s) instead of $numPeersToReplicateTo peers\") return false } logDebug(s\"block $blockId replicated to ${peersReplicatedTo.mkString(\", \")}\") return true } /** * Read a block consisting of a single object. */ def getSingle[T: ClassTag](blockId: BlockId): Option[T] = { get[T](blockId).map(_.data.next().asInstanceOf[T]) } /** * Write a block consisting of a single object. * * @return true if the block was stored or false if the block was already stored or an * error occurred. */ def putSingle[T: ClassTag]( blockId: BlockId, value: T, level: StorageLevel, tellMaster: Boolean = true): Boolean = { putIterator(blockId, Iterator(value), level, tellMaster) } /** * Drop a block from memory, possibly putting it on disk if applicable. Called when the memory * store reaches its limit and needs to free up space. * * If `data` is not put on disk, it won't be created. * * The caller of this method must hold a write lock on the block before calling this method. * This method does not release the write lock. * * @return the block's new effective StorageLevel. */ private[storage] override def dropFromMemory[T: ClassTag]( blockId: BlockId, data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel = { logInfo(s\"Dropping block $blockId from memory\") val info = blockInfoManager.assertBlockIsLockedForWriting(blockId) var blockIsUpdated = false val level = info.level // Drop to disk, if storage level requires if (level.useDisk && !diskStore.contains(blockId)) { logInfo(s\"Writing block $blockId to disk\") data() match { case Left(elements) => diskStore.put(blockId) { channel => val out = Channels.newOutputStream(channel) serializerManager.dataSerializeStream( blockId, out, elements.iterator)(info.classTag.asInstanceOf[ClassTag[T]]) } case Right(bytes) => diskStore.putBytes(blockId, bytes) } blockIsUpdated = true } // Actually drop from memory store val droppedMemorySize = if (memoryStore.contains(blockId)) memoryStore.getSize(blockId) else 0L val blockIsRemoved = memoryStore.remove(blockId) if (blockIsRemoved) { blockIsUpdated = true } else { logWarning(s\"Block $blockId could not be dropped from memory as it does not exist\") } val status = getCurrentBlockStatus(blockId, info) if (info.tellMaster) { reportBlockStatus(blockId, status, droppedMemorySize) } if (blockIsUpdated) { addUpdatedBlockStatusToTaskMetrics(blockId, status) } status.storageLevel } /** * Remove all blocks belonging to the given RDD. * * @return The number of blocks removed. */ def removeRdd(rddId: Int): Int = { // TODO: Avoid a linear scan by creating another mapping of RDD.id to blocks. logInfo(s\"Removing RDD $rddId\") val blocksToRemove = blockInfoManager.entries.flatMap(_._1.asRDDId).filter(_.rddId == rddId) blocksToRemove.foreach { blockId => removeBlock(blockId, tellMaster = false) } blocksToRemove.size } def decommissionBlockManager(): Unit = storageEndpoint.ask(DecommissionBlockManager) private[spark] def decommissionSelf(): Unit = synchronized { decommissioner match { case None => logInfo(\"Starting block manager decommissioning process...\") decommissioner = Some(new BlockManagerDecommissioner(conf, this)) decommissioner.foreach(_.start()) case Some(_) => logDebug(\"Block manager already in decommissioning state\") } } /** * Returns the last migration time and a boolean denoting if all the blocks have been migrated. * If there are any tasks running since that time the boolean may be incorrect. */ private[spark] def lastMigrationInfo(): (Long, Boolean) = { decommissioner.map(_.lastMigrationInfo()).getOrElse((0, false)) } private[storage] def getMigratableRDDBlocks(): Seq[ReplicateBlock] = master.getReplicateInfoForRDDBlocks(blockManagerId) /** * Remove all blocks belonging to the given broadcast. */ def removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int = { logDebug(s\"Removing broadcast $broadcastId\") val blocksToRemove = blockInfoManager.entries.map(_._1).collect { case bid @ BroadcastBlockId(`broadcastId`, _) => bid } blocksToRemove.foreach { blockId => removeBlock(blockId, tellMaster) } blocksToRemove.size } /** * Remove a block from both memory and disk. */ def removeBlock(blockId: BlockId, tellMaster: Boolean = true): Unit = { logDebug(s\"Removing block $blockId\") blockInfoManager.lockForWriting(blockId) match { case None => // The block has already been removed; do nothing. logWarning(s\"Asked to remove block $blockId, which does not exist\") case Some(info) => removeBlockInternal(blockId, tellMaster = tellMaster && info.tellMaster) addUpdatedBlockStatusToTaskMetrics(blockId, BlockStatus.empty) } } /** * Internal version of [[removeBlock()]] which assumes that the caller already holds a write * lock on the block. */ private def removeBlockInternal(blockId: BlockId, tellMaster: Boolean): Unit = { val blockStatus = if (tellMaster) { val blockInfo = blockInfoManager.assertBlockIsLockedForWriting(blockId) Some(getCurrentBlockStatus(blockId, blockInfo)) } else None // Removals are idempotent in disk store and memory store. At worst, we get a warning. val removedFromMemory = memoryStore.remove(blockId) val removedFromDisk = diskStore.remove(blockId) if (!removedFromMemory && !removedFromDisk) { logWarning(s\"Block $blockId could not be removed as it was not found on disk or in memory\") } blockInfoManager.removeBlock(blockId) if (tellMaster) { // Only update storage level from the captured block status before deleting, so that // memory size and disk size are being kept for calculating delta. reportBlockStatus(blockId, blockStatus.get.copy(storageLevel = StorageLevel.NONE)) } } private def addUpdatedBlockStatusToTaskMetrics(blockId: BlockId, status: BlockStatus): Unit = { if (conf.get(config.TASK_METRICS_TRACK_UPDATED_BLOCK_STATUSES)) { Option(TaskContext.get()).foreach { c => c.taskMetrics().incUpdatedBlockStatuses(blockId -> status) } } } def releaseLockAndDispose( blockId: BlockId, data: BlockData, taskContext: Option[TaskContext] = None): Unit = { releaseLock(blockId, taskContext) data.dispose() } def stop(): Unit = { decommissioner.foreach(_.stop()) blockTransferService.close() if (blockStoreClient ne blockTransferService) { // Closing should be idempotent, but maybe not for the NioBlockTransferService. blockStoreClient.close() } remoteBlockTempFileManager.stop() diskBlockManager.stop() rpcEnv.stop(storageEndpoint) blockInfoManager.clear() memoryStore.clear() futureExecutionContext.shutdownNow() logInfo(\"BlockManager stopped\") } } private[spark] object BlockManager { private val ID_GENERATOR = new IdGenerator def blockIdsToLocations( blockIds: Array[BlockId], env: SparkEnv, blockManagerMaster: BlockManagerMaster = null): Map[BlockId, Seq[String]] = { // blockManagerMaster != null is used in tests assert(env != null || blockManagerMaster != null) val blockLocations: Seq[Seq[BlockManagerId]] = if (blockManagerMaster == null) { env.blockManager.getLocationBlockIds(blockIds) } else { blockManagerMaster.getLocations(blockIds) } val blockManagers = new HashMap[BlockId, Seq[String]] for (i <- 0 until blockIds.length) { blockManagers(blockIds(i)) = blockLocations(i).map { loc => ExecutorCacheTaskLocation(loc.host, loc.executorId).toString } } blockManagers.toMap } private class ShuffleMetricsSource( override val sourceName: String, metricSet: MetricSet) extends Source { override val metricRegistry = new MetricRegistry metricRegistry.registerAll(metricSet) } class RemoteBlockDownloadFileManager( blockManager: BlockManager, encryptionKey: Option[Array[Byte]]) extends DownloadFileManager with Logging { private class ReferenceWithCleanup( file: DownloadFile, referenceQueue: JReferenceQueue[DownloadFile] ) extends WeakReference[DownloadFile](file, referenceQueue) { // we cannot use `file.delete()` here otherwise it won't be garbage-collected val filePath = file.path() def cleanUp(): Unit = { logDebug(s\"Clean up file $filePath\") if (!new File(filePath).delete()) { logDebug(s\"Fail to delete file $filePath\") } } } private val referenceQueue = new JReferenceQueue[DownloadFile] private val referenceBuffer = Collections.newSetFromMap[ReferenceWithCleanup]( new ConcurrentHashMap) private val POLL_TIMEOUT = 1000 @volatile private var stopped = false private val cleaningThread = new Thread() { override def run(): Unit = { keepCleaning() } } cleaningThread.setDaemon(true) cleaningThread.setName(\"RemoteBlock-temp-file-clean-thread\") cleaningThread.start() override def createTempFile(transportConf: TransportConf): DownloadFile = { val file = blockManager.diskBlockManager.createTempLocalBlock()._2 encryptionKey match { case Some(key) => // encryption is enabled, so when we read the decrypted data off the network, we need to // encrypt it when writing to disk. Note that the data may have been encrypted when it // was cached on disk on the remote side, but it was already decrypted by now (see // EncryptedBlockData). new EncryptedDownloadFile(file, key) case None => new SimpleDownloadFile(file, transportConf) } } override def registerTempFileToClean(file: DownloadFile): Boolean = { referenceBuffer.add(new ReferenceWithCleanup(file, referenceQueue)) } def stop(): Unit = { stopped = true cleaningThread.interrupt() cleaningThread.join() } private def keepCleaning(): Unit = { while (!stopped) { try { Option(referenceQueue.remove(POLL_TIMEOUT)) .map(_.asInstanceOf[ReferenceWithCleanup]) .foreach { ref => referenceBuffer.remove(ref) ref.cleanUp() } } catch { case _: InterruptedException => // no-op case NonFatal(e) => logError(\"Error in cleaning thread\", e) } } } } /** * A DownloadFile that encrypts data when it is written, and decrypts when it's read. */ private class EncryptedDownloadFile( file: File, key: Array[Byte]) extends DownloadFile { private val env = SparkEnv.get override def delete(): Boolean = file.delete() override def openForWriting(): DownloadFileWritableChannel = { new EncryptedDownloadWritableChannel() } override def path(): String = file.getAbsolutePath private class EncryptedDownloadWritableChannel extends DownloadFileWritableChannel { private val countingOutput: CountingWritableChannel = new CountingWritableChannel( Channels.newChannel(env.serializerManager.wrapForEncryption(new FileOutputStream(file)))) override def closeAndRead(): ManagedBuffer = { countingOutput.close() val size = countingOutput.getCount new EncryptedManagedBuffer(new EncryptedBlockData(file, size, env.conf, key)) } override def write(src: ByteBuffer): Int = countingOutput.write(src) override def isOpen: Boolean = countingOutput.isOpen() override def close(): Unit = countingOutput.close() } } }",
            "## CLASS: org/apache/spark/memory/MemoryManager# (implementation)\n* * In this context, execution memory refers to that used for computation in shuffles, joins, * sorts and aggregations, while storage memory refers to that used for caching and propagating * internal data across the cluster. There exists one MemoryManager per JVM. */ private[spark] abstract class MemoryManager( conf: SparkConf, numCores: Int, onHeapStorageMemory: Long, onHeapExecutionMemory: Long) extends Logging { require(onHeapExecutionMemory > 0, \"onHeapExecutionMemory must be > 0\") // -- Methods related to memory allocation policies and bookkeeping ------------------------------ @GuardedBy(\"this\") protected val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP) @GuardedBy(\"this\") protected val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP) @GuardedBy(\"this\") protected val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP) onHeapStorageMemoryPool.incrementPoolSize(onHeapStorageMemory) onHeapExecutionMemoryPool.incrementPoolSize(onHeapExecutionMemory) protected[this] val maxOffHeapMemory = conf.get(MEMORY_OFFHEAP_SIZE) protected[this] val offHeapStorageMemory = (maxOffHeapMemory * conf.get(MEMORY_STORAGE_FRACTION)).toLong offHeapExecutionMemoryPool.incrementPoolSize(maxOffHeapMemory - offHeapStorageMemory) offHeapStorageMemoryPool.incrementPoolSize(offHeapStorageMemory) /** * Total available on heap memory for storage, in bytes. This amount can vary over time, * depending on the MemoryManager implementation. * In this model, this is equivalent to the amount of memory not occupied by execution. */ def maxOnHeapStorageMemory: Long /** * Total available off heap memory for storage, in bytes. This amount can vary over time, * depending on the MemoryManager implementation. */ def maxOffHeapStorageMemory: Long /** * Set the [[MemoryStore]] used by this manager to evict cached blocks. * This must be set after construction due to initialization ordering constraints. */ final def setMemoryStore(store: MemoryStore): Unit = synchronized { onHeapStorageMemoryPool.setMemoryStore(store) offHeapStorageMemoryPool.setMemoryStore(store) } /** * Acquire N bytes of memory to cache the given block, evicting existing ones if necessary. * * @return whether all N bytes were successfully granted. */ def acquireStorageMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean /** * Acquire N bytes of memory to unroll the given block, evicting existing ones if necessary. * * This extra method allows subclasses to differentiate behavior between acquiring storage * memory and acquiring unroll memory. For instance, the memory management model in Spark * 1.5 and before places a limit on the amount of space that can be freed from unrolling. * * @return whether all N bytes were successfully granted. */ def acquireUnrollMemory(blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean /** * Try to acquire up to `numBytes` of execution memory for the current task and return the * number of bytes obtained, or 0 if none can be allocated. * * This call may block until there is enough free memory in some situations, to make sure each * task has a chance to ramp up to at least 1 / 2N of the total memory pool (where N is the # of * active tasks) before it is forced to spill. This can happen if the number of tasks increase * but an older task had a lot of memory already. */ private[memory] def acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long /** * Release numBytes of execution memory belonging to the given task. */ private[memory] def releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit = synchronized { memoryMode match { case MemoryMode.ON_HEAP => onHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) case MemoryMode.OFF_HEAP => offHeapExecutionMemoryPool.releaseMemory(numBytes, taskAttemptId) } } /** * Release all memory for the given task and mark it as inactive (e.g. when a task ends). * * @return the number of bytes freed. */ private[memory] def releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long = synchronized { onHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) + offHeapExecutionMemoryPool.releaseAllMemoryForTask(taskAttemptId) } /** * Release N bytes of storage memory. */ def releaseStorageMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized { memoryMode match { case MemoryMode.ON_HEAP => onHeapStorageMemoryPool.releaseMemory(numBytes) case MemoryMode.OFF_HEAP => offHeapStorageMemoryPool.releaseMemory(numBytes) } } /** * Release all storage memory acquired. */ final def releaseAllStorageMemory(): Unit = synchronized { onHeapStorageMemoryPool.releaseAllMemory() offHeapStorageMemoryPool.releaseAllMemory() } /** * Release N bytes of unroll memory. */ final def releaseUnrollMemory(numBytes: Long, memoryMode: MemoryMode): Unit = synchronized { releaseStorageMemory(numBytes, memoryMode) } /** * Execution memory currently in use, in bytes. */ final def executionMemoryUsed: Long = synchronized { onHeapExecutionMemoryPool.memoryUsed + offHeapExecutionMemoryPool.memoryUsed } /** * Storage memory currently in use, in bytes. */ final def storageMemoryUsed: Long = synchronized { onHeapStorageMemoryPool.memoryUsed + offHeapStorageMemoryPool.memoryUsed } /** * On heap execution memory currently in use, in bytes. */ final def onHeapExecutionMemoryUsed: Long = synchronized { onHeapExecutionMemoryPool.memoryUsed } /** * Off heap execution memory currently in use, in bytes. */ final def offHeapExecutionMemoryUsed: Long = synchronized { offHeapExecutionMemoryPool.memoryUsed } /** * On heap storage memory currently in use, in bytes. */ final def onHeapStorageMemoryUsed: Long = synchronized { onHeapStorageMemoryPool.memoryUsed } /** * Off heap storage memory currently in use, in bytes. */ final def offHeapStorageMemoryUsed: Long = synchronized { offHeapStorageMemoryPool.memoryUsed } /** * Returns the execution memory consumption, in bytes, for the given task. */ private[memory] def getExecutionMemoryUsageForTask(taskAttemptId: Long): Long = synchronized { onHeapExecutionMemoryPool.getMemoryUsageForTask(taskAttemptId) + offHeapExecutionMemoryPool.getMemoryUsageForTask(taskAttemptId) } // -- Fields related to Tungsten managed memory ------------------------------------------------- /** * Tracks whether Tungsten memory will be allocated on the JVM heap or off-heap using * sun.misc.Unsafe. */ final val tungstenMemoryMode: MemoryMode = { if (conf.get(MEMORY_OFFHEAP_ENABLED)) { require(conf.get(MEMORY_OFFHEAP_SIZE) > 0, \"spark.memory.offHeap.size must be > 0 when spark.memory.offHeap.enabled == true\") require(Platform.unaligned(), \"No support for unaligned Unsafe. Set spark.memory.offHeap.enabled to false.\") MemoryMode.OFF_HEAP } else { MemoryMode.ON_HEAP } } /** * The default page size, in bytes. * * If user didn't explicitly set \"spark.buffer.pageSize\", we figure out the default value * by looking at the number of cores available to the process, and the total amount of memory, * and then divide it by a factor of safety. * * SPARK-37593 If we are using G1GC, it's better to take the LONG_ARRAY_OFFSET * into consideration so that the requested memory size is power of 2 * and can be divided by G1 heap region size to reduce memory waste within one G1 region. */ private lazy val defaultPageSizeBytes = { val minPageSize = 1L * 1024 * 1024 // 1MB val maxPageSize = 64L * minPageSize // 64MB val cores = if (numCores > 0) numCores else Runtime.getRuntime.availableProcessors() // Because of rounding to next power of 2, we may have safetyFactor as 8 in worst case val safetyFactor = 16 val maxTungstenMemory: Long = tungstenMemoryMode match { case MemoryMode.ON_HEAP => onHeapExecutionMemoryPool.poolSize case MemoryMode.OFF_HEAP => offHeapExecutionMemoryPool.poolSize } val size = ByteArrayMethods.nextPowerOf2(maxTungstenMemory / cores / safetyFactor) val chosenPageSize = math.min(maxPageSize, math.max(minPageSize, size)) if (isG1GC && tungstenMemoryMode == MemoryMode.ON_HEAP) { chosenPageSize - Platform.LONG_ARRAY_OFFSET } else { chosenPageSize } } val pageSizeBytes: Long = conf.get(BUFFER_PAGESIZE).getOrElse(defaultPageSizeBytes) /** * Allocates memory for use by Unsafe/Tungsten code. */ private[memory] final val tungstenMemoryAllocator: MemoryAllocator = { tungstenMemoryMode match { case MemoryMode.ON_HEAP => MemoryAllocator.HEAP case MemoryMode.OFF_HEAP => MemoryAllocator.UNSAFE } } /** * Return whether we are using G1GC or not */ private lazy val isG1GC: Boolean = { Try { val clazz = Utils.classForName(\"com.sun.management.HotSpotDiagnosticMXBean\") .asInstanceOf[Class[_ <: PlatformManagedObject]] val vmOptionClazz = Utils.classForName(\"com.sun.management.VMOption\") val hotSpotDiagnosticMXBean = ManagementFactory.getPlatformMXBean(clazz) val vmOptionMethod = clazz.getMethod(\"getVMOption\", classOf[String]) val valueMethod = vmOptionClazz.getMethod(\"getValue\") val useG1GCObject = vmOptionMethod.invoke(hotSpotDiagnosticMXBean, \"UseG1GC\") val useG1GC = valueMethod.invoke(useG1GCObject).asInstanceOf[String] \"true\".equals(useG1GC) }.getOrElse(false) } }",
            "## CLASS: org/apache/spark/api/java/JavaPairRDD# (implementation)\nclass JavaPairRDD[K, V](val rdd: RDD[(K, V)]) (implicit val kClassTag: ClassTag[K], implicit val vClassTag: ClassTag[V]) extends AbstractJavaRDDLike[(K, V), JavaPairRDD[K, V]] { override def wrapRDD(rdd: RDD[(K, V)]): JavaPairRDD[K, V] = JavaPairRDD.fromRDD(rdd) override val classTag: ClassTag[(K, V)] = rdd.elementClassTag import JavaPairRDD._ // Common RDD functions /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def cache(): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.cache()) /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. Can only be called once on each RDD. */ def persist(newLevel: StorageLevel): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.persist(newLevel)) /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * This method blocks until all blocks are deleted. */ def unpersist(): JavaPairRDD[K, V] = wrapRDD(rdd.unpersist()) /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted. */ def unpersist(blocking: Boolean): JavaPairRDD[K, V] = wrapRDD(rdd.unpersist(blocking)) // Transformations (return a new RDD) /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.distinct()) /** * Return a new RDD containing the distinct elements in this RDD. */ def distinct(numPartitions: Int): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.distinct(numPartitions)) /** * Return a new RDD containing only the elements that satisfy a predicate. */ def filter(f: JFunction[(K, V), java.lang.Boolean]): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.filter(x => f.call(x).booleanValue())) /** * Return a new RDD that is reduced into `numPartitions` partitions. */ def coalesce(numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.coalesce(numPartitions)) /** * Return a new RDD that is reduced into `numPartitions` partitions. */ def coalesce(numPartitions: Int, shuffle: Boolean): JavaPairRDD[K, V] = fromRDD(rdd.coalesce(numPartitions, shuffle)) /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle. */ def repartition(numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.repartition(numPartitions)) /** * Return a sampled subset of this RDD. */ def sample(withReplacement: Boolean, fraction: Double): JavaPairRDD[K, V] = sample(withReplacement, fraction, Utils.random.nextLong) /** * Return a sampled subset of this RDD. */ def sample(withReplacement: Boolean, fraction: Double, seed: Long): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.sample(withReplacement, fraction, seed)) /** * Return a subset of this RDD sampled by key (via stratified sampling). * * Create a sample of this RDD using variable sampling rates for different keys as specified by * `fractions`, a key to sampling rate map, via simple random sampling with one pass over the * RDD, to produce a sample of size that's approximately equal to the sum of * math.ceil(numItems * samplingRate) over all key values. */ def sampleByKey(withReplacement: Boolean, fractions: java.util.Map[K, jl.Double], seed: Long): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.sampleByKey( withReplacement, fractions.asScala.mapValues(_.toDouble).toMap, // map to Scala Double; toMap to serialize seed)) /** * Return a subset of this RDD sampled by key (via stratified sampling). * * Create a sample of this RDD using variable sampling rates for different keys as specified by * `fractions`, a key to sampling rate map, via simple random sampling with one pass over the * RDD, to produce a sample of size that's approximately equal to the sum of * math.ceil(numItems * samplingRate) over all key values. * * Use Utils.random.nextLong as the default seed for the random number generator. */ def sampleByKey(withReplacement: Boolean, fractions: java.util.Map[K, jl.Double]): JavaPairRDD[K, V] = sampleByKey(withReplacement, fractions, Utils.random.nextLong) /** * Return a subset of this RDD sampled by key (via stratified sampling) containing exactly * math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). * * This method differs from `sampleByKey` in that we make additional passes over the RDD to * create a sample size that's exactly equal to the sum of math.ceil(numItems * samplingRate) * over all key values with a 99.99% confidence. When sampling without replacement, we need one * additional pass over the RDD to guarantee sample size; when sampling with replacement, we need * two additional passes. */ def sampleByKeyExact(withReplacement: Boolean, fractions: java.util.Map[K, jl.Double], seed: Long): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.sampleByKeyExact( withReplacement, fractions.asScala.mapValues(_.toDouble).toMap, // map to Scala Double; toMap to serialize seed)) /** * Return a subset of this RDD sampled by key (via stratified sampling) containing exactly * math.ceil(numItems * samplingRate) for each stratum (group of pairs with the same key). * * This method differs from `sampleByKey` in that we make additional passes over the RDD to * create a sample size that's exactly equal to the sum of math.ceil(numItems * samplingRate) * over all key values with a 99.99% confidence. When sampling without replacement, we need one * additional pass over the RDD to guarantee sample size; when sampling with replacement, we need * two additional passes. * * Use Utils.random.nextLong as the default seed for the random number generator. */ def sampleByKeyExact( withReplacement: Boolean, fractions: java.util.Map[K, jl.Double]): JavaPairRDD[K, V] = sampleByKeyExact(withReplacement, fractions, Utils.random.nextLong) /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */ def union(other: JavaPairRDD[K, V]): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.union(other.rdd)) /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. */ def intersection(other: JavaPairRDD[K, V]): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.intersection(other.rdd)) // first() has to be overridden here so that the generated method has the signature // 'public scala.Tuple2 first()'; if the trait's definition is used, // then the method has the signature 'public java.lang.Object first()', // causing NoSuchMethodErrors at runtime. override def first(): (K, V) = rdd.first() // Pair RDD functions /** * Generic function to combine the elements for each key using a custom set of aggregation * functions. Turns a JavaPairRDD[(K, V)] into a result of type JavaPairRDD[(K, C)], for a * \"combined type\" C. * * Users provide three functions: * * - `createCombiner`, which turns a V into a C (e.g., creates a one-element list) * - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list) * - `mergeCombiners`, to combine two C's into a single one. * * In addition, users can control the partitioning of the output RDD, the serializer that is use * for the shuffle, and whether to perform map-side aggregation (if a mapper can produce multiple * items with the same key). * * @note V and C can be different -- for example, one might group an RDD of type (Int, Int) into * an RDD of type (Int, List[Int]). */ def combineByKey[C](createCombiner: JFunction[V, C], mergeValue: JFunction2[C, V, C], mergeCombiners: JFunction2[C, C, C], partitioner: Partitioner, mapSideCombine: Boolean, serializer: Serializer): JavaPairRDD[K, C] = { implicit val ctag: ClassTag[C] = fakeClassTag fromRDD(rdd.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine, serializer )) } /** * Generic function to combine the elements for each key using a custom set of aggregation * functions. Turns a JavaPairRDD[(K, V)] into a result of type JavaPairRDD[(K, C)], for a * \"combined type\" C. * * Users provide three functions: * * - `createCombiner`, which turns a V into a C (e.g., creates a one-element list) * - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list) * - `mergeCombiners`, to combine two C's into a single one. * * In addition, users can control the partitioning of the output RDD. This method automatically * uses map-side aggregation in shuffling the RDD. * * @note V and C can be different -- for example, one might group an RDD of type (Int, Int) into * an RDD of type (Int, List[Int]). */ def combineByKey[C](createCombiner: JFunction[V, C], mergeValue: JFunction2[C, V, C], mergeCombiners: JFunction2[C, C, C], partitioner: Partitioner): JavaPairRDD[K, C] = { combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner, true, null) } /** * Simplified version of combineByKey that hash-partitions the output RDD and uses map-side * aggregation. */ def combineByKey[C](createCombiner: JFunction[V, C], mergeValue: JFunction2[C, V, C], mergeCombiners: JFunction2[C, C, C], numPartitions: Int): JavaPairRDD[K, C] = combineByKey(createCombiner, mergeValue, mergeCombiners, new HashPartitioner(numPartitions)) /** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a \"combiner\" in MapReduce. */ def reduceByKey(partitioner: Partitioner, func: JFunction2[V, V, V]): JavaPairRDD[K, V] = fromRDD(rdd.reduceByKey(partitioner, func)) /** * Merge the values for each key using an associative and commutative reduce function, but return * the result immediately to the master as a Map. This will also perform the merging locally on * each mapper before sending results to a reducer, similarly to a \"combiner\" in MapReduce. */ def reduceByKeyLocally(func: JFunction2[V, V, V]): java.util.Map[K, V] = mapAsSerializableJavaMap(rdd.reduceByKeyLocally(func)) /** Count the number of elements for each key, and return the result to the master as a Map. */ def countByKey(): java.util.Map[K, jl.Long] = mapAsSerializableJavaMap(rdd.countByKey()).asInstanceOf[java.util.Map[K, jl.Long]] /** * Approximate version of countByKey that can return a partial result if it does * not finish within a timeout. */ def countByKeyApprox(timeout: Long): PartialResult[java.util.Map[K, BoundedDouble]] = rdd.countByKeyApprox(timeout).map(mapAsSerializableJavaMap) /** * Approximate version of countByKey that can return a partial result if it does * not finish within a timeout. */ def countByKeyApprox(timeout: Long, confidence: Double = 0.95) : PartialResult[java.util.Map[K, BoundedDouble]] = rdd.countByKeyApprox(timeout, confidence).map(mapAsSerializableJavaMap) /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * This function can return a different result type, U, than the type of the values in this RDD, * V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, * as in scala.TraversableOnce. The former operation is used for merging values within a * partition, and the latter is used for merging values between partitions. To avoid memory * allocation, both of these functions are allowed to modify and return their first argument * instead of creating a new U. */ def aggregateByKey[U](zeroValue: U, partitioner: Partitioner, seqFunc: JFunction2[U, V, U], combFunc: JFunction2[U, U, U]): JavaPairRDD[K, U] = { implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.aggregateByKey(zeroValue, partitioner)(seqFunc, combFunc)) } /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * This function can return a different result type, U, than the type of the values in this RDD, * V. Thus, we need one operation for merging a V into a U and one operation for merging two U's, * as in scala.TraversableOnce. The former operation is used for merging values within a * partition, and the latter is used for merging values between partitions. To avoid memory * allocation, both of these functions are allowed to modify and return their first argument * instead of creating a new U. */ def aggregateByKey[U](zeroValue: U, numPartitions: Int, seqFunc: JFunction2[U, V, U], combFunc: JFunction2[U, U, U]): JavaPairRDD[K, U] = { implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.aggregateByKey(zeroValue, numPartitions)(seqFunc, combFunc)) } /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * This function can return a different result type, U, than the type of the values in this RDD, * V. Thus, we need one operation for merging a V into a U and one operation for merging two U's. * The former operation is used for merging values within a partition, and the latter is used for * merging values between partitions. To avoid memory allocation, both of these functions are * allowed to modify and return their first argument instead of creating a new U. */ def aggregateByKey[U](zeroValue: U, seqFunc: JFunction2[U, V, U], combFunc: JFunction2[U, U, U]): JavaPairRDD[K, U] = { implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.aggregateByKey(zeroValue)(seqFunc, combFunc)) } /** * Merge the values for each key using an associative function and a neutral \"zero value\" which * may be added to the result an arbitrary number of times, and must not change the result * (e.g ., Nil for list concatenation, 0 for addition, or 1 for multiplication.). */ def foldByKey(zeroValue: V, partitioner: Partitioner, func: JFunction2[V, V, V]) : JavaPairRDD[K, V] = fromRDD(rdd.foldByKey(zeroValue, partitioner)(func)) /** * Merge the values for each key using an associative function and a neutral \"zero value\" which * may be added to the result an arbitrary number of times, and must not change the result * (e.g ., Nil for list concatenation, 0 for addition, or 1 for multiplication.). */ def foldByKey(zeroValue: V, numPartitions: Int, func: JFunction2[V, V, V]): JavaPairRDD[K, V] = fromRDD(rdd.foldByKey(zeroValue, numPartitions)(func)) /** * Merge the values for each key using an associative function and a neutral \"zero value\" * which may be added to the result an arbitrary number of times, and must not change the result * (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.). */ def foldByKey(zeroValue: V, func: JFunction2[V, V, V]): JavaPairRDD[K, V] = fromRDD(rdd.foldByKey(zeroValue)(func)) /** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a \"combiner\" in MapReduce. Output will be hash-partitioned with numPartitions partitions. */ def reduceByKey(func: JFunction2[V, V, V], numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.reduceByKey(func, numPartitions)) /** * Group the values for each key in the RDD into a single sequence. Allows controlling the * partitioning of the resulting key-value pair RDD by passing a Partitioner. * * @note If you are grouping in order to perform an aggregation (such as a sum or average) over * each key, using `JavaPairRDD.reduceByKey` or `JavaPairRDD.combineByKey` * will provide much better performance. */ def groupByKey(partitioner: Partitioner): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey(partitioner))) /** * Group the values for each key in the RDD into a single sequence. Hash-partitions the * resulting RDD with into `numPartitions` partitions. * * @note If you are grouping in order to perform an aggregation (such as a sum or average) over * each key, using `JavaPairRDD.reduceByKey` or `JavaPairRDD.combineByKey` * will provide much better performance. */ def groupByKey(numPartitions: Int): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey(numPartitions))) /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us. */ def subtract(other: JavaPairRDD[K, V]): JavaPairRDD[K, V] = fromRDD(rdd.subtract(other)) /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: JavaPairRDD[K, V], numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.subtract(other, numPartitions)) /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: JavaPairRDD[K, V], p: Partitioner): JavaPairRDD[K, V] = fromRDD(rdd.subtract(other, p)) /** * Return an RDD with the pairs from `this` whose keys are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us. */ def subtractByKey[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, V] = { implicit val ctag: ClassTag[W] = fakeClassTag fromRDD(rdd.subtractByKey(other)) } /** * Return an RDD with the pairs from `this` whose keys are not in `other`. */ def subtractByKey[W](other: JavaPairRDD[K, W], numPartitions: Int): JavaPairRDD[K, V] = { implicit val ctag: ClassTag[W] = fakeClassTag fromRDD(rdd.subtractByKey(other, numPartitions)) } /** * Return an RDD with the pairs from `this` whose keys are not in `other`. */ def subtractByKey[W](other: JavaPairRDD[K, W], p: Partitioner): JavaPairRDD[K, V] = { implicit val ctag: ClassTag[W] = fakeClassTag fromRDD(rdd.subtractByKey(other, p)) } /** * Return a copy of the RDD partitioned using the specified partitioner. */ def partitionBy(partitioner: Partitioner): JavaPairRDD[K, V] = fromRDD(rdd.partitionBy(partitioner)) /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD. */ def join[W](other: JavaPairRDD[K, W], partitioner: Partitioner): JavaPairRDD[K, (V, W)] = fromRDD(rdd.join(other, partitioner)) /** * Perform a left outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (v, Some(w))) for w in `other`, or the * pair (k, (v, None)) if no elements in `other` have key k. Uses the given Partitioner to * partition the output RDD. */ def leftOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner) : JavaPairRDD[K, (V, Optional[W])] = { val joinResult = rdd.leftOuterJoin(other, partitioner) fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}) } /** * Perform a right outer join of `this` and `other`. For each element (k, w) in `other`, the * resulting RDD will either contain all pairs (k, (Some(v), w)) for v in `this`, or the * pair (k, (None, w)) if no elements in `this` have key k. Uses the given Partitioner to * partition the output RDD. */ def rightOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner) : JavaPairRDD[K, (Optional[V], W)] = { val joinResult = rdd.rightOuterJoin(other, partitioner) fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}) } /** * Perform a full outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in `other`, or * the pair (k, (Some(v), None)) if no elements in `other` have key k. Similarly, for each * element (k, w) in `other`, the resulting RDD will either contain all pairs * (k, (Some(v), Some(w))) for v in `this`, or the pair (k, (None, Some(w))) if no elements * in `this` have key k. Uses the given Partitioner to partition the output RDD. */ def fullOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner) : JavaPairRDD[K, (Optional[V], Optional[W])] = { val joinResult = rdd.fullOuterJoin(other, partitioner) fromRDD(joinResult.mapValues{ case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w)) }) } /** * Simplified version of combineByKey that hash-partitions the resulting RDD using the existing * partitioner/parallelism level and using map-side aggregation. */ def combineByKey[C](createCombiner: JFunction[V, C], mergeValue: JFunction2[C, V, C], mergeCombiners: JFunction2[C, C, C]): JavaPairRDD[K, C] = { implicit val ctag: ClassTag[C] = fakeClassTag fromRDD(combineByKey(createCombiner, mergeValue, mergeCombiners, defaultPartitioner(rdd))) } /** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a \"combiner\" in MapReduce. Output will be hash-partitioned with the existing partitioner/ * parallelism level. */ def reduceByKey(func: JFunction2[V, V, V]): JavaPairRDD[K, V] = { fromRDD(reduceByKey(defaultPartitioner(rdd), func)) } /** * Group the values for each key in the RDD into a single sequence. Hash-partitions the * resulting RDD with the existing partitioner/parallelism level. * * @note If you are grouping in order to perform an aggregation (such as a sum or average) over * each key, using `JavaPairRDD.reduceByKey` or `JavaPairRDD.combineByKey` * will provide much better performance. */ def groupByKey(): JavaPairRDD[K, JIterable[V]] = fromRDD(groupByResultToJava(rdd.groupByKey())) /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and * (k, v2) is in `other`. Performs a hash join across the cluster. */ def join[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (V, W)] = fromRDD(rdd.join(other)) /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and * (k, v2) is in `other`. Performs a hash join across the cluster. */ def join[W](other: JavaPairRDD[K, W], numPartitions: Int): JavaPairRDD[K, (V, W)] = fromRDD(rdd.join(other, numPartitions)) /** * Perform a left outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (v, Some(w))) for w in `other`, or the * pair (k, (v, None)) if no elements in `other` have key k. Hash-partitions the output * using the existing partitioner/parallelism level. */ def leftOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (V, Optional[W])] = { val joinResult = rdd.leftOuterJoin(other) fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}) } /** * Perform a left outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (v, Some(w))) for w in `other`, or the * pair (k, (v, None)) if no elements in `other` have key k. Hash-partitions the output * into `numPartitions` partitions. */ def leftOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int) : JavaPairRDD[K, (V, Optional[W])] = { val joinResult = rdd.leftOuterJoin(other, numPartitions) fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}) } /** * Perform a right outer join of `this` and `other`. For each element (k, w) in `other`, the * resulting RDD will either contain all pairs (k, (Some(v), w)) for v in `this`, or the * pair (k, (None, w)) if no elements in `this` have key k. Hash-partitions the resulting * RDD using the existing partitioner/parallelism level. */ def rightOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (Optional[V], W)] = { val joinResult = rdd.rightOuterJoin(other) fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}) } /** * Perform a right outer join of `this` and `other`. For each element (k, w) in `other`, the * resulting RDD will either contain all pairs (k, (Some(v), w)) for v in `this`, or the * pair (k, (None, w)) if no elements in `this` have key k. Hash-partitions the resulting * RDD into the given number of partitions. */ def rightOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int) : JavaPairRDD[K, (Optional[V], W)] = { val joinResult = rdd.rightOuterJoin(other, numPartitions) fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}) } /** * Perform a full outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in `other`, or * the pair (k, (Some(v), None)) if no elements in `other` have key k. Similarly, for each * element (k, w) in `other`, the resulting RDD will either contain all pairs * (k, (Some(v), Some(w))) for v in `this`, or the pair (k, (None, Some(w))) if no elements * in `this` have key k. Hash-partitions the resulting RDD using the existing partitioner/ * parallelism level. */ def fullOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (Optional[V], Optional[W])] = { val joinResult = rdd.fullOuterJoin(other) fromRDD(joinResult.mapValues{ case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w)) }) } /** * Perform a full outer join of `this` and `other`. For each element (k, v) in `this`, the * resulting RDD will either contain all pairs (k, (Some(v), Some(w))) for w in `other`, or * the pair (k, (Some(v), None)) if no elements in `other` have key k. Similarly, for each * element (k, w) in `other`, the resulting RDD will either contain all pairs * (k, (Some(v), Some(w))) for v in `this`, or the pair (k, (None, Some(w))) if no elements * in `this` have key k. Hash-partitions the resulting RDD into the given number of partitions. */ def fullOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int) : JavaPairRDD[K, (Optional[V], Optional[W])] = { val joinResult = rdd.fullOuterJoin(other, numPartitions) fromRDD(joinResult.mapValues{ case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w)) }) } /** * Return the key-value pairs in this RDD to the master as a Map. * * @note this method should only be used if the resulting data is expected to be small, as * all the data is loaded into the driver's memory. */ def collectAsMap(): java.util.Map[K, V] = mapAsSerializableJavaMap(rdd.collectAsMap()) /** * Pass each value in the key-value pair RDD through a map function without changing the keys; * this also retains the original RDD's partitioning. */ def mapValues[U](f: JFunction[V, U]): JavaPairRDD[K, U] = { implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.mapValues(f)) } /** * Pass each value in the key-value pair RDD through a flatMap function without changing the * keys; this also retains the original RDD's partitioning. */ def flatMapValues[U](f: FlatMapFunction[V, U]): JavaPairRDD[K, U] = { def fn: (V) => Iterator[U] = (x: V) => f.call(x).asScala implicit val ctag: ClassTag[U] = fakeClassTag fromRDD(rdd.flatMapValues(fn)) } /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. */ def cogroup[W](other: JavaPairRDD[K, W], partitioner: Partitioner) : JavaPairRDD[K, (JIterable[V], JIterable[W])] = fromRDD(cogroupResultToJava(rdd.cogroup(other, partitioner))) /** * For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a * tuple with the list of values for that key in `this`, `other1` and `other2`. */ def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], partitioner: Partitioner): JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2])] = fromRDD(cogroupResult2ToJava(rdd.cogroup(other1, other2, partitioner))) /** * For each key k in `this` or `other1` or `other2` or `other3`, * return a resulting RDD that contains a tuple with the list of values * for that key in `this`, `other1`, `other2` and `other3`. */ def cogroup[W1, W2, W3](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], other3: JavaPairRDD[K, W3], partitioner: Partitioner) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3])] = fromRDD(cogroupResult3ToJava(rdd.cogroup(other1, other2, other3, partitioner))) /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. */ def cogroup[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (JIterable[V], JIterable[W])] = fromRDD(cogroupResultToJava(rdd.cogroup(other))) /** * For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a * tuple with the list of values for that key in `this`, `other1` and `other2`. */ def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2]) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2])] = fromRDD(cogroupResult2ToJava(rdd.cogroup(other1, other2))) /** * For each key k in `this` or `other1` or `other2` or `other3`, * return a resulting RDD that contains a tuple with the list of values * for that key in `this`, `other1`, `other2` and `other3`. */ def cogroup[W1, W2, W3](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], other3: JavaPairRDD[K, W3]) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3])] = fromRDD(cogroupResult3ToJava(rdd.cogroup(other1, other2, other3))) /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. */ def cogroup[W](other: JavaPairRDD[K, W], numPartitions: Int) : JavaPairRDD[K, (JIterable[V], JIterable[W])] = fromRDD(cogroupResultToJava(rdd.cogroup(other, numPartitions))) /** * For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a * tuple with the list of values for that key in `this`, `other1` and `other2`. */ def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], numPartitions: Int) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2])] = fromRDD(cogroupResult2ToJava(rdd.cogroup(other1, other2, numPartitions))) /** * For each key k in `this` or `other1` or `other2` or `other3`, * return a resulting RDD that contains a tuple with the list of values * for that key in `this`, `other1`, `other2` and `other3`. */ def cogroup[W1, W2, W3](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], other3: JavaPairRDD[K, W3], numPartitions: Int) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3])] = fromRDD(cogroupResult3ToJava(rdd.cogroup(other1, other2, other3, numPartitions))) /** Alias for cogroup. */ def groupWith[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (JIterable[V], JIterable[W])] = fromRDD(cogroupResultToJava(rdd.groupWith(other))) /** Alias for cogroup. */ def groupWith[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2]) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2])] = fromRDD(cogroupResult2ToJava(rdd.groupWith(other1, other2))) /** Alias for cogroup. */ def groupWith[W1, W2, W3](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], other3: JavaPairRDD[K, W3]) : JavaPairRDD[K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3])] = fromRDD(cogroupResult3ToJava(rdd.groupWith(other1, other2, other3))) /** * Return the list of values in the RDD for key `key`. This operation is done efficiently if the * RDD has a known partitioner by only searching the partition that the key maps to. */ def lookup(key: K): JList[V] = rdd.lookup(key).asJava /** Output the RDD to any Hadoop-supported file system. */ def saveAsHadoopFile[F <: OutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F], conf: JobConf): Unit = { rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, conf) } /** Output the RDD to any Hadoop-supported file system. */ def saveAsHadoopFile[F <: OutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F]): Unit = { rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass) } /** Output the RDD to any Hadoop-supported file system, compressing with the supplied codec. */ def saveAsHadoopFile[F <: OutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F], codec: Class[_ <: CompressionCodec]): Unit = { rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, codec) } /** Output the RDD to any Hadoop-supported file system. */ def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F], conf: Configuration): Unit = { rdd.saveAsNewAPIHadoopFile(path, keyClass, valueClass, outputFormatClass, conf) } /** * Output the RDD to any Hadoop-supported storage system, using * a Configuration object for that storage system. */ def saveAsNewAPIHadoopDataset(conf: Configuration): Unit = { rdd.saveAsNewAPIHadoopDataset(conf) } /** Output the RDD to any Hadoop-supported file system. */ def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[F]): Unit = { rdd.saveAsNewAPIHadoopFile(path, keyClass, valueClass, outputFormatClass) } /** * Output the RDD to any Hadoop-supported storage system, using a Hadoop JobConf object for * that storage system. The JobConf should set an OutputFormat and any output paths required * (e.g. a table name to write to) in the same way as it would be configured for a Hadoop * MapReduce job. */ def saveAsHadoopDataset(conf: JobConf): Unit = { rdd.saveAsHadoopDataset(conf) } /** * Repartition the RDD according to the given partitioner and, within each resulting partition, * sort records by their keys. * * This is more efficient than calling `repartition` and then sorting within each partition * because it can push the sorting down into the shuffle machinery. */ def repartitionAndSortWithinPartitions(partitioner: Partitioner): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] repartitionAndSortWithinPartitions(partitioner, comp) } /** * Repartition the RDD according to the given partitioner and, within each resulting partition, * sort records by their keys. * * This is more efficient than calling `repartition` and then sorting within each partition * because it can push the sorting down into the shuffle machinery. */ def repartitionAndSortWithinPartitions(partitioner: Partitioner, comp: Comparator[K]) : JavaPairRDD[K, V] = { implicit val ordering = comp // Allow implicit conversion of Comparator to Ordering. fromRDD( new OrderedRDDFunctions[K, V, (K, V)](rdd).repartitionAndSortWithinPartitions(partitioner)) } /** * Sort the RDD by key, so that each partition contains a sorted range of the elements in * ascending order. Calling `collect` or `save` on the resulting RDD will return or output an * ordered list of records (in the `save` case, they will be written to multiple `part-X` files * in the filesystem, in order of the keys). */ def sortByKey(): JavaPairRDD[K, V] = sortByKey(true) /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(ascending: Boolean): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] sortByKey(comp, ascending) } /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(ascending: Boolean, numPartitions: Int): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] sortByKey(comp, ascending, numPartitions) } /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(comp: Comparator[K]): JavaPairRDD[K, V] = sortByKey(comp, true) /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(comp: Comparator[K], ascending: Boolean): JavaPairRDD[K, V] = { implicit val ordering = comp // Allow implicit conversion of Comparator to Ordering. fromRDD(new OrderedRDDFunctions[K, V, (K, V)](rdd).sortByKey(ascending)) } /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling * `collect` or `save` on the resulting RDD will return or output an ordered list of records * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in * order of the keys). */ def sortByKey(comp: Comparator[K], ascending: Boolean, numPartitions: Int): JavaPairRDD[K, V] = { implicit val ordering = comp // Allow implicit conversion of Comparator to Ordering. fromRDD(new OrderedRDDFunctions[K, V, (K, V)](rdd).sortByKey(ascending, numPartitions)) } /** * Return a RDD containing only the elements in the inclusive range `lower` to `upper`. * If the RDD has been partitioned using a `RangePartitioner`, then this operation can be * performed efficiently by only scanning the partitions that might contain matching elements. * Otherwise, a standard `filter` is applied to all partitions. * * @since 3.1.0 */ @Since(\"3.1.0\") def filterByRange(lower: K, upper: K): JavaPairRDD[K, V] = { val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]] filterByRange(comp, lower, upper) } /** * Return a RDD containing only the elements in the inclusive range `lower` to `upper`. * If the RDD has been partitioned using a `RangePartitioner`, then this operation can be * performed efficiently by only scanning the partitions that might contain matching elements. * Otherwise, a standard `filter` is applied to all partitions. * * @since 3.1.0 */ @Since(\"3.1.0\") def filterByRange(comp: Comparator[K], lower: K, upper: K): JavaPairRDD[K, V] = { implicit val ordering = comp // Allow implicit conversion of Comparator to Ordering. fromRDD(new OrderedRDDFunctions[K, V, (K, V)](rdd).filterByRange(lower, upper)) } /** * Return an RDD with the keys of each tuple. */ def keys(): JavaRDD[K] = JavaRDD.fromRDD[K](rdd.map(_._1)) /** * Return an RDD with the values of each tuple. */ def values(): JavaRDD[V] = JavaRDD.fromRDD[V](rdd.map(_._2)) /** * Return approximate number of distinct values for each key in this RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. * @param partitioner partitioner of the resulting RDD. */ def countApproxDistinctByKey(relativeSD: Double, partitioner: Partitioner) : JavaPairRDD[K, jl.Long] = { fromRDD(rdd.countApproxDistinctByKey(relativeSD, partitioner)). asInstanceOf[JavaPairRDD[K, jl.Long]] } /** * Return approximate number of distinct values for each key in this RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. * @param numPartitions number of partitions of the resulting RDD. */ def countApproxDistinctByKey(relativeSD: Double, numPartitions: Int): JavaPairRDD[K, jl.Long] = { fromRDD(rdd.countApproxDistinctByKey(relativeSD, numPartitions)). asInstanceOf[JavaPairRDD[K, jl.Long]] } /** * Return approximate number of distinct values for each key in this RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017. */ def countApproxDistinctByKey(relativeSD: Double): JavaPairRDD[K, jl.Long] = { fromRDD(rdd.countApproxDistinctByKey(relativeSD)).asInstanceOf[JavaPairRDD[K, jl.Long]] } /** Assign a name to this RDD */ def setName(name: String): JavaPairRDD[K, V] = { rdd.setName(name) this } } object JavaPairRDD { private[spark] def groupByResultToJava[K: ClassTag, T](rdd: RDD[(K, Iterable[T])]): RDD[(K, JIterable[T])] = { rddToPairRDDFunctions(rdd).mapValues(_.asJava) } private[spark] def cogroupResultToJava[K: ClassTag, V, W]( rdd: RDD[(K, (Iterable[V], Iterable[W]))]): RDD[(K, (JIterable[V], JIterable[W]))] = { rddToPairRDDFunctions(rdd).mapValues(x => (x._1.asJava, x._2.asJava)) } private[spark] def cogroupResult2ToJava[K: ClassTag, V, W1, W2]( rdd: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]) : RDD[(K, (JIterable[V], JIterable[W1], JIterable[W2]))] = { rddToPairRDDFunctions(rdd).mapValues(x => (x._1.asJava, x._2.asJava, x._3.asJava)) } private[spark] def cogroupResult3ToJava[K: ClassTag, V, W1, W2, W3]( rdd: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2], Iterable[W3]))]) : RDD[(K, (JIterable[V], JIterable[W1], JIterable[W2], JIterable[W3]))] = { rddToPairRDDFunctions(rdd).mapValues(x => (x._1.asJava, x._2.asJava, x._3.asJava, x._4.asJava)) } def fromRDD[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)]): JavaPairRDD[K, V] = { new JavaPairRDD[K, V](rdd) } implicit def toRDD[K, V](rdd: JavaPairRDD[K, V]): RDD[(K, V)] = rdd.rdd private[spark] implicit def toScalaFunction2[T1, T2, R](fun: JFunction2[T1, T2, R]): Function2[T1, T2, R] = { (x: T1, x1: T2) => fun.call(x, x1) } private[spark] implicit def toScalaFunction[T, R](fun: JFunction[T, R]): T => R = x => fun.call(x) private[spark] implicit def pairFunToScalaFun[A, B, C](x: PairFunction[A, B, C]): A => (B, C) = y => x.call(y) /** Convert a JavaRDD of key-value pairs to JavaPairRDD. */ def fromJavaRDD[K, V](rdd: JavaRDD[(K, V)]): JavaPairRDD[K, V] = { implicit val ctagK: ClassTag[K] = fakeClassTag implicit val ctagV: ClassTag[V] = fakeClassTag new JavaPairRDD[K, V](rdd.rdd) } }"
          ],
          "tokens": 798,
          "more_info": "Needed `return full answer in .md file`"
        },
        "without_mcp": {
          "answer": "# How this project manages memory allocation (Spark 3.3.0)\n\nThis repository contains Apache Spark 3.3.0. Spark manages memory using a layered design that separates responsibilities between cluster roles (driver/executors), storage vs. execution needs, and on-heap vs. off-heap allocation. Below is a concise but comprehensive overview tailored to this codebase.\n\n## High-level model\n- Process roles:\n  - Driver process: coordinates the application; also uses memory for planning, dataset metadata, broadcast creation, etc.\n  - Executor processes: run tasks and hold cached data. Most of Sparks managed memory logic applies at the executor level.\n- Memory types by usage:\n  - Execution memory: used while computing (joins, aggregations, sorts, shuffles, window ops). It is shortlived and spillable.\n  - Storage memory: used to cache/persist RDD partitions/DataFrame blocks and to hold broadcast variables.\n  - User & JVM memory: everything outside Sparks managed region (JVM overhead, UDFs, app code objects, thread stacks, code cache, etc.).\n- On-heap vs. off-heap:\n  - On-heap: standard Java objects inside the JVM heap; garbage collected by the JVM.\n  - Off-heap: native memory outside the heap, managed by Spark (Tungsten). Reduces GC pressure and can be faster for I/O and serializationfree formats.\n\n## Unified Memory Manager (default)\nSpark 3.3 uses UnifiedMemoryManager by default to manage executor memory.\n- Total managed region size:\n  - On-heap: spark.memory.fraction (default 0.6) of the executor heap is reserved for Sparks managed memory (both execution + storage). The remainder is \"user/JVM\" memory.\n  - Off-heap (optional): when spark.memory.offHeap.enabled=true, Spark uses a separate, fixed pool sized by spark.memory.offHeap.size for managed memory.\n- Dynamic sharing between execution and storage:\n  - spark.memory.storageFraction (default 0.5) sets the initial split between storage and execution inside the managed region.\n  - The split is elastic: execution can borrow from storage when storage is underutilized and vice versa. Storage borrowing can trigger eviction of cached blocks if space is needed.\n\n## Key components in the codebase\n- MemoryManager (core): Abstracts managed memory. UnifiedMemoryManager is the concrete default; StaticMemoryManager is legacy.\n- TaskMemoryManager (core): A per-task facade that handles fine-grained allocation requests for operators during a task. It allocates memory in fixed-size pages and tracks ownership so memory is reclaimed at task completion or on failure.\n- MemoryConsumer (core): Operators like sort, aggregate, and hash join implement MemoryConsumer to request/release pages from TaskMemoryManager and implement spilling when memory is scarce.\n- MemoryAllocator (core): Backing allocator for on-heap (byte arrays) or off-heap (direct ByteBuffers/native). Page size is controlled by spark.buffer.pageSize.\n- BlockManager (core): Orchestrates storage of blocks in memory and on disk across executors (and the driver). Includes:\n  - MemoryStore: in-JVM or off-heap cached blocks\n  - DiskStore: on-disk persistence and spill files\n  - BlockTransferService/Netty for network transfers\n- Shuffle subsystem (core): Uses execution memory to buffer map outputs and perform merges; spills to disk when necessary (e.g., ExternalSorter, UnsafeExternalSorter, ShuffleWriteProcessor).\n\n## Allocation and spilling during task execution\n- Operators request memory via TaskMemoryManager through MemoryConsumer.acquire(nBytes).\n- If enough space exists in the execution pool, pages are granted.\n- Under pressure, operators implement spill() to write intermediate data to disk and free pages. Sparks external algorithms ensure progress even with limited memory.\n- All per-task allocations are released when the task completes (success, failure, or kill), preventing leaks.\n\n## Storage memory and eviction\n- Caching (RDD/DataFrame persist) and broadcast variables occupy storage memory managed by BlockManager/MemoryStore.\n- When a new block needs storage space:\n  - Spark attempts to use free storage memory or borrow from execution if permissible.\n  - If insufficient, it evicts existing cached blocks based on an LRU-like policy, respecting storage levels and replication.\n- Unroll memory: temporary memory used to materialize a block before putting it in the cache; if unroll cannot be satisfied, the block is not cached and may be stored on disk depending on the chosen StorageLevel.\n\n## On-heap vs. off-heap details\n- On-heap managed memory lives inside the JVM heap and is subject to GC.\n- Off-heap managed memory (spark.memory.offHeap.enabled=true):\n  - Allocated from native memory; not directly visible to the JVM GC.\n  - Requires spark.memory.offHeap.size to be set.\n  - Used heavily by Tungsten/Unsafe operators to store encoded rows and shuffle data efficiently.\n\n## Important configuration knobs\n- Managed region sizing:\n  - spark.memory.fraction (default: 0.6)\n  - spark.memory.storageFraction (default: 0.5 of the managed region)\n  - spark.memory.offHeap.enabled (default: false)\n  - spark.memory.offHeap.size (bytes; required when off-heap is enabled)\n- Page and buffer sizes:\n  - spark.buffer.pageSize (default derived from JVM; commonly 64KB1MB depending on platform)\n  - spark.shuffle.file.buffer (default 32KB)  write-side buffer per shuffle output stream\n  - spark.reducer.maxSizeInFlight / spark.reducer.maxReqsInFlight  controls in-flight shuffle fetches (affects network & memory pressure)\n- Storage/caching behavior:\n  - spark.storage.unrollMemoryThreshold\n  - spark.storage.decommission.* (in newer versions) for graceful decommissioning\n- Networking:\n  - spark.network.io.preferDirectBufs (default: true)  uses direct (off-heap) Netty buffers for IO\n\n## Failure safety and cleanup\n- Per-task memory is released deterministically when tasks end.\n- Spilled files are cleaned up on task completion.\n- BlockManager removes evicted/obsolete blocks and handles replication bookkeeping.\n- Heartbeat + Executor/Task metrics report memory metrics used in the UI for observability.\n\n## Where to look in this repo\n- core/src/main/scala/org/apache/spark/memory/*  MemoryManager, UnifiedMemoryManager, TaskMemoryManager, MemoryConsumer, MemoryAllocator\n- core/src/main/scala/org/apache/spark/storage/*  BlockManager, MemoryStore, DiskStore, StorageLevel\n- core/src/main/scala/org/apache/spark/shuffle/*  shuffle IO and spilling logic\n- sql/core/... execution operators  joins, aggregations, sorts using managed memory\n\n## Summary\nSpark 3.3.0 manages memory with a unified, spill-aware design:\n- A managed memory region (on-heap and/or off-heap) is elastically shared between execution and storage.\n- Task-level page-based allocators provide fast, tracked allocations; operators must spill under pressure.\n- Cached data uses storage memory; eviction and on-disk persistence maintain forward progress.\n- Configuration flags let you tune the split, enable off-heap, and size buffers for workload characteristics.\n",
          "answer_time": 45,
          "tokens": 1509
        }
      },
       "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 3,
          "reasoning": "Answer A provides a comprehensive, well-structured explanation of memory management with clear architectural overview, detailed mechanisms for each component, and practical allocation patterns. Answer B is more fragmented, focuses heavily on code snippets and implementation details without sufficient explanation of the overall system design, and lacks the clarity and logical flow that makes A more accessible and complete."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 3,
          "reasoning": "Answer A provides a more complete and detailed analysis with specific implementation details, code patterns, and concrete examples from the codebase. It covers all major components with clear structure and actionable insights. Answer B, while well-structured conceptually, appears incomplete (cuts off mid-sentence) and provides more general information without the same depth of implementation-specific details that would be valuable for understanding how this particular project manages memory allocation."
        }
      }
    },
    {
      "id": "Q009",
      "question": "What is the architecture for logging and error reporting?",
      "category": "general",
      "parameters": {
        "kinds": [
          "CLASS",
          "METHOD",
          "TRAIT"
        ],
        "keywords": [
          "logging",
          "info",
          "debug",
          "error",
          "exception",
          "trace",
          "throwable",
          "logger",
          "throw",
          "message"
        ],
        "top_nodes": 8,
        "max_neighbors": 4
      },
      "ground_truth_contexts": [
        "import org.apache.spark.internal.config._ import org.apache.spark.network.util.JavaUtils import org.apache.spark.util.{ThreadUtils, Utils} private[spark] class DriverLogger(conf: SparkConf) extends Logging { private val UPLOAD_CHUNK_SIZE = 1024 * 1024 private val UPLOAD_INTERVAL_IN_SECS = 5 private val DEFAULT_LAYOUT = \"%d{yy/MM/dd HH:mm:ss.SSS} %t %p %c{1}: %m%n%ex\" private val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort) private val localLogFile: String = FileUtils.getFile( Utils.getLocalDir(conf), DriverLogger.DRIVER_LOG_DIR, DriverLogger.DRIVER_LOG_FILE).getAbsolutePath() private var writer: Option[DfsAsyncWriter] = None addLogAppender() private def addLogAppender(): Unit = { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val layout = if (conf.contains(DRIVER_LOG_LAYOUT)) { PatternLayout.newBuilder().withPattern(conf.get(DRIVER_LOG_LAYOUT).get).build() } else { PatternLayout.newBuilder().withPattern(DEFAULT_LAYOUT).build() } val config = logger.getContext.getConfiguration() def log4jFileAppender() = { // SPARK-37853: We can't use the chained API invocation mode because // `AbstractFilterable.Builder.asBuilder()` method will return `Any` in Scala. val builder: Log4jFileAppender.Builder[_] = Log4jFileAppender.newBuilder() builder.withAppend(false) builder.withBufferedIo(false) builder.setConfiguration(config) builder.withFileName(localLogFile) builder.setIgnoreExceptions(false) builder.setLayout(layout) builder.setName(DriverLogger.APPENDER_NAME) builder.build() } val fa = log4jFileAppender() logger.addAppender(fa) fa.start() logInfo(s\"Added a local log appender at: $localLogFile\") } def startSync(hadoopConf: Configuration): Unit = { try { // Setup a writer which moves the local file to hdfs continuously val appId = Utils.sanitizeDirName(conf.getAppId) writer = Some(new DfsAsyncWriter(appId, hadoopConf)) } catch { case e: Exception => logError(s\"Could not persist driver logs to dfs\", e) } } def stop(): Unit = { try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
        "try { val logger = LogManager.getRootLogger().asInstanceOf[Logger] val fa = logger.getAppenders.get(DriverLogger.APPENDER_NAME) logger.removeAppender(fa) Utils.tryLogNonFatalError(fa.stop()) writer.foreach(_.closeWriter()) } catch { case e: Exception => logError(s\"Error in persisting driver logs\", e) } finally { Utils.tryLogNonFatalError { JavaUtils.deleteRecursively(FileUtils.getFile(localLogFile).getParentFile()) } } } // Visible for testing private[spark] class DfsAsyncWriter(appId: String, hadoopConf: Configuration) extends Runnable with Logging { private var streamClosed = false private var inStream: InputStream = null private var outputStream: FSDataOutputStream = null private val tmpBuffer = new Array[Byte](UPLOAD_CHUNK_SIZE) private var threadpool: ScheduledExecutorService = _ init() private def init(): Unit = { val rootDir = conf.get(DRIVER_LOG_DFS_DIR).get val fileSystem: FileSystem = new Path(rootDir).getFileSystem(hadoopConf) if (!fileSystem.exists(new Path(rootDir))) { throw new RuntimeException(s\"${rootDir} does not exist.\" + s\" Please create this dir in order to persist driver logs\") } val dfsLogFile: String = FileUtils.getFile(rootDir, appId + DriverLogger.DRIVER_LOG_FILE_SUFFIX).getAbsolutePath() try { inStream = new BufferedInputStream(new FileInputStream(localLogFile)) outputStream = SparkHadoopUtil.createFile(fileSystem, new Path(dfsLogFile), conf.get(DRIVER_LOG_ALLOW_EC)) fileSystem.setPermission(new Path(dfsLogFile), LOG_FILE_PERMISSIONS) } catch { case e: Exception => JavaUtils.closeQuietly(inStream) JavaUtils.closeQuietly(outputStream) throw e } threadpool = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"dfsSyncThread\") threadpool.scheduleWithFixedDelay(this, UPLOAD_INTERVAL_IN_SECS, UPLOAD_INTERVAL_IN_SECS, TimeUnit.SECONDS) logInfo(s\"Started driver log file sync to: ${dfsLogFile}\") } def run(): Unit = { if (streamClosed) { return } try { var remaining = inStream.available() val hadData = remaining > 0 while (remaining > 0) { val read = inStream.read(tmpBuffer, 0, math.min(remaining, UPLOAD_CHUNK_SIZE)) outputStream.write(tmpBuffer, 0, read) remaining -= read } if (hadData) { outputStream match { case hdfsStream: HdfsDataOutputStream => hdfsStream.hsync(EnumSet.allOf(classOf[HdfsDataOutputStream.SyncFlag])) case other => other.hflush() } } } catch { case e: Exception => logError(\"Failed writing driver logs to dfs\", e) } } private def close(): Unit = { if (streamClosed) { return } try { // Write all remaining bytes run() } finally { try { streamClosed = true inStream.close() outputStream.close() } catch { case e: Exception => logError(\"Error in closing driver log input/output stream\", e) } } } def closeWriter(): Unit = { try { threadpool.execute(() => DfsAsyncWriter.this.close()) threadpool.shutdown() threadpool.awaitTermination(1, TimeUnit.MINUTES) } catch { case e: Exception => logError(\"Error in shutting down threadpool\", e) } } } } private[spark] object DriverLogger extends Logging { val DRIVER_LOG_DIR = \"__driver_logs__\" val DRIVER_LOG_FILE = \"driver.log\" val DRIVER_LOG_FILE_SUFFIX = \"_\" + DRIVER_LOG_FILE val APPENDER_NAME = \"_DriverLogAppender\" def apply(conf: SparkConf): Option[DriverLogger] = { if (conf.get(DRIVER_LOG_PERSISTTODFS) && Utils.isClientMode(conf)) { if (conf.contains(DRIVER_LOG_DFS_DIR)) { try { Some(new DriverLogger(conf)) } catch { case e: Exception => logError(\"Could not add driver logger\", e) None } } else { logWarning(s\"Driver logs are not persisted because\" + s\" ${DRIVER_LOG_DFS_DIR.key} is not configured\") None } } else { None } } }",
        "*/ private[spark] case class ErrorInfo(message: Seq[String], sqlState: Option[String]) { // For compatibility with multi-line error messages @JsonIgnore val messageFormat: String = message.mkString(\"\\n\") } /** * Companion object used by instances of [[SparkThrowable]] to access error class information and * construct error messages. */ private[spark] object SparkThrowableHelper { val errorClassesUrl: URL = Utils.getSparkClassLoader.getResource(\"error/error-classes.json\") val errorClassToInfoMap: SortedMap[String, ErrorInfo] = { val mapper: JsonMapper = JsonMapper.builder() .addModule(DefaultScalaModule) .build() mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {}) } def getMessage( errorClass: String, messageParameters: Array[String], queryContext: String = \"\"): String = { val errorInfo = errorClassToInfoMap.getOrElse(errorClass, throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\")) val displayQueryContext = if (queryContext.isEmpty) { \"\" } else { s\"\\n$queryContext\" } String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"), messageParameters: _*) + displayQueryContext } def getSqlState(errorClass: String): String = { Option(errorClass).flatMap(errorClassToInfoMap.get).flatMap(_.sqlState).orNull } def isInternalError(errorClass: String): Boolean = { errorClass == \"INTERNAL_ERROR\" } }",
        "private[streaming] case class ReceiverErrorInfo( lastErrorMessage: String = \"\", lastError: String = \"\", lastErrorTime: Long = -1L) /** * Class having information about a receiver. * * @param receiverId the unique receiver id * @param state the current Receiver state * @param scheduledLocations the scheduled locations provided by ReceiverSchedulingPolicy * @param runningExecutor the running executor if the receiver is active * @param name the receiver name * @param endpoint the receiver endpoint. It can be used to send messages to the receiver * @param errorInfo the receiver error information if it fails */ private[streaming] case class ReceiverTrackingInfo( receiverId: Int, state: ReceiverState, scheduledLocations: Option[Seq[TaskLocation]], runningExecutor: Option[ExecutorCacheTaskLocation], name: Option[String] = None, endpoint: Option[RpcEndpointRef] = None, errorInfo: Option[ReceiverErrorInfo] = None) { def toReceiverInfo: ReceiverInfo = ReceiverInfo( receiverId, name.getOrElse(\"\"), state == ReceiverState.ACTIVE, location = runningExecutor.map(_.host).getOrElse(\"\"), executorId = runningExecutor.map(_.executorId).getOrElse(\"\"), lastErrorMessage = errorInfo.map(_.lastErrorMessage).getOrElse(\"\"), lastError = errorInfo.map(_.lastError).getOrElse(\"\"), lastErrorTime = errorInfo.map(_.lastErrorTime).getOrElse(-1L) ) }",
        ".build() mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {}) } def getMessage( errorClass: String, messageParameters: Array[String], queryContext: String = \"\"): String = { val errorInfo = errorClassToInfoMap.getOrElse(errorClass, throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\")) val displayQueryContext = if (queryContext.isEmpty) { \"\" } else { s\"\\n$queryContext\" } String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"), messageParameters: _*) + displayQueryContext } def getSqlState(errorClass: String): String = { Option(errorClass).flatMap(errorClassToInfoMap.get).flatMap(_.sqlState).orNull } def isInternalError(errorClass: String): Boolean = { errorClass == \"INTERNAL_ERROR\" } }"
      ],
      "key_entities": [
        "org/apache/spark/util/logging/DriverLogger#",
        "org/apache/spark/util/logging/DriverLogger#DfsAsyncWriter#",
        "org/apache/spark/ErrorInfo#",
        "org/apache/spark/streaming/scheduler/ReceiverErrorInfo#",
        "local2:core/src/main/scala/org/apache/spark/ErrorInfo.scala"
      ],
      "claude_stats": {
        "answer": "Based on the code analysis, I can describe the logging and error reporting architecture:\n\n## Logging Architecture\n\nThe codebase uses **SLF4J-based logging** through a `Logging` trait that provides standard logging functionality:\n\n**Core Components:**\n1. **Logging Trait** - Mixed into classes that need logging capabilities (like `SparkContext`, `SparkSession`, `LogicalPlan`)\n2. **Log Levels** - Support for multiple levels: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n3. **Dynamic Configuration** - Log levels can be changed at runtime via `SparkContext.setLogLevel()`\n\n**Key Implementation Details:**\n```scala\n// From SparkContext\ndef setLogLevel(logLevel: String): Unit = {\n  val upperCased = logLevel.toUpperCase(Locale.ROOT)\n  require(SparkContext.VALID_LOG_LEVELS.contains(upperCased), ...)\n  Utils.setLogLevel(Level.toLevel(upperCased))\n}\n```\n\n## Error Reporting Architecture\n\nThe system uses a **structured error handling approach** with specialized exception types:\n\n**Exception Hierarchy:**\n1. **SparkException** - Base exception for general Spark errors\n2. **SparkRuntimeException** - Runtime errors with error classes\n3. **SparkSQLException** - SQL-specific errors\n4. **SparkArithmeticException** - Arithmetic/overflow errors\n5. **SparkDateTimeException** - Date/time parsing errors\n6. **SparkUpgradeException** - Version compatibility issues\n\n**Error Reporting Features:**\n- **Error Classes**: Structured error codes for categorization (e.g., \"INVALID_PARAMETER_VALUE\")\n- **Message Parameters**: Parameterized error messages for i18n support\n- **Query Context**: Includes SQL context information for parse errors\n- **Stack Traces**: Full exception chains preserved\n\n**QueryExecutionErrors Object:**\nContains 200+ specialized error factory methods for different failure scenarios:\n- Parse errors\n- Type conversion errors\n- Invalid operations\n- Resource constraints\n- Data validation failures\n\n",
        "prompt_time": 19.248,
        "answer_time": 49,
        "context_tokens": 91676,
        "tokens": 490,
        "used_context": [
          "## CLASS: org/apache/spark/sql/catalyst/plans/logical/LogicalPlan#\nabstract class LogicalPlan extends QueryPlan[LogicalPlan] with AnalysisHelper with LogicalPlanStats with LogicalPlanDistinctKeys with QueryPlanConstraints with Logging { /** * Metadata fields that can be projected from this node. * Should be overridden if the plan does not propagate its children's output.  def metadataOutput: Seq[Attribute] = children.flatMap(_.metadataOutput) /** Returns true if this subtree has data from a streaming data source.  def isStreaming: Boolean = _isStreaming private[this] lazy val _isStreaming = children.exists(_.isStreaming) override def verboseStringWithSuffix(maxFields: Int): String = { super.verboseString(maxFields) + statsCache.map(\", \" + _.toString).getOrElse(\"\") } /** * Returns the maximum number of rows that this plan may compute. * * Any operator that a Limit can be pushed passed should override this function (e.g., Union). * Any operator that can push through a Limit should override this function (e.g., Project).  def maxRows: Option[Long] = None /** * Returns the maximum number of rows this plan may compute on each partition.  def maxRowsPerPartition: Option[Long] = maxRows /** * Returns true if this expression and all its children have been resolved to a specific schema * and false if it still contains any unresolved placeholders. Implementations of LogicalPlan * can override this (e.g. * [[org.apache.spark.sql.catalyst.analysis.UnresolvedRelation UnresolvedRelation]] * should return `false`).  lazy val resolved: Boolean = expressions.forall(_.resolved) && childrenResolved override protected def statePrefix = if (!resolved) \"'\" else super.statePrefix /** * Returns true if all its children of this query plan have been resolved.  def childrenResolved: Boolean = children.forall(_.resolved) /** * Resolves a given schema to concrete [[Attribute]] references in this query plan. This function * should only be called on analyzed plans since it will throw [[AnalysisException]] for * unresolved [[Attribute]]s.  def resolve(schema: StructType, resolver: Resolver): Seq[Attribute] = { schema.map { field => resolve(field.name :: Nil, resolver).map { case a: AttributeReference => a case _ => throw QueryExecutionErrors.resolveCannotHandleNestedSchema(this) }.getOrElse { throw QueryCompilationErrors.cannotResolveAttributeError( field.name, output.map(_.name).mkString(\", \")) } } } private[this] lazy val childAttributes = AttributeSeq(children.flatMap(_.output)) private[this] lazy val childMetadataAttributes = AttributeSeq(children.flatMap(_.metadataOutput)) private[this] lazy val outputAttributes = AttributeSeq(output) private[this] lazy val outputMetadataAttributes = AttributeSeq(metadataOutput) /** * Optionally resolves the given strings to a [[NamedExpression]] using the input from all child * nodes of this LogicalPlan. The attribute is expressed as * string in the following form: `[scope].AttributeName.[nested].[fields]...`.  def resolveChildren( nameParts: Seq[String], resolver: Resolver): Option[NamedExpression] = childAttributes.resolve(nameParts, resolver) .orElse(childMetadataAttributes.resolve(nameParts, resolver)) /** * Optionally resolves the given strings to a [[NamedExpression]] based on the output of this * LogicalPlan. The attribute is expressed as string in the following form: * `[scope].AttributeName.[nested].[fields]...`.  def resolve( nameParts: Seq[String], resolver: Resolver): Option[NamedExpression] = outputAttributes.resolve(nameParts, resolver) .orElse(outputMetadataAttributes.resolve(nameParts, resolver)) /** * Given an attribute name, split it to name parts by dot, but * don't split the name parts quoted by backticks, for example, * `ab.cd`.`efg` should be split into two parts \"ab.cd\" and \"efg\".  def resolveQuoted( name: String, resolver: Resolver): Option[NamedExpression] = { resolve(UnresolvedAttribute.parseAttributeName(name), resolver) } /** * Refreshes (or invalidates) any metadata/data cached in the plan recursively.  def refresh(): Unit = children.foreach(_.refresh()) /** * Returns the output ordering that this plan generates.  def outputOrdering: Seq[SortOrder] = Nil /** * Returns true iff `other`'s output is semantically the same, i.e.: * - it contains the same number of `Attribute`s; * - references are the same; * - the order is equal too.  def sameOutput(other: LogicalPlan): Boolean = { val thisOutput = this.output val otherOutput = other.output thisOutput.length == otherOutput.length && thisOutput.zip(otherOutput).forall { case (a1, a2) => a1.semanticEquals(a2) } } } /** * A logical plan node with no children.  trait LeafNode extends LogicalPlan with LeafLike[LogicalPlan] { override def producedAttributes: AttributeSet = outputSet /** Leaf nodes that can survive analysis must define their own statistics.  def computeStats(): Statistics = throw new UnsupportedOperationException } /** * A logical plan node with single child.  trait UnaryNode extends LogicalPlan with UnaryLike[LogicalPlan] { /** * Generates all valid constraints including an set of aliased constraints by replacing the * original constraint expressions with the corresponding alias  protected def getAllValidConstraints(projectList: Seq[NamedExpression]): ExpressionSet = { var allConstraints = child.constraints projectList.foreach { case a @ Alias(l: Literal, _) => allConstraints += EqualNullSafe(a.toAttribute, l) case a @ Alias(e, _) if e.deterministic => // For every alias in `projectList`, replace the reference in constraints by its attribute. allConstraints ++= allConstraints.map(_ transform { case expr: Expression if expr.semanticEquals(e) => a.toAttribute }) allConstraints += EqualNullSafe(e, a.toAttribute) case _ => // Don't change. } allConstraints } override protected lazy val validConstraints: ExpressionSet = child.constraints } /** * A logical plan node with a left and right child.  trait BinaryNode extends LogicalPlan with BinaryLike[LogicalPlan] abstract class OrderPreservingUnaryNode extends UnaryNode { override final def outputOrdering: Seq[SortOrder] = child.outputOrdering } object LogicalPlanIntegrity { private def canGetOutputAttrs(p: LogicalPlan): Boolean = { p.resolved && !p.expressions.exists { e => e.exists { // We cannot call `output` in plans with a `ScalarSubquery` expr having no column, // so, we filter out them in advance. case s: ScalarSubquery => s.plan.schema.fields.isEmpty case _ => false } } } /** * Since some logical plans (e.g., `Union`) can build `AttributeReference`s in their `output`, * this method checks if the same `ExprId` refers to attributes having the same data type * in plan output.  def hasUniqueExprIdsForOutput(plan: LogicalPlan): Boolean = { val exprIds = plan.collect { case p if canGetOutputAttrs(p) => // NOTE: we still need to filter resolved expressions here because the output of // some resolved logical plans can have unresolved references, // e.g., outer references in `ExistenceJoin`. p.output.filter(_.resolved).map { a => (a.exprId, a.dataType.asNullable) } }.flatten val ignoredExprIds = plan.collect { // NOTE: `Union` currently reuses input `ExprId`s for output references, but we cannot // simply modify the code for assigning new `ExprId`s in `Union#output` because // the modification will make breaking changes (See SPARK-32741(#29585)). // So, this check just ignores the `exprId`s of `Union` output. case u: Union if u.resolved => u.output.map(_.exprId) }.flatten.toSet val groupedDataTypesByExprId = exprIds.filterNot { case (exprId, _) => ignoredExprIds.contains(exprId) }.groupBy(_._1).values.map(_.distinct) groupedDataTypesByExprId.forall(_.length == 1) } /** * This method checks if reference `ExprId`s are not reused when assigning a new `ExprId`. * For example, it returns false if plan transformers create an alias having the same `ExprId` * with one of reference attributes, e.g., `a#1 + 1 AS a#1`.  def checkIfSameExprIdNotReused(plan: LogicalPlan): Boolean = { plan.collect { case p if p.resolved => p.expressions.forall { case a: Alias => // Even if a plan is resolved, `a.references` can return unresolved references, // e.g., in `Grouping`/`GroupingID`, so we need to filter out them and // check if the same `exprId` in `Alias` does not exist // among reference `exprId`s. !a.references.filter(_.resolved).map(_.exprId).exists(_ == a.exprId) case _ => true } }.forall(identity) } /** * This method checks if the same `ExprId` refers to an unique attribute in a plan tree. * Some plan transformers (e.g., `RemoveNoopOperators`) rewrite logical * plans based on this assumption.  def checkIfExprIdsAreGloballyUnique(plan: LogicalPlan): Boolean = { checkIfSameExprIdNotReused(plan) && hasUniqueExprIdsForOutput(plan) } } /** * A logical plan node that can generate metadata columns  trait ExposesMetadataColumns extends LogicalPlan { def withMetadataColumns(): LogicalPlan }",
          "## OBJECT: org/apache/spark/sql/errors/QueryExecutionErrors.\n private[sql] object QueryExecutionErrors extends QueryErrorsBase { def cannotEvaluateExpressionError(expression: Expression): Throwable = { new SparkUnsupportedOperationException(errorClass = \"INTERNAL_ERROR\", messageParameters = Array(s\"Cannot evaluate expression: $expression\")) } def cannotGenerateCodeForExpressionError(expression: Expression): Throwable = { new SparkUnsupportedOperationException(errorClass = \"INTERNAL_ERROR\", messageParameters = Array(s\"Cannot generate code for expression: $expression\")) } def cannotTerminateGeneratorError(generator: UnresolvedGenerator): Throwable = { new SparkUnsupportedOperationException(errorClass = \"INTERNAL_ERROR\", messageParameters = Array(s\"Cannot terminate expression: $generator\")) } def castingCauseOverflowError(t: Any, from: DataType, to: DataType): ArithmeticException = { new SparkArithmeticException( errorClass = \"CAST_OVERFLOW\", messageParameters = Array( toSQLValue(t, from), toSQLType(from), toSQLType(to), toSQLConf(SQLConf.ANSI_ENABLED.key))) } def cannotChangeDecimalPrecisionError( value: Decimal, decimalPrecision: Int, decimalScale: Int, context: String): ArithmeticException = { new SparkArithmeticException( errorClass = \"CANNOT_CHANGE_DECIMAL_PRECISION\", messageParameters = Array( value.toDebugString, decimalPrecision.toString, decimalScale.toString, toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = context) } def invalidInputInCastToDatetimeError( value: Any, from: DataType, to: DataType, errorContext: String): Throwable = { new SparkDateTimeException( errorClass = \"CAST_INVALID_INPUT\", messageParameters = Array( toSQLValue(value, from), toSQLType(from), toSQLType(to), toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = errorContext) } def invalidInputSyntaxForBooleanError( s: UTF8String, errorContext: String): SparkRuntimeException = { new SparkRuntimeException( errorClass = \"CAST_INVALID_INPUT\", messageParameters = Array( toSQLValue(s, StringType), toSQLType(StringType), toSQLType(BooleanType), toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = errorContext) } def invalidInputInCastToNumberError( to: DataType, s: UTF8String, errorContext: String): SparkNumberFormatException = { new SparkNumberFormatException( errorClass = \"CAST_INVALID_INPUT\", messageParameters = Array( toSQLValue(s, StringType), toSQLType(StringType), toSQLType(to), toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = errorContext) } def cannotCastFromNullTypeError(to: DataType): Throwable = { new SparkException(errorClass = \"CANNOT_CAST_DATATYPE\", messageParameters = Array(NullType.typeName, to.typeName), null) } def cannotCastError(from: DataType, to: DataType): Throwable = { new SparkException(errorClass = \"CANNOT_CAST_DATATYPE\", messageParameters = Array(from.typeName, to.typeName), null) } def cannotParseDecimalError(): Throwable = { new SparkRuntimeException( errorClass = \"CANNOT_PARSE_DECIMAL\", messageParameters = Array.empty) } def dataTypeUnsupportedError(dataType: String, failure: String): Throwable = { new SparkIllegalArgumentException(errorClass = \"UNSUPPORTED_DATATYPE\", messageParameters = Array(dataType + failure)) } def failedExecuteUserDefinedFunctionError(funcCls: String, inputTypes: String, outputType: String, e: Throwable): Throwable = { new SparkException(errorClass = \"FAILED_EXECUTE_UDF\", messageParameters = Array(funcCls, inputTypes, outputType), e) } def divideByZeroError(context: String): ArithmeticException = { new SparkArithmeticException( errorClass = \"DIVIDE_BY_ZERO\", messageParameters = Array(toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = context) } def invalidArrayIndexError(index: Int, numElements: Int): ArrayIndexOutOfBoundsException = { invalidArrayIndexErrorInternal(index, numElements, SQLConf.ANSI_ENABLED.key) } def invalidInputIndexError(index: Int, numElements: Int): ArrayIndexOutOfBoundsException = { invalidArrayIndexErrorInternal(index, numElements, SQLConf.ANSI_ENABLED.key) } private def invalidArrayIndexErrorInternal( index: Int, numElements: Int, key: String): ArrayIndexOutOfBoundsException = { new SparkArrayIndexOutOfBoundsException( errorClass = \"INVALID_ARRAY_INDEX\", messageParameters = Array( toSQLValue(index, IntegerType), toSQLValue(numElements, IntegerType), toSQLConf(key))) } def invalidElementAtIndexError( index: Int, numElements: Int): ArrayIndexOutOfBoundsException = { new SparkArrayIndexOutOfBoundsException( errorClass = \"INVALID_ARRAY_INDEX_IN_ELEMENT_AT\", messageParameters = Array( toSQLValue(index, IntegerType), toSQLValue(numElements, IntegerType), toSQLConf(SQLConf.ANSI_ENABLED.key))) } def mapKeyNotExistError(key: Any, dataType: DataType, context: String): NoSuchElementException = { new SparkNoSuchElementException( errorClass = \"MAP_KEY_DOES_NOT_EXIST\", messageParameters = Array( toSQLValue(key, dataType), toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = context) } def inputTypeUnsupportedError(dataType: DataType): Throwable = { new IllegalArgumentException(s\"Unsupported input type ${dataType.catalogString}\") } def invalidFractionOfSecondError(): DateTimeException = { new SparkDateTimeException( errorClass = \"INVALID_FRACTION_OF_SECOND\", Array(toSQLConf(SQLConf.ANSI_ENABLED.key))) } def ansiDateTimeParseError(e: DateTimeParseException): DateTimeParseException = { val newMessage = s\"${e.getMessage}. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" new DateTimeParseException(newMessage, e.getParsedString, e.getErrorIndex, e.getCause) } def ansiDateTimeError(e: DateTimeException): DateTimeException = { val newMessage = s\"${e.getMessage}. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" new DateTimeException(newMessage, e.getCause) } def ansiParseError(e: JavaParseException): JavaParseException = { val newMessage = s\"${e.getMessage}. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" new JavaParseException(newMessage, e.getErrorOffset) } def ansiIllegalArgumentError(message: String): IllegalArgumentException = { val newMessage = s\"$message. If necessary set ${SQLConf.ANSI_ENABLED.key} \" + s\"to false to bypass this error.\" new IllegalArgumentException(newMessage) } def ansiIllegalArgumentError(e: IllegalArgumentException): IllegalArgumentException = { ansiIllegalArgumentError(e.getMessage) } def overflowInSumOfDecimalError(context: String): ArithmeticException = { arithmeticOverflowError(\"Overflow in sum of decimals\", errorContext = context) } def overflowInIntegralDivideError(context: String): ArithmeticException = { arithmeticOverflowError(\"Overflow in integral divide\", \"try_divide\", context) } def mapSizeExceedArraySizeWhenZipMapError(size: Int): RuntimeException = { new RuntimeException(s\"Unsuccessful try to zip maps with $size \" + \"unique keys due to exceeding the array size limit \" + s\"${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}.\") } def copyNullFieldNotAllowedError(): Throwable = { new IllegalStateException(\"Do not attempt to copy a null field\") } def literalTypeUnsupportedError(v: Any): RuntimeException = { new SparkRuntimeException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(s\"literal for '${v.toString}' of ${v.getClass.toString}.\")) } def pivotColumnUnsupportedError(v: Any, dataType: DataType): RuntimeException = { new SparkRuntimeException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array( s\"pivoting by the value '${v.toString}' of the column data type ${toSQLType(dataType)}.\")) } def noDefaultForDataTypeError(dataType: DataType): RuntimeException = { new RuntimeException(s\"no default for type $dataType\") } def doGenCodeOfAliasShouldNotBeCalledError(): Throwable = { new IllegalStateException(\"Alias.doGenCode should not be called.\") } def orderedOperationUnsupportedByDataTypeError(dataType: DataType): Throwable = { new IllegalArgumentException(s\"Type $dataType does not support ordered operations\") } def regexGroupIndexLessThanZeroError(): Throwable = { new IllegalArgumentException(\"The specified group index cannot be less than zero\") } def regexGroupIndexExceedGroupCountError( groupCount: Int, groupIndex: Int): Throwable = { new IllegalArgumentException( s\"Regex group count is $groupCount, but the specified group index is $groupIndex\") } def invalidUrlError(url: UTF8String, e: URISyntaxException): Throwable = { new IllegalArgumentException(s\"Find an invalid url string ${url.toString}. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\", e) } def dataTypeOperationUnsupportedError(): Throwable = { new UnsupportedOperationException(\"dataType\") } def mergeUnsupportedByWindowFunctionError(): Throwable = { new UnsupportedOperationException(\"Window Functions do not support merging.\") } def dataTypeUnexpectedError(dataType: DataType): Throwable = { new UnsupportedOperationException(s\"Unexpected data type ${dataType.catalogString}\") } def typeUnsupportedError(dataType: DataType): Throwable = { new IllegalArgumentException(s\"Unexpected type $dataType\") } def negativeValueUnexpectedError(frequencyExpression : Expression): Throwable = { new SparkException(s\"Negative values found in ${frequencyExpression.sql}\") } def addNewFunctionMismatchedWithFunctionError(funcName: String): Throwable = { new IllegalArgumentException(s\"$funcName is not matched at addNewFunction\") } def cannotGenerateCodeForUncomparableTypeError( codeType: String, dataType: DataType): Throwable = { new IllegalArgumentException( s\"cannot generate $codeType code for un-comparable type: ${dataType.catalogString}\") } def cannotGenerateCodeForUnsupportedTypeError(dataType: DataType): Throwable = { new IllegalArgumentException(s\"cannot generate code for unsupported type: $dataType\") } def cannotInterpolateClassIntoCodeBlockError(arg: Any): Throwable = { new IllegalArgumentException( s\"Can not interpolate ${arg.getClass.getName} into code block.\") } def customCollectionClsNotResolvedError(): Throwable = { new UnsupportedOperationException(\"not resolved\") } def classUnsupportedByMapObjectsError(cls: Class[_]): RuntimeException = { new RuntimeException(s\"class `${cls.getName}` is not supported by `MapObjects` as \" + \"resulting collection.\") } def nullAsMapKeyNotAllowedError(): RuntimeException = { new RuntimeException(\"Cannot use null as map key!\") } def methodNotDeclaredError(name: String): Throwable = { new SparkNoSuchMethodException(errorClass = \"INTERNAL_ERROR\", messageParameters = Array( s\"\"\"A method named \"$name\" is not declared in any enclosing class nor any supertype\"\"\")) } def constructorNotFoundError(cls: String): Throwable = { new RuntimeException(s\"Couldn't find a valid constructor on $cls\") } def primaryConstructorNotFoundError(cls: Class[_]): Throwable = { new RuntimeException(s\"Couldn't find a primary constructor on $cls\") } def unsupportedNaturalJoinTypeError(joinType: JoinType): Throwable = { new RuntimeException(\"Unsupported natural join type \" + joinType) } def notExpectedUnresolvedEncoderError(attr: AttributeReference): Throwable = { new RuntimeException(s\"Unresolved encoder expected, but $attr was found.\") } def unsupportedEncoderError(): Throwable = { new RuntimeException(\"Only expression encoders are supported for now.\") } def notOverrideExpectedMethodsError(className: String, m1: String, m2: String): Throwable = { new RuntimeException(s\"$className must override either $m1 or $m2\") } def failToConvertValueToJsonError(value: AnyRef, cls: Class[_], dataType: DataType): Throwable = { new RuntimeException(s\"Failed to convert value $value (class of $cls) \" + s\"with the type of $dataType to JSON.\") } def unexpectedOperatorInCorrelatedSubquery(op: LogicalPlan, pos: String = \"\"): Throwable = { new RuntimeException(s\"Unexpected operator $op in correlated subquery\" + pos) } def unreachableError(err: String = \"\"): Throwable = { new RuntimeException(\"This line should be unreachable\" + err) } def unsupportedRoundingMode(roundMode: BigDecimal.RoundingMode.Value): Throwable = { new RuntimeException(s\"Not supported rounding mode: $roundMode\") } def resolveCannotHandleNestedSchema(plan: LogicalPlan): Throwable = { new RuntimeException(s\"Can not handle nested schema yet... plan $plan\") } def inputExternalRowCannotBeNullError(): RuntimeException = { new RuntimeException(\"The input external row cannot be null.\") } def fieldCannotBeNullMsg(index: Int, fieldName: String): String = { s\"The ${index}th field '$fieldName' of input row cannot be null.\" } def fieldCannotBeNullError(index: Int, fieldName: String): RuntimeException = { new RuntimeException(fieldCannotBeNullMsg(index, fieldName)) } def unableToCreateDatabaseAsFailedToCreateDirectoryError( dbDefinition: CatalogDatabase, e: IOException): Throwable = { new SparkException(s\"Unable to create database ${dbDefinition.name} as failed \" + s\"to create its directory ${dbDefinition.locationUri}\", e) } def unableToDropDatabaseAsFailedToDeleteDirectoryError( dbDefinition: CatalogDatabase, e: IOException): Throwable = { new SparkException(s\"Unable to drop database ${dbDefinition.name} as failed \" + s\"to delete its directory ${dbDefinition.locationUri}\", e) } def unableToCreateTableAsFailedToCreateDirectoryError( table: String, defaultTableLocation: Path, e: IOException): Throwable = { new SparkException(s\"Unable to create table $table as failed \" + s\"to create its directory $defaultTableLocation\", e) } def unableToDeletePartitionPathError(partitionPath: Path, e: IOException): Throwable = { new SparkException(s\"Unable to delete partition path $partitionPath\", e) } def unableToDropTableAsFailedToDeleteDirectoryError( table: String, dir: Path, e: IOException): Throwable = { new SparkException(s\"Unable to drop table $table as failed \" + s\"to delete its directory $dir\", e) } def unableToRenameTableAsFailedToRenameDirectoryError( oldName: String, newName: String, oldDir: Path, e: IOException): Throwable = { new SparkException(s\"Unable to rename table $oldName to $newName as failed \" + s\"to rename its directory $oldDir\", e) } def unableToCreatePartitionPathError(partitionPath: Path, e: IOException): Throwable = { new SparkException(s\"Unable to create partition path $partitionPath\", e) } def unableToRenamePartitionPathError(oldPartPath: Path, e: IOException): Throwable = { new SparkException(s\"Unable to rename partition path $oldPartPath\", e) } def methodNotImplementedError(methodName: String): Throwable = { new UnsupportedOperationException(s\"$methodName is not implemented\") } def tableStatsNotSpecifiedError(): Throwable = { new IllegalStateException(\"table stats must be specified.\") } def arithmeticOverflowError(e: ArithmeticException): ArithmeticException = { new ArithmeticException(s\"${e.getMessage}. If necessary set ${SQLConf.ANSI_ENABLED.key} \" + s\"to false to bypass this error.\") } def arithmeticOverflowError( message: String, hint: String = \"\", errorContext: String = \"\"): ArithmeticException = { val alternative = if (hint.nonEmpty) { s\" Use '$hint' to tolerate overflow and return NULL instead.\" } else \"\" new SparkArithmeticException( errorClass = \"ARITHMETIC_OVERFLOW\", messageParameters = Array(message, alternative, SQLConf.ANSI_ENABLED.key), queryContext = errorContext) } def unaryMinusCauseOverflowError(originValue: Int): ArithmeticException = { arithmeticOverflowError(s\"- ${toSQLValue(originValue, IntegerType)} caused overflow\") } def binaryArithmeticCauseOverflowError( eval1: Short, symbol: String, eval2: Short): ArithmeticException = { arithmeticOverflowError( s\"${toSQLValue(eval1, ShortType)} $symbol ${toSQLValue(eval2, ShortType)} caused overflow\") } def failedSplitSubExpressionMsg(length: Int): String = { \"Failed to split subexpression code into small functions because \" + s\"the parameter length of at least one split function went over the JVM limit: $length\" } def failedSplitSubExpressionError(length: Int): Throwable = { new IllegalStateException(failedSplitSubExpressionMsg(length)) } def failedToCompileMsg(e: Exception): String = { s\"failed to compile: $e\" } def internalCompilerError(e: InternalCompilerException): Throwable = { new InternalCompilerException(failedToCompileMsg(e), e) } def compilerError(e: CompileException): Throwable = { new CompileException(failedToCompileMsg(e), e.getLocation) } def unsupportedTableChangeError(e: IllegalArgumentException): Throwable = { new SparkException(s\"Unsupported table change: ${e.getMessage}\", e) } def notADatasourceRDDPartitionError(split: Partition): Throwable = { new SparkException(s\"[BUG] Not a DataSourceRDDPartition: $split\") } def dataPathNotSpecifiedError(): Throwable = { new IllegalArgumentException(\"'path' is not specified\") } def createStreamingSourceNotSpecifySchemaError(): Throwable = { new IllegalArgumentException( s\"\"\" |Schema must be specified when creating a streaming source DataFrame. If some |files already exist in the directory, then depending on the file format you |may be able to create a static DataFrame on that directory with |'spark.read.load(directory)' and infer schema from it. \"\"\".stripMargin) } def streamedOperatorUnsupportedByDataSourceError( className: String, operator: String): Throwable = { new UnsupportedOperationException( s\"Data source $className does not support streamed $operator\") } def multiplePathsSpecifiedError(allPaths: Seq[String]): Throwable = { new IllegalArgumentException(\"Expected exactly one path to be specified, but \" + s\"got: ${allPaths.mkString(\", \")}\") } def failedToFindDataSourceError(provider: String, error: Throwable): Throwable = { new ClassNotFoundException( s\"\"\" |Failed to find data source: $provider. Please find packages at |https://spark.apache.org/third-party-projects.html \"\"\".stripMargin, error) } def removedClassInSpark2Error(className: String, e: Throwable): Throwable = { new ClassNotFoundException(s\"$className was removed in Spark 2.0. \" + \"Please check if your library is compatible with Spark 2.0\", e) } def incompatibleDataSourceRegisterError(e: Throwable): Throwable = { new SparkClassNotFoundException(\"INCOMPATIBLE_DATASOURCE_REGISTER\", Array(e.getMessage), e) } def unrecognizedFileFormatError(format: String): Throwable = { new IllegalStateException(s\"unrecognized format $format\") } // scalastyle:off line.size.limit def sparkUpgradeInReadingDatesError( format: String, config: String, option: String): SparkUpgradeException = { new SparkUpgradeException( errorClass = \"INCONSISTENT_BEHAVIOR_CROSS_VERSION\", messageParameters = Array( \"3.0\", s\"\"\" |reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z |from $format files can be ambiguous, as the files may be written by |Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar |that is different from Spark 3.0+'s Proleptic Gregorian calendar. |See more details in SPARK-31404. You can set the SQL config ${toSQLConf(config)} or |the datasource option ${toDSOption(option)} to \"LEGACY\" to rebase the datetime values |w.r.t. the calendar difference during reading. To read the datetime values |as it is, set the SQL config ${toSQLConf(config)} or the datasource option ${toDSOption(option)} |to \"CORRECTED\". |\"\"\".stripMargin), cause = null ) } // scalastyle:on line.size.limit def sparkUpgradeInWritingDatesError(format: String, config: String): SparkUpgradeException = { new SparkUpgradeException( errorClass = \"INCONSISTENT_BEHAVIOR_CROSS_VERSION\", messageParameters = Array( \"3.0\", s\"\"\" |writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z |into $format files can be dangerous, as the files may be read by Spark 2.x |or legacy versions of Hive later, which uses a legacy hybrid calendar that |is different from Spark 3.0+'s Proleptic Gregorian calendar. See more |details in SPARK-31404. You can set ${toSQLConf(config)} to \"LEGACY\" to rebase the |datetime values w.r.t. the calendar difference during writing, to get maximum |interoperability. Or set ${toSQLConf(config)} to \"CORRECTED\" to write the datetime |values as it is, if you are 100% sure that the written files will only be read by |Spark 3.0+ or other systems that use Proleptic Gregorian calendar. |\"\"\".stripMargin), cause = null ) } def buildReaderUnsupportedForFileFormatError(format: String): Throwable = { new UnsupportedOperationException(s\"buildReader is not supported for $format\") } def jobAbortedError(cause: Throwable): Throwable = { new SparkException(\"Job aborted.\", cause) } def taskFailedWhileWritingRowsError(cause: Throwable): Throwable = { new SparkException(\"Task failed while writing rows.\", cause) } def readCurrentFileNotFoundError(e: FileNotFoundException): Throwable = { new FileNotFoundException( s\"\"\" |${e.getMessage}\\n |It is possible the underlying files have been updated. You can explicitly invalidate |the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by |recreating the Dataset/DataFrame involved. \"\"\".stripMargin) } def unsupportedSaveModeError(saveMode: String, pathExists: Boolean): Throwable = { new IllegalStateException(s\"unsupported save mode $saveMode ($pathExists)\") } def cannotClearOutputDirectoryError(staticPrefixPath: Path): Throwable = { new IOException(s\"Unable to clear output directory $staticPrefixPath prior to writing to it\") } def cannotClearPartitionDirectoryError(path: Path): Throwable = { new IOException(s\"Unable to clear partition directory $path prior to writing to it\") } def failedToCastValueToDataTypeForPartitionColumnError( value: String, dataType: DataType, columnName: String): Throwable = { new RuntimeException(s\"Failed to cast value `$value` to \" + s\"`$dataType` for partition column `$columnName`\") } def endOfStreamError(): Throwable = { new NoSuchElementException(\"End of stream\") } def fallbackV1RelationReportsInconsistentSchemaError( v2Schema: StructType, v1Schema: StructType): Throwable = { new IllegalArgumentException( \"The fallback v1 relation reports inconsistent schema:\\n\" + \"Schema of v2 scan: \" + v2Schema + \"\\n\" + \"Schema of v1 relation: \" + v1Schema) } def noRecordsFromEmptyDataReaderError(): Throwable = { new IOException(\"No records should be returned from EmptyDataReader\") } def fileNotFoundError(e: FileNotFoundException): Throwable = { new FileNotFoundException( e.getMessage + \"\\n\" + \"It is possible the underlying files have been updated. \" + \"You can explicitly invalidate the cache in Spark by \" + \"recreating the Dataset/DataFrame involved.\") } def unsupportedSchemaColumnConvertError( filePath: String, column: String, logicalType: String, physicalType: String, e: Exception): Throwable = { val message = \"Parquet column cannot be converted in \" + s\"file $filePath. Column: $column, \" + s\"Expected: $logicalType, Found: $physicalType\" new QueryExecutionException(message, e) } def cannotReadFilesError( e: Throwable, path: String): Throwable = { val message = s\"Encountered error while reading file $path. Details: \" new QueryExecutionException(message, e) } def cannotCreateColumnarReaderError(): Throwable = { new UnsupportedOperationException(\"Cannot create columnar reader.\") } def invalidNamespaceNameError(namespace: Array[String]): Throwable = { new IllegalArgumentException(s\"Invalid namespace name: ${namespace.quoted}\") } def unsupportedPartitionTransformError(transform: Transform): Throwable = { new UnsupportedOperationException( s\"Unsupported partition transform: $transform\") } def missingDatabaseLocationError(): Throwable = { new IllegalArgumentException(\"Missing database location\") } def cannotRemoveReservedPropertyError(property: String): Throwable = { new UnsupportedOperationException(s\"Cannot remove reserved property: $property\") } def namespaceNotEmptyError(namespace: Array[String]): Throwable = { new IllegalStateException(s\"Namespace ${namespace.quoted} is not empty\") } def writingJobFailedError(cause: Throwable): Throwable = { new SparkException(\"Writing job failed.\", cause) } def writingJobAbortedError(e: Throwable): Throwable = { new SparkException( errorClass = \"WRITING_JOB_ABORTED\", messageParameters = Array.empty, cause = e) } def commitDeniedError( partId: Int, taskId: Long, attemptId: Int, stageId: Int, stageAttempt: Int): Throwable = { val message = s\"Commit denied for partition $partId (task $taskId, attempt $attemptId, \" + s\"stage $stageId.$stageAttempt)\" new CommitDeniedException(message, stageId, partId, attemptId) } def unsupportedTableWritesError(ident: Identifier): Throwable = { new SparkException( s\"Table implementation does not support writes: ${ident.quoted}\") } def cannotCreateJDBCTableWithPartitionsError(): Throwable = { new UnsupportedOperationException(\"Cannot create JDBC table with partition\") } def unsupportedUserSpecifiedSchemaError(): Throwable = { new UnsupportedOperationException(\"user-specified schema\") } def writeUnsupportedForBinaryFileDataSourceError(): Throwable = { new UnsupportedOperationException(\"Write is not supported for binary file data source\") } def fileLengthExceedsMaxLengthError(status: FileStatus, maxLength: Int): Throwable = { new SparkException( s\"The length of ${status.getPath} is ${status.getLen}, \" + s\"which exceeds the max length allowed: ${maxLength}.\") } def unsupportedFieldNameError(fieldName: String): Throwable = { new RuntimeException(s\"Unsupported field name: ${fieldName}\") } def cannotSpecifyBothJdbcTableNameAndQueryError( jdbcTableName: String, jdbcQueryString: String): Throwable = { new IllegalArgumentException( s\"Both '$jdbcTableName' and '$jdbcQueryString' can not be specified at the same time.\") } def missingJdbcTableNameAndQueryError( jdbcTableName: String, jdbcQueryString: String): Throwable = { new IllegalArgumentException( s\"Option '$jdbcTableName' or '$jdbcQueryString' is required.\" ) } def emptyOptionError(optionName: String): Throwable = { new IllegalArgumentException(s\"Option `$optionName` can not be empty.\") } def invalidJdbcTxnIsolationLevelError(jdbcTxnIsolationLevel: String, value: String): Throwable = { new IllegalArgumentException( s\"Invalid value `$value` for parameter `$jdbcTxnIsolationLevel`. This can be \" + \"`NONE`, `READ_UNCOMMITTED`, `READ_COMMITTED`, `REPEATABLE_READ` or `SERIALIZABLE`.\") } def cannotGetJdbcTypeError(dt: DataType): Throwable = { new IllegalArgumentException(s\"Can't get JDBC type for ${dt.catalogString}\") } def unrecognizedSqlTypeError(sqlType: Int): Throwable = { new SparkSQLException(errorClass = \"UNRECOGNIZED_SQL_TYPE\", Array(sqlType.toString)) } def unsupportedJdbcTypeError(content: String): Throwable = { new SQLException(s\"Unsupported type $content\") } def unsupportedArrayElementTypeBasedOnBinaryError(dt: DataType): Throwable = { new IllegalArgumentException(s\"Unsupported array element \" + s\"type ${dt.catalogString} based on binary\") } def nestedArraysUnsupportedError(): Throwable = { new IllegalArgumentException(\"Nested arrays unsupported\") } def cannotTranslateNonNullValueForFieldError(pos: Int): Throwable = { new IllegalArgumentException(s\"Can't translate non-null value for field $pos\") } def invalidJdbcNumPartitionsError(n: Int, jdbcNumPartitions: String): Throwable = { new IllegalArgumentException( s\"Invalid value `$n` for parameter `$jdbcNumPartitions` in table writing \" + \"via JDBC. The minimum value is 1.\") } def transactionUnsupportedByJdbcServerError(): Throwable = { new SparkSQLFeatureNotSupportedException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(\"the target JDBC server does not support transaction and \" + \"can only support ALTER TABLE with a single action.\")) } def dataTypeUnsupportedYetError(dataType: DataType): Throwable = { new UnsupportedOperationException(s\"$dataType is not supported yet.\") } def unsupportedOperationForDataTypeError(dataType: DataType): Throwable = { new UnsupportedOperationException(s\"DataType: ${dataType.catalogString}\") } def inputFilterNotFullyConvertibleError(owner: String): Throwable = { new SparkException(s\"The input filter of $owner should be fully convertible.\") } def cannotReadFooterForFileError(file: Path, e: IOException): Throwable = { new SparkException(s\"Could not read footer for file: $file\", e) } def cannotReadFooterForFileError(file: FileStatus, e: RuntimeException): Throwable = { new IOException(s\"Could not read footer for file: $file\", e) } def foundDuplicateFieldInCaseInsensitiveModeError( requiredFieldName: String, matchedOrcFields: String): Throwable = { new RuntimeException( s\"\"\" |Found duplicate field(s) \"$requiredFieldName\": $matchedOrcFields |in case-insensitive mode \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def foundDuplicateFieldInFieldIdLookupModeError( requiredId: Int, matchedFields: String): Throwable = { new RuntimeException( s\"\"\" |Found duplicate field(s) \"$requiredId\": $matchedFields |in id mapping mode \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def failedToMergeIncompatibleSchemasError( left: StructType, right: StructType, e: Throwable): Throwable = { new SparkException(s\"Failed to merge incompatible schemas $left and $right\", e) } def ddlUnsupportedTemporarilyError(ddl: String): Throwable = { new UnsupportedOperationException(s\"$ddl is not supported temporarily.\") } def operatingOnCanonicalizationPlanError(): Throwable = { new IllegalStateException(\"operating on canonicalization plan\") } def executeBroadcastTimeoutError(timeout: Long, ex: Option[TimeoutException]): Throwable = { new SparkException( s\"\"\" |Could not execute broadcast in $timeout secs. You can increase the timeout |for broadcasts via ${SQLConf.BROADCAST_TIMEOUT.key} or disable broadcast join |by setting ${SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key} to -1 \"\"\".stripMargin.replaceAll(\"\\n\", \" \"), ex.getOrElse(null)) } def cannotCompareCostWithTargetCostError(cost: String): Throwable = { new IllegalArgumentException(s\"Could not compare cost with $cost\") } def unsupportedDataTypeError(dt: String): Throwable = { new UnsupportedOperationException(s\"Unsupported data type: ${dt}\") } def notSupportTypeError(dataType: DataType): Throwable = { new Exception(s\"not support type: $dataType\") } def notSupportNonPrimitiveTypeError(): Throwable = { new RuntimeException(\"Not support non-primitive type now\") } def unsupportedTypeError(dataType: DataType): Throwable = { new Exception(s\"Unsupported type: ${dataType.catalogString}\") } def useDictionaryEncodingWhenDictionaryOverflowError(): Throwable = { new IllegalStateException( \"Dictionary encoding should not be used because of dictionary overflow.\") } def endOfIteratorError(): Throwable = { new NoSuchElementException(\"End of the iterator\") } def cannotAllocateMemoryToGrowBytesToBytesMapError(): Throwable = { new IOException(\"Could not allocate memory to grow BytesToBytesMap\") } def cannotAcquireMemoryToBuildLongHashedRelationError(size: Long, got: Long): Throwable = { new SparkException(s\"Can't acquire $size bytes memory to build hash relation, \" + s\"got $got bytes\") } def cannotAcquireMemoryToBuildUnsafeHashedRelationError(): Throwable = { new SparkOutOfMemoryError(\"There is not enough memory to build hash map\") } def rowLargerThan256MUnsupportedError(): Throwable = { new UnsupportedOperationException(\"Does not support row that is larger than 256M\") } def cannotBuildHashedRelationWithUniqueKeysExceededError(): Throwable = { new UnsupportedOperationException( \"Cannot build HashedRelation with more than 1/3 billions unique keys\") } def cannotBuildHashedRelationLargerThan8GError(): Throwable = { new UnsupportedOperationException( \"Can not build a HashedRelation that is larger than 8G\") } def failedToPushRowIntoRowQueueError(rowQueue: String): Throwable = { new SparkException(s\"failed to push a row into $rowQueue\") } def unexpectedWindowFunctionFrameError(frame: String): Throwable = { new RuntimeException(s\"Unexpected window function frame $frame.\") } def cannotParseStatisticAsPercentileError( stats: String, e: NumberFormatException): Throwable = { new IllegalArgumentException(s\"Unable to parse $stats as a percentile\", e) } def statisticNotRecognizedError(stats: String): Throwable = { new IllegalArgumentException(s\"$stats is not a recognised statistic\") } def unknownColumnError(unknownColumn: String): Throwable = { new IllegalArgumentException(s\"Unknown column: $unknownColumn\") } def unexpectedAccumulableUpdateValueError(o: Any): Throwable = { new IllegalArgumentException(s\"Unexpected: $o\") } def unscaledValueTooLargeForPrecisionError(): Throwable = { new ArithmeticException(\"Unscaled value too large for precision. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\") } def decimalPrecisionExceedsMaxPrecisionError(precision: Int, maxPrecision: Int): Throwable = { new ArithmeticException( s\"Decimal precision $precision exceeds max precision $maxPrecision\") } def outOfDecimalTypeRangeError(str: UTF8String): Throwable = { new ArithmeticException(s\"out of decimal type range: $str\") } def unsupportedArrayTypeError(clazz: Class[_]): Throwable = { new RuntimeException(s\"Do not support array of type $clazz.\") } def unsupportedJavaTypeError(clazz: Class[_]): Throwable = { new RuntimeException(s\"Do not support type $clazz.\") } def failedParsingStructTypeError(raw: String): Throwable = { new RuntimeException(s\"Failed parsing ${StructType.simpleString}: $raw\") } def failedMergingFieldsError(leftName: String, rightName: String, e: Throwable): Throwable = { new SparkException(s\"Failed to merge fields '$leftName' and '$rightName'. ${e.getMessage}\") } def cannotMergeDecimalTypesWithIncompatiblePrecisionAndScaleError( leftPrecision: Int, rightPrecision: Int, leftScale: Int, rightScale: Int): Throwable = { new SparkException(\"Failed to merge decimal types with incompatible \" + s\"precision $leftPrecision and $rightPrecision & scale $leftScale and $rightScale\") } def cannotMergeDecimalTypesWithIncompatiblePrecisionError( leftPrecision: Int, rightPrecision: Int): Throwable = { new SparkException(\"Failed to merge decimal types with incompatible \" + s\"precision $leftPrecision and $rightPrecision\") } def cannotMergeDecimalTypesWithIncompatibleScaleError( leftScale: Int, rightScale: Int): Throwable = { new SparkException(\"Failed to merge decimal types with incompatible \" + s\"scale $leftScale and $rightScale\") } def cannotMergeIncompatibleDataTypesError(left: DataType, right: DataType): Throwable = { new SparkException(s\"Failed to merge incompatible data types ${left.catalogString}\" + s\" and ${right.catalogString}\") } def exceedMapSizeLimitError(size: Int): Throwable = { new RuntimeException(s\"Unsuccessful attempt to build maps with $size elements \" + s\"due to exceeding the map size limit ${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}.\") } def duplicateMapKeyFoundError(key: Any): Throwable = { new RuntimeException(s\"Duplicate map key $key was found, please check the input \" + \"data. If you want to remove the duplicated keys, you can set \" + s\"${SQLConf.MAP_KEY_DEDUP_POLICY.key} to ${SQLConf.MapKeyDedupPolicy.LAST_WIN} so that \" + \"the key inserted at last takes precedence.\") } def mapDataKeyArrayLengthDiffersFromValueArrayLengthError(): Throwable = { new RuntimeException(\"The key array and value array of MapData must have the same length.\") } def fieldDiffersFromDerivedLocalDateError( field: ChronoField, actual: Int, expected: Int, candidate: LocalDate): Throwable = { new DateTimeException(s\"Conflict found: Field $field $actual differs from\" + s\" $field $expected derived from $candidate\") } def failToParseDateTimeInNewParserError(s: String, e: Throwable): Throwable = { new SparkUpgradeException(\"3.0\", s\"Fail to parse '$s' in the new parser. You can \" + s\"set ${SQLConf.LEGACY_TIME_PARSER_POLICY.key} to LEGACY to restore the behavior \" + s\"before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\", e) } def failToFormatDateTimeInNewFormatterError( resultCandidate: String, e: Throwable): Throwable = { new SparkUpgradeException(\"3.0\", s\"\"\" |Fail to format it to '$resultCandidate' in the new formatter. You can set |${SQLConf.LEGACY_TIME_PARSER_POLICY.key} to LEGACY to restore the behavior before |Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string. \"\"\".stripMargin.replaceAll(\"\\n\", \" \"), e) } def failToRecognizePatternAfterUpgradeError(pattern: String, e: Throwable): Throwable = { new SparkUpgradeException(\"3.0\", s\"Fail to recognize '$pattern' pattern in the\" + s\" DateTimeFormatter. 1) You can set ${SQLConf.LEGACY_TIME_PARSER_POLICY.key} to LEGACY\" + s\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern\" + s\" with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\", e) } def failToRecognizePatternError(pattern: String, e: Throwable): Throwable = { new RuntimeException(s\"Fail to recognize '$pattern' pattern in the\" + \" DateTimeFormatter. You can form a valid datetime pattern\" + \" with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\", e) } def cannotCastToDateTimeError( value: Any, from: DataType, to: DataType, errorContext: String): Throwable = { val valueString = toSQLValue(value, from) new DateTimeException(s\"Invalid input syntax for type ${toSQLType(to)}: $valueString. \" + s\"Use `try_cast` to tolerate malformed input and return NULL instead. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} \" + s\"to false to bypass this error.\" + errorContext) } def registeringStreamingQueryListenerError(e: Exception): Throwable = { new SparkException(\"Exception when registering StreamingQueryListener\", e) } def concurrentQueryInstanceError(): Throwable = { new SparkConcurrentModificationException(\"CONCURRENT_QUERY\", Array.empty) } def cannotParseJsonArraysAsStructsError(): Throwable = { new RuntimeException(\"Parsing JSON arrays as structs is forbidden.\") } def cannotParseStringAsDataTypeError(parser: JsonParser, token: JsonToken, dataType: DataType) : Throwable = { new RuntimeException( s\"Cannot parse field name ${parser.getCurrentName}, \" + s\"field value ${parser.getText}, \" + s\"[$token] as target spark data type [$dataType].\") } def cannotParseStringAsDataTypeError(pattern: String, value: String, dataType: DataType) : Throwable = { new RuntimeException( s\"Cannot parse field value ${toSQLValue(value, StringType)} \" + s\"for pattern ${toSQLValue(pattern, StringType)} \" + s\"as target spark data type [$dataType].\") } def failToParseEmptyStringForDataTypeError(dataType: DataType): Throwable = { new RuntimeException( s\"Failed to parse an empty string for data type ${dataType.catalogString}\") } def failToParseValueForDataTypeError(parser: JsonParser, token: JsonToken, dataType: DataType) : Throwable = { new RuntimeException( s\"Failed to parse field name ${parser.getCurrentName}, \" + s\"field value ${parser.getText}, \" + s\"[$token] to target spark data type [$dataType].\") } def rootConverterReturnNullError(): Throwable = { new RuntimeException(\"Root converter returned null\") } def cannotHaveCircularReferencesInBeanClassError(clazz: Class[_]): Throwable = { new UnsupportedOperationException( \"Cannot have circular references in bean class, but got the circular reference \" + s\"of class $clazz\") } def cannotHaveCircularReferencesInClassError(t: String): Throwable = { new UnsupportedOperationException( s\"cannot have circular references in class, but got the circular reference of class $t\") } def cannotUseInvalidJavaIdentifierAsFieldNameError( fieldName: String, walkedTypePath: WalkedTypePath): Throwable = { new UnsupportedOperationException(s\"`$fieldName` is not a valid identifier of \" + s\"Java and cannot be used as field name\\n$walkedTypePath\") } def cannotFindEncoderForTypeError( tpe: String, walkedTypePath: WalkedTypePath): Throwable = { new UnsupportedOperationException(s\"No Encoder found for $tpe\\n$walkedTypePath\") } def attributesForTypeUnsupportedError(schema: Schema): Throwable = { new UnsupportedOperationException(s\"Attributes for type $schema is not supported\") } def schemaForTypeUnsupportedError(tpe: String): Throwable = { new UnsupportedOperationException(s\"Schema for type $tpe is not supported\") } def cannotFindConstructorForTypeError(tpe: String): Throwable = { new UnsupportedOperationException( s\"\"\" |Unable to find constructor for $tpe. |This could happen if $tpe is an interface, or a trait without companion object |constructor. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def paramExceedOneCharError(paramName: String): Throwable = { new RuntimeException(s\"$paramName cannot be more than one character\") } def paramIsNotIntegerError(paramName: String, value: String): Throwable = { new RuntimeException(s\"$paramName should be an integer. Found ${toSQLValue(value, StringType)}\") } def paramIsNotBooleanValueError(paramName: String): Throwable = { new Exception(s\"$paramName flag can be true or false\") } def foundNullValueForNotNullableFieldError(name: String): Throwable = { new RuntimeException(s\"null value found but field $name is not nullable.\") } def malformedCSVRecordError(): Throwable = { new RuntimeException(\"Malformed CSV record\") } def elementsOfTupleExceedLimitError(): Throwable = { new UnsupportedOperationException(\"Due to Scala's limited support of tuple, \" + \"tuple with more than 22 elements are not supported.\") } def expressionDecodingError(e: Exception, expressions: Seq[Expression]): Throwable = { new RuntimeException(s\"Error while decoding: $e\\n\" + s\"${expressions.map(_.simpleString(SQLConf.get.maxToStringFields)).mkString(\"\\n\")}\", e) } def expressionEncodingError(e: Exception, expressions: Seq[Expression]): Throwable = { new RuntimeException(s\"Error while encoding: $e\\n\" + s\"${expressions.map(_.simpleString(SQLConf.get.maxToStringFields)).mkString(\"\\n\")}\", e) } def classHasUnexpectedSerializerError(clsName: String, objSerializer: Expression): Throwable = { new RuntimeException(s\"class $clsName has unexpected serializer: $objSerializer\") } def cannotGetOuterPointerForInnerClassError(innerCls: Class[_]): Throwable = { new RuntimeException(s\"Failed to get outer pointer for ${innerCls.getName}\") } def userDefinedTypeNotAnnotatedAndRegisteredError(udt: UserDefinedType[_]): Throwable = { new SparkException(s\"${udt.userClass.getName} is not annotated with \" + \"SQLUserDefinedType nor registered with UDTRegistration.}\") } def unsupportedOperandTypeForSizeFunctionError(dataType: DataType): Throwable = { new UnsupportedOperationException( s\"The size function doesn't support the operand type ${dataType.getClass.getCanonicalName}\") } def unexpectedValueForStartInFunctionError(prettyName: String): RuntimeException = { new RuntimeException( s\"Unexpected value for start in function $prettyName: SQL array indices start at 1.\") } def unexpectedValueForLengthInFunctionError(prettyName: String): RuntimeException = { new RuntimeException(s\"Unexpected value for length in function $prettyName: \" + \"length must be greater than or equal to 0.\") } def sqlArrayIndexNotStartAtOneError(): ArrayIndexOutOfBoundsException = { new ArrayIndexOutOfBoundsException(\"SQL array indices start at 1\") } def concatArraysWithElementsExceedLimitError(numberOfElements: Long): Throwable = { new RuntimeException( s\"\"\" |Unsuccessful try to concat arrays with $numberOfElements |elements due to exceeding the array size limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def flattenArraysWithElementsExceedLimitError(numberOfElements: Long): Throwable = { new RuntimeException( s\"\"\" |Unsuccessful try to flatten an array of arrays with $numberOfElements |elements due to exceeding the array size limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def createArrayWithElementsExceedLimitError(count: Any): RuntimeException = { new RuntimeException( s\"\"\" |Unsuccessful try to create array with $count elements |due to exceeding the array size limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def unionArrayWithElementsExceedLimitError(length: Int): Throwable = { new RuntimeException( s\"\"\" |Unsuccessful try to union arrays with $length |elements due to exceeding the array size limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def initialTypeNotTargetDataTypeError(dataType: DataType, target: String): Throwable = { new UnsupportedOperationException(s\"Initial type ${dataType.catalogString} must be a $target\") } def initialTypeNotTargetDataTypesError(dataType: DataType): Throwable = { new UnsupportedOperationException( s\"Initial type ${dataType.catalogString} must be \" + s\"an ${ArrayType.simpleString}, a ${StructType.simpleString} or a ${MapType.simpleString}\") } def cannotConvertColumnToJSONError(name: String, dataType: DataType): Throwable = { new UnsupportedOperationException( s\"Unable to convert column $name of type ${dataType.catalogString} to JSON.\") } def malformedRecordsDetectedInSchemaInferenceError(e: Throwable): Throwable = { new SparkException(\"Malformed records are detected in schema inference. \" + s\"Parse Mode: ${FailFastMode.name}.\", e) } def malformedJSONError(): Throwable = { new SparkException(\"Malformed JSON\") } def malformedRecordsDetectedInSchemaInferenceError(dataType: DataType): Throwable = { new SparkException( s\"\"\" |Malformed records are detected in schema inference. |Parse Mode: ${FailFastMode.name}. Reasons: Failed to infer a common schema. |Struct types are expected, but `${dataType.catalogString}` was found. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def cannotRewriteDomainJoinWithConditionsError( conditions: Seq[Expression], d: DomainJoin): Throwable = { new IllegalStateException( s\"Unable to rewrite domain join with conditions: $conditions\\n$d\") } def decorrelateInnerQueryThroughPlanUnsupportedError(plan: LogicalPlan): Throwable = { new UnsupportedOperationException( s\"Decorrelate inner query through ${plan.nodeName} is not supported.\") } def methodCalledInAnalyzerNotAllowedError(): Throwable = { new RuntimeException(\"This method should not be called in the analyzer\") } def cannotSafelyMergeSerdePropertiesError( props1: Map[String, String], props2: Map[String, String], conflictKeys: Set[String]): Throwable = { new UnsupportedOperationException( s\"\"\" |Cannot safely merge SERDEPROPERTIES: |${props1.map { case (k, v) => s\"$k=$v\" }.mkString(\"{\", \",\", \"}\")} |${props2.map { case (k, v) => s\"$k=$v\" }.mkString(\"{\", \",\", \"}\")} |The conflict keys: ${conflictKeys.mkString(\", \")} |\"\"\".stripMargin) } def pairUnsupportedAtFunctionError( r1: ValueInterval, r2: ValueInterval, function: String): Throwable = { new UnsupportedOperationException(s\"Not supported pair: $r1, $r2 at $function()\") } def onceStrategyIdempotenceIsBrokenForBatchError[TreeType <: TreeNode[_]]( batchName: String, plan: TreeType, reOptimized: TreeType): Throwable = { new RuntimeException( s\"\"\" |Once strategy's idempotence is broken for batch $batchName |${sideBySide(plan.treeString, reOptimized.treeString).mkString(\"\\n\")} \"\"\".stripMargin) } def structuralIntegrityOfInputPlanIsBrokenInClassError(className: String): Throwable = { new RuntimeException(\"The structural integrity of the input plan is broken in \" + s\"$className.\") } def structuralIntegrityIsBrokenAfterApplyingRuleError( ruleName: String, batchName: String): Throwable = { new RuntimeException(s\"After applying rule $ruleName in batch $batchName, \" + \"the structural integrity of the plan is broken.\") } def ruleIdNotFoundForRuleError(ruleName: String): Throwable = { new NoSuchElementException(s\"Rule id not found for $ruleName\") } def cannotCreateArrayWithElementsExceedLimitError( numElements: Long, additionalErrorMessage: String): Throwable = { new RuntimeException( s\"\"\" |Cannot create array with $numElements |elements of data due to exceeding the limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH} elements for ArrayData. |$additionalErrorMessage \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def indexOutOfBoundsOfArrayDataError(idx: Int): Throwable = { new SparkIndexOutOfBoundsException( errorClass = \"INDEX_OUT_OF_BOUNDS\", Array(toSQLValue(idx, IntegerType))) } def malformedRecordsDetectedInRecordParsingError(e: BadRecordException): Throwable = { new SparkException(\"Malformed records are detected in record parsing. \" + s\"Parse Mode: ${FailFastMode.name}. To process malformed records as null \" + \"result, try setting the option 'mode' as 'PERMISSIVE'.\", e) } def remoteOperationsUnsupportedError(): Throwable = { new RuntimeException(\"Remote operations not supported\") } def invalidKerberosConfigForHiveServer2Error(): Throwable = { new IOException( \"HiveServer2 Kerberos principal or keytab is not correctly configured\") } def parentSparkUIToAttachTabNotFoundError(): Throwable = { new SparkException(\"Parent SparkUI to attach this tab to not found!\") } def inferSchemaUnsupportedForHiveError(): Throwable = { new UnsupportedOperationException(\"inferSchema is not supported for hive data source.\") } def requestedPartitionsMismatchTablePartitionsError( table: CatalogTable, partition: Map[String, Option[String]]): Throwable = { new SparkException( s\"\"\" |Requested partitioning does not match the ${table.identifier.table} table: |Requested partitions: ${partition.keys.mkString(\",\")} |Table partitions: ${table.partitionColumnNames.mkString(\",\")} \"\"\".stripMargin) } def dynamicPartitionKeyNotAmongWrittenPartitionPathsError(key: String): Throwable = { new SparkException( s\"Dynamic partition key ${toSQLValue(key, StringType)} is not among written partition paths.\") } def cannotRemovePartitionDirError(partitionPath: Path): Throwable = { new RuntimeException(s\"Cannot remove partition directory '$partitionPath'\") } def cannotCreateStagingDirError(message: String, e: IOException): Throwable = { new RuntimeException(s\"Cannot create staging directory: $message\", e) } def serDeInterfaceNotFoundError(e: NoClassDefFoundError): Throwable = { new ClassNotFoundException(\"The SerDe interface removed since Hive 2.3(HIVE-15167).\" + \" Please migrate your custom SerDes to Hive 2.3. See HIVE-15167 for more details.\", e) } def convertHiveTableToCatalogTableError( e: SparkException, dbName: String, tableName: String): Throwable = { new SparkException(s\"${e.getMessage}, db: $dbName, table: $tableName\", e) } def cannotRecognizeHiveTypeError( e: ParseException, fieldType: String, fieldName: String): Throwable = { new SparkException( s\"Cannot recognize hive type string: $fieldType, column: $fieldName\", e) } def getTablesByTypeUnsupportedByHiveVersionError(): Throwable = { new UnsupportedOperationException(\"Hive 2.2 and lower versions don't support \" + \"getTablesByType. Please use Hive 2.3 or higher version.\") } def dropTableWithPurgeUnsupportedError(): Throwable = { new UnsupportedOperationException(\"DROP TABLE ... PURGE\") } def alterTableWithDropPartitionAndPurgeUnsupportedError(): Throwable = { new UnsupportedOperationException(\"ALTER TABLE ... DROP PARTITION ... PURGE\") } def invalidPartitionFilterError(): Throwable = { new UnsupportedOperationException( \"\"\"Partition filter cannot have both `\"` and `'` characters\"\"\") } def getPartitionMetadataByFilterError(e: InvocationTargetException): Throwable = { new RuntimeException( s\"\"\" |Caught Hive MetaException attempting to get partition metadata by filter |from Hive. You can set the Spark configuration setting |${SQLConf.HIVE_METASTORE_PARTITION_PRUNING_FALLBACK_ON_EXCEPTION.key} to true to work |around this problem, however this will result in degraded performance. Please |report a bug: https://issues.apache.org/jira/browse/SPARK \"\"\".stripMargin.replaceAll(\"\\n\", \" \"), e) } def unsupportedHiveMetastoreVersionError(version: String, key: String): Throwable = { new UnsupportedOperationException(s\"Unsupported Hive Metastore version ($version). \" + s\"Please set $key with a valid version.\") } def loadHiveClientCausesNoClassDefFoundError( cnf: NoClassDefFoundError, execJars: Seq[URL], key: String, e: InvocationTargetException): Throwable = { new ClassNotFoundException( s\"\"\" |$cnf when creating Hive client using classpath: ${execJars.mkString(\", \")}\\n |Please make sure that jars for your version of hive and hadoop are included in the |paths passed to $key. \"\"\".stripMargin.replaceAll(\"\\n\", \" \"), e) } def cannotFetchTablesOfDatabaseError(dbName: String, e: Exception): Throwable = { new SparkException(s\"Unable to fetch tables of db $dbName\", e) } def illegalLocationClauseForViewPartitionError(): Throwable = { new SparkException(\"LOCATION clause illegal for view partition\") } def renamePathAsExistsPathError(srcPath: Path, dstPath: Path): Throwable = { new SparkFileAlreadyExistsException(errorClass = \"FAILED_RENAME_PATH\", Array(srcPath.toString, dstPath.toString)) } def renameAsExistsPathError(dstPath: Path): Throwable = { new FileAlreadyExistsException(s\"Failed to rename as $dstPath already exists\") } def renameSrcPathNotFoundError(srcPath: Path): Throwable = { new SparkFileNotFoundException(errorClass = \"RENAME_SRC_PATH_NOT_FOUND\", Array(srcPath.toString)) } def failedRenameTempFileError(srcPath: Path, dstPath: Path): Throwable = { new IOException(s\"Failed to rename temp file $srcPath to $dstPath as rename returned false\") } def legacyMetadataPathExistsError(metadataPath: Path, legacyMetadataPath: Path): Throwable = { new SparkException( s\"\"\" |Error: we detected a possible problem with the location of your \"_spark_metadata\" |directory and you likely need to move it before restarting this query. | |Earlier version of Spark incorrectly escaped paths when writing out the |\"_spark_metadata\" directory for structured streaming. While this was corrected in |Spark 3.0, it appears that your query was started using an earlier version that |incorrectly handled the \"_spark_metadata\" path. | |Correct \"_spark_metadata\" Directory: $metadataPath |Incorrect \"_spark_metadata\" Directory: $legacyMetadataPath | |Please move the data from the incorrect directory to the correct one, delete the |incorrect directory, and then restart this query. If you believe you are receiving |this message in error, you can disable it with the SQL conf |${SQLConf.STREAMING_CHECKPOINT_ESCAPED_PATH_CHECK_ENABLED.key}. \"\"\".stripMargin) } def partitionColumnNotFoundInSchemaError(col: String, schema: StructType): Throwable = { new RuntimeException(s\"Partition column $col not found in schema $schema\") } def stateNotDefinedOrAlreadyRemovedError(): Throwable = { new NoSuchElementException(\"State is either not defined or has already been removed\") } def cannotSetTimeoutDurationError(): Throwable = { new UnsupportedOperationException( \"Cannot set timeout duration without enabling processing time timeout in \" + \"[map|flatMap]GroupsWithState\") } def cannotGetEventTimeWatermarkError(): Throwable = { new UnsupportedOperationException( \"Cannot get event time watermark timestamp without setting watermark before \" + \"[map|flatMap]GroupsWithState\") } def cannotSetTimeoutTimestampError(): Throwable = { new UnsupportedOperationException( \"Cannot set timeout timestamp without enabling event time timeout in \" + \"[map|flatMapGroupsWithState\") } def batchMetadataFileNotFoundError(batchMetadataFile: Path): Throwable = { new FileNotFoundException(s\"Unable to find batch $batchMetadataFile\") } def multiStreamingQueriesUsingPathConcurrentlyError( path: String, e: FileAlreadyExistsException): Throwable = { new ConcurrentModificationException( s\"Multiple streaming queries are concurrently using $path\", e) } def addFilesWithAbsolutePathUnsupportedError(commitProtocol: String): Throwable = { new UnsupportedOperationException( s\"$commitProtocol does not support adding files with an absolute path\") } def microBatchUnsupportedByDataSourceError(srcName: String): Throwable = { new UnsupportedOperationException( s\"Data source $srcName does not support microbatch processing.\") } def cannotExecuteStreamingRelationExecError(): Throwable = { new UnsupportedOperationException(\"StreamingRelationExec cannot be executed\") } def invalidStreamingOutputModeError(outputMode: Option[OutputMode]): Throwable = { new UnsupportedOperationException(s\"Invalid output mode: $outputMode\") } def catalogPluginClassNotFoundError(name: String): Throwable = { new CatalogNotFoundException( s\"Catalog '$name' plugin class not found: spark.sql.catalog.$name is not defined\") } def catalogPluginClassNotImplementedError(name: String, pluginClassName: String): Throwable = { new SparkException( s\"Plugin class for catalog '$name' does not implement CatalogPlugin: $pluginClassName\") } def catalogPluginClassNotFoundForCatalogError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException(s\"Cannot find catalog plugin class for catalog '$name': $pluginClassName\", e) } def catalogFailToFindPublicNoArgConstructorError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException( s\"Failed to find public no-arg constructor for catalog '$name': $pluginClassName)\", e) } def catalogFailToCallPublicNoArgConstructorError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException( s\"Failed to call public no-arg constructor for catalog '$name': $pluginClassName)\", e) } def cannotInstantiateAbstractCatalogPluginClassError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException(\"Cannot instantiate abstract catalog plugin class for \" + s\"catalog '$name': $pluginClassName\", e.getCause) } def failedToInstantiateConstructorForCatalogError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException(\"Failed during instantiating constructor for catalog \" + s\"'$name': $pluginClassName\", e.getCause) } def noSuchElementExceptionError(): Throwable = { new NoSuchElementException } def noSuchElementExceptionError(key: String): Throwable = { new NoSuchElementException(key) } def cannotMutateReadOnlySQLConfError(): Throwable = { new UnsupportedOperationException(\"Cannot mutate ReadOnlySQLConf.\") } def cannotCloneOrCopyReadOnlySQLConfError(): Throwable = { new UnsupportedOperationException(\"Cannot clone/copy ReadOnlySQLConf.\") } def cannotGetSQLConfInSchedulerEventLoopThreadError(): Throwable = { new RuntimeException(\"Cannot get SQLConf inside scheduler event loop thread.\") } def unsupportedOperationExceptionError(): Throwable = { new UnsupportedOperationException } def nullLiteralsCannotBeCastedError(name: String): Throwable = { new UnsupportedOperationException(s\"null literals can't be casted to $name\") } def notUserDefinedTypeError(name: String, userClass: String): Throwable = { new SparkException(s\"$name is not an UserDefinedType. Please make sure registering \" + s\"an UserDefinedType for ${userClass}\") } def cannotLoadUserDefinedTypeError(name: String, userClass: String): Throwable = { new SparkException(s\"Can not load in UserDefinedType ${name} for user class ${userClass}.\") } def timeZoneIdNotSpecifiedForTimestampTypeError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_OPERATION\", messageParameters = Array( s\"${toSQLType(TimestampType)} must supply timeZoneId parameter \" + s\"while converting to the arrow timestamp type.\") ) } def notPublicClassError(name: String): Throwable = { new UnsupportedOperationException( s\"$name is not a public class. Only public classes are supported.\") } def primitiveTypesNotSupportedError(): Throwable = { new UnsupportedOperationException(\"Primitive types are not supported.\") } def fieldIndexOnRowWithoutSchemaError(): Throwable = { new UnsupportedOperationException(\"fieldIndex on a Row without schema is undefined.\") } def valueIsNullError(index: Int): Throwable = { new NullPointerException(s\"Value at index ${toSQLValue(index, IntegerType)} is null\") } def onlySupportDataSourcesProvidingFileFormatError(providingClass: String): Throwable = { new SparkException(s\"Only Data Sources providing FileFormat are supported: $providingClass\") } def failToSetOriginalPermissionBackError( permission: FsPermission, path: Path, e: Throwable): Throwable = { new SparkSecurityException(errorClass = \"RESET_PERMISSION_TO_ORIGINAL\", Array(permission.toString, path.toString, e.getMessage)) } def failToSetOriginalACLBackError(aclEntries: String, path: Path, e: Throwable): Throwable = { new SecurityException(s\"Failed to set original ACL $aclEntries back to \" + s\"the created path: $path. Exception: ${e.getMessage}\") } def multiFailuresInStageMaterializationError(error: Throwable): Throwable = { new SparkException(\"Multiple failures in stage materialization.\", error) } def unrecognizedCompressionSchemaTypeIDError(typeId: Int): Throwable = { new UnsupportedOperationException(s\"Unrecognized compression scheme type ID: $typeId\") } def getParentLoggerNotImplementedError(className: String): Throwable = { new SQLFeatureNotSupportedException(s\"$className.getParentLogger is not yet implemented.\") } def cannotCreateParquetConverterForTypeError(t: DecimalType, parquetType: String): Throwable = { new RuntimeException( s\"\"\" |Unable to create Parquet converter for ${t.typeName} |whose Parquet type is $parquetType without decimal metadata. Please read this |column/field as Spark BINARY type. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def cannotCreateParquetConverterForDecimalTypeError( t: DecimalType, parquetType: String): Throwable = { new RuntimeException( s\"\"\" |Unable to create Parquet converter for decimal type ${t.json} whose Parquet type is |$parquetType. Parquet DECIMAL type can only be backed by INT32, INT64, |FIXED_LEN_BYTE_ARRAY, or BINARY. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def cannotCreateParquetConverterForDataTypeError( t: DataType, parquetType: String): Throwable = { new RuntimeException(s\"Unable to create Parquet converter for data type ${t.json} \" + s\"whose Parquet type is $parquetType\") } def cannotAddMultiPartitionsOnNonatomicPartitionTableError(tableName: String): Throwable = { new UnsupportedOperationException( s\"Nonatomic partition table $tableName can not add multiple partitions.\") } def userSpecifiedSchemaUnsupportedByDataSourceError(provider: TableProvider): Throwable = { new UnsupportedOperationException( s\"${provider.getClass.getSimpleName} source does not support user-specified schema.\") } def cannotDropMultiPartitionsOnNonatomicPartitionTableError(tableName: String): Throwable = { new UnsupportedOperationException( s\"Nonatomic partition table $tableName can not drop multiple partitions.\") } def truncateMultiPartitionUnsupportedError(tableName: String): Throwable = { new UnsupportedOperationException( s\"The table $tableName does not support truncation of multiple partition.\") } def overwriteTableByUnsupportedExpressionError(table: Table): Throwable = { new SparkException(s\"Table does not support overwrite by expression: $table\") } def dynamicPartitionOverwriteUnsupportedByTableError(table: Table): Throwable = { new SparkException(s\"Table does not support dynamic partition overwrite: $table\") } def failedMergingSchemaError(schema: StructType, e: SparkException): Throwable = { new SparkException(s\"Failed merging schema:\\n${schema.treeString}\", e) } def cannotBroadcastTableOverMaxTableRowsError( maxBroadcastTableRows: Long, numRows: Long): Throwable = { new SparkException( s\"Cannot broadcast the table over $maxBroadcastTableRows rows: $numRows rows\") } def cannotBroadcastTableOverMaxTableBytesError( maxBroadcastTableBytes: Long, dataSize: Long): Throwable = { new SparkException(\"Cannot broadcast the table that is larger than\" + s\" ${maxBroadcastTableBytes >> 30}GB: ${dataSize >> 30} GB\") } def notEnoughMemoryToBuildAndBroadcastTableError(oe: OutOfMemoryError): Throwable = { new OutOfMemoryError(\"Not enough memory to build and broadcast the table to all \" + \"worker nodes. As a workaround, you can either disable broadcast by setting \" + s\"${SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key} to -1 or increase the spark \" + s\"driver memory by setting ${SparkLauncher.DRIVER_MEMORY} to a higher value.\") .initCause(oe.getCause) } def executeCodePathUnsupportedError(execName: String): Throwable = { new UnsupportedOperationException(s\"$execName does not support the execute() code path.\") } def cannotMergeClassWithOtherClassError(className: String, otherClass: String): Throwable = { new UnsupportedOperationException( s\"Cannot merge $className with $otherClass\") } def continuousProcessingUnsupportedByDataSourceError(sourceName: String): Throwable = { new UnsupportedOperationException( s\"Data source $sourceName does not support continuous processing.\") } def failedToReadDataError(failureReason: Throwable): Throwable = { new SparkException(\"Data read failed\", failureReason) } def failedToGenerateEpochMarkerError(failureReason: Throwable): Throwable = { new SparkException(\"Epoch marker generation failed\", failureReason) } def foreachWriterAbortedDueToTaskFailureError(): Throwable = { new SparkException(\"Foreach writer has been aborted due to a task failure\") } def integerOverflowError(message: String): Throwable = { new ArithmeticException(s\"Integer overflow. $message\") } def failedToReadDeltaFileError(fileToRead: Path, clazz: String, keySize: Int): Throwable = { new IOException( s\"Error reading delta file $fileToRead of $clazz: key size cannot be $keySize\") } def failedToReadSnapshotFileError(fileToRead: Path, clazz: String, message: String): Throwable = { new IOException(s\"Error reading snapshot file $fileToRead of $clazz: $message\") } def cannotPurgeAsBreakInternalStateError(): Throwable = { new UnsupportedOperationException(\"Cannot purge as it might break internal state.\") } def cleanUpSourceFilesUnsupportedError(): Throwable = { new UnsupportedOperationException(\"Clean up source files is not supported when\" + \" reading from the output directory of FileStreamSink.\") } def latestOffsetNotCalledError(): Throwable = { new UnsupportedOperationException( \"latestOffset(Offset, ReadLimit) should be called instead of this method\") } def legacyCheckpointDirectoryExistsError( checkpointPath: Path, legacyCheckpointDir: String): Throwable = { new SparkException( s\"\"\" |Error: we detected a possible problem with the location of your checkpoint and you |likely need to move it before restarting this query. | |Earlier version of Spark incorrectly escaped paths when writing out checkpoints for |structured streaming. While this was corrected in Spark 3.0, it appears that your |query was started using an earlier version that incorrectly handled the checkpoint |path. | |Correct Checkpoint Directory: $checkpointPath |Incorrect Checkpoint Directory: $legacyCheckpointDir | |Please move the data from the incorrect directory to the correct one, delete the |incorrect directory, and then restart this query. If you believe you are receiving |this message in error, you can disable it with the SQL conf |${SQLConf.STREAMING_CHECKPOINT_ESCAPED_PATH_CHECK_ENABLED.key}. \"\"\".stripMargin) } def subprocessExitedError( exitCode: Int, stderrBuffer: CircularBuffer, cause: Throwable): Throwable = { new SparkException(s\"Subprocess exited with status $exitCode. \" + s\"Error: ${stderrBuffer.toString}\", cause) } def outputDataTypeUnsupportedByNodeWithoutSerdeError( nodeName: String, dt: DataType): Throwable = { new SparkException(s\"$nodeName without serde does not support \" + s\"${dt.getClass.getSimpleName} as output data type\") } def invalidStartIndexError(numRows: Int, startIndex: Int): Throwable = { new ArrayIndexOutOfBoundsException( \"Invalid `startIndex` provided for generating iterator over the array. \" + s\"Total elements: $numRows, requested `startIndex`: $startIndex\") } def concurrentModificationOnExternalAppendOnlyUnsafeRowArrayError( className: String): Throwable = { new ConcurrentModificationException( s\"The backing $className has been modified since the creation of this Iterator\") } def doExecuteBroadcastNotImplementedError(nodeName: String): Throwable = { new UnsupportedOperationException(s\"$nodeName does not implement doExecuteBroadcast\") } def databaseNameConflictWithSystemPreservedDatabaseError(globalTempDB: String): Throwable = { new SparkException( s\"\"\" |$globalTempDB is a system preserved database, please rename your existing database |to resolve the name conflict, or set a different value for |${GLOBAL_TEMP_DATABASE.key}, and launch your Spark application again. \"\"\".stripMargin.split(\"\\n\").mkString(\" \")) } def commentOnTableUnsupportedError(): Throwable = { new SQLFeatureNotSupportedException(\"comment on table is not supported\") } def unsupportedUpdateColumnNullabilityError(): Throwable = { new SQLFeatureNotSupportedException(\"UpdateColumnNullability is not supported\") } def renameColumnUnsupportedForOlderMySQLError(): Throwable = { new SQLFeatureNotSupportedException( \"Rename column is only supported for MySQL version 8.0 and above.\") } def failedToExecuteQueryError(e: Throwable): QueryExecutionException = { val message = \"Hit an error when executing a query\" + (if (e.getMessage == null) \"\" else s\": ${e.getMessage}\") new QueryExecutionException(message, e) } def nestedFieldUnsupportedError(colName: String): Throwable = { new UnsupportedOperationException(s\"Nested field $colName is not supported.\") } def transformationsAndActionsNotInvokedByDriverError(): Throwable = { new SparkException( \"\"\" |Dataset transformations and actions can only be invoked by the driver, not inside of |other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() |* x) is invalid because the values transformation and count action cannot be |performed inside of the dataset1.map transformation. For more information, |see SPARK-28702. \"\"\".stripMargin.split(\"\\n\").mkString(\" \")) } def repeatedPivotsUnsupportedError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(s\"Repeated ${toSQLStmt(\"pivot\")}s.\")) } def pivotNotAfterGroupByUnsupportedError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(s\"${toSQLStmt(\"pivot\")} not after a ${toSQLStmt(\"group by\")}.\")) } private val aesFuncName = toSQLId(\"aes_encrypt\") + \"/\" + toSQLId(\"aes_decrypt\") def invalidAesKeyLengthError(actualLength: Int): RuntimeException = { new SparkRuntimeException( errorClass = \"INVALID_PARAMETER_VALUE\", messageParameters = Array( \"key\", s\"the $aesFuncName function\", s\"expects a binary value with 16, 24 or 32 bytes, but got ${actualLength.toString} bytes.\")) } def aesModeUnsupportedError(mode: String, padding: String): RuntimeException = { new SparkRuntimeException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array( s\"AES-$mode with the padding $padding by the $aesFuncName function.\")) } def aesCryptoError(detailMessage: String): RuntimeException = { new SparkRuntimeException( errorClass = \"INVALID_PARAMETER_VALUE\", messageParameters = Array( \"expr, key\", s\"the $aesFuncName function\", s\"Detail message: $detailMessage\")) } def hiveTableWithAnsiIntervalsError(tableName: String): Throwable = { new UnsupportedOperationException(s\"Hive table $tableName with ANSI intervals is not supported\") } def cannotConvertOrcTimestampToTimestampNTZError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_OPERATION\", messageParameters = Array( s\"Unable to convert ${toSQLType(TimestampType)} of Orc to \" + s\"data type ${toSQLType(TimestampNTZType)}.\")) } def cannotConvertOrcTimestampNTZToTimestampLTZError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_OPERATION\", messageParameters = Array( s\"Unable to convert ${toSQLType(TimestampNTZType)} of Orc to \" + s\"data type ${toSQLType(TimestampType)}.\")) } def writePartitionExceedConfigSizeWhenDynamicPartitionError( numWrittenParts: Int, maxDynamicPartitions: Int, maxDynamicPartitionsKey: String): Throwable = { new SparkException( s\"Number of dynamic partitions created is $numWrittenParts\" + s\", which is more than $maxDynamicPartitions\" + s\". To solve this try to set $maxDynamicPartitionsKey\" + s\" to at least $numWrittenParts.\") } def invalidNumberFormatError(input: UTF8String, format: String): Throwable = { new IllegalArgumentException( s\"The input string '$input' does not match the given number format: '$format'\") } def multipleBucketTransformsError(): Throwable = { new UnsupportedOperationException(\"Multiple bucket transforms are not supported.\") } def unsupportedCreateNamespaceCommentError(): Throwable = { new SQLFeatureNotSupportedException(\"Create namespace comment is not supported\") } def unsupportedRemoveNamespaceCommentError(): Throwable = { new SQLFeatureNotSupportedException(\"Remove namespace comment is not supported\") } def unsupportedDropNamespaceRestrictError(): Throwable = { new SQLFeatureNotSupportedException(\"Drop namespace restrict is not supported\") } def timestampAddOverflowError(micros: Long, amount: Int, unit: String): ArithmeticException = { new SparkArithmeticException( errorClass = \"DATETIME_OVERFLOW\", messageParameters = Array( s\"add ${toSQLValue(amount, IntegerType)} $unit to \" + s\"${toSQLValue(DateTimeUtils.microsToInstant(micros), TimestampType)}\")) } }",
          "## OBJECT: org/apache/spark/sql/errors/QueryCompilationErrors.\n private[sql] object QueryCompilationErrors extends QueryErrorsBase { def groupingIDMismatchError(groupingID: GroupingID, groupByExprs: Seq[Expression]): Throwable = { new AnalysisException( errorClass = \"GROUPING_ID_COLUMN_MISMATCH\", messageParameters = Array(groupingID.groupByExprs.mkString(\",\"), groupByExprs.mkString(\",\"))) } def groupingColInvalidError(groupingCol: Expression, groupByExprs: Seq[Expression]): Throwable = { new AnalysisException( errorClass = \"GROUPING_COLUMN_MISMATCH\", messageParameters = Array(groupingCol.toString, groupByExprs.mkString(\",\"))) } def groupingSizeTooLargeError(sizeLimit: Int): Throwable = { new AnalysisException( errorClass = \"GROUPING_SIZE_LIMIT_EXCEEDED\", messageParameters = Array(sizeLimit.toString)) } def zeroArgumentIndexError(): Throwable = { new AnalysisException( errorClass = \"INVALID_PARAMETER_VALUE\", messageParameters = Array( \"strfmt\", toSQLId(\"format_string\"), \"expects %1$, %2$ and so on, but got %0$.\")) } def unorderablePivotColError(pivotCol: Expression): Throwable = { new AnalysisException( errorClass = \"INCOMPARABLE_PIVOT_COLUMN\", messageParameters = Array(toSQLId(pivotCol.sql))) } def nonLiteralPivotValError(pivotVal: Expression): Throwable = { new AnalysisException( errorClass = \"NON_LITERAL_PIVOT_VALUES\", messageParameters = Array(toSQLExpr(pivotVal))) } def pivotValDataTypeMismatchError(pivotVal: Expression, pivotCol: Expression): Throwable = { new AnalysisException( errorClass = \"PIVOT_VALUE_DATA_TYPE_MISMATCH\", messageParameters = Array( pivotVal.toString, pivotVal.dataType.simpleString, pivotCol.dataType.catalogString)) } def unsupportedIfNotExistsError(tableName: String): Throwable = { new AnalysisException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array( s\"${toSQLStmt(\"IF NOT EXISTS\")} for the table ${toSQLId(tableName)} \" + s\"by ${toSQLStmt(\"INSERT INTO\")}.\")) } def nonPartitionColError(partitionName: String): Throwable = { new AnalysisException( errorClass = \"NON_PARTITION_COLUMN\", messageParameters = Array(toSQLId(partitionName))) } def missingStaticPartitionColumn(staticName: String): Throwable = { new AnalysisException( errorClass = \"MISSING_STATIC_PARTITION_COLUMN\", messageParameters = Array(staticName)) } def nestedGeneratorError(trimmedNestedGenerator: Expression): Throwable = { new AnalysisException( \"Generators are not supported when it's nested in \" + \"expressions, but got: \" + toPrettySQL(trimmedNestedGenerator)) } def moreThanOneGeneratorError(generators: Seq[Expression], clause: String): Throwable = { new AnalysisException( s\"Only one generator allowed per $clause clause but found \" + generators.size + \": \" + generators.map(toPrettySQL).mkString(\", \")) } def generatorOutsideSelectError(plan: LogicalPlan): Throwable = { new AnalysisException( \"Generators are not supported outside the SELECT clause, but \" + \"got: \" + plan.simpleString(SQLConf.get.maxToStringFields)) } def legacyStoreAssignmentPolicyError(): Throwable = { val configKey = SQLConf.STORE_ASSIGNMENT_POLICY.key new AnalysisException( \"LEGACY store assignment policy is disallowed in Spark data source V2. \" + s\"Please set the configuration $configKey to other values.\") } def unresolvedUsingColForJoinError( colName: String, plan: LogicalPlan, side: String): Throwable = { new AnalysisException( s\"USING column `$colName` cannot be resolved on the $side \" + s\"side of the join. The $side-side columns: [${plan.output.map(_.name).mkString(\", \")}]\") } def dataTypeMismatchForDeserializerError( dataType: DataType, desiredType: String): Throwable = { val quantifier = if (desiredType.equals(\"array\")) \"an\" else \"a\" new AnalysisException( s\"need $quantifier $desiredType field but got \" + dataType.catalogString) } def fieldNumberMismatchForDeserializerError( schema: StructType, maxOrdinal: Int): Throwable = { new AnalysisException( s\"Try to map ${schema.catalogString} to Tuple${maxOrdinal + 1}, \" + \"but failed as the number of fields does not line up.\") } def upCastFailureError( fromStr: String, from: Expression, to: DataType, walkedTypePath: Seq[String]): Throwable = { new AnalysisException( errorClass = \"CANNOT_UP_CAST_DATATYPE\", messageParameters = Array( fromStr, toSQLType(from.dataType), toSQLType(to), s\"The type path of the target object is:\\n\" + walkedTypePath.mkString(\"\", \"\\n\", \"\\n\") + \"You can either add an explicit cast to the input data or choose a higher precision \" + \"type of the field in the target object\" ) ) } def outerScopeFailureForNewInstanceError(className: String): Throwable = { new AnalysisException( s\"Unable to generate an encoder for inner class `$className` without \" + \"access to the scope that this class was defined in.\\n\" + \"Try moving this class out of its parent class.\") } def referenceColNotFoundForAlterTableChangesError( after: TableChange.After, parentName: String): Throwable = { new AnalysisException( s\"Couldn't find the reference column for $after at $parentName\") } def windowSpecificationNotDefinedError(windowName: String): Throwable = { new AnalysisException(s\"Window specification $windowName is not defined in the WINDOW clause.\") } def selectExprNotInGroupByError(expr: Expression, groupByAliases: Seq[Alias]): Throwable = { new AnalysisException(s\"$expr doesn't show up in the GROUP BY list $groupByAliases\") } def groupingMustWithGroupingSetsOrCubeOrRollupError(): Throwable = { new AnalysisException( errorClass = \"UNSUPPORTED_GROUPING_EXPRESSION\", messageParameters = Array.empty) } def pandasUDFAggregateNotSupportedInPivotError(): Throwable = { new AnalysisException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(\"Pandas UDF aggregate expressions don't support pivot.\")) } def aggregateExpressionRequiredForPivotError(sql: String): Throwable = { new AnalysisException(s\"Aggregate expression required for pivot, but '$sql' \" + \"did not appear in any aggregate function.\") } def writeIntoTempViewNotAllowedError(quoted: String): Throwable = { new AnalysisException(\"Cannot write into temp view \" + s\"$quoted as it's not a data source v2 relation.\") } def expectTableOrPermanentViewNotTempViewError( quoted: String, cmd: String, t: TreeNode[_]): Throwable = { new AnalysisException(s\"$quoted is a temp view. '$cmd' expects a table or permanent view.\", t.origin.line, t.origin.startPosition) } def readNonStreamingTempViewError(quoted: String): Throwable = { new AnalysisException(s\"$quoted is not a temp view of streaming \" + \"logical plan, please use batch API such as `DataFrameReader.table` to read it.\") } def viewDepthExceedsMaxResolutionDepthError( identifier: TableIdentifier, maxNestedViewDepth: Int, t: TreeNode[_]): Throwable = { new AnalysisException(s\"The depth of view $identifier exceeds the maximum \" + s\"view resolution depth ($maxNestedViewDepth). Analysis is aborted to \" + s\"avoid errors. Increase the value of ${SQLConf.MAX_NESTED_VIEW_DEPTH.key} to work \" + \"around this.\", t.origin.line, t.origin.startPosition) } def insertIntoViewNotAllowedError(identifier: TableIdentifier, t: TreeNode[_]): Throwable = { new AnalysisException(s\"Inserting into a view is not allowed. View: $identifier.\", t.origin.line, t.origin.startPosition) } def writeIntoViewNotAllowedError(identifier: TableIdentifier, t: TreeNode[_]): Throwable = { new AnalysisException(s\"Writing into a view is not allowed. View: $identifier.\", t.origin.line, t.origin.startPosition) } def writeIntoV1TableNotAllowedError(identifier: TableIdentifier, t: TreeNode[_]): Throwable = { new AnalysisException(s\"Cannot write into v1 table: $identifier.\", t.origin.line, t.origin.startPosition) } def expectTableNotViewError( v: ResolvedView, cmd: String, mismatchHint: Option[String], t: TreeNode[_]): Throwable = { val viewStr = if (v.isTemp) \"temp view\" else \"view\" val hintStr = mismatchHint.map(\" \" + _).getOrElse(\"\") new AnalysisException(s\"${v.identifier.quoted} is a $viewStr. '$cmd' expects a table.$hintStr\", t.origin.line, t.origin.startPosition) } def expectViewNotTableError( v: ResolvedTable, cmd: String, mismatchHint: Option[String], t: TreeNode[_]): Throwable = { val hintStr = mismatchHint.map(\" \" + _).getOrElse(\"\") new AnalysisException(s\"${v.identifier.quoted} is a table. '$cmd' expects a view.$hintStr\", t.origin.line, t.origin.startPosition) } def expectPersistentFuncError( name: String, cmd: String, mismatchHint: Option[String], t: TreeNode[_]): Throwable = { val hintStr = mismatchHint.map(\" \" + _).getOrElse(\"\") new AnalysisException( s\"$name is a built-in/temporary function. '$cmd' expects a persistent function.$hintStr\", t.origin.line, t.origin.startPosition) } def permanentViewNotSupportedByStreamingReadingAPIError(quoted: String): Throwable = { new AnalysisException(s\"$quoted is a permanent view, which is not supported by \" + \"streaming reading API such as `DataStreamReader.table` yet.\") } def starNotAllowedWhenGroupByOrdinalPositionUsedError(): Throwable = { new AnalysisException( \"Star (*) is not allowed in select list when GROUP BY ordinal position is used\") } def invalidStarUsageError(prettyName: String, stars: Seq[Star]): Throwable = { val regExpr = stars.collect{ case UnresolvedRegex(pattern, _, _) => s\"'$pattern'\" } val resExprMsg = Option(regExpr.distinct).filter(_.nonEmpty).map { case Seq(p) => s\"regular expression $p\" case patterns => s\"regular expressions ${patterns.mkString(\", \")}\" } val starMsg = if (stars.length - regExpr.length > 0) { Some(\"'*'\") } else { None } val elem = Seq(starMsg, resExprMsg).flatten.mkString(\" and \") new AnalysisException(s\"Invalid usage of $elem in $prettyName\") } def singleTableStarInCountNotAllowedError(targetString: String): Throwable = { new AnalysisException(s\"count($targetString.*) is not allowed. \" + \"Please use count(*) or expand the columns manually, e.g. count(col1, col2)\") } def orderByPositionRangeError(index: Int, size: Int, t: TreeNode[_]): Throwable = { new AnalysisException(s\"ORDER BY position $index is not in select list \" + s\"(valid range is [1, $size])\", t.origin.line, t.origin.startPosition) } def groupByPositionRefersToAggregateFunctionError( index: Int, expr: Expression): Throwable = { new AnalysisException(s\"GROUP BY $index refers to an expression that is or contains \" + \"an aggregate function. Aggregate functions are not allowed in GROUP BY, \" + s\"but got ${expr.sql}\") } def groupByPositionRangeError(index: Int, size: Int): Throwable = { new AnalysisException(s\"GROUP BY position $index is not in select list \" + s\"(valid range is [1, $size])\") } def generatorNotExpectedError(name: FunctionIdentifier, classCanonicalName: String): Throwable = { new AnalysisException(s\"$name is expected to be a generator. However, \" + s\"its class is $classCanonicalName, which is not a generator.\") } def functionWithUnsupportedSyntaxError(prettyName: String, syntax: String): Throwable = { new AnalysisException(s\"Function $prettyName does not support $syntax\") } def nonDeterministicFilterInAggregateError(): Throwable = { new AnalysisException(\"FILTER expression is non-deterministic, \" + \"it cannot be used in aggregate functions\") } def nonBooleanFilterInAggregateError(): Throwable = { new AnalysisException(\"FILTER expression is not of type boolean. \" + \"It cannot be used in an aggregate function\") } def aggregateInAggregateFilterError(): Throwable = { new AnalysisException(\"FILTER expression contains aggregate. \" + \"It cannot be used in an aggregate function\") } def windowFunctionInAggregateFilterError(): Throwable = { new AnalysisException(\"FILTER expression contains window function. \" + \"It cannot be used in an aggregate function\") } def aliasNumberNotMatchColumnNumberError( columnSize: Int, outputSize: Int, t: TreeNode[_]): Throwable = { new AnalysisException(\"Number of column aliases does not match number of columns. \" + s\"Number of column aliases: $columnSize; \" + s\"number of columns: $outputSize.\", t.origin.line, t.origin.startPosition) } def aliasesNumberNotMatchUDTFOutputError( aliasesSize: Int, aliasesNames: String): Throwable = { new AnalysisException(\"The number of aliases supplied in the AS clause does not \" + s\"match the number of columns output by the UDTF expected $aliasesSize \" + s\"aliases but got $aliasesNames \") } def windowAggregateFunctionWithFilterNotSupportedError(): Throwable = { new AnalysisException(\"window aggregate function with filter predicate is not supported yet.\") } def windowFunctionInsideAggregateFunctionNotAllowedError(): Throwable = { new AnalysisException(\"It is not allowed to use a window function inside an aggregate \" + \"function. Please use the inner window function in a sub-query.\") } def expressionWithoutWindowExpressionError(expr: NamedExpression): Throwable = { new AnalysisException(s\"$expr does not have any WindowExpression.\") } def expressionWithMultiWindowExpressionsError( expr: NamedExpression, distinctWindowSpec: Seq[WindowSpecDefinition]): Throwable = { new AnalysisException(s\"$expr has multiple Window Specifications ($distinctWindowSpec).\" + \"Please file a bug report with this error message, stack trace, and the query.\") } def windowFunctionNotAllowedError(clauseName: String): Throwable = { new AnalysisException(s\"It is not allowed to use window functions inside $clauseName clause\") } def cannotSpecifyWindowFrameError(prettyName: String): Throwable = { new AnalysisException(s\"Cannot specify window frame for $prettyName function\") } def windowFrameNotMatchRequiredFrameError( f: SpecifiedWindowFrame, required: WindowFrame): Throwable = { new AnalysisException(s\"Window Frame $f must match the required frame $required\") } def windowFunctionWithWindowFrameNotOrderedError(wf: WindowFunction): Throwable = { new AnalysisException(s\"Window function $wf requires window to be ordered, please add \" + s\"ORDER BY clause. For example SELECT $wf(value_expr) OVER (PARTITION BY window_partition \" + \"ORDER BY window_ordering) from table\") } def cannotResolveUserSpecifiedColumnsError(col: String, t: TreeNode[_]): Throwable = { new AnalysisException(s\"Cannot resolve column name $col\", t.origin.line, t.origin.startPosition) } def writeTableWithMismatchedColumnsError( columnSize: Int, outputSize: Int, t: TreeNode[_]): Throwable = { new AnalysisException(\"Cannot write to table due to mismatched user specified column \" + s\"size($columnSize) and data column size($outputSize)\", t.origin.line, t.origin.startPosition) } def multiTimeWindowExpressionsNotSupportedError(t: TreeNode[_]): Throwable = { new AnalysisException(\"Multiple time/session window expressions would result in a cartesian \" + \"product of rows, therefore they are currently not supported.\", t.origin.line, t.origin.startPosition) } def sessionWindowGapDurationDataTypeError(dt: DataType): Throwable = { new AnalysisException(\"Gap duration expression used in session window must be \" + s\"CalendarIntervalType, but got ${dt}\") } def viewOutputNumberMismatchQueryColumnNamesError( output: Seq[Attribute], queryColumnNames: Seq[String]): Throwable = { new AnalysisException( s\"The view output ${output.mkString(\"[\", \",\", \"]\")} doesn't have the same\" + \"number of columns with the query column names \" + s\"${queryColumnNames.mkString(\"[\", \",\", \"]\")}\") } def attributeNotFoundError(colName: String, child: LogicalPlan): Throwable = { new AnalysisException( s\"Attribute with name '$colName' is not found in \" + s\"'${child.output.map(_.name).mkString(\"(\", \",\", \")\")}'\") } def functionUndefinedError(name: FunctionIdentifier): Throwable = { new AnalysisException(s\"undefined function $name\") } def invalidFunctionArgumentsError( name: String, expectedInfo: String, actualNumber: Int): Throwable = { new AnalysisException(s\"Invalid number of arguments for function $name. \" + s\"Expected: $expectedInfo; Found: $actualNumber\") } def invalidFunctionArgumentNumberError( validParametersCount: Seq[Int], name: String, actualNumber: Int): Throwable = { if (validParametersCount.length == 0) { new AnalysisException(s\"Invalid arguments for function $name\") } else { val expectedNumberOfParameters = if (validParametersCount.length == 1) { validParametersCount.head.toString } else { validParametersCount.init.mkString(\"one of \", \", \", \" and \") + validParametersCount.last } invalidFunctionArgumentsError(name, expectedNumberOfParameters, actualNumber) } } def functionAcceptsOnlyOneArgumentError(name: String): Throwable = { new AnalysisException(s\"Function $name accepts only one argument\") } def alterV2TableSetLocationWithPartitionNotSupportedError(): Throwable = { new AnalysisException(\"ALTER TABLE SET LOCATION does not support partition for v2 tables.\") } def joinStrategyHintParameterNotSupportedError(unsupported: Any): Throwable = { new AnalysisException(\"Join strategy hint parameter \" + s\"should be an identifier or string but was $unsupported (${unsupported.getClass}\") } def invalidHintParameterError( hintName: String, invalidParams: Seq[Any]): Throwable = { new AnalysisException(s\"$hintName Hint parameter should include columns, but \" + s\"${invalidParams.mkString(\", \")} found\") } def invalidCoalesceHintParameterError(hintName: String): Throwable = { new AnalysisException(s\"$hintName Hint expects a partition number as a parameter\") } def attributeNameSyntaxError(name: String): Throwable = { new AnalysisException(s\"syntax error in attribute name: $name\") } def starExpandDataTypeNotSupportedError(attributes: Seq[String]): Throwable = { new AnalysisException(s\"Can only star expand struct data types. Attribute: `$attributes`\") } def cannotResolveStarExpandGivenInputColumnsError( targetString: String, columns: String): Throwable = { new AnalysisException(s\"cannot resolve '$targetString.*' given input columns '$columns'\") } def addColumnWithV1TableCannotSpecifyNotNullError(): Throwable = { new AnalysisException(\"ADD COLUMN with v1 tables cannot specify NOT NULL.\") } def operationOnlySupportedWithV2TableError(operation: String): Throwable = { new AnalysisException(s\"$operation is only supported with v2 tables.\") } def alterColumnWithV1TableCannotSpecifyNotNullError(): Throwable = { new AnalysisException(\"ALTER COLUMN with v1 tables cannot specify NOT NULL.\") } def alterColumnCannotFindColumnInV1TableError(colName: String, v1Table: V1Table): Throwable = { new AnalysisException( s\"ALTER COLUMN cannot find column $colName in v1 table. \" + s\"Available: ${v1Table.schema.fieldNames.mkString(\", \")}\") } def invalidDatabaseNameError(quoted: String): Throwable = { new AnalysisException(s\"The database name is not valid: $quoted\") } def cannotDropViewWithDropTableError(): Throwable = { new AnalysisException(\"Cannot drop a view with DROP TABLE. Please use DROP VIEW instead\") } def showColumnsWithConflictDatabasesError( db: Seq[String], v1TableName: TableIdentifier): Throwable = { new AnalysisException(\"SHOW COLUMNS with conflicting databases: \" + s\"'${db.head}' != '${v1TableName.database.get}'\") } def sqlOnlySupportedWithV1TablesError(sql: String): Throwable = { new AnalysisException(s\"$sql is only supported with v1 tables.\") } def cannotCreateTableWithBothProviderAndSerdeError( provider: Option[String], maybeSerdeInfo: Option[SerdeInfo]): Throwable = { new AnalysisException( s\"Cannot create table with both USING $provider and ${maybeSerdeInfo.get.describe}\") } def invalidFileFormatForStoredAsError(serdeInfo: SerdeInfo): Throwable = { new AnalysisException( s\"STORED AS with file format '${serdeInfo.storedAs.get}' is invalid.\") } def commandNotSupportNestedColumnError(command: String, quoted: String): Throwable = { new AnalysisException(s\"$command does not support nested column: $quoted\") } def columnDoesNotExistError(colName: String): Throwable = { new AnalysisException(s\"Column $colName does not exist\") } def renameTempViewToExistingViewError(oldName: String, newName: String): Throwable = { new AnalysisException( s\"rename temporary view from '$oldName' to '$newName': destination view already exists\") } def cannotDropNonemptyDatabaseError(db: String): Throwable = { new AnalysisException(s\"Cannot drop a non-empty database: $db. \" + \"Use CASCADE option to drop a non-empty database.\") } def cannotDropNonemptyNamespaceError(namespace: Seq[String]): Throwable = { new AnalysisException(s\"Cannot drop a non-empty namespace: ${namespace.quoted}. \" + \"Use CASCADE option to drop a non-empty namespace.\") } def invalidNameForTableOrDatabaseError(name: String): Throwable = { new AnalysisException(s\"`$name` is not a valid name for tables/databases. \" + \"Valid names only contain alphabet characters, numbers and _.\") } def cannotCreateDatabaseWithSameNameAsPreservedDatabaseError(database: String): Throwable = { new AnalysisException(s\"$database is a system preserved database, \" + \"you cannot create a database with this name.\") } def cannotDropDefaultDatabaseError(): Throwable = { new AnalysisException(\"Can not drop default database\") } def cannotUsePreservedDatabaseAsCurrentDatabaseError(database: String): Throwable = { new AnalysisException(s\"$database is a system preserved database, you cannot use it as \" + \"current database. To access global temporary views, you should use qualified name with \" + s\"the GLOBAL_TEMP_DATABASE, e.g. SELECT * FROM $database.viewName.\") } def createExternalTableWithoutLocationError(): Throwable = { new AnalysisException(\"CREATE EXTERNAL TABLE must be accompanied by LOCATION\") } def cannotOperateManagedTableWithExistingLocationError( methodName: String, tableIdentifier: TableIdentifier, tableLocation: Path): Throwable = { new AnalysisException(s\"Can not $methodName the managed table('$tableIdentifier')\" + s\". The associated location('${tableLocation.toString}') already exists.\") } def dropNonExistentColumnsNotSupportedError( nonExistentColumnNames: Seq[String]): Throwable = { new AnalysisException( s\"\"\" |Some existing schema fields (${nonExistentColumnNames.mkString(\"[\", \",\", \"]\")}) are |not present in the new schema. We don't support dropping columns yet. \"\"\".stripMargin) } def cannotRetrieveTableOrViewNotInSameDatabaseError( qualifiedTableNames: Seq[QualifiedTableName]): Throwable = { new AnalysisException(\"Only the tables/views belong to the same database can be retrieved. \" + s\"Querying tables/views are $qualifiedTableNames\") } def renameTableSourceAndDestinationMismatchError(db: String, newDb: String): Throwable = { new AnalysisException( s\"RENAME TABLE source and destination databases do not match: '$db' != '$newDb'\") } def cannotRenameTempViewWithDatabaseSpecifiedError( oldName: TableIdentifier, newName: TableIdentifier): Throwable = { new AnalysisException(s\"RENAME TEMPORARY VIEW from '$oldName' to '$newName': cannot \" + s\"specify database name '${newName.database.get}' in the destination table\") } def cannotRenameTempViewToExistingTableError( oldName: TableIdentifier, newName: TableIdentifier): Throwable = { new AnalysisException(s\"RENAME TEMPORARY VIEW from '$oldName' to '$newName': \" + \"destination table already exists\") } def invalidPartitionSpecError(details: String): Throwable = { new AnalysisException(s\"Partition spec is invalid. $details\") } def functionAlreadyExistsError(func: FunctionIdentifier): Throwable = { new AnalysisException(s\"Function $func already exists\") } def cannotLoadClassWhenRegisteringFunctionError( className: String, func: FunctionIdentifier): Throwable = { new AnalysisException(s\"Can not load class '$className' when registering \" + s\"the function '$func', please make sure it is on the classpath\") } def resourceTypeNotSupportedError(resourceType: String): Throwable = { new AnalysisException(s\"Resource Type '$resourceType' is not supported.\") } def tableNotSpecifyDatabaseError(identifier: TableIdentifier): Throwable = { new AnalysisException(s\"table $identifier did not specify database\") } def tableNotSpecifyLocationUriError(identifier: TableIdentifier): Throwable = { new AnalysisException(s\"table $identifier did not specify locationUri\") } def partitionNotSpecifyLocationUriError(specString: String): Throwable = { new AnalysisException(s\"Partition [$specString] did not specify locationUri\") } def invalidBucketNumberError(bucketingMaxBuckets: Int, numBuckets: Int): Throwable = { new AnalysisException( s\"Number of buckets should be greater than 0 but less than or equal to \" + s\"bucketing.maxBuckets (`$bucketingMaxBuckets`). Got `$numBuckets`\") } def corruptedTableNameContextInCatalogError(numParts: Int, index: Int): Throwable = { new AnalysisException(\"Corrupted table name context in catalog: \" + s\"$numParts parts expected, but part $index is missing.\") } def corruptedViewSQLConfigsInCatalogError(e: Exception): Throwable = { new AnalysisException(\"Corrupted view SQL configs in catalog\", cause = Some(e)) } def corruptedViewQueryOutputColumnsInCatalogError(numCols: String, index: Int): Throwable = { new AnalysisException(\"Corrupted view query output column names in catalog: \" + s\"$numCols parts expected, but part $index is missing.\") } def corruptedViewReferredTempViewInCatalogError(e: Exception): Throwable = { new AnalysisException(\"corrupted view referred temp view names in catalog\", cause = Some(e)) } def corruptedViewReferredTempFunctionsInCatalogError(e: Exception): Throwable = { new AnalysisException( \"corrupted view referred temp functions names in catalog\", cause = Some(e)) } def columnStatisticsDeserializationNotSupportedError( name: String, dataType: DataType): Throwable = { new AnalysisException(\"Column statistics deserialization is not supported for \" + s\"column $name of data type: $dataType.\") } def columnStatisticsSerializationNotSupportedError( colName: String, dataType: DataType): Throwable = { new AnalysisException(\"Column statistics serialization is not supported for \" + s\"column $colName of data type: $dataType.\") } def cannotReadCorruptedTablePropertyError(key: String, details: String = \"\"): Throwable = { new AnalysisException(s\"Cannot read table property '$key' as it's corrupted.$details\") } def invalidSchemaStringError(exp: Expression): Throwable = { new AnalysisException(s\"The expression '${exp.sql}' is not a valid schema string.\") } def schemaNotFoldableError(exp: Expression): Throwable = { new AnalysisException( \"Schema should be specified in DDL format as a string literal or output of \" + s\"the schema_of_json/schema_of_csv functions instead of ${exp.sql}\") } def schemaIsNotStructTypeError(dataType: DataType): Throwable = { new AnalysisException(s\"Schema should be struct type but got ${dataType.sql}.\") } def keyValueInMapNotStringError(m: CreateMap): Throwable = { new AnalysisException( s\"A type of keys and values in map() must be string, but got ${m.dataType.catalogString}\") } def nonMapFunctionNotAllowedError(): Throwable = { new AnalysisException(\"Must use a map() function for options\") } def invalidFieldTypeForCorruptRecordError(): Throwable = { new AnalysisException(\"The field for corrupt records must be string type and nullable\") } def dataTypeUnsupportedByClassError(x: DataType, className: String): Throwable = { new AnalysisException(s\"DataType '$x' is not supported by $className.\") } def parseModeUnsupportedError(funcName: String, mode: ParseMode): Throwable = { new AnalysisException(s\"$funcName() doesn't support the ${mode.name} mode. \" + s\"Acceptable modes are ${PermissiveMode.name} and ${FailFastMode.name}.\") } def requireLiteralParameter( funcName: String, argName: String, requiredType: String): Throwable = { new AnalysisException( s\"The '$argName' parameter of function '$funcName' needs to be a $requiredType literal.\") } def invalidStringLiteralParameter( funcName: String, argName: String, invalidValue: String, allowedValues: Option[String] = None): Throwable = { val endingMsg = allowedValues.map(\" \" + _).getOrElse(\"\") new AnalysisException(s\"Invalid value for the '$argName' parameter of function '$funcName': \" + s\"$invalidValue.$endingMsg\") } def literalTypeUnsupportedForSourceTypeError(field: String, source: Expression): Throwable = { new AnalysisException(s\"Literals of type '$field' are currently not supported \" + s\"for the ${source.dataType.catalogString} type.\") } def arrayComponentTypeUnsupportedError(clz: Class[_]): Throwable = { new AnalysisException(s\"Unsupported component type $clz in arrays\") } def secondArgumentNotDoubleLiteralError(): Throwable = { new AnalysisException(\"The second argument should be a double literal.\") } def dataTypeUnsupportedByExtractValueError( dataType: DataType, extraction: Expression, child: Expression): Throwable = { val errorMsg = dataType match { case StructType(_) => s\"Field name should be String Literal, but it's $extraction\" case other => s\"Can't extract value from $child: need struct type but got ${other.catalogString}\" } new AnalysisException(errorMsg) } def noHandlerForUDAFError(name: String): Throwable = { new InvalidUDFClassException(s\"No handler for UDAF '$name'. \" + \"Use sparkSession.udf.register(...) instead.\") } def batchWriteCapabilityError( table: Table, v2WriteClassName: String, v1WriteClassName: String): Throwable = { new AnalysisException( s\"Table ${table.name} declares ${TableCapability.V1_BATCH_WRITE} capability but \" + s\"$v2WriteClassName is not an instance of $v1WriteClassName\") } def unsupportedDeleteByConditionWithSubqueryError(condition: Expression): Throwable = { new AnalysisException( s\"Delete by condition with subquery is not supported: $condition\") } def cannotTranslateExpressionToSourceFilterError(f: Expression): Throwable = { new AnalysisException(\"Exec update failed:\" + s\" cannot translate expression to source filter: $f\") } def cannotDeleteTableWhereFiltersError(table: Table, filters: Array[Filter]): Throwable = { new AnalysisException( s\"Cannot delete from table ${table.name} where ${filters.mkString(\"[\", \", \", \"]\")}\") } def deleteOnlySupportedWithV2TablesError(): Throwable = { new AnalysisException(\"DELETE is only supported with v2 tables.\") } def describeDoesNotSupportPartitionForV2TablesError(): Throwable = { new AnalysisException(\"DESCRIBE does not support partition for v2 tables.\") } def cannotReplaceMissingTableError( tableIdentifier: Identifier): Throwable = { new CannotReplaceMissingTableException(tableIdentifier) } def cannotReplaceMissingTableError( tableIdentifier: Identifier, cause: Option[Throwable]): Throwable = { new CannotReplaceMissingTableException(tableIdentifier, cause) } def unsupportedTableOperationError(table: Table, cmd: String): Throwable = { new AnalysisException(s\"Table ${table.name} does not support $cmd.\") } def unsupportedBatchReadError(table: Table): Throwable = { unsupportedTableOperationError(table, \"batch scan\") } def unsupportedMicroBatchOrContinuousScanError(table: Table): Throwable = { unsupportedTableOperationError(table, \"either micro-batch or continuous scan\") } def unsupportedAppendInBatchModeError(table: Table): Throwable = { unsupportedTableOperationError(table, \"append in batch mode\") } def unsupportedDynamicOverwriteInBatchModeError(table: Table): Throwable = { unsupportedTableOperationError(table, \"dynamic overwrite in batch mode\") } def unsupportedTruncateInBatchModeError(table: Table): Throwable = { unsupportedTableOperationError(table, \"truncate in batch mode\") } def unsupportedOverwriteByFilterInBatchModeError(table: Table): Throwable = { unsupportedTableOperationError(table, \"overwrite by filter in batch mode\") } def streamingSourcesDoNotSupportCommonExecutionModeError( microBatchSources: Seq[String], continuousSources: Seq[String]): Throwable = { new AnalysisException( \"The streaming sources in a query do not have a common supported execution mode.\\n\" + \"Sources support micro-batch: \" + microBatchSources.mkString(\", \") + \"\\n\" + \"Sources support continuous: \" + continuousSources.mkString(\", \")) } def noSuchTableError(ident: Identifier): Throwable = { new NoSuchTableException(ident) } def noSuchNamespaceError(namespace: Array[String]): Throwable = { new NoSuchNamespaceException(namespace) } def tableAlreadyExistsError(ident: Identifier): Throwable = { new TableAlreadyExistsException(ident) } def requiresSinglePartNamespaceError(ns: Seq[String]): Throwable = { new AnalysisException(CatalogManager.SESSION_CATALOG_NAME + \" requires a single-part namespace, but got \" + ns.mkString(\"[\", \", \", \"]\")) } def namespaceAlreadyExistsError(namespace: Array[String]): Throwable = { new NamespaceAlreadyExistsException(namespace) } private def notSupportedInJDBCCatalog(cmd: String): Throwable = { new AnalysisException(s\"$cmd is not supported in JDBC catalog.\") } def cannotCreateJDBCTableUsingProviderError(): Throwable = { notSupportedInJDBCCatalog(\"CREATE TABLE ... USING ...\") } def cannotCreateJDBCTableUsingLocationError(): Throwable = { notSupportedInJDBCCatalog(\"CREATE TABLE ... LOCATION ...\") } def cannotCreateJDBCNamespaceUsingProviderError(): Throwable = { notSupportedInJDBCCatalog(\"CREATE NAMESPACE ... LOCATION ...\") } def cannotCreateJDBCNamespaceWithPropertyError(k: String): Throwable = { notSupportedInJDBCCatalog(s\"CREATE NAMESPACE with property $k\") } def cannotSetJDBCNamespaceWithPropertyError(k: String): Throwable = { notSupportedInJDBCCatalog(s\"SET NAMESPACE with property $k\") } def cannotUnsetJDBCNamespaceWithPropertyError(k: String): Throwable = { notSupportedInJDBCCatalog(s\"Remove NAMESPACE property $k\") } def unsupportedJDBCNamespaceChangeInCatalogError(changes: Seq[NamespaceChange]): Throwable = { new AnalysisException(s\"Unsupported NamespaceChange $changes in JDBC catalog.\") } private def tableDoesNotSupportError(cmd: String, table: Table): Throwable = { new AnalysisException(s\"Table does not support $cmd: ${table.name}\") } def tableDoesNotSupportReadsError(table: Table): Throwable = { tableDoesNotSupportError(\"reads\", table) } def tableDoesNotSupportWritesError(table: Table): Throwable = { tableDoesNotSupportError(\"writes\", table) } def tableDoesNotSupportDeletesError(table: Table): Throwable = { tableDoesNotSupportError(\"deletes\", table) } def tableDoesNotSupportTruncatesError(table: Table): Throwable = { tableDoesNotSupportError(\"truncates\", table) } def tableDoesNotSupportPartitionManagementError(table: Table): Throwable = { tableDoesNotSupportError(\"partition management\", table) } def tableDoesNotSupportAtomicPartitionManagementError(table: Table): Throwable = { tableDoesNotSupportError(\"atomic partition management\", table) } def tableIsNotRowLevelOperationTableError(table: Table): Throwable = { throw new AnalysisException(s\"Table ${table.name} is not a row-level operation table\") } def cannotRenameTableWithAlterViewError(): Throwable = { new AnalysisException( \"Cannot rename a table with ALTER VIEW. Please use ALTER TABLE instead.\") } private def notSupportedForV2TablesError(cmd: String): Throwable = { new AnalysisException(s\"$cmd is not supported for v2 tables.\") } def analyzeTableNotSupportedForV2TablesError(): Throwable = { notSupportedForV2TablesError(\"ANALYZE TABLE\") } def alterTableRecoverPartitionsNotSupportedForV2TablesError(): Throwable = { notSupportedForV2TablesError(\"ALTER TABLE ... RECOVER PARTITIONS\") } def alterTableSerDePropertiesNotSupportedForV2TablesError(): Throwable = { notSupportedForV2TablesError(\"ALTER TABLE ... SET [SERDE|SERDEPROPERTIES]\") } def loadDataNotSupportedForV2TablesError(): Throwable = { notSupportedForV2TablesError(\"LOAD DATA\") } def showCreateTableAsSerdeNotSupportedForV2TablesError(): Throwable = { notSupportedForV2TablesError(\"SHOW CREATE TABLE AS SERDE\") } def showColumnsNotSupportedForV2TablesError(): Throwable = { notSupportedForV2TablesError(\"SHOW COLUMNS\") } def repairTableNotSupportedForV2TablesError(): Throwable = { notSupportedForV2TablesError(\"MSCK REPAIR TABLE\") } def databaseFromV1SessionCatalogNotSpecifiedError(): Throwable = { new AnalysisException(\"Database from v1 session catalog is not specified\") } def nestedDatabaseUnsupportedByV1SessionCatalogError(catalog: String): Throwable = { new AnalysisException(s\"Nested databases are not supported by v1 session catalog: $catalog\") } def invalidRepartitionExpressionsError(sortOrders: Seq[Any]): Throwable = { new AnalysisException(s\"Invalid partitionExprs specified: $sortOrders For range \" + \"partitioning use REPARTITION_BY_RANGE instead.\") } def partitionColumnNotSpecifiedError(format: String, partitionColumn: String): Throwable = { new AnalysisException(s\"Failed to resolve the schema for $format for \" + s\"the partition column: $partitionColumn. It must be specified manually.\") } def dataSchemaNotSpecifiedError(format: String): Throwable = { new AnalysisException(s\"Unable to infer schema for $format. It must be specified manually.\") } def dataPathNotExistError(path: String): Throwable = { new AnalysisException(s\"Path does not exist: $path\") } def dataSourceOutputModeUnsupportedError( className: String, outputMode: OutputMode): Throwable = { new AnalysisException(s\"Data source $className does not support $outputMode output mode\") } def schemaNotSpecifiedForSchemaRelationProviderError(className: String): Throwable = { new AnalysisException(s\"A schema needs to be specified when using $className.\") } def userSpecifiedSchemaMismatchActualSchemaError( schema: StructType, actualSchema: StructType): Throwable = { new AnalysisException( s\"\"\" |The user-specified schema doesn't match the actual schema: |user-specified: ${schema.toDDL}, actual: ${actualSchema.toDDL}. If you're using |DataFrameReader.schema API or creating a table, please do not specify the schema. |Or if you're scanning an existed table, please drop it and re-create it. \"\"\".stripMargin) } def dataSchemaNotSpecifiedError(format: String, fileCatalog: String): Throwable = { new AnalysisException( s\"Unable to infer schema for $format at $fileCatalog. It must be specified manually\") } def invalidDataSourceError(className: String): Throwable = { new AnalysisException(s\"$className is not a valid Spark SQL Data Source.\") } def cannotSaveIntervalIntoExternalStorageError(): Throwable = { new AnalysisException(\"Cannot save interval data type into external storage.\") } def cannotResolveAttributeError(name: String, outputStr: String): Throwable = { new AnalysisException( s\"Unable to resolve $name given [$outputStr]\") } def orcNotUsedWithHiveEnabledError(): Throwable = { new AnalysisException( s\"\"\" |Hive built-in ORC data source must be used with Hive support enabled. |Please use the native ORC data source by setting 'spark.sql.orc.impl' to 'native' \"\"\".stripMargin) } def failedToFindAvroDataSourceError(provider: String): Throwable = { new AnalysisException( s\"\"\" |Failed to find data source: $provider. Avro is built-in but external data |source module since Spark 2.4. Please deploy the application as per |the deployment section of \"Apache Avro Data Source Guide\". \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def failedToFindKafkaDataSourceError(provider: String): Throwable = { new AnalysisException( s\"\"\" |Failed to find data source: $provider. Please deploy the application as |per the deployment section of \"Structured Streaming + Kafka Integration Guide\". \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def findMultipleDataSourceError(provider: String, sourceNames: Seq[String]): Throwable = { new AnalysisException( s\"\"\" |Multiple sources found for $provider (${sourceNames.mkString(\", \")}), | please specify the fully qualified class name. \"\"\".stripMargin) } def writeEmptySchemasUnsupportedByDataSourceError(): Throwable = { new AnalysisException( s\"\"\" |Datasource does not support writing empty or nested empty schemas. |Please make sure the data schema has at least one or more column(s). \"\"\".stripMargin) } def insertMismatchedColumnNumberError( targetAttributes: Seq[Attribute], sourceAttributes: Seq[Attribute], staticPartitionsSize: Int): Throwable = { new AnalysisException( s\"\"\" |The data to be inserted needs to have the same number of columns as the |target table: target table has ${targetAttributes.size} column(s) but the |inserted data has ${sourceAttributes.size + staticPartitionsSize} column(s), |which contain $staticPartitionsSize partition column(s) having assigned |constant values. \"\"\".stripMargin) } def insertMismatchedPartitionNumberError( targetPartitionSchema: StructType, providedPartitionsSize: Int): Throwable = { new AnalysisException( s\"\"\" |The data to be inserted needs to have the same number of partition columns |as the target table: target table has ${targetPartitionSchema.fields.size} |partition column(s) but the inserted data has $providedPartitionsSize |partition columns specified. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def invalidPartitionColumnError( partKey: String, targetPartitionSchema: StructType): Throwable = { new AnalysisException( s\"\"\" |$partKey is not a partition column. Partition columns are |${targetPartitionSchema.fields.map(_.name).mkString(\"[\", \",\", \"]\")} \"\"\".stripMargin) } def multiplePartitionColumnValuesSpecifiedError( field: StructField, potentialSpecs: Map[String, String]): Throwable = { new AnalysisException( s\"\"\" |Partition column ${field.name} have multiple values specified, |${potentialSpecs.mkString(\"[\", \", \", \"]\")}. Please only specify a single value. \"\"\".stripMargin) } def invalidOrderingForConstantValuePartitionColumnError( targetPartitionSchema: StructType): Throwable = { new AnalysisException( s\"\"\" |The ordering of partition columns is |${targetPartitionSchema.fields.map(_.name).mkString(\"[\", \",\", \"]\")} |All partition columns having constant values need to appear before other |partition columns that do not have an assigned constant value. \"\"\".stripMargin) } def cannotWriteDataToRelationsWithMultiplePathsError(): Throwable = { new AnalysisException(\"Can only write data to relations with a single path.\") } def failedToRebuildExpressionError(filter: Filter): Throwable = { new AnalysisException( s\"Fail to rebuild expression: missing key $filter in `translatedFilterToExpr`\") } def dataTypeUnsupportedByDataSourceError(format: String, field: StructField): Throwable = { new AnalysisException( s\"$format data source does not support ${field.dataType.catalogString} data type.\") } def failToResolveDataSourceForTableError(table: CatalogTable, key: String): Throwable = { new AnalysisException( s\"\"\" |Fail to resolve data source for the table ${table.identifier} since the table |serde property has the duplicated key $key with extra options specified for this |scan operation. To fix this, you can rollback to the legacy behavior of ignoring |the extra options by setting the config |${SQLConf.LEGACY_EXTRA_OPTIONS_BEHAVIOR.key} to `false`, or address the |conflicts of the same config. \"\"\".stripMargin) } def outputPathAlreadyExistsError(outputPath: Path): Throwable = { new AnalysisException(s\"path $outputPath already exists.\") } def cannotUseDataTypeForPartitionColumnError(field: StructField): Throwable = { new AnalysisException(s\"Cannot use ${field.dataType} for partition column\") } def cannotUseAllColumnsForPartitionColumnsError(): Throwable = { new AnalysisException(s\"Cannot use all columns for partition columns\") } def partitionColumnNotFoundInSchemaError(col: String, schemaCatalog: String): Throwable = { new AnalysisException(s\"Partition column `$col` not found in schema $schemaCatalog\") } def columnNotFoundInSchemaError( col: StructField, tableSchema: Option[StructType]): Throwable = { new AnalysisException(s\"\"\"Column \"${col.name}\" not found in schema $tableSchema\"\"\") } def unsupportedDataSourceTypeForDirectQueryOnFilesError(className: String): Throwable = { new AnalysisException(s\"Unsupported data source type for direct query on files: $className\") } def saveDataIntoViewNotAllowedError(): Throwable = { new AnalysisException(\"Saving data into a view is not allowed.\") } def mismatchedTableFormatError( tableName: String, existingProvider: Class[_], specifiedProvider: Class[_]): Throwable = { new AnalysisException( s\"\"\" |The format of the existing table $tableName is `${existingProvider.getSimpleName}`. |It doesn't match the specified format `${specifiedProvider.getSimpleName}`. \"\"\".stripMargin) } def mismatchedTableLocationError( identifier: TableIdentifier, existingTable: CatalogTable, tableDesc: CatalogTable): Throwable = { new AnalysisException( s\"\"\" |The location of the existing table ${identifier.quotedString} is |`${existingTable.location}`. It doesn't match the specified location |`${tableDesc.location}`. \"\"\".stripMargin) } def mismatchedTableColumnNumberError( tableName: String, existingTable: CatalogTable, query: LogicalPlan): Throwable = { new AnalysisException( s\"\"\" |The column number of the existing table $tableName |(${existingTable.schema.catalogString}) doesn't match the data schema |(${query.schema.catalogString}) \"\"\".stripMargin) } def cannotResolveColumnGivenInputColumnsError(col: String, inputColumns: String): Throwable = { new AnalysisException(s\"cannot resolve '$col' given input columns: [$inputColumns]\") } def mismatchedTablePartitionColumnError( tableName: String, specifiedPartCols: Seq[String], existingPartCols: String): Throwable = { new AnalysisException( s\"\"\" |Specified partitioning does not match that of the existing table $tableName. |Specified partition columns: [${specifiedPartCols.mkString(\", \")}] |Existing partition columns: [$existingPartCols] \"\"\".stripMargin) } def mismatchedTableBucketingError( tableName: String, specifiedBucketString: String, existingBucketString: String): Throwable = { new AnalysisException( s\"\"\" |Specified bucketing does not match that of the existing table $tableName. |Specified bucketing: $specifiedBucketString |Existing bucketing: $existingBucketString \"\"\".stripMargin) } def specifyPartitionNotAllowedWhenTableSchemaNotDefinedError(): Throwable = { new AnalysisException(\"It is not allowed to specify partitioning when the \" + \"table schema is not defined.\") } def bucketingColumnCannotBePartOfPartitionColumnsError( bucketCol: String, normalizedPartCols: Seq[String]): Throwable = { new AnalysisException(s\"bucketing column '$bucketCol' should not be part of \" + s\"partition columns '${normalizedPartCols.mkString(\", \")}'\") } def bucketSortingColumnCannotBePartOfPartitionColumnsError( sortCol: String, normalizedPartCols: Seq[String]): Throwable = { new AnalysisException(s\"bucket sorting column '$sortCol' should not be part of \" + s\"partition columns '${normalizedPartCols.mkString(\", \")}'\") } def mismatchedInsertedDataColumnNumberError( tableName: String, insert: InsertIntoStatement, staticPartCols: Set[String]): Throwable = { new AnalysisException( s\"$tableName requires that the data to be inserted have the same number of columns as \" + s\"the target table: target table has ${insert.table.output.size} column(s) but the \" + s\"inserted data has ${insert.query.output.length + staticPartCols.size} column(s), \" + s\"including ${staticPartCols.size} partition column(s) having constant value(s).\") } def requestedPartitionsMismatchTablePartitionsError( tableName: String, normalizedPartSpec: Map[String, Option[String]], partColNames: StructType): Throwable = { new AnalysisException( s\"\"\" |Requested partitioning does not match the table $tableName: |Requested partitions: ${normalizedPartSpec.keys.mkString(\",\")} |Table partitions: ${partColNames.mkString(\",\")} \"\"\".stripMargin) } def ddlWithoutHiveSupportEnabledError(detail: String): Throwable = { new AnalysisException(s\"Hive support is required to $detail\") } def createTableColumnTypesOptionColumnNotFoundInSchemaError( col: String, schema: StructType): Throwable = { new AnalysisException( s\"createTableColumnTypes option column $col not found in schema ${schema.catalogString}\") } def parquetTypeUnsupportedYetError(parquetType: String): Throwable = { new AnalysisException(s\"Parquet type not yet supported: $parquetType\") } def illegalParquetTypeError(parquetType: String): Throwable = { new AnalysisException(s\"Illegal Parquet type: $parquetType\") } def unrecognizedParquetTypeError(field: String): Throwable = { new AnalysisException(s\"Unrecognized Parquet type: $field\") } def cannotConvertDataTypeToParquetTypeError(field: StructField): Throwable = { new AnalysisException(s\"Unsupported data type ${field.dataType.catalogString}\") } def incompatibleViewSchemaChange( viewName: String, colName: String, expectedNum: Int, actualCols: Seq[Attribute], viewDDL: Option[String]): Throwable = { new AnalysisException(s\"The SQL query of view $viewName has an incompatible schema change \" + s\"and column $colName cannot be resolved. Expected $expectedNum columns named $colName but \" + s\"got ${actualCols.map(_.name).mkString(\"[\", \",\", \"]\")}\" + viewDDL.map(s => s\"\\nPlease try to re-create the view by running: $s\").getOrElse(\"\")) } def numberOfPartitionsNotAllowedWithUnspecifiedDistributionError(): Throwable = { throw new AnalysisException(\"The number of partitions can't be specified with unspecified\" + \" distribution. Invalid writer requirements detected.\") } def cannotApplyTableValuedFunctionError( name: String, arguments: String, usage: String, details: String = \"\"): Throwable = { new AnalysisException(s\"Table-valued function $name with alternatives: $usage\\n\" + s\"cannot be applied to ($arguments): $details\") } def incompatibleRangeInputDataTypeError( expression: Expression, dataType: DataType): Throwable = { new AnalysisException(s\"Incompatible input data type. \" + s\"Expected: ${dataType.typeName}; Found: ${expression.dataType.typeName}\") } def streamJoinStreamWithoutEqualityPredicateUnsupportedError(plan: LogicalPlan): Throwable = { new AnalysisException( \"Stream-stream join without equality predicate is not supported\", plan = Some(plan)) } def cannotUseMixtureOfAggFunctionAndGroupAggPandasUDFError(): Throwable = { new AnalysisException( errorClass = \"CANNOT_USE_MIXTURE\", messageParameters = Array.empty) } def ambiguousAttributesInSelfJoinError( ambiguousAttrs: Seq[AttributeReference]): Throwable = { new AnalysisException( s\"\"\" |Column ${ambiguousAttrs.mkString(\", \")} are ambiguous. It's probably because |you joined several Datasets together, and some of these Datasets are the same. |This column points to one of the Datasets but Spark is unable to figure out |which one. Please alias the Datasets with different names via `Dataset.as` |before joining them, and specify the column using qualified name, e.g. |`df.as(\"a\").join(df.as(\"b\"), $$\"a.id\" > $$\"b.id\")`. You can also set |${SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED.key} to false to disable this check. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def unexpectedEvalTypesForUDFsError(evalTypes: Set[Int]): Throwable = { new AnalysisException( s\"Expected udfs have the same evalType but got different evalTypes: \" + s\"${evalTypes.mkString(\",\")}\") } def ambiguousFieldNameError( fieldName: Seq[String], numMatches: Int, context: Origin): Throwable = { new AnalysisException( errorClass = \"AMBIGUOUS_FIELD_NAME\", messageParameters = Array(fieldName.quoted, numMatches.toString), origin = context) } def cannotUseIntervalTypeInTableSchemaError(): Throwable = { new AnalysisException(\"Cannot use interval type in the table schema.\") } def cannotPartitionByNestedColumnError(reference: NamedReference): Throwable = { new AnalysisException(s\"Cannot partition by nested column: $reference\") } def missingCatalogAbilityError(plugin: CatalogPlugin, ability: String): Throwable = { new AnalysisException(s\"Catalog ${plugin.name} does not support $ability\") } def identifierHavingMoreThanTwoNamePartsError( quoted: String, identifier: String): Throwable = { new AnalysisException(s\"$quoted is not a valid $identifier as it has more than 2 name parts.\") } def emptyMultipartIdentifierError(): Throwable = { new AnalysisException(\"multi-part identifier cannot be empty.\") } def cannotOperateOnHiveDataSourceFilesError(operation: String): Throwable = { new AnalysisException(\"Hive data source can only be used with tables, you can not \" + s\"$operation files of Hive data source directly.\") } def setPathOptionAndCallWithPathParameterError(method: String): Throwable = { new AnalysisException( s\"\"\" |There is a 'path' option set and $method() is called with a path |parameter. Either remove the path option, or call $method() without the parameter. |To ignore this check, set '${SQLConf.LEGACY_PATH_OPTION_BEHAVIOR.key}' to 'true'. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def userSpecifiedSchemaUnsupportedError(operation: String): Throwable = { new AnalysisException(s\"User specified schema not supported with `$operation`\") } def tempViewNotSupportStreamingWriteError(viewName: String): Throwable = { new AnalysisException(s\"Temporary view $viewName doesn't support streaming write\") } def streamingIntoViewNotSupportedError(viewName: String): Throwable = { new AnalysisException(s\"Streaming into views $viewName is not supported.\") } def inputSourceDiffersFromDataSourceProviderError( source: String, tableName: String, table: CatalogTable): Throwable = { new AnalysisException(s\"The input source($source) is different from the table \" + s\"$tableName's data source provider(${table.provider.get}).\") } def tableNotSupportStreamingWriteError(tableName: String, t: Table): Throwable = { new AnalysisException(s\"Table $tableName doesn't support streaming write - $t\") } def queryNameNotSpecifiedForMemorySinkError(): Throwable = { new AnalysisException(\"queryName must be specified for memory sink\") } def sourceNotSupportedWithContinuousTriggerError(source: String): Throwable = { new AnalysisException(s\"'$source' is not supported with continuous trigger\") } def columnNotFoundInExistingColumnsError( columnType: String, columnName: String, validColumnNames: Seq[String]): Throwable = { new AnalysisException(s\"$columnType column $columnName not found in \" + s\"existing columns (${validColumnNames.mkString(\", \")})\") } def operationNotSupportPartitioningError(operation: String): Throwable = { new AnalysisException(s\"'$operation' does not support partitioning\") } def mixedRefsInAggFunc(funcStr: String): Throwable = { val msg = \"Found an aggregate function in a correlated predicate that has both \" + \"outer and local references, which is not supported: \" + funcStr new AnalysisException(msg) } def lookupFunctionInNonFunctionCatalogError( ident: Identifier, catalog: CatalogPlugin): Throwable = { new AnalysisException(s\"Trying to lookup function '$ident' in \" + s\"catalog '${catalog.name()}', but it is not a FunctionCatalog.\") } def functionCannotProcessInputError( unbound: UnboundFunction, arguments: Seq[Expression], unsupported: UnsupportedOperationException): Throwable = { new AnalysisException(s\"Function '${unbound.name}' cannot process \" + s\"input: (${arguments.map(_.dataType.simpleString).mkString(\", \")}): \" + unsupported.getMessage, cause = Some(unsupported)) } def v2FunctionInvalidInputTypeLengthError( bound: BoundFunction, args: Seq[Expression]): Throwable = { new AnalysisException(s\"Invalid bound function '${bound.name()}: there are ${args.length} \" + s\"arguments but ${bound.inputTypes().length} parameters returned from 'inputTypes()'\") } def ambiguousRelationAliasNameInNestedCTEError(name: String): Throwable = { new AnalysisException(s\"Name $name is ambiguous in nested CTE. \" + s\"Please set ${LEGACY_CTE_PRECEDENCE_POLICY.key} to CORRECTED so that name \" + \"defined in inner CTE takes precedence. If set it to LEGACY, outer CTE \" + \"definitions will take precedence. See more details in SPARK-28228.\") } def commandUnsupportedInV2TableError(name: String): Throwable = { new AnalysisException(s\"$name is not supported for v2 tables.\") } def cannotResolveColumnNameAmongAttributesError( colName: String, fieldNames: String): Throwable = { new AnalysisException(s\"\"\"Cannot resolve column name \"$colName\" among ($fieldNames)\"\"\") } def cannotWriteTooManyColumnsToTableError( tableName: String, expected: Seq[Attribute], query: LogicalPlan): Throwable = { new AnalysisException( s\"\"\" |Cannot write to '$tableName', too many data columns: |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")} |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")} \"\"\".stripMargin) } def cannotWriteNotEnoughColumnsToTableError( tableName: String, expected: Seq[Attribute], query: LogicalPlan): Throwable = { new AnalysisException( s\"\"\"Cannot write to '$tableName', not enough data columns: |Table columns: ${expected.map(c => s\"'${c.name}'\").mkString(\", \")} |Data columns: ${query.output.map(c => s\"'${c.name}'\").mkString(\", \")}\"\"\" .stripMargin) } def cannotWriteIncompatibleDataToTableError(tableName: String, errors: Seq[String]): Throwable = { new AnalysisException( s\"Cannot write incompatible data to table '$tableName':\\n- ${errors.mkString(\"\\n- \")}\") } def secondArgumentOfFunctionIsNotIntegerError( function: String, e: NumberFormatException): Throwable = { // The second argument of '{function}' function needs to be an integer new AnalysisException( errorClass = \"SECOND_FUNCTION_ARGUMENT_NOT_INTEGER\", messageParameters = Array(function), cause = Some(e)) } def nonPartitionPruningPredicatesNotExpectedError( nonPartitionPruningPredicates: Seq[Expression]): Throwable = { new AnalysisException( s\"Expected only partition pruning predicates: $nonPartitionPruningPredicates\") } def columnNotDefinedInTableError( colType: String, colName: String, tableName: String, tableCols: Seq[String]): Throwable = { new AnalysisException(s\"$colType column $colName is not defined in table $tableName, \" + s\"defined table columns are: ${tableCols.mkString(\", \")}\") } def invalidLiteralForWindowDurationError(): Throwable = { new AnalysisException(\"The duration and time inputs to window must be \" + \"an integer, long or string literal.\") } def noSuchStructFieldInGivenFieldsError( fieldName: String, fields: Array[StructField]): Throwable = { new AnalysisException( s\"No such struct field $fieldName in ${fields.map(_.name).mkString(\", \")}\") } def ambiguousReferenceToFieldsError(fields: String): Throwable = { new AnalysisException(s\"Ambiguous reference to fields $fields\") } def secondArgumentInFunctionIsNotBooleanLiteralError(funcName: String): Throwable = { new AnalysisException(s\"The second argument in $funcName should be a boolean literal.\") } def joinConditionMissingOrTrivialError( join: Join, left: LogicalPlan, right: LogicalPlan): Throwable = { new AnalysisException( s\"\"\"Detected implicit cartesian product for ${join.joinType.sql} join between logical plans |${left.treeString(false).trim} |and |${right.treeString(false).trim} |Join condition is missing or trivial. |Either: use the CROSS JOIN syntax to allow cartesian products between these |relations, or: enable implicit cartesian products by setting the configuration |variable spark.sql.crossJoin.enabled=true\"\"\" .stripMargin) } def usePythonUDFInJoinConditionUnsupportedError(joinType: JoinType): Throwable = { new AnalysisException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array( \"Using PythonUDF in join condition of join type \" + s\"${toSQLStmt(joinType.sql)} is not supported.\")) } def conflictingAttributesInJoinConditionError( conflictingAttrs: AttributeSet, outerPlan: LogicalPlan, subplan: LogicalPlan): Throwable = { new AnalysisException(\"Found conflicting attributes \" + s\"${conflictingAttrs.mkString(\",\")} in the condition joining outer plan:\\n \" + s\"$outerPlan\\nand subplan:\\n $subplan\") } def emptyWindowExpressionError(expr: Window): Throwable = { new AnalysisException(s\"Window expression is empty in $expr\") } def foundDifferentWindowFunctionTypeError(windowExpressions: Seq[NamedExpression]): Throwable = { new AnalysisException( s\"Found different window function type in $windowExpressions\") } def charOrVarcharTypeAsStringUnsupportedError(): Throwable = { new AnalysisException(\"char/varchar type can only be used in the table schema. \" + s\"You can set ${SQLConf.LEGACY_CHAR_VARCHAR_AS_STRING.key} to true, so that Spark\" + s\" treat them as string type as same as Spark 3.0 and earlier\") } def invalidPatternError(pattern: String, message: String): Throwable = { new AnalysisException( s\"the pattern '$pattern' is invalid, $message\") } def tableIdentifierExistsError(tableIdentifier: TableIdentifier): Throwable = { new AnalysisException(s\"$tableIdentifier already exists.\") } def tableIdentifierNotConvertedToHadoopFsRelationError( tableIdentifier: TableIdentifier): Throwable = { new AnalysisException(s\"$tableIdentifier should be converted to HadoopFsRelation.\") } def alterDatabaseLocationUnsupportedError(version: String): Throwable = { new AnalysisException(s\"Hive $version does not support altering database location\") } def hiveTableTypeUnsupportedError(tableType: String): Throwable = { new AnalysisException(s\"Hive $tableType is not supported.\") } def hiveCreatePermanentFunctionsUnsupportedError(): Throwable = { new AnalysisException(\"Hive 0.12 doesn't support creating permanent functions. \" + \"Please use Hive 0.13 or higher.\") } def unknownHiveResourceTypeError(resourceType: String): Throwable = { new AnalysisException(s\"Unknown resource type: $resourceType\") } def invalidDayTimeField(field: Byte): Throwable = { val supportedIds = DayTimeIntervalType.dayTimeFields .map(i => s\"$i (${DayTimeIntervalType.fieldToString(i)})\") new AnalysisException(s\"Invalid field id '$field' in day-time interval. \" + s\"Supported interval fields: ${supportedIds.mkString(\", \")}.\") } def invalidDayTimeIntervalType(startFieldName: String, endFieldName: String): Throwable = { new AnalysisException(s\"'interval $startFieldName to $endFieldName' is invalid.\") } def invalidYearMonthField(field: Byte): Throwable = { val supportedIds = YearMonthIntervalType.yearMonthFields .map(i => s\"$i (${YearMonthIntervalType.fieldToString(i)})\") new AnalysisException(s\"Invalid field id '$field' in year-month interval. \" + s\"Supported interval fields: ${supportedIds.mkString(\", \")}.\") } def invalidYearMonthIntervalType(startFieldName: String, endFieldName: String): Throwable = { new AnalysisException(s\"'interval $startFieldName to $endFieldName' is invalid.\") } def configRemovedInVersionError( configName: String, version: String, comment: String): Throwable = { new AnalysisException( s\"The SQL config '$configName' was removed in the version $version. $comment\") } def failedFallbackParsingError(msg: String, e1: Throwable, e2: Throwable): Throwable = { new AnalysisException(s\"$msg${e1.getMessage}\\nFailed fallback parsing: ${e2.getMessage}\", cause = Some(e1.getCause)) } def decimalCannotGreaterThanPrecisionError(scale: Int, precision: Int): Throwable = { new AnalysisException(s\"Decimal scale ($scale) cannot be greater than precision ($precision).\") } def decimalOnlySupportPrecisionUptoError(decimalType: String, precision: Int): Throwable = { new AnalysisException(s\"$decimalType can only support precision up to $precision\") } def negativeScaleNotAllowedError(scale: Int): Throwable = { new AnalysisException( s\"\"\"|Negative scale is not allowed: $scale. |You can use ${LEGACY_ALLOW_NEGATIVE_SCALE_OF_DECIMAL_ENABLED.key}=true |to enable legacy mode to allow it.\"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def invalidPartitionColumnKeyInTableError(key: String, tblName: String): Throwable = { new AnalysisException(s\"$key is not a valid partition column in table $tblName.\") } def invalidPartitionSpecError( specKeys: String, partitionColumnNames: Seq[String], tableName: String): Throwable = { new AnalysisException( s\"\"\"|Partition spec is invalid. The spec ($specKeys) must match |the partition spec (${partitionColumnNames.mkString(\", \")}) defined in |table '$tableName'\"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def foundDuplicateColumnError(colType: String, duplicateCol: Seq[String]): Throwable = { new AnalysisException( s\"Found duplicate column(s) $colType: ${duplicateCol.sorted.mkString(\", \")}\") } def noSuchTableError(db: String, table: String): Throwable = { new NoSuchTableException(db = db, table = table) } def tempViewNotCachedForAnalyzingColumnsError(tableIdent: TableIdentifier): Throwable = { new AnalysisException(s\"Temporary view $tableIdent is not cached for analyzing columns.\") } def columnTypeNotSupportStatisticsCollectionError( name: String, tableIdent: TableIdentifier, dataType: DataType): Throwable = { new AnalysisException(s\"Column $name in table $tableIdent is of type $dataType, \" + \"and Spark does not support statistics collection on this column type.\") } def analyzeTableNotSupportedOnViewsError(): Throwable = { new AnalysisException(\"ANALYZE TABLE is not supported on views.\") } def unexpectedPartitionColumnPrefixError( table: String, database: String, schemaColumns: String, specColumns: String): Throwable = { new AnalysisException( s\"\"\" |The list of partition columns with values |in partition specification for table '${table}' |in database '${database}' is not a prefix of the list of |partition columns defined in the table schema. |Expected a prefix of [${schemaColumns}], but got [${specColumns}]. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def noSuchPartitionError( db: String, table: String, partition: TablePartitionSpec): Throwable = { new NoSuchPartitionException(db, table, partition) } def analyzingColumnStatisticsNotSupportedForColumnTypeError( name: String, dataType: DataType): Throwable = { new AnalysisException(\"Analyzing column statistics is not supported for column \" + s\"$name of data type: $dataType.\") } def tableAlreadyExistsError(table: String, guide: String = \"\"): Throwable = { new AnalysisException(s\"Table $table already exists.\" + guide) } def createTableAsSelectWithNonEmptyDirectoryError(tablePath: String): Throwable = { new AnalysisException( s\"CREATE-TABLE-AS-SELECT cannot create table with location to a non-empty directory \" + s\"${tablePath} . To allow overwriting the existing non-empty directory, \" + s\"set '${SQLConf.ALLOW_NON_EMPTY_LOCATION_IN_CTAS.key}' to true.\") } def tableOrViewNotFoundError(table: String): Throwable = { new AnalysisException(s\"Table or view not found: $table\") } def noSuchFunctionError( rawName: Seq[String], t: TreeNode[_], fullName: Option[Seq[String]] = None): Throwable = { if (rawName.length == 1 && fullName.isDefined) { new AnalysisException(s\"Undefined function: ${rawName.head}. \" + \"This function is neither a built-in/temporary function, nor a persistent \" + s\"function that is qualified as ${fullName.get.quoted}.\", t.origin.line, t.origin.startPosition) } else { new AnalysisException(s\"Undefined function: ${rawName.quoted}\", t.origin.line, t.origin.startPosition) } } def unsetNonExistentPropertyError(property: String, table: TableIdentifier): Throwable = { new AnalysisException(s\"Attempted to unset non-existent property '$property' in table '$table'\") } def alterTableChangeColumnNotSupportedForColumnTypeError( originColumn: StructField, newColumn: StructField): Throwable = { new AnalysisException(\"ALTER TABLE CHANGE COLUMN is not supported for changing column \" + s\"'${originColumn.name}' with type '${originColumn.dataType}' to \" + s\"'${newColumn.name}' with type '${newColumn.dataType}'\") } def cannotFindColumnError(name: String, fieldNames: Array[String]): Throwable = { new AnalysisException(s\"Can't find column `$name` given table data columns \" + s\"${fieldNames.mkString(\"[`\", \"`, `\", \"`]\")}\") } def alterTableSetSerdeForSpecificPartitionNotSupportedError(): Throwable = { new AnalysisException(\"Operation not allowed: ALTER TABLE SET \" + \"[SERDE | SERDEPROPERTIES] for a specific partition is not supported \" + \"for tables created with the datasource API\") } def alterTableSetSerdeNotSupportedError(): Throwable = { new AnalysisException(\"Operation not allowed: ALTER TABLE SET SERDE is \" + \"not supported for tables created with the datasource API\") } def cmdOnlyWorksOnPartitionedTablesError(cmd: String, tableIdentWithDB: String): Throwable = { new AnalysisException( s\"Operation not allowed: $cmd only works on partitioned tables: $tableIdentWithDB\") } def cmdOnlyWorksOnTableWithLocationError(cmd: String, tableIdentWithDB: String): Throwable = { new AnalysisException(s\"Operation not allowed: $cmd only works on table with \" + s\"location provided: $tableIdentWithDB\") } def actionNotAllowedOnTableWithFilesourcePartitionManagementDisabledError( action: String, tableName: String): Throwable = { new AnalysisException( s\"$action is not allowed on $tableName since filesource partition management is \" + \"disabled (spark.sql.hive.manageFilesourcePartitions = false).\") } def actionNotAllowedOnTableSincePartitionMetadataNotStoredError( action: String, tableName: String): Throwable = { new AnalysisException( s\"$action is not allowed on $tableName since its partition metadata is not stored in \" + \"the Hive metastore. To import this information into the metastore, run \" + s\"`msck repair table $tableName`\") } def cannotAlterViewWithAlterTableError(): Throwable = { new AnalysisException( \"Cannot alter a view with ALTER TABLE. Please use ALTER VIEW instead\") } def cannotAlterTableWithAlterViewError(): Throwable = { new AnalysisException( \"Cannot alter a table with ALTER VIEW. Please use ALTER TABLE instead\") } def cannotOverwritePathBeingReadFromError(): Throwable = { new AnalysisException(\"Cannot overwrite a path that is also being read from.\") } def cannotDropBuiltinFuncError(functionName: String): Throwable = { new AnalysisException(s\"Cannot drop built-in function '$functionName'\") } def cannotRefreshBuiltInFuncError(functionName: String): Throwable = { new AnalysisException(s\"Cannot refresh built-in function $functionName\") } def cannotRefreshTempFuncError(functionName: String): Throwable = { new AnalysisException(s\"Cannot refresh temporary function $functionName\") } def noSuchFunctionError(identifier: FunctionIdentifier): Throwable = { new NoSuchFunctionException(identifier.database.get, identifier.funcName) } def alterAddColNotSupportViewError(table: TableIdentifier): Throwable = { new AnalysisException( s\"\"\" |ALTER ADD COLUMNS does not support views. |You must drop and re-create the views for adding the new columns. Views: $table \"\"\".stripMargin) } def alterAddColNotSupportDatasourceTableError( tableType: Any, table: TableIdentifier): Throwable = { new AnalysisException( s\"\"\" |ALTER ADD COLUMNS does not support datasource table with type $tableType. |You must drop and re-create the table for adding the new columns. Tables: $table \"\"\".stripMargin) } def loadDataNotSupportedForDatasourceTablesError(tableIdentWithDB: String): Throwable = { new AnalysisException(s\"LOAD DATA is not supported for datasource tables: $tableIdentWithDB\") } def loadDataWithoutPartitionSpecProvidedError(tableIdentWithDB: String): Throwable = { new AnalysisException(s\"LOAD DATA target table $tableIdentWithDB is partitioned, \" + s\"but no partition spec is provided\") } def loadDataPartitionSizeNotMatchNumPartitionColumnsError( tableIdentWithDB: String, partitionSize: Int, targetTableSize: Int): Throwable = { new AnalysisException( s\"\"\" |LOAD DATA target table $tableIdentWithDB is partitioned, |but number of columns in provided partition spec ($partitionSize) |do not match number of partitioned columns in table ($targetTableSize) \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def loadDataTargetTableNotPartitionedButPartitionSpecWasProvidedError( tableIdentWithDB: String): Throwable = { new AnalysisException(s\"LOAD DATA target table $tableIdentWithDB is not \" + s\"partitioned, but a partition spec was provided.\") } def loadDataInputPathNotExistError(path: String): Throwable = { new AnalysisException(s\"LOAD DATA input path does not exist: $path\") } def truncateTableOnExternalTablesError(tableIdentWithDB: String): Throwable = { new AnalysisException( s\"Operation not allowed: TRUNCATE TABLE on external tables: $tableIdentWithDB\") } def truncateTablePartitionNotSupportedForNotPartitionedTablesError( tableIdentWithDB: String): Throwable = { new AnalysisException(s\"Operation not allowed: TRUNCATE TABLE ... PARTITION is not supported\" + s\" for tables that are not partitioned: $tableIdentWithDB\") } def failToTruncateTableWhenRemovingDataError( tableIdentWithDB: String, path: Path, e: Throwable): Throwable = { new AnalysisException(s\"Failed to truncate table $tableIdentWithDB when \" + s\"removing data of the path: $path because of ${e.toString}\") } def descPartitionNotAllowedOnTempView(table: String): Throwable = { new AnalysisException(s\"DESC PARTITION is not allowed on a temporary view: $table\") } def descPartitionNotAllowedOnView(table: String): Throwable = { new AnalysisException(s\"DESC PARTITION is not allowed on a view: $table\") } def showPartitionNotAllowedOnTableNotPartitionedError(tableIdentWithDB: String): Throwable = { new AnalysisException( s\"SHOW PARTITIONS is not allowed on a table that is not partitioned: $tableIdentWithDB\") } def showCreateTableNotSupportedOnTempView(table: String): Throwable = { new AnalysisException(s\"SHOW CREATE TABLE is not supported on a temporary view: $table\") } def showCreateTableFailToExecuteUnsupportedFeatureError(table: CatalogTable): Throwable = { new AnalysisException(\"Failed to execute SHOW CREATE TABLE against table \" + s\"${table.identifier}, which is created by Hive and uses the \" + s\"following unsupported feature(s)\\n\" + table.unsupportedFeatures.map(\" - \" + _).mkString(\"\\n\") + \". \" + s\"Please use `SHOW CREATE TABLE ${table.identifier} AS SERDE` to show Hive DDL instead.\") } def showCreateTableNotSupportTransactionalHiveTableError(table: CatalogTable): Throwable = { new AnalysisException(\"SHOW CREATE TABLE doesn't support transactional Hive table. \" + s\"Please use `SHOW CREATE TABLE ${table.identifier} AS SERDE` \" + \"to show Hive DDL instead.\") } def showCreateTableFailToExecuteUnsupportedConfError( table: TableIdentifier, builder: mutable.StringBuilder): Throwable = { new AnalysisException(\"Failed to execute SHOW CREATE TABLE against table \" + s\"${table.identifier}, which is created by Hive and uses the \" + \"following unsupported serde configuration\\n\" + builder.toString() ) } def descPartitionNotAllowedOnViewError(table: String): Throwable = { new AnalysisException(s\"DESC PARTITION is not allowed on a view: $table\") } def showCreateTableAsSerdeNotAllowedOnSparkDataSourceTableError( table: TableIdentifier): Throwable = { new AnalysisException( s\"$table is a Spark data source table. Use `SHOW CREATE TABLE` without `AS SERDE` instead.\") } def showCreateTableOrViewFailToExecuteUnsupportedFeatureError( table: CatalogTable, features: Seq[String]): Throwable = { new AnalysisException( s\"Failed to execute SHOW CREATE TABLE against table/view ${table.identifier}, \" + \"which is created by Hive and uses the following unsupported feature(s)\\n\" + features.map(\" - \" + _).mkString(\"\\n\")) } def logicalPlanForViewNotAnalyzedError(): Throwable = { new AnalysisException(\"The logical plan that represents the view is not analyzed.\") } def createViewNumColumnsMismatchUserSpecifiedColumnLengthError( analyzedPlanLength: Int, userSpecifiedColumnsLength: Int): Throwable = { new AnalysisException(s\"The number of columns produced by the SELECT clause \" + s\"(num: `$analyzedPlanLength`) does not match the number of column names \" + s\"specified by CREATE VIEW (num: `$userSpecifiedColumnsLength`).\") } def tableIsNotViewError(name: TableIdentifier): Throwable = { new AnalysisException(s\"$name is not a view\") } def viewAlreadyExistsError(name: TableIdentifier): Throwable = { new AnalysisException( s\"View $name already exists. If you want to update the view definition, \" + \"please use ALTER VIEW AS or CREATE OR REPLACE VIEW AS\") } def createPersistedViewFromDatasetAPINotAllowedError(): Throwable = { new AnalysisException(\"It is not allowed to create a persisted view from the Dataset API\") } def recursiveViewDetectedError( viewIdent: TableIdentifier, newPath: Seq[TableIdentifier]): Throwable = { new AnalysisException(s\"Recursive view $viewIdent detected \" + s\"(cycle: ${newPath.mkString(\" -> \")})\") } def notAllowedToCreatePermanentViewWithoutAssigningAliasForExpressionError( name: TableIdentifier, attrName: String): Throwable = { new AnalysisException(s\"Not allowed to create a permanent view $name without \" + s\"explicitly assigning an alias for expression $attrName\") } def notAllowedToCreatePermanentViewByReferencingTempViewError( name: TableIdentifier, nameParts: String): Throwable = { new AnalysisException(s\"Not allowed to create a permanent view $name by \" + s\"referencing a temporary view $nameParts. \" + \"Please create a temp view instead by CREATE TEMP VIEW\") } def notAllowedToCreatePermanentViewByReferencingTempFuncError( name: TableIdentifier, funcName: String): Throwable = { new AnalysisException(s\"Not allowed to create a permanent view $name by \" + s\"referencing a temporary function `$funcName`\") } def queryFromRawFilesIncludeCorruptRecordColumnError(): Throwable = { new AnalysisException( \"\"\" |Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the |referenced columns only include the internal corrupt record column |(named _corrupt_record by default). For example: |spark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count() |and spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show(). |Instead, you can cache or save the parsed results and then send the same query. |For example, val df = spark.read.schema(schema).csv(file).cache() and then |df.filter($\"_corrupt_record\".isNotNull).count(). \"\"\".stripMargin) } def userDefinedPartitionNotFoundInJDBCRelationError( columnName: String, schema: String): Throwable = { new AnalysisException(s\"User-defined partition column $columnName not \" + s\"found in the JDBC relation: $schema\") } def invalidPartitionColumnTypeError(column: StructField): Throwable = { new AnalysisException( s\"\"\" |Partition column type should be ${NumericType.simpleString}, |${DateType.catalogString}, or ${TimestampType.catalogString}, but |${column.dataType.catalogString} found. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def tableOrViewAlreadyExistsError(name: String): Throwable = { new AnalysisException( s\"Table or view '$name' already exists. SaveMode: ErrorIfExists.\") } def columnNameContainsInvalidCharactersError(name: String): Throwable = { new AnalysisException( s\"\"\" |Column name \"$name\" contains invalid character(s). |Please use alias to rename it. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def textDataSourceWithMultiColumnsError(schema: StructType): Throwable = { new AnalysisException( s\"Text data source supports only a single column, and you have ${schema.size} columns.\") } def cannotFindPartitionColumnInPartitionSchemaError( readField: StructField, partitionSchema: StructType): Throwable = { new AnalysisException(s\"Can't find required partition column ${readField.name} \" + s\"in partition schema $partitionSchema\") } def cannotSpecifyDatabaseForTempViewError(tableIdent: TableIdentifier): Throwable = { new AnalysisException( s\"Temporary view '$tableIdent' should not have specified a database\") } def cannotCreateTempViewUsingHiveDataSourceError(): Throwable = { new AnalysisException(\"Hive data source can only be used with tables, \" + \"you can't use it with CREATE TEMP VIEW USING\") } def invalidTimestampProvidedForStrategyError( strategy: String, timeString: String): Throwable = { new AnalysisException( s\"The timestamp provided for the '$strategy' option is invalid. The expected format \" + s\"is 'YYYY-MM-DDTHH:mm:ss', but the provided timestamp: $timeString\") } def hostOptionNotSetError(): Throwable = { new AnalysisException(\"Set a host to read from with option(\\\"host\\\", ...).\") } def portOptionNotSetError(): Throwable = { new AnalysisException(\"Set a port to read from with option(\\\"port\\\", ...).\") } def invalidIncludeTimestampValueError(): Throwable = { new AnalysisException(\"includeTimestamp must be set to either \\\"true\\\" or \\\"false\\\"\") } def checkpointLocationNotSpecifiedError(): Throwable = { new AnalysisException( s\"\"\" |checkpointLocation must be specified either |through option(\"checkpointLocation\", ...) or |SparkSession.conf.set(\"${SQLConf.CHECKPOINT_LOCATION.key}\", ...) \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def recoverQueryFromCheckpointUnsupportedError(checkpointPath: Path): Throwable = { new AnalysisException(\"This query does not support recovering from checkpoint location. \" + s\"Delete $checkpointPath to start over.\") } def cannotFindColumnInRelationOutputError( colName: String, relation: LogicalPlan): Throwable = { new AnalysisException(s\"Unable to find the column `$colName` \" + s\"given [${relation.output.map(_.name).mkString(\", \")}]\") } def invalidBoundaryStartError(start: Long): Throwable = { new AnalysisException(s\"Boundary start is not a valid integer: $start\") } def invalidBoundaryEndError(end: Long): Throwable = { new AnalysisException(s\"Boundary end is not a valid integer: $end\") } def databaseDoesNotExistError(dbName: String): Throwable = { new AnalysisException(s\"Database '$dbName' does not exist.\") } def tableDoesNotExistInDatabaseError(tableName: String, dbName: String): Throwable = { new AnalysisException(s\"Table '$tableName' does not exist in database '$dbName'.\") } def tableOrViewNotFoundInDatabaseError(tableName: String, dbName: String): Throwable = { new AnalysisException(s\"Table or view '$tableName' not found in database '$dbName'\") } def unexpectedTypeOfRelationError(relation: LogicalPlan, tableName: String): Throwable = { new AnalysisException( s\"Unexpected type ${relation.getClass.getCanonicalName} of the relation $tableName\") } def unsupportedTableChangeInJDBCCatalogError(change: TableChange): Throwable = { new AnalysisException(s\"Unsupported TableChange $change in JDBC catalog.\") } def pathOptionNotSetCorrectlyWhenReadingError(): Throwable = { new AnalysisException( s\"\"\" |There is a 'path' or 'paths' option set and load() is called |with path parameters. Either remove the path option if it's the same as the path |parameter, or add it to the load() parameter if you do want to read multiple paths. |To ignore this check, set '${SQLConf.LEGACY_PATH_OPTION_BEHAVIOR.key}' to 'true'. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def pathOptionNotSetCorrectlyWhenWritingError(): Throwable = { new AnalysisException( s\"\"\" |There is a 'path' option set and save() is called with a path |parameter. Either remove the path option, or call save() without the parameter. |To ignore this check, set '${SQLConf.LEGACY_PATH_OPTION_BEHAVIOR.key}' to 'true'. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def writeWithSaveModeUnsupportedBySourceError(source: String, createMode: String): Throwable = { new AnalysisException(s\"TableProvider implementation $source cannot be \" + s\"written with $createMode mode, please use Append or Overwrite modes instead.\") } def partitionByDoesNotAllowedWhenUsingInsertIntoError(): Throwable = { new AnalysisException( \"\"\" |insertInto() can't be used together with partitionBy(). |Partition columns have already been defined for the table. |It is not necessary to use partitionBy(). \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def cannotFindCatalogToHandleIdentifierError(quote: String): Throwable = { new AnalysisException(s\"Couldn't find a catalog to handle the identifier $quote.\") } def sortByNotUsedWithBucketByError(): Throwable = { new AnalysisException(\"sortBy must be used together with bucketBy\") } def bucketByUnsupportedByOperationError(operation: String): Throwable = { new AnalysisException(s\"'$operation' does not support bucketBy right now\") } def bucketByAndSortByUnsupportedByOperationError(operation: String): Throwable = { new AnalysisException(s\"'$operation' does not support bucketBy and sortBy right now\") } def tableAlreadyExistsError(tableIdent: TableIdentifier): Throwable = { new AnalysisException(s\"Table $tableIdent already exists.\") } def cannotOverwriteTableThatIsBeingReadFromError(tableName: String): Throwable = { new AnalysisException(s\"Cannot overwrite table $tableName that is also being read from\") } def invalidPartitionTransformationError(expr: Expression): Throwable = { new AnalysisException(s\"Invalid partition transformation: ${expr.sql}\") } def cannotResolveColumnNameAmongFieldsError( colName: String, fieldsStr: String, extraMsg: String): AnalysisException = { new AnalysisException( s\"\"\"Cannot resolve column name \"$colName\" among (${fieldsStr})${extraMsg}\"\"\") } def cannotParseIntervalError(delayThreshold: String, e: Throwable): Throwable = { new AnalysisException(s\"Unable to parse '$delayThreshold'\", cause = Some(e)) } def invalidJoinTypeInJoinWithError(joinType: JoinType): Throwable = { new AnalysisException(s\"Invalid join type in joinWith: ${joinType.sql}\") } def cannotPassTypedColumnInUntypedSelectError(typedCol: String): Throwable = { new AnalysisException(s\"Typed column $typedCol that needs input type and schema \" + \"cannot be passed in untyped `select` API. Use the typed `Dataset.select` API instead.\") } def invalidViewNameError(viewName: String): Throwable = { new AnalysisException(s\"Invalid view name: $viewName\") } def invalidBucketsNumberError(numBuckets: String, e: String): Throwable = { new AnalysisException(s\"Invalid number of buckets: bucket($numBuckets, $e)\") } def usingUntypedScalaUDFError(): Throwable = { new AnalysisException(\"You're using untyped Scala UDF, which does not have the input type \" + \"information. Spark may blindly pass null to the Scala closure with primitive-type \" + \"argument, and the closure will see the default value of the Java type for the null \" + \"argument, e.g. `udf((x: Int) => x, IntegerType)`, the result is 0 for null input. \" + \"To get rid of this error, you could:\\n\" + \"1. use typed Scala UDF APIs(without return type parameter), e.g. `udf((x: Int) => x)`\\n\" + \"2. use Java UDF APIs, e.g. `udf(new UDF1[String, Integer] { \" + \"override def call(s: String): Integer = s.length() }, IntegerType)`, \" + \"if input types are all non primitive\\n\" + s\"3. set ${SQLConf.LEGACY_ALLOW_UNTYPED_SCALA_UDF.key} to true and \" + s\"use this API with caution\") } def aggregationFunctionAppliedOnNonNumericColumnError(colName: String): Throwable = { new AnalysisException(s\"\"\"\"$colName\" is not a numeric column. \"\"\" + \"Aggregation function can only be applied on a numeric column.\") } def aggregationFunctionAppliedOnNonNumericColumnError( pivotColumn: String, maxValues: Int): Throwable = { new AnalysisException( s\"\"\" |The pivot column $pivotColumn has more than $maxValues distinct values, |this could indicate an error. |If this was intended, set ${SQLConf.DATAFRAME_PIVOT_MAX_VALUES.key} |to at least the number of distinct values of the pivot column. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def cannotModifyValueOfStaticConfigError(key: String): Throwable = { new AnalysisException(s\"Cannot modify the value of a static config: $key\") } def cannotModifyValueOfSparkConfigError(key: String): Throwable = { new AnalysisException( s\"\"\" |Cannot modify the value of a Spark config: $key. |See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements' \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def commandExecutionInRunnerUnsupportedError(runner: String): Throwable = { new AnalysisException(s\"Command execution is not supported in runner $runner\") } def udfClassDoesNotImplementAnyUDFInterfaceError(className: String): Throwable = { new AnalysisException(s\"UDF class $className doesn't implement any UDF interface\") } def udfClassNotAllowedToImplementMultiUDFInterfacesError(className: String): Throwable = { new AnalysisException( s\"It is invalid to implement multiple UDF interfaces, UDF class $className\") } def udfClassWithTooManyTypeArgumentsError(n: Int): Throwable = { new AnalysisException(s\"UDF class with $n type arguments is not supported.\") } def classWithoutPublicNonArgumentConstructorError(className: String): Throwable = { new AnalysisException(s\"Can not instantiate class $className, please make sure\" + \" it has public non argument constructor\") } def cannotLoadClassNotOnClassPathError(className: String): Throwable = { new AnalysisException(s\"Can not load class $className, please make sure it is on the classpath\") } def classDoesNotImplementUserDefinedAggregateFunctionError(className: String): Throwable = { new AnalysisException( s\"class $className doesn't implement interface UserDefinedAggregateFunction\") } def missingFieldError( fieldName: Seq[String], table: ResolvedTable, context: Origin): Throwable = { throw new AnalysisException( s\"Missing field ${fieldName.quoted} in table ${table.name} with schema:\\n\" + table.schema.treeString, context.line, context.startPosition) } def invalidFieldName(fieldName: Seq[String], path: Seq[String], context: Origin): Throwable = { new AnalysisException( errorClass = \"INVALID_FIELD_NAME\", messageParameters = Array(toSQLId(fieldName), toSQLId(path)), origin = context) } def invalidJsonSchema(schema: DataType): Throwable = { new AnalysisException( errorClass = \"INVALID_JSON_SCHEMA_MAP_TYPE\", messageParameters = Array(toSQLType(schema))) } def tableIndexNotSupportedError(errorMessage: String): Throwable = { new AnalysisException(errorMessage) } def invalidViewText(viewText: String, tableName: String): Throwable = { new AnalysisException( s\"Invalid view text: $viewText. The view $tableName may have been tampered with\") } def invalidTimeTravelSpecError(): Throwable = { new AnalysisException( \"Cannot specify both version and timestamp when time travelling the table.\") } def invalidTimestampExprForTimeTravel(expr: Expression): Throwable = { new AnalysisException(s\"${expr.sql} is not a valid timestamp expression for time travel.\") } def timeTravelUnsupportedError(target: String): Throwable = { new AnalysisException(s\"Cannot time travel $target.\") } def tableNotSupportTimeTravelError(tableName: Identifier): UnsupportedOperationException = { new UnsupportedOperationException(s\"Table $tableName does not support time travel.\") } def writeDistributionAndOrderingNotSupportedInContinuousExecution(): Throwable = { new AnalysisException( \"Sinks cannot request distribution and ordering in continuous execution mode\") } }",
          "## CLASS: org/apache/spark/rdd/RDD#\n abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent  def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition.  @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`  protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it.  protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences.  protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned.  @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD.  def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext).  val id: Int = sc.newRddId() /** A friendly name for this RDD  @transient var name: String = _ /** Assign a name to this RDD  def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one  private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception.  def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD.  def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set.  def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver).  private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not.  final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing.  final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not.  final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD.  @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed.  final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD.  final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned.  private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached.  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD.  def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results.  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate.  def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle.  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable.  def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition.  val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]].  def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array  def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement.  private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function.  def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally.  def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD  def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD  def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array.  def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`.  def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD  def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning.  private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other).  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition.  def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD.  def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD.  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first.  def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`.  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us.  def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator.  def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]]  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions  def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2)  def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor  def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD.  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map.  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped.  def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017.  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`.  def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD.  def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD *  def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD *  def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition.  def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements.  def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements.  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects.  def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`.  def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition  private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation.  def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used.  def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally.  def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing.  private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing.  private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized.  private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally.  def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned  @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents.  def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a>  @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD.  @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified  @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`).  @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations.  @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD  protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T]  protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on.  def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs.  private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions.  private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example.  protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging.  def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier.  private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level.  // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`.  object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE.  private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
          "## METHOD: org/apache/spark/sql/SQLContext#streams().\n def streams: StreamingQueryManager = sparkSession.streams /** * Returns the names of tables in the current database as an array. * * @group ddl_ops * @since 1.3.0  def tableNames(): Array[String] = { tableNames(sparkSession.catalog.currentDatabase) } /** * Returns the names of tables in the given database as an array. * * @group ddl_ops * @since 1.3.0  def tableNames(databaseName: String): Array[String] = { sessionState.catalog.listTables(databaseName).map(_.table).toArray } //////////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////////// // Deprecated methods //////////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////////// /** * @deprecated As of 1.3.0, replaced by `createDataFrame()`.  @deprecated(\"Use createDataFrame instead.\", \"1.3.0\") def applySchema(rowRDD: RDD[Row], schema: StructType): DataFrame = { createDataFrame(rowRDD, schema) } /** * @deprecated As of 1.3.0, replaced by `createDataFrame()`.  @deprecated(\"Use createDataFrame instead.\", \"1.3.0\") def applySchema(rowRDD: JavaRDD[Row], schema: StructType): DataFrame = { createDataFrame(rowRDD, schema) } /** * @deprecated As of 1.3.0, replaced by `createDataFrame()`.  @deprecated(\"Use createDataFrame instead.\", \"1.3.0\") def applySchema(rdd: RDD[_], beanClass: Class[_]): DataFrame = { createDataFrame(rdd, beanClass) } /** * @deprecated As of 1.3.0, replaced by `createDataFrame()`.  @deprecated(\"Use createDataFrame instead.\", \"1.3.0\") def applySchema(rdd: JavaRDD[_], beanClass: Class[_]): DataFrame = { createDataFrame(rdd, beanClass) } /** * Loads a Parquet file, returning the result as a `DataFrame`. This function returns an empty * `DataFrame` if no paths are passed in. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().parquet()`.  @deprecated(\"Use read.parquet() instead.\", \"1.4.0\") @scala.annotation.varargs def parquetFile(paths: String*): DataFrame = { if (paths.isEmpty) { emptyDataFrame } else { read.parquet(paths : _*) } } /** * Loads a JSON file (one object per line), returning the result as a `DataFrame`. * It goes through the entire dataset once to determine the schema. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonFile(path: String): DataFrame = { read.json(path) } /** * Loads a JSON file (one object per line) and applies the given schema, * returning the result as a `DataFrame`. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonFile(path: String, schema: StructType): DataFrame = { read.schema(schema).json(path) } /** * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonFile(path: String, samplingRatio: Double): DataFrame = { read.option(\"samplingRatio\", samplingRatio.toString).json(path) } /** * Loads an RDD[String] storing JSON objects (one object per record), returning the result as a * `DataFrame`. * It goes through the entire dataset once to determine the schema. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonRDD(json: RDD[String]): DataFrame = read.json(json) /** * Loads an RDD[String] storing JSON objects (one object per record), returning the result as a * `DataFrame`. * It goes through the entire dataset once to determine the schema. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonRDD(json: JavaRDD[String]): DataFrame = read.json(json) /** * Loads an RDD[String] storing JSON objects (one object per record) and applies the given schema, * returning the result as a `DataFrame`. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonRDD(json: RDD[String], schema: StructType): DataFrame = { read.schema(schema).json(json) } /** * Loads an JavaRDD[String] storing JSON objects (one object per record) and applies the given * schema, returning the result as a `DataFrame`. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonRDD(json: JavaRDD[String], schema: StructType): DataFrame = { read.schema(schema).json(json) } /** * Loads an RDD[String] storing JSON objects (one object per record) inferring the * schema, returning the result as a `DataFrame`. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonRDD(json: RDD[String], samplingRatio: Double): DataFrame = { read.option(\"samplingRatio\", samplingRatio.toString).json(json) } /** * Loads a JavaRDD[String] storing JSON objects (one object per record) inferring the * schema, returning the result as a `DataFrame`. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().json()`.  @deprecated(\"Use read.json() instead.\", \"1.4.0\") def jsonRDD(json: JavaRDD[String], samplingRatio: Double): DataFrame = { read.option(\"samplingRatio\", samplingRatio.toString).json(json) } /** * Returns the dataset stored at path as a DataFrame, * using the default data source configured by spark.sql.sources.default. * * @group genericdata * @deprecated As of 1.4.0, replaced by `read().load(path)`.  @deprecated(\"Use read.load(path) instead.\", \"1.4.0\") def load(path: String): DataFrame = { read.load(path) } /** * Returns the dataset stored at path as a DataFrame, using the given data source. * * @group genericdata * @deprecated As of 1.4.0, replaced by `read().format(source).load(path)`.  @deprecated(\"Use read.format(source).load(path) instead.\", \"1.4.0\") def load(path: String, source: String): DataFrame = { read.format(source).load(path) } /** * (Java-specific) Returns the dataset specified by the given data source and * a set of options as a DataFrame. * * @group genericdata * @deprecated As of 1.4.0, replaced by `read().format(source).options(options).load()`.  @deprecated(\"Use read.format(source).options(options).load() instead.\", \"1.4.0\") def load(source: String, options: java.util.Map[String, String]): DataFrame = { read.options(options).format(source).load() } /** * (Scala-specific) Returns the dataset specified by the given data source and * a set of options as a DataFrame. * * @group genericdata * @deprecated As of 1.4.0, replaced by `read().format(source).options(options).load()`.  @deprecated(\"Use read.format(source).options(options).load() instead.\", \"1.4.0\") def load(source: String, options: Map[String, String]): DataFrame = { read.options(options).format(source).load() } /** * (Java-specific) Returns the dataset specified by the given data source and * a set of options as a DataFrame, using the given schema as the schema of the DataFrame. * * @group genericdata * @deprecated As of 1.4.0, replaced by * `read().format(source).schema(schema).options(options).load()`.  @deprecated(\"Use read.format(source).schema(schema).options(options).load() instead.\", \"1.4.0\") def load( source: String, schema: StructType, options: java.util.Map[String, String]): DataFrame = { read.format(source).schema(schema).options(options).load() } /** * (Scala-specific) Returns the dataset specified by the given data source and * a set of options as a DataFrame, using the given schema as the schema of the DataFrame. * * @group genericdata * @deprecated As of 1.4.0, replaced by * `read().format(source).schema(schema).options(options).load()`.  @deprecated(\"Use read.format(source).schema(schema).options(options).load() instead.\", \"1.4.0\") def load(source: String, schema: StructType, options: Map[String, String]): DataFrame = { read.format(source).schema(schema).options(options).load() } /** * Construct a `DataFrame` representing the database table accessible via JDBC URL * url named table. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().jdbc()`.  @deprecated(\"Use read.jdbc() instead.\", \"1.4.0\") def jdbc(url: String, table: String): DataFrame = { read.jdbc(url, table, new Properties) } /** * Construct a `DataFrame` representing the database table accessible via JDBC URL * url named table. Partitions of the table will be retrieved in parallel based on the parameters * passed to this function. * * @param columnName the name of a column of integral type that will be used for partitioning. * @param lowerBound the minimum value of `columnName` used to decide partition stride * @param upperBound the maximum value of `columnName` used to decide partition stride * @param numPartitions the number of partitions. the range `minValue`-`maxValue` will be split * evenly into this many partitions * @group specificdata * @deprecated As of 1.4.0, replaced by `read().jdbc()`.  @deprecated(\"Use read.jdbc() instead.\", \"1.4.0\") def jdbc( url: String, table: String, columnName: String, lowerBound: Long, upperBound: Long, numPartitions: Int): DataFrame = { read.jdbc(url, table, columnName, lowerBound, upperBound, numPartitions, new Properties) } /** * Construct a `DataFrame` representing the database table accessible via JDBC URL * url named table. The theParts parameter gives a list expressions * suitable for inclusion in WHERE clauses; each one defines one partition * of the `DataFrame`. * * @group specificdata * @deprecated As of 1.4.0, replaced by `read().jdbc()`.  @deprecated(\"Use read.jdbc() instead.\", \"1.4.0\") def jdbc(url: String, table: String, theParts: Array[String]): DataFrame = { read.jdbc(url, table, theParts, new Properties) } } /** * This SQLContext object contains utility functions to create a singleton SQLContext instance, * or to get the created SQLContext instance. * * It also provides utility functions to support preference for threads in multiple sessions * scenario, setActive could set a SQLContext for current thread, which will be returned by * getOrCreate instead of the global one.  object SQLContext { /** * Get the singleton SQLContext if it exists or create a new one using the given SparkContext. * * This function can be used to create a singleton SQLContext object that can be shared across * the JVM. * * If there is an active SQLContext for current thread, it will be returned instead of the global * one. * * @since 1.5.0  @deprecated(\"Use SparkSession.builder instead\", \"2.0.0\") def getOrCreate(sparkContext: SparkContext): SQLContext = { SparkSession.builder().sparkContext(sparkContext).getOrCreate().sqlContext } /** * Changes the SQLContext that will be returned in this thread and its children when * SQLContext.getOrCreate() is called. This can be used to ensure that a given thread receives * a SQLContext with an isolated session, instead of the global (first created) context. * * @since 1.6.0  @deprecated(\"Use SparkSession.setActiveSession instead\", \"2.0.0\") def setActive(sqlContext: SQLContext): Unit = { SparkSession.setActiveSession(sqlContext.sparkSession) } /** * Clears the active SQLContext for current thread. Subsequent calls to getOrCreate will * return the first created context instead of a thread-local override. * * @since 1.6.0  @deprecated(\"Use SparkSession.clearActiveSession instead\", \"2.0.0\") def clearActive(): Unit = { SparkSession.clearActiveSession() } /** * Converts an iterator of Java Beans to InternalRow using the provided * bean info & schema. This is not related to the singleton, but is a static * method for internal use.  private[sql] def beansToRows( data: Iterator[_], beanClass: Class[_], attrs: Seq[AttributeReference]): Iterator[InternalRow] = { def createStructConverter(cls: Class[_], fieldTypes: Seq[DataType]): Any => InternalRow = { val methodConverters = JavaTypeInference.getJavaBeanReadableProperties(cls).zip(fieldTypes) .map { case (property, fieldType) => val method = property.getReadMethod method -> createConverter(method.getReturnType, fieldType) } value => if (value == null) { null } else { new GenericInternalRow( methodConverters.map { case (method, converter) => converter(method.invoke(value)) }) } } def createConverter(cls: Class[_], dataType: DataType): Any => Any = dataType match { case struct: StructType => createStructConverter(cls, struct.map(_.dataType)) case _ => CatalystTypeConverters.createToCatalystConverter(dataType) } val dataConverter = createStructConverter(beanClass, attrs.map(_.dataType)) data.map(dataConverter) } /** * Extract `spark.sql.*` properties from the conf and return them as a [[Properties]].  private[sql] def getSQLProperties(sparkConf: SparkConf): Properties = { val properties = new Properties sparkConf.getAll.foreach { case (key, value) => if (key.startsWith(\"spark.sql\")) { properties.setProperty(key, value) } } properties } }",
          "## METHOD: org/apache/spark/SparkContext#addFile(+1).\n* @note A path can be added only once. Subsequent additions of the same path are ignored.  def addFile(path: String, recursive: Boolean): Unit = { addFile(path, recursive, false) } private def addFile( path: String, recursive: Boolean, addedOnSubmit: Boolean, isArchive: Boolean = false ): Unit = { val uri = Utils.resolveURI(path) val schemeCorrectedURI = uri.getScheme match { case null => new File(path).getCanonicalFile.toURI case \"local\" => logWarning(s\"File with 'local' scheme $path is not supported to add to file server, \" + s\"since it is already available on every node.\") return case _ => uri } val hadoopPath = new Path(schemeCorrectedURI) val scheme = schemeCorrectedURI.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) { val fs = hadoopPath.getFileSystem(hadoopConfiguration) val isDir = fs.getFileStatus(hadoopPath).isDirectory if (!isLocal && scheme == \"file\" && isDir) { throw new SparkException(s\"addFile does not support local directories when not running \" + \"local mode.\") } if (!recursive && isDir) { throw new SparkException(s\"Added file $hadoopPath is a directory and recursive is not \" + \"turned on.\") } } else { // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) } val key = if (!isLocal && scheme == \"file\") { env.rpcEnv.fileServer.addFile(new File(uri.getPath)) } else if (uri.getScheme == null) { schemeCorrectedURI.toString } else { uri.toString } val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis if (!isArchive && addedFiles.putIfAbsent(key, timestamp).isEmpty) { logInfo(s\"Added file $path at $key with timestamp $timestamp\") // Fetch the file locally so that closures which are run on the driver can still use the // SparkFiles API to access files. Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf, hadoopConfiguration, timestamp, useCache = false) postEnvironmentUpdate() } else if ( isArchive && addedArchives.putIfAbsent( UriBuilder.fromUri(new URI(key)).fragment(uri.getFragment).build().toString, timestamp).isEmpty) { logInfo(s\"Added archive $path at $key with timestamp $timestamp\") // If the scheme is file, use URI to simply copy instead of downloading. val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key) val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build() val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false) val dest = new File( SparkFiles.getRootDirectory(), if (uri.getFragment != null) uri.getFragment else source.getName) logInfo( s\"Unpacking an archive $path from ${source.getAbsolutePath} to ${dest.getAbsolutePath}\") Utils.deleteRecursively(dest) Utils.unpack(source, dest) postEnvironmentUpdate() } else { logWarning(s\"The path $path has been added already. Overwriting of added paths \" + \"is not supported in the current version.\") } } /** * :: DeveloperApi :: * Register a listener to receive up-calls from events that happen during execution.  @DeveloperApi def addSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.addToSharedQueue(listener) } /** * :: DeveloperApi :: * Deregister the listener from Spark's listener bus.  @DeveloperApi def removeSparkListener(listener: SparkListenerInterface): Unit = { listenerBus.removeListener(listener) } private[spark] def getExecutorIds(): Seq[String] = { schedulerBackend match { case b: ExecutorAllocationClient => b.getExecutorIds() case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") Nil } } /** * Get the max number of tasks that can be concurrent launched based on the ResourceProfile * could be used, even if some of them are being used at the moment. * Note that please don't cache the value returned by this method, because the number can change * due to add/remove executors. * * @param rp ResourceProfile which to use to calculate max concurrent tasks. * @return The max number of tasks that can be concurrent launched currently.  private[spark] def maxNumConcurrentTasks(rp: ResourceProfile): Int = { schedulerBackend.maxNumConcurrentTasks(rp) } /** * Update the cluster manager on our scheduling needs. Three bits of information are included * to help it make decisions. This applies to the default ResourceProfile. * @param numExecutors The total number of executors we'd like to have. The cluster manager * shouldn't kill any running executor to reach this number, but, * if all existing executors were to die, this is the number of executors * we'd want to be allocated. * @param localityAwareTasks The number of tasks in all active stages that have a locality * preferences. This includes running, pending, and completed tasks. * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages * that would like to like to run on that host. * This includes running, pending, and completed tasks. * @return whether the request is acknowledged by the cluster manager.  @DeveloperApi def requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: immutable.Map[String, Int] ): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => // this is being applied to the default resource profile, would need to add api to support // others val defaultProfId = resourceProfileManager.defaultResourceProfile.id b.requestTotalExecutors(immutable.Map(defaultProfId-> numExecutors), immutable.Map(localityAwareTasks -> defaultProfId), immutable.Map(defaultProfId -> hostToLocalTaskCount)) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request an additional number of executors from the cluster manager. * @return whether the request is received.  @DeveloperApi def requestExecutors(numAdditionalExecutors: Int): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.requestExecutors(numAdditionalExecutors) case _ => logWarning(\"Requesting executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executors. * * This is not supported when dynamic allocation is turned on. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executors it kills * through this method with new ones, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutors(executorIds: Seq[String]): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => require(executorAllocationManager.isEmpty, \"killExecutors() unsupported with Dynamic Allocation turned on\") b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** * :: DeveloperApi :: * Request that the cluster manager kill the specified executor. * * @note This is an indication to the cluster manager that the application wishes to adjust * its resource usage downwards. If the application wishes to replace the executor it kills * through this method with a new one, it should follow up explicitly with a call to * {{SparkContext#requestExecutors}}. * * @return whether the request is received.  @DeveloperApi def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId)) /** * Request that the cluster manager kill the specified executor without adjusting the * application resource requirements. * * The effect is that a new executor will be launched in place of the one killed by * this request. This assumes the cluster manager will automatically and eventually * fulfill all missing application resource requests. * * @note The replace is by no means guaranteed; another application on the same cluster * can steal the window of opportunity and acquire this application's resources in the * mean time. * * @return whether the request is received.  private[spark] def killAndReplaceExecutor(executorId: String): Boolean = { schedulerBackend match { case b: ExecutorAllocationClient => b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true, force = true).nonEmpty case _ => logWarning(\"Killing executors is not supported by current scheduler.\") false } } /** The version of Spark on which this application is running.  def version: String = SPARK_VERSION /** * Return a map from the block manager to the max memory available for caching and the remaining * memory available for caching.  def getExecutorMemoryStatus: Map[String, (Long, Long)] = { assertNotStopped() env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) => (blockManagerId.host + \":\" + blockManagerId.port, mem) } } /** * :: DeveloperApi :: * Return information about what RDDs are cached, if they are in mem or on disk, how much space * they take, etc.  @DeveloperApi def getRDDStorageInfo: Array[RDDInfo] = { getRDDStorageInfo(_ => true) } private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = { assertNotStopped() val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray rddInfos.foreach { rddInfo => val rddId = rddInfo.id val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId)) rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0) rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L) rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L) } rddInfos.filter(_.isCached) } /** * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call. * * @note This does not necessarily mean the caching or computation was successful.  def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap /** * :: DeveloperApi :: * Return pools for fair scheduler  @DeveloperApi def getAllPools: Seq[Schedulable] = { assertNotStopped() // TODO(xiajunluan): We should take nested pools into account taskScheduler.rootPool.schedulableQueue.asScala.toSeq } /** * :: DeveloperApi :: * Return the pool associated with the given name, if one exists  @DeveloperApi def getPoolForName(pool: String): Option[Schedulable] = { assertNotStopped() Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool)) } /** * Return current scheduling mode  def getSchedulingMode: SchedulingMode.SchedulingMode = { assertNotStopped() taskScheduler.schedulingMode } /** * Gets the locality information associated with the partition in a particular rdd * @param rdd of interest * @param partition to be looked up for locality * @return list of preferred locations for the partition  private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = { dagScheduler.getPreferredLocs(rdd, partition) } /** * Register an RDD to be persisted in memory and/or disk storage  private[spark] def persistRDD(rdd: RDD[_]): Unit = { persistentRdds(rdd.id) = rdd } /** * Unpersist an RDD from memory and/or disk storage  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean): Unit = { env.blockManager.master.removeRdd(rddId, blocking) persistentRdds.remove(rddId) listenerBus.post(SparkListenerUnpersistRDD(rddId)) } /** * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future. * * If a jar is added during execution, it will not be available until the next TaskSet starts. * * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. * * @note A path can be added only once. Subsequent additions of the same path are ignored.  def addJar(path: String): Unit = { addJar(path, false) } private def addJar(path: String, addedOnSubmit: Boolean): Unit = { def addLocalJarFile(file: File): Seq[String] = { try { if (!file.exists()) { throw new FileNotFoundException(s\"Jar ${file.getAbsolutePath} not found\") } if (file.isDirectory) { throw new IllegalArgumentException( s\"Directory ${file.getAbsoluteFile} is not allowed for addJar\") } Seq(env.rpcEnv.fileServer.addJar(file)) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } def checkRemoteJarFile(path: String): Seq[String] = { val hadoopPath = new Path(path) val scheme = hadoopPath.toUri.getScheme if (!Array(\"http\", \"https\", \"ftp\").contains(scheme)) { try { val fs = hadoopPath.getFileSystem(hadoopConfiguration) if (!fs.exists(hadoopPath)) { throw new FileNotFoundException(s\"Jar ${path} not found\") } if (fs.getFileStatus(hadoopPath).isDirectory) { throw new IllegalArgumentException( s\"Directory ${path} is not allowed for addJar\") } Seq(path) } catch { case NonFatal(e) => logError(s\"Failed to add $path to Spark environment\", e) Nil } } else { Seq(path) } } if (path == null || path.isEmpty) { logWarning(\"null or empty path specified as parameter to addJar\") } else { val (keys, scheme) = if (path.contains(\"\\\\\") && Utils.isWindows) { // For local paths with backslashes on Windows, URI throws an exception (addLocalJarFile(new File(path)), \"local\") } else { val uri = Utils.resolveURI(path) // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies Utils.validateURL(uri) val uriScheme = uri.getScheme val jarPaths = uriScheme match { // A JAR file which exists only on the driver node case null => // SPARK-22585 path without schema is not url encoded addLocalJarFile(new File(uri.getPath)) // A JAR file which exists only on the driver node case \"file\" => addLocalJarFile(new File(uri.getPath)) // A JAR file which exists locally on every worker node case \"local\" => Seq(\"file:\" + uri.getPath) case \"ivy\" => // Since `new Path(path).toUri` will lose query information, // so here we use `URI.create(path)` DependencyUtils.resolveMavenDependencies(URI.create(path)) .flatMap(jar => addLocalJarFile(new File(jar))) case _ => checkRemoteJarFile(path) } (jarPaths, uriScheme) } if (keys.nonEmpty) { val timestamp = if (addedOnSubmit) startTime else System.currentTimeMillis val (added, existed) = keys.partition(addedJars.putIfAbsent(_, timestamp).isEmpty) if (added.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"Added $jarMessage $path at ${added.mkString(\",\")} with timestamp $timestamp\") postEnvironmentUpdate() } if (existed.nonEmpty) { val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\" logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" + \" Overwriting of added jar is not supported in the current version.\") } } } } /** * Returns a list of jar files that are added to resources.  def listJars(): Seq[String] = addedJars.keySet.toSeq /** * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark * may wait for some internal threads to finish. It's better to use this method to stop * SparkContext instead.  private[spark] def stopInNewThread(): Unit = { new Thread(\"stop-spark-context\") { setDaemon(true) override def run(): Unit = { try { SparkContext.this.stop() } catch { case e: Throwable => logError(e.getMessage, e) throw e } } }.start() } /** * Shut down the SparkContext.  def stop(): Unit = { if (LiveListenerBus.withinListenerThread.value) { throw new SparkException(s\"Cannot stop SparkContext within listener bus thread.\") } // Use the stopping variable to ensure no contention for the stop scenario. // Still track the stopped variable for use elsewhere in the code. if (!stopped.compareAndSet(false, true)) { logInfo(\"SparkContext already stopped.\") return } if (_shutdownHookRef != null) { ShutdownHookManager.removeShutdownHook(_shutdownHookRef) } if (listenerBus != null) { Utils.tryLogNonFatalError { postApplicationEnd() } } Utils.tryLogNonFatalError { _driverLogger.foreach(_.stop()) } Utils.tryLogNonFatalError { _ui.foreach(_.stop()) } Utils.tryLogNonFatalError { _cleaner.foreach(_.stop()) } Utils.tryLogNonFatalError { _executorAllocationManager.foreach(_.stop()) } if (_dagScheduler != null) { Utils.tryLogNonFatalError { _dagScheduler.stop() } _dagScheduler = null } if (_listenerBusStarted) { Utils.tryLogNonFatalError { listenerBus.stop() _listenerBusStarted = false } } if (env != null) { Utils.tryLogNonFatalError { env.metricsSystem.report() } } Utils.tryLogNonFatalError { _plugins.foreach(_.shutdown()) } FallbackStorage.cleanUp(_conf, _hadoopConfiguration) Utils.tryLogNonFatalError { _eventLogger.foreach(_.stop()) } if (_heartbeater != null) { Utils.tryLogNonFatalError { _heartbeater.stop() } _heartbeater = null } if (_shuffleDriverComponents != null) { Utils.tryLogNonFatalError { _shuffleDriverComponents.cleanupApplication() } } if (env != null && _heartbeatReceiver != null) { Utils.tryLogNonFatalError { env.rpcEnv.stop(_heartbeatReceiver) } } Utils.tryLogNonFatalError { _progressBar.foreach(_.stop()) } _taskScheduler = null // TODO: Cache.stop()? if (_env != null) { Utils.tryLogNonFatalError { _env.stop() } SparkEnv.set(null) } if (_statusStore != null) { _statusStore.close() } // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this // `SparkContext` is stopped. localProperties.remove() ResourceProfile.clearDefaultProfile() // Unset YARN mode system env variable, to allow switching between cluster types. SparkContext.clearActiveContext() logInfo(\"Successfully stopped SparkContext\") } /** * Get Spark's home location from either a value set through the constructor, * or the spark.home Java property, or the SPARK_HOME environment variable * (in that order of preference). If neither of these is set, return None.  private[spark] def getSparkHome(): Option[String] = { conf.getOption(\"spark.home\").orElse(Option(System.getenv(\"SPARK_HOME\"))) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  def setCallSite(shortCallSite: String): Unit = { setLocalProperty(CallSite.SHORT_FORM, shortCallSite) } /** * Set the thread-local property for overriding the call sites * of actions and RDDs.  private[spark] def setCallSite(callSite: CallSite): Unit = { setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm) setLocalProperty(CallSite.LONG_FORM, callSite.longForm) } /** * Clear the thread-local property for overriding the call sites * of actions and RDDs.  def clearCallSite(): Unit = { setLocalProperty(CallSite.SHORT_FORM, null) setLocalProperty(CallSite.LONG_FORM, null) } /** * Capture the current user callsite and return a formatted version for printing. If the user * has overridden the call site using `setCallSite()`, this will return the user's version.  private[spark] def getCallSite(): CallSite = { lazy val callSite = Utils.getCallSite() CallSite( Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm), Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm) ) } /** * Run a function on a given set of partitions in an RDD and pass the results to the given * handler function. This is the main entry point for all actions in Spark. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = { if (stopped.get()) { throw new IllegalStateException(\"SparkContext has been shutdown\") } val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) { logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) } dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * The function that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, partitions: Seq[Int]): Array[U] = { val results = new Array[U](partitions.size) runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res) results } /** * Run a function on a given set of partitions in an RDD and return the results as an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag]( rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int]): Array[U] = { val cleanedFunc = clean(func) runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions) } /** * Run a job on all partitions in an RDD and return the results in an array. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and return the results in an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition)  def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = { runJob(rdd, func, 0 until rdd.partitions.length) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. The function * that is run against each partition additionally takes `TaskContext` argument. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: (TaskContext, Iterator[T]) => U, resultHandler: (Int, U) => Unit): Unit = { runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler) } /** * Run a job on all partitions in an RDD and pass the results to a handler function. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param resultHandler callback to pass each result to  def runJob[T, U: ClassTag]( rdd: RDD[T], processPartition: Iterator[T] => U, resultHandler: (Int, U) => Unit): Unit = { val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter) runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler) } /** * :: DeveloperApi :: * Run a job that can return approximate results. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @param evaluator `ApproximateEvaluator` to receive the partial results * @param timeout maximum time to wait for the job, in milliseconds * @return partial result (how partial depends on whether the job was finished before or * after timeout)  @DeveloperApi def runApproximateJob[T, U, R]( rdd: RDD[T], func: (TaskContext, Iterator[T]) => U, evaluator: ApproximateEvaluator[U, R], timeout: Long): PartialResult[R] = { assertNotStopped() val callSite = getCallSite logInfo(\"Starting job: \" + callSite.shortForm) val start = System.nanoTime val cleanedFunc = clean(func) val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout, localProperties.get) logInfo( \"Job finished: \" + callSite.shortForm + \", took \" + (System.nanoTime - start) / 1e9 + \" s\") result } /** * Submit a job for execution and return a FutureJob holding the result. * * @param rdd target RDD to run tasks on * @param processPartition a function to run on each partition of the RDD * @param partitions set of partitions to run on; some jobs may not want to compute on all * partitions of the target RDD, e.g. for operations like `first()` * @param resultHandler callback to pass each result to * @param resultFunc function to be executed when the result is ready  def submitJob[T, U, R]( rdd: RDD[T], processPartition: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit, resultFunc: => R): SimpleFutureAction[R] = { assertNotStopped() val cleanF = clean(processPartition) val callSite = getCallSite val waiter = dagScheduler.submitJob( rdd, (context: TaskContext, iter: Iterator[T]) => cleanF(iter), partitions, callSite, resultHandler, localProperties.get) new SimpleFutureAction(waiter, resultFunc) } /** * Submit a map stage for execution. This is currently an internal API only, but might be * promoted to DeveloperApi in the future.  private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C]) : SimpleFutureAction[MapOutputStatistics] = { assertNotStopped() val callSite = getCallSite() var result: MapOutputStatistics = null val waiter = dagScheduler.submitMapStage( dependency, (r: MapOutputStatistics) => { result = r }, callSite, localProperties.get) new SimpleFutureAction[MapOutputStatistics](waiter, result) } /** * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup` * for more information.  def cancelJobGroup(groupId: String): Unit = { assertNotStopped() dagScheduler.cancelJobGroup(groupId) } /** Cancel all jobs that have been scheduled or are running.  def cancelAllJobs(): Unit = { assertNotStopped() dagScheduler.cancelAllJobs() } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @param reason optional reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int, reason: String): Unit = { dagScheduler.cancelJob(jobId, Option(reason)) } /** * Cancel a given job if it's scheduled or running. * * @param jobId the job ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelJob(jobId: Int): Unit = { dagScheduler.cancelJob(jobId, None) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @param reason reason for cancellation * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int, reason: String): Unit = { dagScheduler.cancelStage(stageId, Option(reason)) } /** * Cancel a given stage and all jobs associated with it. * * @param stageId the stage ID to cancel * @note Throws `InterruptedException` if the cancel message cannot be sent  def cancelStage(stageId: Int): Unit = { dagScheduler.cancelStage(stageId, None) } /** * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI * or through SparkListener.onTaskStart. * * @param taskId the task ID to kill. This id uniquely identifies the task attempt. * @param interruptThread whether to interrupt the thread running the task. * @param reason the reason for killing the task, which should be a short string. If a task * is killed multiple times with different reasons, only one reason will be reported. * * @return Whether the task was successfully killed.  def killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean = { dagScheduler.killTaskAttempt(taskId, interruptThread, reason) } /** * Clean a closure to make it ready to be serialized and sent to tasks * (removes unreferenced variables in $outer's, updates REPL variables) * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt> * if not. * * @param f the closure to clean * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not * serializable * @return the cleaned closure  private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = { ClosureCleaner.clean(f, checkSerializable) f } /** * Set the directory under which RDDs are going to be checkpointed. * @param directory path to the directory where checkpoint files will be stored * (must be HDFS path if running in cluster)  def setCheckpointDir(directory: String): Unit = { // If we are running on a cluster, log a warning if the directory is local. // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from // its own local file system, which is incorrect because the checkpoint files // are actually on the executor machines. if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) { logWarning(\"Spark is not running in local mode, therefore the checkpoint directory \" + s\"must not be on the local filesystem. Directory '$directory' \" + \"appears to be on the local filesystem.\") } checkpointDir = Option(directory).map { dir => val path = new Path(dir, UUID.randomUUID().toString) val fs = path.getFileSystem(hadoopConfiguration) fs.mkdirs(path) fs.getFileStatus(path).getPath.toString } } def getCheckpointDir: Option[String] = checkpointDir /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD).  def defaultParallelism: Int = { assertNotStopped() taskScheduler.defaultParallelism } /** * Default min number of partitions for Hadoop RDDs when not given by user * Notice that we use math.min so the \"defaultMinPartitions\" cannot be higher than 2. * The reasons for this are discussed in https://github.com/mesos/spark/pull/718  def defaultMinPartitions: Int = math.min(defaultParallelism, 2) private val nextShuffleId = new AtomicInteger(0) private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement() private val nextRddId = new AtomicInteger(0) /** Register a new RDD, returning its RDD ID  private[spark] def newRddId(): Int = nextRddId.getAndIncrement() /** * Registers listeners specified in spark.extraListeners, then starts the listener bus. * This should be called after all internal listeners have been registered with the listener bus * (e.g. after the web UI and event logging listeners have been registered).  private def setupAndStartListenerBus(): Unit = { try { conf.get(EXTRA_LISTENERS).foreach { classNames => val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf) listeners.foreach { listener => listenerBus.addToSharedQueue(listener) logInfo(s\"Registered listener ${listener.getClass().getName()}\") } } } catch { case e: Exception => try { stop() } finally { throw new SparkException(s\"Exception when registering SparkListener\", e) } } listenerBus.start(this, _env.metricsSystem) _listenerBusStarted = true } /** Post the application start event  private def postApplicationStart(): Unit = { // Note: this code assumes that the task scheduler has been initialized and has contacted // the cluster manager to get an application ID (in case the cluster manager provides one). listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId), startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls, schedulerBackend.getDriverAttributes)) _driverLogger.foreach(_.startSync(_hadoopConfiguration)) } /** Post the application end event  private def postApplicationEnd(): Unit = { listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis)) } /** Post the environment update event once the task scheduler is ready  private def postEnvironmentUpdate(): Unit = { if (taskScheduler != null) { val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val addedArchivePaths = addedArchives.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, hadoopConfiguration, schedulingMode, addedJarPaths, addedFilePaths, addedArchivePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) } } /** Reports heartbeat metrics for the driver.  private def reportHeartBeat(executorMetricsSource: Option[ExecutorMetricsSource]): Unit = { val currentMetrics = ExecutorMetrics.getCurrentMetrics(env.memoryManager) executorMetricsSource.foreach(_.updateMetricsSnapshot(currentMetrics)) val driverUpdates = new HashMap[(Int, Int), ExecutorMetrics] // In the driver, we do not track per-stage metrics, so use a dummy stage for the key driverUpdates.put(EventLoggingListener.DRIVER_STAGE_KEY, new ExecutorMetrics(currentMetrics)) val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0) listenerBus.post(SparkListenerExecutorMetricsUpdate(\"driver\", accumUpdates, driverUpdates)) } // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having finished construction. // NOTE: this must be placed at the end of the SparkContext constructor. SparkContext.setActiveContext(this) } /** * The SparkContext object contains a number of implicit conversions and parameters for use with * various Spark features.  object SparkContext extends Logging { private val VALID_LOG_LEVELS = Set(\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\") /** * Lock that guards access to global variables that track SparkContext construction.  private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object() /** * The active, fully-constructed SparkContext. If no SparkContext is active, then this is `null`. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private val activeContext: AtomicReference[SparkContext] = new AtomicReference[SparkContext](null) /** * Points to a partially-constructed SparkContext if another thread is in the SparkContext * constructor, or `None` if no SparkContext is being constructed. * * Access to this field is guarded by `SPARK_CONTEXT_CONSTRUCTOR_LOCK`.  private var contextBeingConstructed: Option[SparkContext] = None /** * Called to ensure that no other SparkContext is running in this JVM. * * Throws an exception if a running context is detected and logs a warning if another thread is * constructing a SparkContext. This warning is necessary because the current locking scheme * prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private def assertNoOtherContextIsRunning(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()).filter(_ ne sc).foreach { ctx => val errMsg = \"Only one SparkContext should be running in this JVM (see SPARK-2243).\" + s\"The currently running SparkContext was created at:\\n${ctx.creationSite.longForm}\" throw new SparkException(errMsg) } contextBeingConstructed.filter(_ ne sc).foreach { otherContext => // Since otherContext might point to a partially-constructed context, guard against // its creationSite field being null: val otherContextCreationSite = Option(otherContext.creationSite).map(_.longForm).getOrElse(\"unknown location\") val warnMsg = \"Another SparkContext is being constructed (or threw an exception in its\" + \" constructor). This may indicate an error, since only one SparkContext should be\" + \" running in this JVM (see SPARK-2243).\" + s\" The other SparkContext was created at:\\n$otherContextCreationSite\" logWarning(warnMsg) } } } /** * Called to ensure that SparkContext is created or accessed only on the Driver. * * Throws an exception if a SparkContext is about to be created in executors.  private def assertOnDriver(): Unit = { if (Utils.isInRunningSparkTask) { // we're accessing it during task execution, fail. throw new IllegalStateException( \"SparkContext should only be created and accessed on the driver.\") } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * @param config `SparkConfig` that will be used for initialisation of the `SparkContext` * @return current `SparkContext` (or a new one if it wasn't created before the function call)  def getOrCreate(config: SparkConf): SparkContext = { // Synchronize to ensure that multiple create requests don't trigger an exception // from assertNoOtherContextIsRunning within setActiveContext SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext(config)) } else { if (config.getAll.nonEmpty) { logWarning(\"Using an existing SparkContext; some configuration may not take effect.\") } } activeContext.get() } } /** * This function may be used to get or instantiate a SparkContext and register it as a * singleton object. Because we can only have one active SparkContext per JVM, * this is useful when applications may wish to share a SparkContext. * * This method allows not passing a SparkConf (useful if just retrieving). * * @return current `SparkContext` (or a new one if wasn't created before the function call)  def getOrCreate(): SparkContext = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { if (activeContext.get() == null) { setActiveContext(new SparkContext()) } activeContext.get() } } /** Return the current active [[SparkContext]] if any.  private[spark] def getActive: Option[SparkContext] = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { Option(activeContext.get()) } } /** * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is * running. Throws an exception if a running context is detected and logs a warning if another * thread is constructing a SparkContext. This warning is necessary because the current locking * scheme prevents us from reliably distinguishing between cases where another context is being * constructed and cases where another constructor threw an exception.  private[spark] def markPartiallyConstructed(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = Some(sc) } } /** * Called at the end of the SparkContext constructor to ensure that no other SparkContext has * raced with this constructor and started.  private[spark] def setActiveContext(sc: SparkContext): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { assertNoOtherContextIsRunning(sc) contextBeingConstructed = None activeContext.set(sc) } } /** * Clears the active SparkContext metadata. This is called by `SparkContext#stop()`. It's * also called in unit tests to prevent a flood of warnings from test suites that don't / can't * properly clean up their SparkContexts.  private[spark] def clearActiveContext(): Unit = { SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized { activeContext.set(null) } } private[spark] val SPARK_JOB_DESCRIPTION = \"spark.job.description\" private[spark] val SPARK_JOB_GROUP_ID = \"spark.jobGroup.id\" private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = \"spark.job.interruptOnCancel\" private[spark] val SPARK_SCHEDULER_POOL = \"spark.scheduler.pool\" private[spark] val RDD_SCOPE_KEY = \"spark.rdd.scope\" private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = \"spark.rdd.scope.noOverride\" /** * Executor id for the driver. In earlier versions of Spark, this was `<driver>`, but this was * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see * SPARK-6716 for more details).  private[spark] val DRIVER_IDENTIFIER = \"driver\" private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Iterable[T]) : ArrayWritable = { def anyToWritable[U <: Writable](u: U): Writable = u new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]], arr.map(x => anyToWritable(x)).toArray) } /** * Find the JAR from which a given class was loaded, to make it easy for users to pass * their JARs to SparkContext. * * @param cls class that should be inside of the jar * @return jar that contains the Class, `None` if not found  def jarOfClass(cls: Class[_]): Option[String] = { val uri = cls.getResource(\"/\" + cls.getName.replace('.', '/') + \".class\") if (uri != null) { val uriStr = uri.toString if (uriStr.startsWith(\"jar:file:\")) { // URI will be of the form \"jar:file:/path/foo.jar!/package/cls.class\", // so pull out the /path/foo.jar Some(uriStr.substring(\"jar:file:\".length, uriStr.indexOf('!'))) } else { None } } else { None } } /** * Find the JAR that contains the class of a particular object, to make it easy for users * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in * your driver program. * * @param obj reference to an instance which class should be inside of the jar * @return jar that contains the class of the instance, `None` if not found  def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass) /** * Creates a modified version of a SparkConf with the parameters that can be passed separately * to SparkContext, to make it easier to write SparkContext's constructors. This ignores * parameters that are passed as the default value of null, instead of throwing an exception * like SparkConf would.  private[spark] def updatedConf( conf: SparkConf, master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()): SparkConf = { val res = conf.clone() res.setMaster(master) res.setAppName(appName) if (sparkHome != null) { res.setSparkHome(sparkHome) } if (jars != null && !jars.isEmpty) { res.setJars(jars) } res.setExecutorEnv(environment.toSeq) res } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String): Int = { numDriverCores(master, null) } /** * The number of cores available to the driver to use for tasks such as I/O with Netty  private[spark] def numDriverCores(master: String, conf: SparkConf): Int = { def convertToInt(threads: String): Int = { if (threads == \"*\") Runtime.getRuntime.availableProcessors() else threads.toInt } master match { case \"local\" => 1 case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads) case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads) case \"yarn\" | SparkMasterRegex.KUBERNETES_REGEX(_) => if (conf != null && conf.get(SUBMIT_DEPLOY_MODE) == \"cluster\") { conf.getInt(DRIVER_CORES.key, 0) } else { 0 } case _ => 0 // Either driver is not being used, or its core count will be interpolated later } } /** * Create a task scheduler based on a given master URL. * Return a 2-tuple of the scheduler backend and the task scheduler.  private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = { import SparkMasterRegex._ // When running locally, don't try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 // Ensure that default executor's resources satisfies one or more tasks requirement. // This function is for cluster managers that don't set the executor cores config, for // others its checked in ResourceProfile. def checkResourcesPerTask(executorCores: Int): Unit = { val taskCores = sc.conf.get(CPUS_PER_TASK) if (!sc.conf.get(SKIP_VALIDATE_CORES_TESTING)) { validateTaskCpusLargeEnough(sc.conf, executorCores, taskCores) } val defaultProf = sc.resourceProfileManager.defaultResourceProfile ResourceUtils.warnOnWastedResources(defaultProf, sc.conf, Some(executorCores)) } master match { case \"local\" => checkResourcesPerTask(1) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_REGEX(threads) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads. val threadCount = if (threads == \"*\") localCpuCount else threads.toInt if (threadCount <= 0) { throw new SparkException(s\"Asked to run locally with $threadCount threads\") } checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case LOCAL_N_FAILURES_REGEX(threads, maxFailures) => def localCpuCount: Int = Runtime.getRuntime.availableProcessors() // local[*, M] means the number of cores on the computer with M failures // local[N, M] means exactly N threads with M failures val threadCount = if (threads == \"*\") localCpuCount else threads.toInt checkResourcesPerTask(threadCount) val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true) val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount) scheduler.initialize(backend) (backend, scheduler) case SPARK_REGEX(sparkUrl) => val scheduler = new TaskSchedulerImpl(sc) val masterUrls = sparkUrl.split(\",\").map(\"spark://\" + _) val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) (backend, scheduler) case LOCAL_CLUSTER_REGEX(numWorkers, coresPerWorker, memoryPerWorker) => checkResourcesPerTask(coresPerWorker.toInt) // Check to make sure memory requested <= memoryPerWorker. Otherwise Spark will just hang. val memoryPerWorkerInt = memoryPerWorker.toInt if (sc.executorMemory > memoryPerWorkerInt) { throw new SparkException( \"Asked to launch cluster with %d MiB/worker but requested %d MiB/executor\".format( memoryPerWorkerInt, sc.executorMemory)) } // For host local mode setting the default of SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED // to false because this mode is intended to be used for testing and in this case all the // executors are running on the same host. So if host local reading was enabled here then // testing of the remote fetching would be secondary as setting this config explicitly to // false would be required in most of the unit test (despite the fact that remote fetching // is much more frequent in production). sc.conf.setIfMissing(SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED, false) val scheduler = new TaskSchedulerImpl(sc) val localCluster = LocalSparkCluster( numWorkers.toInt, coresPerWorker.toInt, memoryPerWorkerInt, sc.conf) val masterUrls = localCluster.start() val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls) scheduler.initialize(backend) backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => { localCluster.stop() } (backend, scheduler) case masterUrl => val cm = getClusterManager(masterUrl) match { case Some(clusterMgr) => clusterMgr case None => throw new SparkException(\"Could not parse Master URL: '\" + master + \"'\") } try { val scheduler = cm.createTaskScheduler(sc, masterUrl) val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) cm.initialize(scheduler, backend) (backend, scheduler) } catch { case se: SparkException => throw se case NonFatal(e) => throw new SparkException(\"External scheduler cannot be instantiated\", e) } } } private def getClusterManager(url: String): Option[ExternalClusterManager] = { val loader = Utils.getContextOrSparkClassLoader val serviceLoaders = ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url)) if (serviceLoaders.size > 1) { throw new SparkException( s\"Multiple external cluster managers registered for the url $url: $serviceLoaders\") } serviceLoaders.headOption } /** * This is a helper function to complete the missing S3A magic committer configurations * based on a single conf: `spark.hadoop.fs.s3a.bucket.<bucket>.committer.magic.enabled`  private def fillMissingMagicCommitterConfsIfNeeded(conf: SparkConf): Unit = { val magicCommitterConfs = conf .getAllWithPrefix(\"spark.hadoop.fs.s3a.bucket.\") .filter(_._1.endsWith(\".committer.magic.enabled\")) .filter(_._2.equalsIgnoreCase(\"true\")) if (magicCommitterConfs.nonEmpty) { // Try to enable S3 magic committer if missing conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\") if (conf.get(\"spark.hadoop.fs.s3a.committer.magic.enabled\").equals(\"true\")) { conf.setIfMissing(\"spark.hadoop.fs.s3a.committer.name\", \"magic\") conf.setIfMissing(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\") conf.setIfMissing(\"spark.sql.parquet.output.committer.class\", \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\") conf.setIfMissing(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\") } } } /** * SPARK-36796: This is a helper function to supplement `--add-opens` options to * `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`.  private def supplementJavaModuleOptions(conf: SparkConf): Unit = { def supplement(key: OptionalConfigEntry[String]): Unit = { val v = conf.get(key) match { case Some(opts) => s\"${JavaModuleOptions.defaultModuleOptions()} $opts\" case None => JavaModuleOptions.defaultModuleOptions() } conf.set(key.key, v) } supplement(DRIVER_JAVA_OPTIONS) supplement(EXECUTOR_JAVA_OPTIONS) } } /** * A collection of regexes for extracting information from the master string.  private object SparkMasterRegex { // Regular expression used for local[N] and local[*] master formats val LOCAL_N_REGEX = \"\"\"local\\[([0-9]+|\\*)\\]\"\"\".r // Regular expression for local[N, maxRetries], used in tests with failing tasks val LOCAL_N_FAILURES_REGEX = \"\"\"local\\[([0-9]+|\\*)\\s*,\\s*([0-9]+)\\]\"\"\".r // Regular expression for simulating a Spark cluster of [N, cores, memory] locally val LOCAL_CLUSTER_REGEX = \"\"\"local-cluster\\[\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*]\"\"\".r // Regular expression for connecting to Spark deploy clusters val SPARK_REGEX = \"\"\"spark://(.*)\"\"\".r // Regular expression for connecting to kubernetes clusters val KUBERNETES_REGEX = \"\"\"k8s://(.*)\"\"\".r } /** * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`).  private[spark] class WritableConverter[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: Writable => T) extends Serializable object WritableConverter { // Helper objects for converting common types to Writable private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T) : WritableConverter[T] = { val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]] new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W])) } // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. // The following implicit declarations have been added on top of the very similar ones // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta // expansion of zero-arg methods and thus won't match a no-arg method where it expects // an implicit that is a function of no args. implicit val intWritableConverterFn: () => WritableConverter[Int] = () => simpleWritableConverter[Int, IntWritable](_.get) implicit val longWritableConverterFn: () => WritableConverter[Long] = () => simpleWritableConverter[Long, LongWritable](_.get) implicit val doubleWritableConverterFn: () => WritableConverter[Double] = () => simpleWritableConverter[Double, DoubleWritable](_.get) implicit val floatWritableConverterFn: () => WritableConverter[Float] = () => simpleWritableConverter[Float, FloatWritable](_.get) implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] = () => simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = { () => simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit val stringWritableConverterFn: () => WritableConverter[String] = () => simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] = () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) // These implicits remain included for backwards-compatibility. They fulfill the // same role as those above. implicit def intWritableConverter(): WritableConverter[Int] = simpleWritableConverter[Int, IntWritable](_.get) implicit def longWritableConverter(): WritableConverter[Long] = simpleWritableConverter[Long, LongWritable](_.get) implicit def doubleWritableConverter(): WritableConverter[Double] = simpleWritableConverter[Double, DoubleWritable](_.get) implicit def floatWritableConverter(): WritableConverter[Float] = simpleWritableConverter[Float, FloatWritable](_.get) implicit def booleanWritableConverter(): WritableConverter[Boolean] = simpleWritableConverter[Boolean, BooleanWritable](_.get) implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = { simpleWritableConverter[Array[Byte], BytesWritable] { bw => // getBytes method returns array which is longer then data to be returned Arrays.copyOfRange(bw.getBytes, 0, bw.getLength) } } implicit def stringWritableConverter(): WritableConverter[String] = simpleWritableConverter[String, Text](_.toString) implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] = new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T]) } /** * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable` * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the * conversion. * The `Writable` class will be used in `SequenceFileRDDFunctions`.  private[spark] class WritableFactory[T]( val writableClass: ClassTag[T] => Class[_ <: Writable], val convert: T => Writable) extends Serializable object WritableFactory { private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W) : WritableFactory[T] = { val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]] new WritableFactory[T](_ => writableClass, convert) } implicit def intWritableFactory: WritableFactory[Int] = simpleWritableFactory(new IntWritable(_)) implicit def longWritableFactory: WritableFactory[Long] = simpleWritableFactory(new LongWritable(_)) implicit def floatWritableFactory: WritableFactory[Float] = simpleWritableFactory(new FloatWritable(_)) implicit def doubleWritableFactory: WritableFactory[Double] = simpleWritableFactory(new DoubleWritable(_)) implicit def booleanWritableFactory: WritableFactory[Boolean] = simpleWritableFactory(new BooleanWritable(_)) implicit def bytesWritableFactory: WritableFactory[Array[Byte]] = simpleWritableFactory(new BytesWritable(_)) implicit def stringWritableFactory: WritableFactory[String] = simpleWritableFactory(new Text(_)) implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] = simpleWritableFactory(w => w) }"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### Logging and Error-Reporting Architecture in Apache Spark 3.3.0\n\nUpdated: 2026-01-03 11:21 (local time)\n\nThis document summarizes the architecture for logging and error reporting in the provided Apache Spark 3.3.0 codebase snapshot, based on code inspection during this session. It is self-contained and can be read independently of the tooling that generated it.\n\n---\n\n#### Core logging mechanism\n\n- Widespread mix-in via a common trait\n  - Many classes and singleton objects mix in a `Logging` trait to gain logging methods such as `logInfo`, `logWarning`, and `logError`.\n  - Examples observed in the code:\n    - `RDD[T]` (core) extends `Serializable with Logging` and invokes `logWarning(\"Spark does not support nested RDDs ...\")`.\n    - `LogicalPlan` (SQL Catalyst) extends multiple traits including `Logging`.\n    - `Utils` (core utilities) is defined as an `object` that `extends Logging`.\n\n- Centralized uncaught exception handling\n  - `Utils` defines a `sparkUncaughtExceptionHandler = new SparkUncaughtExceptionHandler`, indicating a centralized handler for uncaught exceptions at process/thread boundaries. This integrates with logging to surface fatal errors consistently.\n\n---\n\n#### Centralized, structured error reporting (SQL and core)\n\n- Factory-style error helpers producing typed exceptions with structured metadata\n  - `object QueryExecutionErrors extends QueryErrorsBase` (SQL) exposes numerous helper methods that construct typed exceptions such as `SparkUnsupportedOperationException`, `SparkArithmeticException`, `SparkDateTimeException`, and `SparkRuntimeException`.\n  - Each factory method typically provides:\n    - `errorClass` (e.g., `CAST_OVERFLOW`, `CAST_INVALID_INPUT`)\n    - `messageParameters` (array of strings, often derived with helpers like `toSQLValue`, `toSQLType`, `toSQLConf`)\n    - Optional `queryContext` to include additional diagnostics\n\n- Similar pattern in core components\n  - In core, classes reference component-specific error factories, e.g., `RDD` throws from `SparkCoreErrors.rddLacksSparkContextError()` when there is no `SparkContext`.\n\n- Benefits of the design\n  - Consistency: Uniform error types and formatting across Spark subsystems (core and SQL).\n  - Structure: Machine-readable `errorClass` and parameterization support message templating, localization, downstream tooling, and better diagnostics.\n\n---\n\n#### Example touchpoints\n\n- `RDD`\n  - Logging: Uses `logWarning(...)` (e.g., for nested RDDs warning).\n  - Errors: Throws typed exceptions via `SparkCoreErrors.rddLacksSparkContextError()`.\n\n- `LogicalPlan`\n  - Mixes in `Logging` (even if specific log statements are not shown in the inspected snippet), inheriting standard diagnostic logging methods.\n\n- `Utils`\n  - Extends `Logging` and maintains the top-level `sparkUncaughtExceptionHandler` for consistent handling of fatal/unhandled exceptions.\n\n- `QueryExecutionErrors` (SQL)\n  - Central hub to construct exceptions carrying structured metadata and optional query context to aid debugging and user messaging.\n\n---\n\n#### Gaps and suggested deeper dives\n\nThe following implementations were not inspected in full within the provided snippets but are logical next steps if more detail is needed:\n\n- `org.apache.spark.internal.Logging`\n  - To confirm exact log levels, integration with SLF4J/Log4j backends, message formatting rules, and any MDC/structured logging support.\n\n- `org.apache.spark.sql.errors.QueryErrorsBase` and `org.apache.spark.errors.SparkCoreErrors`\n  - To show how `errorClass` templates are resolved, parameter substitution mechanics, localization/i18n (if any), and mapping to specific exception subtypes.\n\n- Event/listener plumbing and sinks\n  - E.g., SparkListenerBus, event logging, metrics emission, and how they interoperate with logging and error handling.\n\nIf you want a deeper dive into any of the above, fetch and review those components directly; they are the likely source-of-truth for formatting and routing specifics.\n\n---\n\n#### Summary\n\n- Logging is implemented through a shared `Logging` trait mixed into core components (`RDD`, `LogicalPlan`, `Utils`, etc.).\n- Top-level failures are captured by a `SparkUncaughtExceptionHandler` maintained in `Utils`.\n- Error reporting uses centralized, component-scoped factories (`QueryExecutionErrors`, `SparkCoreErrors`) to construct strongly typed exceptions with structured fields (`errorClass`, `messageParameters`, `queryContext`).\n- This yields consistent and diagnosable behavior across Sparks subsystems.\n\n---\n\n#### Appendix: How to locate the mentioned elements (paths as seen in the provided project snapshot)\n\n- Examples shown in recent files list during analysis:\n  - `core/src/main/scala/org/apache/spark/rdd/RDD.scala`\n  - `sql/core/src/main/scala/org/apache/spark/sql/LogicalPlan.scala` (referenced by name in discussion; path representative)\n  - `core/src/main/scala/org/apache/spark/util/Utils.scala` (referenced as `Utils`; actual path may vary as `org/apache/spark/util/Utils.scala`)\n  - `sql/core/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala`\n\nNote: Exact file paths may differ slightly within your local checkout; the above are representative of their typical locations in Spark 3.3.0.\n",
          "prompt_time": 17.791,
          "answer_time": 120,
          "context_tokens": 93927,
          "used_context": [
            "## CLASS: org/apache/spark/sql/catalyst/plans/logical/LogicalPlan#\nabstract class LogicalPlan extends QueryPlan[LogicalPlan] with AnalysisHelper with LogicalPlanStats with LogicalPlanDistinctKeys with QueryPlanConstraints with Logging { /** * Metadata fields that can be projected from this node. * Should be overridden if the plan does not propagate its children's output.  def metadataOutput: Seq[Attribute] = children.flatMap(_.metadataOutput) /** Returns true if this subtree has data from a streaming data source.  def isStreaming: Boolean = _isStreaming private[this] lazy val _isStreaming = children.exists(_.isStreaming) override def verboseStringWithSuffix(maxFields: Int): String = { super.verboseString(maxFields) + statsCache.map(\", \" + _.toString).getOrElse(\"\") } /** * Returns the maximum number of rows that this plan may compute. * * Any operator that a Limit can be pushed passed should override this function (e.g., Union). * Any operator that can push through a Limit should override this function (e.g., Project).  def maxRows: Option[Long] = None /** * Returns the maximum number of rows this plan may compute on each partition.  def maxRowsPerPartition: Option[Long] = maxRows /** * Returns true if this expression and all its children have been resolved to a specific schema * and false if it still contains any unresolved placeholders. Implementations of LogicalPlan * can override this (e.g. * [[org.apache.spark.sql.catalyst.analysis.UnresolvedRelation UnresolvedRelation]] * should return `false`).  lazy val resolved: Boolean = expressions.forall(_.resolved) && childrenResolved override protected def statePrefix = if (!resolved) \"'\" else super.statePrefix /** * Returns true if all its children of this query plan have been resolved.  def childrenResolved: Boolean = children.forall(_.resolved) /** * Resolves a given schema to concrete [[Attribute]] references in this query plan. This function * should only be called on analyzed plans since it will throw [[AnalysisException]] for * unresolved [[Attribute]]s.  def resolve(schema: StructType, resolver: Resolver): Seq[Attribute] = { schema.map { field => resolve(field.name :: Nil, resolver).map { case a: AttributeReference => a case _ => throw QueryExecutionErrors.resolveCannotHandleNestedSchema(this) }.getOrElse { throw QueryCompilationErrors.cannotResolveAttributeError( field.name, output.map(_.name).mkString(\", \")) } } } private[this] lazy val childAttributes = AttributeSeq(children.flatMap(_.output)) private[this] lazy val childMetadataAttributes = AttributeSeq(children.flatMap(_.metadataOutput)) private[this] lazy val outputAttributes = AttributeSeq(output) private[this] lazy val outputMetadataAttributes = AttributeSeq(metadataOutput) /** * Optionally resolves the given strings to a [[NamedExpression]] using the input from all child * nodes of this LogicalPlan. The attribute is expressed as * string in the following form: `[scope].AttributeName.[nested].[fields]...`.  def resolveChildren( nameParts: Seq[String], resolver: Resolver): Option[NamedExpression] = childAttributes.resolve(nameParts, resolver) .orElse(childMetadataAttributes.resolve(nameParts, resolver)) /** * Optionally resolves the given strings to a [[NamedExpression]] based on the output of this * LogicalPlan. The attribute is expressed as string in the following form: * `[scope].AttributeName.[nested].[fields]...`.  def resolve( nameParts: Seq[String], resolver: Resolver): Option[NamedExpression] = outputAttributes.resolve(nameParts, resolver) .orElse(outputMetadataAttributes.resolve(nameParts, resolver)) /** * Given an attribute name, split it to name parts by dot, but * don't split the name parts quoted by backticks, for example, * `ab.cd`.`efg` should be split into two parts \"ab.cd\" and \"efg\".  def resolveQuoted( name: String, resolver: Resolver): Option[NamedExpression] = { resolve(UnresolvedAttribute.parseAttributeName(name), resolver) } /** * Refreshes (or invalidates) any metadata/data cached in the plan recursively.  def refresh(): Unit = children.foreach(_.refresh()) /** * Returns the output ordering that this plan generates.  def outputOrdering: Seq[SortOrder] = Nil /** * Returns true iff `other`'s output is semantically the same, i.e.: * - it contains the same number of `Attribute`s; * - references are the same; * - the order is equal too.  def sameOutput(other: LogicalPlan): Boolean = { val thisOutput = this.output val otherOutput = other.output thisOutput.length == otherOutput.length && thisOutput.zip(otherOutput).forall { case (a1, a2) => a1.semanticEquals(a2) } } } /** * A logical plan node with no children.  trait LeafNode extends LogicalPlan with LeafLike[LogicalPlan] { override def producedAttributes: AttributeSet = outputSet /** Leaf nodes that can survive analysis must define their own statistics.  def computeStats(): Statistics = throw new UnsupportedOperationException } /** * A logical plan node with single child.  trait UnaryNode extends LogicalPlan with UnaryLike[LogicalPlan] { /** * Generates all valid constraints including an set of aliased constraints by replacing the * original constraint expressions with the corresponding alias  protected def getAllValidConstraints(projectList: Seq[NamedExpression]): ExpressionSet = { var allConstraints = child.constraints projectList.foreach { case a @ Alias(l: Literal, _) => allConstraints += EqualNullSafe(a.toAttribute, l) case a @ Alias(e, _) if e.deterministic => // For every alias in `projectList`, replace the reference in constraints by its attribute. allConstraints ++= allConstraints.map(_ transform { case expr: Expression if expr.semanticEquals(e) => a.toAttribute }) allConstraints += EqualNullSafe(e, a.toAttribute) case _ => // Don't change. } allConstraints } override protected lazy val validConstraints: ExpressionSet = child.constraints } /** * A logical plan node with a left and right child.  trait BinaryNode extends LogicalPlan with BinaryLike[LogicalPlan] abstract class OrderPreservingUnaryNode extends UnaryNode { override final def outputOrdering: Seq[SortOrder] = child.outputOrdering } object LogicalPlanIntegrity { private def canGetOutputAttrs(p: LogicalPlan): Boolean = { p.resolved && !p.expressions.exists { e => e.exists { // We cannot call `output` in plans with a `ScalarSubquery` expr having no column, // so, we filter out them in advance. case s: ScalarSubquery => s.plan.schema.fields.isEmpty case _ => false } } } /** * Since some logical plans (e.g., `Union`) can build `AttributeReference`s in their `output`, * this method checks if the same `ExprId` refers to attributes having the same data type * in plan output.  def hasUniqueExprIdsForOutput(plan: LogicalPlan): Boolean = { val exprIds = plan.collect { case p if canGetOutputAttrs(p) => // NOTE: we still need to filter resolved expressions here because the output of // some resolved logical plans can have unresolved references, // e.g., outer references in `ExistenceJoin`. p.output.filter(_.resolved).map { a => (a.exprId, a.dataType.asNullable) } }.flatten val ignoredExprIds = plan.collect { // NOTE: `Union` currently reuses input `ExprId`s for output references, but we cannot // simply modify the code for assigning new `ExprId`s in `Union#output` because // the modification will make breaking changes (See SPARK-32741(#29585)). // So, this check just ignores the `exprId`s of `Union` output. case u: Union if u.resolved => u.output.map(_.exprId) }.flatten.toSet val groupedDataTypesByExprId = exprIds.filterNot { case (exprId, _) => ignoredExprIds.contains(exprId) }.groupBy(_._1).values.map(_.distinct) groupedDataTypesByExprId.forall(_.length == 1) } /** * This method checks if reference `ExprId`s are not reused when assigning a new `ExprId`. * For example, it returns false if plan transformers create an alias having the same `ExprId` * with one of reference attributes, e.g., `a#1 + 1 AS a#1`.  def checkIfSameExprIdNotReused(plan: LogicalPlan): Boolean = { plan.collect { case p if p.resolved => p.expressions.forall { case a: Alias => // Even if a plan is resolved, `a.references` can return unresolved references, // e.g., in `Grouping`/`GroupingID`, so we need to filter out them and // check if the same `exprId` in `Alias` does not exist // among reference `exprId`s. !a.references.filter(_.resolved).map(_.exprId).exists(_ == a.exprId) case _ => true } }.forall(identity) } /** * This method checks if the same `ExprId` refers to an unique attribute in a plan tree. * Some plan transformers (e.g., `RemoveNoopOperators`) rewrite logical * plans based on this assumption.  def checkIfExprIdsAreGloballyUnique(plan: LogicalPlan): Boolean = { checkIfSameExprIdNotReused(plan) && hasUniqueExprIdsForOutput(plan) } } /** * A logical plan node that can generate metadata columns  trait ExposesMetadataColumns extends LogicalPlan { def withMetadataColumns(): LogicalPlan }",
            "## OBJECT: org/apache/spark/sql/errors/QueryExecutionErrors.\n private[sql] object QueryExecutionErrors extends QueryErrorsBase { def cannotEvaluateExpressionError(expression: Expression): Throwable = { new SparkUnsupportedOperationException(errorClass = \"INTERNAL_ERROR\", messageParameters = Array(s\"Cannot evaluate expression: $expression\")) } def cannotGenerateCodeForExpressionError(expression: Expression): Throwable = { new SparkUnsupportedOperationException(errorClass = \"INTERNAL_ERROR\", messageParameters = Array(s\"Cannot generate code for expression: $expression\")) } def cannotTerminateGeneratorError(generator: UnresolvedGenerator): Throwable = { new SparkUnsupportedOperationException(errorClass = \"INTERNAL_ERROR\", messageParameters = Array(s\"Cannot terminate expression: $generator\")) } def castingCauseOverflowError(t: Any, from: DataType, to: DataType): ArithmeticException = { new SparkArithmeticException( errorClass = \"CAST_OVERFLOW\", messageParameters = Array( toSQLValue(t, from), toSQLType(from), toSQLType(to), toSQLConf(SQLConf.ANSI_ENABLED.key))) } def cannotChangeDecimalPrecisionError( value: Decimal, decimalPrecision: Int, decimalScale: Int, context: String): ArithmeticException = { new SparkArithmeticException( errorClass = \"CANNOT_CHANGE_DECIMAL_PRECISION\", messageParameters = Array( value.toDebugString, decimalPrecision.toString, decimalScale.toString, toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = context) } def invalidInputInCastToDatetimeError( value: Any, from: DataType, to: DataType, errorContext: String): Throwable = { new SparkDateTimeException( errorClass = \"CAST_INVALID_INPUT\", messageParameters = Array( toSQLValue(value, from), toSQLType(from), toSQLType(to), toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = errorContext) } def invalidInputSyntaxForBooleanError( s: UTF8String, errorContext: String): SparkRuntimeException = { new SparkRuntimeException( errorClass = \"CAST_INVALID_INPUT\", messageParameters = Array( toSQLValue(s, StringType), toSQLType(StringType), toSQLType(BooleanType), toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = errorContext) } def invalidInputInCastToNumberError( to: DataType, s: UTF8String, errorContext: String): SparkNumberFormatException = { new SparkNumberFormatException( errorClass = \"CAST_INVALID_INPUT\", messageParameters = Array( toSQLValue(s, StringType), toSQLType(StringType), toSQLType(to), toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = errorContext) } def cannotCastFromNullTypeError(to: DataType): Throwable = { new SparkException(errorClass = \"CANNOT_CAST_DATATYPE\", messageParameters = Array(NullType.typeName, to.typeName), null) } def cannotCastError(from: DataType, to: DataType): Throwable = { new SparkException(errorClass = \"CANNOT_CAST_DATATYPE\", messageParameters = Array(from.typeName, to.typeName), null) } def cannotParseDecimalError(): Throwable = { new SparkRuntimeException( errorClass = \"CANNOT_PARSE_DECIMAL\", messageParameters = Array.empty) } def dataTypeUnsupportedError(dataType: String, failure: String): Throwable = { new SparkIllegalArgumentException(errorClass = \"UNSUPPORTED_DATATYPE\", messageParameters = Array(dataType + failure)) } def failedExecuteUserDefinedFunctionError(funcCls: String, inputTypes: String, outputType: String, e: Throwable): Throwable = { new SparkException(errorClass = \"FAILED_EXECUTE_UDF\", messageParameters = Array(funcCls, inputTypes, outputType), e) } def divideByZeroError(context: String): ArithmeticException = { new SparkArithmeticException( errorClass = \"DIVIDE_BY_ZERO\", messageParameters = Array(toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = context) } def invalidArrayIndexError(index: Int, numElements: Int): ArrayIndexOutOfBoundsException = { invalidArrayIndexErrorInternal(index, numElements, SQLConf.ANSI_ENABLED.key) } def invalidInputIndexError(index: Int, numElements: Int): ArrayIndexOutOfBoundsException = { invalidArrayIndexErrorInternal(index, numElements, SQLConf.ANSI_ENABLED.key) } private def invalidArrayIndexErrorInternal( index: Int, numElements: Int, key: String): ArrayIndexOutOfBoundsException = { new SparkArrayIndexOutOfBoundsException( errorClass = \"INVALID_ARRAY_INDEX\", messageParameters = Array( toSQLValue(index, IntegerType), toSQLValue(numElements, IntegerType), toSQLConf(key))) } def invalidElementAtIndexError( index: Int, numElements: Int): ArrayIndexOutOfBoundsException = { new SparkArrayIndexOutOfBoundsException( errorClass = \"INVALID_ARRAY_INDEX_IN_ELEMENT_AT\", messageParameters = Array( toSQLValue(index, IntegerType), toSQLValue(numElements, IntegerType), toSQLConf(SQLConf.ANSI_ENABLED.key))) } def mapKeyNotExistError(key: Any, dataType: DataType, context: String): NoSuchElementException = { new SparkNoSuchElementException( errorClass = \"MAP_KEY_DOES_NOT_EXIST\", messageParameters = Array( toSQLValue(key, dataType), toSQLConf(SQLConf.ANSI_ENABLED.key)), queryContext = context) } def inputTypeUnsupportedError(dataType: DataType): Throwable = { new IllegalArgumentException(s\"Unsupported input type ${dataType.catalogString}\") } def invalidFractionOfSecondError(): DateTimeException = { new SparkDateTimeException( errorClass = \"INVALID_FRACTION_OF_SECOND\", Array(toSQLConf(SQLConf.ANSI_ENABLED.key))) } def ansiDateTimeParseError(e: DateTimeParseException): DateTimeParseException = { val newMessage = s\"${e.getMessage}. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" new DateTimeParseException(newMessage, e.getParsedString, e.getErrorIndex, e.getCause) } def ansiDateTimeError(e: DateTimeException): DateTimeException = { val newMessage = s\"${e.getMessage}. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" new DateTimeException(newMessage, e.getCause) } def ansiParseError(e: JavaParseException): JavaParseException = { val newMessage = s\"${e.getMessage}. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" new JavaParseException(newMessage, e.getErrorOffset) } def ansiIllegalArgumentError(message: String): IllegalArgumentException = { val newMessage = s\"$message. If necessary set ${SQLConf.ANSI_ENABLED.key} \" + s\"to false to bypass this error.\" new IllegalArgumentException(newMessage) } def ansiIllegalArgumentError(e: IllegalArgumentException): IllegalArgumentException = { ansiIllegalArgumentError(e.getMessage) } def overflowInSumOfDecimalError(context: String): ArithmeticException = { arithmeticOverflowError(\"Overflow in sum of decimals\", errorContext = context) } def overflowInIntegralDivideError(context: String): ArithmeticException = { arithmeticOverflowError(\"Overflow in integral divide\", \"try_divide\", context) } def mapSizeExceedArraySizeWhenZipMapError(size: Int): RuntimeException = { new RuntimeException(s\"Unsuccessful try to zip maps with $size \" + \"unique keys due to exceeding the array size limit \" + s\"${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}.\") } def copyNullFieldNotAllowedError(): Throwable = { new IllegalStateException(\"Do not attempt to copy a null field\") } def literalTypeUnsupportedError(v: Any): RuntimeException = { new SparkRuntimeException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(s\"literal for '${v.toString}' of ${v.getClass.toString}.\")) } def pivotColumnUnsupportedError(v: Any, dataType: DataType): RuntimeException = { new SparkRuntimeException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array( s\"pivoting by the value '${v.toString}' of the column data type ${toSQLType(dataType)}.\")) } def noDefaultForDataTypeError(dataType: DataType): RuntimeException = { new RuntimeException(s\"no default for type $dataType\") } def doGenCodeOfAliasShouldNotBeCalledError(): Throwable = { new IllegalStateException(\"Alias.doGenCode should not be called.\") } def orderedOperationUnsupportedByDataTypeError(dataType: DataType): Throwable = { new IllegalArgumentException(s\"Type $dataType does not support ordered operations\") } def regexGroupIndexLessThanZeroError(): Throwable = { new IllegalArgumentException(\"The specified group index cannot be less than zero\") } def regexGroupIndexExceedGroupCountError( groupCount: Int, groupIndex: Int): Throwable = { new IllegalArgumentException( s\"Regex group count is $groupCount, but the specified group index is $groupIndex\") } def invalidUrlError(url: UTF8String, e: URISyntaxException): Throwable = { new IllegalArgumentException(s\"Find an invalid url string ${url.toString}. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\", e) } def dataTypeOperationUnsupportedError(): Throwable = { new UnsupportedOperationException(\"dataType\") } def mergeUnsupportedByWindowFunctionError(): Throwable = { new UnsupportedOperationException(\"Window Functions do not support merging.\") } def dataTypeUnexpectedError(dataType: DataType): Throwable = { new UnsupportedOperationException(s\"Unexpected data type ${dataType.catalogString}\") } def typeUnsupportedError(dataType: DataType): Throwable = { new IllegalArgumentException(s\"Unexpected type $dataType\") } def negativeValueUnexpectedError(frequencyExpression : Expression): Throwable = { new SparkException(s\"Negative values found in ${frequencyExpression.sql}\") } def addNewFunctionMismatchedWithFunctionError(funcName: String): Throwable = { new IllegalArgumentException(s\"$funcName is not matched at addNewFunction\") } def cannotGenerateCodeForUncomparableTypeError( codeType: String, dataType: DataType): Throwable = { new IllegalArgumentException( s\"cannot generate $codeType code for un-comparable type: ${dataType.catalogString}\") } def cannotGenerateCodeForUnsupportedTypeError(dataType: DataType): Throwable = { new IllegalArgumentException(s\"cannot generate code for unsupported type: $dataType\") } def cannotInterpolateClassIntoCodeBlockError(arg: Any): Throwable = { new IllegalArgumentException( s\"Can not interpolate ${arg.getClass.getName} into code block.\") } def customCollectionClsNotResolvedError(): Throwable = { new UnsupportedOperationException(\"not resolved\") } def classUnsupportedByMapObjectsError(cls: Class[_]): RuntimeException = { new RuntimeException(s\"class `${cls.getName}` is not supported by `MapObjects` as \" + \"resulting collection.\") } def nullAsMapKeyNotAllowedError(): RuntimeException = { new RuntimeException(\"Cannot use null as map key!\") } def methodNotDeclaredError(name: String): Throwable = { new SparkNoSuchMethodException(errorClass = \"INTERNAL_ERROR\", messageParameters = Array( s\"\"\"A method named \"$name\" is not declared in any enclosing class nor any supertype\"\"\")) } def constructorNotFoundError(cls: String): Throwable = { new RuntimeException(s\"Couldn't find a valid constructor on $cls\") } def primaryConstructorNotFoundError(cls: Class[_]): Throwable = { new RuntimeException(s\"Couldn't find a primary constructor on $cls\") } def unsupportedNaturalJoinTypeError(joinType: JoinType): Throwable = { new RuntimeException(\"Unsupported natural join type \" + joinType) } def notExpectedUnresolvedEncoderError(attr: AttributeReference): Throwable = { new RuntimeException(s\"Unresolved encoder expected, but $attr was found.\") } def unsupportedEncoderError(): Throwable = { new RuntimeException(\"Only expression encoders are supported for now.\") } def notOverrideExpectedMethodsError(className: String, m1: String, m2: String): Throwable = { new RuntimeException(s\"$className must override either $m1 or $m2\") } def failToConvertValueToJsonError(value: AnyRef, cls: Class[_], dataType: DataType): Throwable = { new RuntimeException(s\"Failed to convert value $value (class of $cls) \" + s\"with the type of $dataType to JSON.\") } def unexpectedOperatorInCorrelatedSubquery(op: LogicalPlan, pos: String = \"\"): Throwable = { new RuntimeException(s\"Unexpected operator $op in correlated subquery\" + pos) } def unreachableError(err: String = \"\"): Throwable = { new RuntimeException(\"This line should be unreachable\" + err) } def unsupportedRoundingMode(roundMode: BigDecimal.RoundingMode.Value): Throwable = { new RuntimeException(s\"Not supported rounding mode: $roundMode\") } def resolveCannotHandleNestedSchema(plan: LogicalPlan): Throwable = { new RuntimeException(s\"Can not handle nested schema yet... plan $plan\") } def inputExternalRowCannotBeNullError(): RuntimeException = { new RuntimeException(\"The input external row cannot be null.\") } def fieldCannotBeNullMsg(index: Int, fieldName: String): String = { s\"The ${index}th field '$fieldName' of input row cannot be null.\" } def fieldCannotBeNullError(index: Int, fieldName: String): RuntimeException = { new RuntimeException(fieldCannotBeNullMsg(index, fieldName)) } def unableToCreateDatabaseAsFailedToCreateDirectoryError( dbDefinition: CatalogDatabase, e: IOException): Throwable = { new SparkException(s\"Unable to create database ${dbDefinition.name} as failed \" + s\"to create its directory ${dbDefinition.locationUri}\", e) } def unableToDropDatabaseAsFailedToDeleteDirectoryError( dbDefinition: CatalogDatabase, e: IOException): Throwable = { new SparkException(s\"Unable to drop database ${dbDefinition.name} as failed \" + s\"to delete its directory ${dbDefinition.locationUri}\", e) } def unableToCreateTableAsFailedToCreateDirectoryError( table: String, defaultTableLocation: Path, e: IOException): Throwable = { new SparkException(s\"Unable to create table $table as failed \" + s\"to create its directory $defaultTableLocation\", e) } def unableToDeletePartitionPathError(partitionPath: Path, e: IOException): Throwable = { new SparkException(s\"Unable to delete partition path $partitionPath\", e) } def unableToDropTableAsFailedToDeleteDirectoryError( table: String, dir: Path, e: IOException): Throwable = { new SparkException(s\"Unable to drop table $table as failed \" + s\"to delete its directory $dir\", e) } def unableToRenameTableAsFailedToRenameDirectoryError( oldName: String, newName: String, oldDir: Path, e: IOException): Throwable = { new SparkException(s\"Unable to rename table $oldName to $newName as failed \" + s\"to rename its directory $oldDir\", e) } def unableToCreatePartitionPathError(partitionPath: Path, e: IOException): Throwable = { new SparkException(s\"Unable to create partition path $partitionPath\", e) } def unableToRenamePartitionPathError(oldPartPath: Path, e: IOException): Throwable = { new SparkException(s\"Unable to rename partition path $oldPartPath\", e) } def methodNotImplementedError(methodName: String): Throwable = { new UnsupportedOperationException(s\"$methodName is not implemented\") } def tableStatsNotSpecifiedError(): Throwable = { new IllegalStateException(\"table stats must be specified.\") } def arithmeticOverflowError(e: ArithmeticException): ArithmeticException = { new ArithmeticException(s\"${e.getMessage}. If necessary set ${SQLConf.ANSI_ENABLED.key} \" + s\"to false to bypass this error.\") } def arithmeticOverflowError( message: String, hint: String = \"\", errorContext: String = \"\"): ArithmeticException = { val alternative = if (hint.nonEmpty) { s\" Use '$hint' to tolerate overflow and return NULL instead.\" } else \"\" new SparkArithmeticException( errorClass = \"ARITHMETIC_OVERFLOW\", messageParameters = Array(message, alternative, SQLConf.ANSI_ENABLED.key), queryContext = errorContext) } def unaryMinusCauseOverflowError(originValue: Int): ArithmeticException = { arithmeticOverflowError(s\"- ${toSQLValue(originValue, IntegerType)} caused overflow\") } def binaryArithmeticCauseOverflowError( eval1: Short, symbol: String, eval2: Short): ArithmeticException = { arithmeticOverflowError( s\"${toSQLValue(eval1, ShortType)} $symbol ${toSQLValue(eval2, ShortType)} caused overflow\") } def failedSplitSubExpressionMsg(length: Int): String = { \"Failed to split subexpression code into small functions because \" + s\"the parameter length of at least one split function went over the JVM limit: $length\" } def failedSplitSubExpressionError(length: Int): Throwable = { new IllegalStateException(failedSplitSubExpressionMsg(length)) } def failedToCompileMsg(e: Exception): String = { s\"failed to compile: $e\" } def internalCompilerError(e: InternalCompilerException): Throwable = { new InternalCompilerException(failedToCompileMsg(e), e) } def compilerError(e: CompileException): Throwable = { new CompileException(failedToCompileMsg(e), e.getLocation) } def unsupportedTableChangeError(e: IllegalArgumentException): Throwable = { new SparkException(s\"Unsupported table change: ${e.getMessage}\", e) } def notADatasourceRDDPartitionError(split: Partition): Throwable = { new SparkException(s\"[BUG] Not a DataSourceRDDPartition: $split\") } def dataPathNotSpecifiedError(): Throwable = { new IllegalArgumentException(\"'path' is not specified\") } def createStreamingSourceNotSpecifySchemaError(): Throwable = { new IllegalArgumentException( s\"\"\" |Schema must be specified when creating a streaming source DataFrame. If some |files already exist in the directory, then depending on the file format you |may be able to create a static DataFrame on that directory with |'spark.read.load(directory)' and infer schema from it. \"\"\".stripMargin) } def streamedOperatorUnsupportedByDataSourceError( className: String, operator: String): Throwable = { new UnsupportedOperationException( s\"Data source $className does not support streamed $operator\") } def multiplePathsSpecifiedError(allPaths: Seq[String]): Throwable = { new IllegalArgumentException(\"Expected exactly one path to be specified, but \" + s\"got: ${allPaths.mkString(\", \")}\") } def failedToFindDataSourceError(provider: String, error: Throwable): Throwable = { new ClassNotFoundException( s\"\"\" |Failed to find data source: $provider. Please find packages at |https://spark.apache.org/third-party-projects.html \"\"\".stripMargin, error) } def removedClassInSpark2Error(className: String, e: Throwable): Throwable = { new ClassNotFoundException(s\"$className was removed in Spark 2.0. \" + \"Please check if your library is compatible with Spark 2.0\", e) } def incompatibleDataSourceRegisterError(e: Throwable): Throwable = { new SparkClassNotFoundException(\"INCOMPATIBLE_DATASOURCE_REGISTER\", Array(e.getMessage), e) } def unrecognizedFileFormatError(format: String): Throwable = { new IllegalStateException(s\"unrecognized format $format\") } // scalastyle:off line.size.limit def sparkUpgradeInReadingDatesError( format: String, config: String, option: String): SparkUpgradeException = { new SparkUpgradeException( errorClass = \"INCONSISTENT_BEHAVIOR_CROSS_VERSION\", messageParameters = Array( \"3.0\", s\"\"\" |reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z |from $format files can be ambiguous, as the files may be written by |Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar |that is different from Spark 3.0+'s Proleptic Gregorian calendar. |See more details in SPARK-31404. You can set the SQL config ${toSQLConf(config)} or |the datasource option ${toDSOption(option)} to \"LEGACY\" to rebase the datetime values |w.r.t. the calendar difference during reading. To read the datetime values |as it is, set the SQL config ${toSQLConf(config)} or the datasource option ${toDSOption(option)} |to \"CORRECTED\". |\"\"\".stripMargin), cause = null ) } // scalastyle:on line.size.limit def sparkUpgradeInWritingDatesError(format: String, config: String): SparkUpgradeException = { new SparkUpgradeException( errorClass = \"INCONSISTENT_BEHAVIOR_CROSS_VERSION\", messageParameters = Array( \"3.0\", s\"\"\" |writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z |into $format files can be dangerous, as the files may be read by Spark 2.x |or legacy versions of Hive later, which uses a legacy hybrid calendar that |is different from Spark 3.0+'s Proleptic Gregorian calendar. See more |details in SPARK-31404. You can set ${toSQLConf(config)} to \"LEGACY\" to rebase the |datetime values w.r.t. the calendar difference during writing, to get maximum |interoperability. Or set ${toSQLConf(config)} to \"CORRECTED\" to write the datetime |values as it is, if you are 100% sure that the written files will only be read by |Spark 3.0+ or other systems that use Proleptic Gregorian calendar. |\"\"\".stripMargin), cause = null ) } def buildReaderUnsupportedForFileFormatError(format: String): Throwable = { new UnsupportedOperationException(s\"buildReader is not supported for $format\") } def jobAbortedError(cause: Throwable): Throwable = { new SparkException(\"Job aborted.\", cause) } def taskFailedWhileWritingRowsError(cause: Throwable): Throwable = { new SparkException(\"Task failed while writing rows.\", cause) } def readCurrentFileNotFoundError(e: FileNotFoundException): Throwable = { new FileNotFoundException( s\"\"\" |${e.getMessage}\\n |It is possible the underlying files have been updated. You can explicitly invalidate |the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by |recreating the Dataset/DataFrame involved. \"\"\".stripMargin) } def unsupportedSaveModeError(saveMode: String, pathExists: Boolean): Throwable = { new IllegalStateException(s\"unsupported save mode $saveMode ($pathExists)\") } def cannotClearOutputDirectoryError(staticPrefixPath: Path): Throwable = { new IOException(s\"Unable to clear output directory $staticPrefixPath prior to writing to it\") } def cannotClearPartitionDirectoryError(path: Path): Throwable = { new IOException(s\"Unable to clear partition directory $path prior to writing to it\") } def failedToCastValueToDataTypeForPartitionColumnError( value: String, dataType: DataType, columnName: String): Throwable = { new RuntimeException(s\"Failed to cast value `$value` to \" + s\"`$dataType` for partition column `$columnName`\") } def endOfStreamError(): Throwable = { new NoSuchElementException(\"End of stream\") } def fallbackV1RelationReportsInconsistentSchemaError( v2Schema: StructType, v1Schema: StructType): Throwable = { new IllegalArgumentException( \"The fallback v1 relation reports inconsistent schema:\\n\" + \"Schema of v2 scan: \" + v2Schema + \"\\n\" + \"Schema of v1 relation: \" + v1Schema) } def noRecordsFromEmptyDataReaderError(): Throwable = { new IOException(\"No records should be returned from EmptyDataReader\") } def fileNotFoundError(e: FileNotFoundException): Throwable = { new FileNotFoundException( e.getMessage + \"\\n\" + \"It is possible the underlying files have been updated. \" + \"You can explicitly invalidate the cache in Spark by \" + \"recreating the Dataset/DataFrame involved.\") } def unsupportedSchemaColumnConvertError( filePath: String, column: String, logicalType: String, physicalType: String, e: Exception): Throwable = { val message = \"Parquet column cannot be converted in \" + s\"file $filePath. Column: $column, \" + s\"Expected: $logicalType, Found: $physicalType\" new QueryExecutionException(message, e) } def cannotReadFilesError( e: Throwable, path: String): Throwable = { val message = s\"Encountered error while reading file $path. Details: \" new QueryExecutionException(message, e) } def cannotCreateColumnarReaderError(): Throwable = { new UnsupportedOperationException(\"Cannot create columnar reader.\") } def invalidNamespaceNameError(namespace: Array[String]): Throwable = { new IllegalArgumentException(s\"Invalid namespace name: ${namespace.quoted}\") } def unsupportedPartitionTransformError(transform: Transform): Throwable = { new UnsupportedOperationException( s\"Unsupported partition transform: $transform\") } def missingDatabaseLocationError(): Throwable = { new IllegalArgumentException(\"Missing database location\") } def cannotRemoveReservedPropertyError(property: String): Throwable = { new UnsupportedOperationException(s\"Cannot remove reserved property: $property\") } def namespaceNotEmptyError(namespace: Array[String]): Throwable = { new IllegalStateException(s\"Namespace ${namespace.quoted} is not empty\") } def writingJobFailedError(cause: Throwable): Throwable = { new SparkException(\"Writing job failed.\", cause) } def writingJobAbortedError(e: Throwable): Throwable = { new SparkException( errorClass = \"WRITING_JOB_ABORTED\", messageParameters = Array.empty, cause = e) } def commitDeniedError( partId: Int, taskId: Long, attemptId: Int, stageId: Int, stageAttempt: Int): Throwable = { val message = s\"Commit denied for partition $partId (task $taskId, attempt $attemptId, \" + s\"stage $stageId.$stageAttempt)\" new CommitDeniedException(message, stageId, partId, attemptId) } def unsupportedTableWritesError(ident: Identifier): Throwable = { new SparkException( s\"Table implementation does not support writes: ${ident.quoted}\") } def cannotCreateJDBCTableWithPartitionsError(): Throwable = { new UnsupportedOperationException(\"Cannot create JDBC table with partition\") } def unsupportedUserSpecifiedSchemaError(): Throwable = { new UnsupportedOperationException(\"user-specified schema\") } def writeUnsupportedForBinaryFileDataSourceError(): Throwable = { new UnsupportedOperationException(\"Write is not supported for binary file data source\") } def fileLengthExceedsMaxLengthError(status: FileStatus, maxLength: Int): Throwable = { new SparkException( s\"The length of ${status.getPath} is ${status.getLen}, \" + s\"which exceeds the max length allowed: ${maxLength}.\") } def unsupportedFieldNameError(fieldName: String): Throwable = { new RuntimeException(s\"Unsupported field name: ${fieldName}\") } def cannotSpecifyBothJdbcTableNameAndQueryError( jdbcTableName: String, jdbcQueryString: String): Throwable = { new IllegalArgumentException( s\"Both '$jdbcTableName' and '$jdbcQueryString' can not be specified at the same time.\") } def missingJdbcTableNameAndQueryError( jdbcTableName: String, jdbcQueryString: String): Throwable = { new IllegalArgumentException( s\"Option '$jdbcTableName' or '$jdbcQueryString' is required.\" ) } def emptyOptionError(optionName: String): Throwable = { new IllegalArgumentException(s\"Option `$optionName` can not be empty.\") } def invalidJdbcTxnIsolationLevelError(jdbcTxnIsolationLevel: String, value: String): Throwable = { new IllegalArgumentException( s\"Invalid value `$value` for parameter `$jdbcTxnIsolationLevel`. This can be \" + \"`NONE`, `READ_UNCOMMITTED`, `READ_COMMITTED`, `REPEATABLE_READ` or `SERIALIZABLE`.\") } def cannotGetJdbcTypeError(dt: DataType): Throwable = { new IllegalArgumentException(s\"Can't get JDBC type for ${dt.catalogString}\") } def unrecognizedSqlTypeError(sqlType: Int): Throwable = { new SparkSQLException(errorClass = \"UNRECOGNIZED_SQL_TYPE\", Array(sqlType.toString)) } def unsupportedJdbcTypeError(content: String): Throwable = { new SQLException(s\"Unsupported type $content\") } def unsupportedArrayElementTypeBasedOnBinaryError(dt: DataType): Throwable = { new IllegalArgumentException(s\"Unsupported array element \" + s\"type ${dt.catalogString} based on binary\") } def nestedArraysUnsupportedError(): Throwable = { new IllegalArgumentException(\"Nested arrays unsupported\") } def cannotTranslateNonNullValueForFieldError(pos: Int): Throwable = { new IllegalArgumentException(s\"Can't translate non-null value for field $pos\") } def invalidJdbcNumPartitionsError(n: Int, jdbcNumPartitions: String): Throwable = { new IllegalArgumentException( s\"Invalid value `$n` for parameter `$jdbcNumPartitions` in table writing \" + \"via JDBC. The minimum value is 1.\") } def transactionUnsupportedByJdbcServerError(): Throwable = { new SparkSQLFeatureNotSupportedException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(\"the target JDBC server does not support transaction and \" + \"can only support ALTER TABLE with a single action.\")) } def dataTypeUnsupportedYetError(dataType: DataType): Throwable = { new UnsupportedOperationException(s\"$dataType is not supported yet.\") } def unsupportedOperationForDataTypeError(dataType: DataType): Throwable = { new UnsupportedOperationException(s\"DataType: ${dataType.catalogString}\") } def inputFilterNotFullyConvertibleError(owner: String): Throwable = { new SparkException(s\"The input filter of $owner should be fully convertible.\") } def cannotReadFooterForFileError(file: Path, e: IOException): Throwable = { new SparkException(s\"Could not read footer for file: $file\", e) } def cannotReadFooterForFileError(file: FileStatus, e: RuntimeException): Throwable = { new IOException(s\"Could not read footer for file: $file\", e) } def foundDuplicateFieldInCaseInsensitiveModeError( requiredFieldName: String, matchedOrcFields: String): Throwable = { new RuntimeException( s\"\"\" |Found duplicate field(s) \"$requiredFieldName\": $matchedOrcFields |in case-insensitive mode \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def foundDuplicateFieldInFieldIdLookupModeError( requiredId: Int, matchedFields: String): Throwable = { new RuntimeException( s\"\"\" |Found duplicate field(s) \"$requiredId\": $matchedFields |in id mapping mode \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def failedToMergeIncompatibleSchemasError( left: StructType, right: StructType, e: Throwable): Throwable = { new SparkException(s\"Failed to merge incompatible schemas $left and $right\", e) } def ddlUnsupportedTemporarilyError(ddl: String): Throwable = { new UnsupportedOperationException(s\"$ddl is not supported temporarily.\") } def operatingOnCanonicalizationPlanError(): Throwable = { new IllegalStateException(\"operating on canonicalization plan\") } def executeBroadcastTimeoutError(timeout: Long, ex: Option[TimeoutException]): Throwable = { new SparkException( s\"\"\" |Could not execute broadcast in $timeout secs. You can increase the timeout |for broadcasts via ${SQLConf.BROADCAST_TIMEOUT.key} or disable broadcast join |by setting ${SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key} to -1 \"\"\".stripMargin.replaceAll(\"\\n\", \" \"), ex.getOrElse(null)) } def cannotCompareCostWithTargetCostError(cost: String): Throwable = { new IllegalArgumentException(s\"Could not compare cost with $cost\") } def unsupportedDataTypeError(dt: String): Throwable = { new UnsupportedOperationException(s\"Unsupported data type: ${dt}\") } def notSupportTypeError(dataType: DataType): Throwable = { new Exception(s\"not support type: $dataType\") } def notSupportNonPrimitiveTypeError(): Throwable = { new RuntimeException(\"Not support non-primitive type now\") } def unsupportedTypeError(dataType: DataType): Throwable = { new Exception(s\"Unsupported type: ${dataType.catalogString}\") } def useDictionaryEncodingWhenDictionaryOverflowError(): Throwable = { new IllegalStateException( \"Dictionary encoding should not be used because of dictionary overflow.\") } def endOfIteratorError(): Throwable = { new NoSuchElementException(\"End of the iterator\") } def cannotAllocateMemoryToGrowBytesToBytesMapError(): Throwable = { new IOException(\"Could not allocate memory to grow BytesToBytesMap\") } def cannotAcquireMemoryToBuildLongHashedRelationError(size: Long, got: Long): Throwable = { new SparkException(s\"Can't acquire $size bytes memory to build hash relation, \" + s\"got $got bytes\") } def cannotAcquireMemoryToBuildUnsafeHashedRelationError(): Throwable = { new SparkOutOfMemoryError(\"There is not enough memory to build hash map\") } def rowLargerThan256MUnsupportedError(): Throwable = { new UnsupportedOperationException(\"Does not support row that is larger than 256M\") } def cannotBuildHashedRelationWithUniqueKeysExceededError(): Throwable = { new UnsupportedOperationException( \"Cannot build HashedRelation with more than 1/3 billions unique keys\") } def cannotBuildHashedRelationLargerThan8GError(): Throwable = { new UnsupportedOperationException( \"Can not build a HashedRelation that is larger than 8G\") } def failedToPushRowIntoRowQueueError(rowQueue: String): Throwable = { new SparkException(s\"failed to push a row into $rowQueue\") } def unexpectedWindowFunctionFrameError(frame: String): Throwable = { new RuntimeException(s\"Unexpected window function frame $frame.\") } def cannotParseStatisticAsPercentileError( stats: String, e: NumberFormatException): Throwable = { new IllegalArgumentException(s\"Unable to parse $stats as a percentile\", e) } def statisticNotRecognizedError(stats: String): Throwable = { new IllegalArgumentException(s\"$stats is not a recognised statistic\") } def unknownColumnError(unknownColumn: String): Throwable = { new IllegalArgumentException(s\"Unknown column: $unknownColumn\") } def unexpectedAccumulableUpdateValueError(o: Any): Throwable = { new IllegalArgumentException(s\"Unexpected: $o\") } def unscaledValueTooLargeForPrecisionError(): Throwable = { new ArithmeticException(\"Unscaled value too large for precision. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\") } def decimalPrecisionExceedsMaxPrecisionError(precision: Int, maxPrecision: Int): Throwable = { new ArithmeticException( s\"Decimal precision $precision exceeds max precision $maxPrecision\") } def outOfDecimalTypeRangeError(str: UTF8String): Throwable = { new ArithmeticException(s\"out of decimal type range: $str\") } def unsupportedArrayTypeError(clazz: Class[_]): Throwable = { new RuntimeException(s\"Do not support array of type $clazz.\") } def unsupportedJavaTypeError(clazz: Class[_]): Throwable = { new RuntimeException(s\"Do not support type $clazz.\") } def failedParsingStructTypeError(raw: String): Throwable = { new RuntimeException(s\"Failed parsing ${StructType.simpleString}: $raw\") } def failedMergingFieldsError(leftName: String, rightName: String, e: Throwable): Throwable = { new SparkException(s\"Failed to merge fields '$leftName' and '$rightName'. ${e.getMessage}\") } def cannotMergeDecimalTypesWithIncompatiblePrecisionAndScaleError( leftPrecision: Int, rightPrecision: Int, leftScale: Int, rightScale: Int): Throwable = { new SparkException(\"Failed to merge decimal types with incompatible \" + s\"precision $leftPrecision and $rightPrecision & scale $leftScale and $rightScale\") } def cannotMergeDecimalTypesWithIncompatiblePrecisionError( leftPrecision: Int, rightPrecision: Int): Throwable = { new SparkException(\"Failed to merge decimal types with incompatible \" + s\"precision $leftPrecision and $rightPrecision\") } def cannotMergeDecimalTypesWithIncompatibleScaleError( leftScale: Int, rightScale: Int): Throwable = { new SparkException(\"Failed to merge decimal types with incompatible \" + s\"scale $leftScale and $rightScale\") } def cannotMergeIncompatibleDataTypesError(left: DataType, right: DataType): Throwable = { new SparkException(s\"Failed to merge incompatible data types ${left.catalogString}\" + s\" and ${right.catalogString}\") } def exceedMapSizeLimitError(size: Int): Throwable = { new RuntimeException(s\"Unsuccessful attempt to build maps with $size elements \" + s\"due to exceeding the map size limit ${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}.\") } def duplicateMapKeyFoundError(key: Any): Throwable = { new RuntimeException(s\"Duplicate map key $key was found, please check the input \" + \"data. If you want to remove the duplicated keys, you can set \" + s\"${SQLConf.MAP_KEY_DEDUP_POLICY.key} to ${SQLConf.MapKeyDedupPolicy.LAST_WIN} so that \" + \"the key inserted at last takes precedence.\") } def mapDataKeyArrayLengthDiffersFromValueArrayLengthError(): Throwable = { new RuntimeException(\"The key array and value array of MapData must have the same length.\") } def fieldDiffersFromDerivedLocalDateError( field: ChronoField, actual: Int, expected: Int, candidate: LocalDate): Throwable = { new DateTimeException(s\"Conflict found: Field $field $actual differs from\" + s\" $field $expected derived from $candidate\") } def failToParseDateTimeInNewParserError(s: String, e: Throwable): Throwable = { new SparkUpgradeException(\"3.0\", s\"Fail to parse '$s' in the new parser. You can \" + s\"set ${SQLConf.LEGACY_TIME_PARSER_POLICY.key} to LEGACY to restore the behavior \" + s\"before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\", e) } def failToFormatDateTimeInNewFormatterError( resultCandidate: String, e: Throwable): Throwable = { new SparkUpgradeException(\"3.0\", s\"\"\" |Fail to format it to '$resultCandidate' in the new formatter. You can set |${SQLConf.LEGACY_TIME_PARSER_POLICY.key} to LEGACY to restore the behavior before |Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string. \"\"\".stripMargin.replaceAll(\"\\n\", \" \"), e) } def failToRecognizePatternAfterUpgradeError(pattern: String, e: Throwable): Throwable = { new SparkUpgradeException(\"3.0\", s\"Fail to recognize '$pattern' pattern in the\" + s\" DateTimeFormatter. 1) You can set ${SQLConf.LEGACY_TIME_PARSER_POLICY.key} to LEGACY\" + s\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern\" + s\" with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\", e) } def failToRecognizePatternError(pattern: String, e: Throwable): Throwable = { new RuntimeException(s\"Fail to recognize '$pattern' pattern in the\" + \" DateTimeFormatter. You can form a valid datetime pattern\" + \" with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\", e) } def cannotCastToDateTimeError( value: Any, from: DataType, to: DataType, errorContext: String): Throwable = { val valueString = toSQLValue(value, from) new DateTimeException(s\"Invalid input syntax for type ${toSQLType(to)}: $valueString. \" + s\"Use `try_cast` to tolerate malformed input and return NULL instead. \" + s\"If necessary set ${SQLConf.ANSI_ENABLED.key} \" + s\"to false to bypass this error.\" + errorContext) } def registeringStreamingQueryListenerError(e: Exception): Throwable = { new SparkException(\"Exception when registering StreamingQueryListener\", e) } def concurrentQueryInstanceError(): Throwable = { new SparkConcurrentModificationException(\"CONCURRENT_QUERY\", Array.empty) } def cannotParseJsonArraysAsStructsError(): Throwable = { new RuntimeException(\"Parsing JSON arrays as structs is forbidden.\") } def cannotParseStringAsDataTypeError(parser: JsonParser, token: JsonToken, dataType: DataType) : Throwable = { new RuntimeException( s\"Cannot parse field name ${parser.getCurrentName}, \" + s\"field value ${parser.getText}, \" + s\"[$token] as target spark data type [$dataType].\") } def cannotParseStringAsDataTypeError(pattern: String, value: String, dataType: DataType) : Throwable = { new RuntimeException( s\"Cannot parse field value ${toSQLValue(value, StringType)} \" + s\"for pattern ${toSQLValue(pattern, StringType)} \" + s\"as target spark data type [$dataType].\") } def failToParseEmptyStringForDataTypeError(dataType: DataType): Throwable = { new RuntimeException( s\"Failed to parse an empty string for data type ${dataType.catalogString}\") } def failToParseValueForDataTypeError(parser: JsonParser, token: JsonToken, dataType: DataType) : Throwable = { new RuntimeException( s\"Failed to parse field name ${parser.getCurrentName}, \" + s\"field value ${parser.getText}, \" + s\"[$token] to target spark data type [$dataType].\") } def rootConverterReturnNullError(): Throwable = { new RuntimeException(\"Root converter returned null\") } def cannotHaveCircularReferencesInBeanClassError(clazz: Class[_]): Throwable = { new UnsupportedOperationException( \"Cannot have circular references in bean class, but got the circular reference \" + s\"of class $clazz\") } def cannotHaveCircularReferencesInClassError(t: String): Throwable = { new UnsupportedOperationException( s\"cannot have circular references in class, but got the circular reference of class $t\") } def cannotUseInvalidJavaIdentifierAsFieldNameError( fieldName: String, walkedTypePath: WalkedTypePath): Throwable = { new UnsupportedOperationException(s\"`$fieldName` is not a valid identifier of \" + s\"Java and cannot be used as field name\\n$walkedTypePath\") } def cannotFindEncoderForTypeError( tpe: String, walkedTypePath: WalkedTypePath): Throwable = { new UnsupportedOperationException(s\"No Encoder found for $tpe\\n$walkedTypePath\") } def attributesForTypeUnsupportedError(schema: Schema): Throwable = { new UnsupportedOperationException(s\"Attributes for type $schema is not supported\") } def schemaForTypeUnsupportedError(tpe: String): Throwable = { new UnsupportedOperationException(s\"Schema for type $tpe is not supported\") } def cannotFindConstructorForTypeError(tpe: String): Throwable = { new UnsupportedOperationException( s\"\"\" |Unable to find constructor for $tpe. |This could happen if $tpe is an interface, or a trait without companion object |constructor. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def paramExceedOneCharError(paramName: String): Throwable = { new RuntimeException(s\"$paramName cannot be more than one character\") } def paramIsNotIntegerError(paramName: String, value: String): Throwable = { new RuntimeException(s\"$paramName should be an integer. Found ${toSQLValue(value, StringType)}\") } def paramIsNotBooleanValueError(paramName: String): Throwable = { new Exception(s\"$paramName flag can be true or false\") } def foundNullValueForNotNullableFieldError(name: String): Throwable = { new RuntimeException(s\"null value found but field $name is not nullable.\") } def malformedCSVRecordError(): Throwable = { new RuntimeException(\"Malformed CSV record\") } def elementsOfTupleExceedLimitError(): Throwable = { new UnsupportedOperationException(\"Due to Scala's limited support of tuple, \" + \"tuple with more than 22 elements are not supported.\") } def expressionDecodingError(e: Exception, expressions: Seq[Expression]): Throwable = { new RuntimeException(s\"Error while decoding: $e\\n\" + s\"${expressions.map(_.simpleString(SQLConf.get.maxToStringFields)).mkString(\"\\n\")}\", e) } def expressionEncodingError(e: Exception, expressions: Seq[Expression]): Throwable = { new RuntimeException(s\"Error while encoding: $e\\n\" + s\"${expressions.map(_.simpleString(SQLConf.get.maxToStringFields)).mkString(\"\\n\")}\", e) } def classHasUnexpectedSerializerError(clsName: String, objSerializer: Expression): Throwable = { new RuntimeException(s\"class $clsName has unexpected serializer: $objSerializer\") } def cannotGetOuterPointerForInnerClassError(innerCls: Class[_]): Throwable = { new RuntimeException(s\"Failed to get outer pointer for ${innerCls.getName}\") } def userDefinedTypeNotAnnotatedAndRegisteredError(udt: UserDefinedType[_]): Throwable = { new SparkException(s\"${udt.userClass.getName} is not annotated with \" + \"SQLUserDefinedType nor registered with UDTRegistration.}\") } def unsupportedOperandTypeForSizeFunctionError(dataType: DataType): Throwable = { new UnsupportedOperationException( s\"The size function doesn't support the operand type ${dataType.getClass.getCanonicalName}\") } def unexpectedValueForStartInFunctionError(prettyName: String): RuntimeException = { new RuntimeException( s\"Unexpected value for start in function $prettyName: SQL array indices start at 1.\") } def unexpectedValueForLengthInFunctionError(prettyName: String): RuntimeException = { new RuntimeException(s\"Unexpected value for length in function $prettyName: \" + \"length must be greater than or equal to 0.\") } def sqlArrayIndexNotStartAtOneError(): ArrayIndexOutOfBoundsException = { new ArrayIndexOutOfBoundsException(\"SQL array indices start at 1\") } def concatArraysWithElementsExceedLimitError(numberOfElements: Long): Throwable = { new RuntimeException( s\"\"\" |Unsuccessful try to concat arrays with $numberOfElements |elements due to exceeding the array size limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def flattenArraysWithElementsExceedLimitError(numberOfElements: Long): Throwable = { new RuntimeException( s\"\"\" |Unsuccessful try to flatten an array of arrays with $numberOfElements |elements due to exceeding the array size limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def createArrayWithElementsExceedLimitError(count: Any): RuntimeException = { new RuntimeException( s\"\"\" |Unsuccessful try to create array with $count elements |due to exceeding the array size limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def unionArrayWithElementsExceedLimitError(length: Int): Throwable = { new RuntimeException( s\"\"\" |Unsuccessful try to union arrays with $length |elements due to exceeding the array size limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def initialTypeNotTargetDataTypeError(dataType: DataType, target: String): Throwable = { new UnsupportedOperationException(s\"Initial type ${dataType.catalogString} must be a $target\") } def initialTypeNotTargetDataTypesError(dataType: DataType): Throwable = { new UnsupportedOperationException( s\"Initial type ${dataType.catalogString} must be \" + s\"an ${ArrayType.simpleString}, a ${StructType.simpleString} or a ${MapType.simpleString}\") } def cannotConvertColumnToJSONError(name: String, dataType: DataType): Throwable = { new UnsupportedOperationException( s\"Unable to convert column $name of type ${dataType.catalogString} to JSON.\") } def malformedRecordsDetectedInSchemaInferenceError(e: Throwable): Throwable = { new SparkException(\"Malformed records are detected in schema inference. \" + s\"Parse Mode: ${FailFastMode.name}.\", e) } def malformedJSONError(): Throwable = { new SparkException(\"Malformed JSON\") } def malformedRecordsDetectedInSchemaInferenceError(dataType: DataType): Throwable = { new SparkException( s\"\"\" |Malformed records are detected in schema inference. |Parse Mode: ${FailFastMode.name}. Reasons: Failed to infer a common schema. |Struct types are expected, but `${dataType.catalogString}` was found. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def cannotRewriteDomainJoinWithConditionsError( conditions: Seq[Expression], d: DomainJoin): Throwable = { new IllegalStateException( s\"Unable to rewrite domain join with conditions: $conditions\\n$d\") } def decorrelateInnerQueryThroughPlanUnsupportedError(plan: LogicalPlan): Throwable = { new UnsupportedOperationException( s\"Decorrelate inner query through ${plan.nodeName} is not supported.\") } def methodCalledInAnalyzerNotAllowedError(): Throwable = { new RuntimeException(\"This method should not be called in the analyzer\") } def cannotSafelyMergeSerdePropertiesError( props1: Map[String, String], props2: Map[String, String], conflictKeys: Set[String]): Throwable = { new UnsupportedOperationException( s\"\"\" |Cannot safely merge SERDEPROPERTIES: |${props1.map { case (k, v) => s\"$k=$v\" }.mkString(\"{\", \",\", \"}\")} |${props2.map { case (k, v) => s\"$k=$v\" }.mkString(\"{\", \",\", \"}\")} |The conflict keys: ${conflictKeys.mkString(\", \")} |\"\"\".stripMargin) } def pairUnsupportedAtFunctionError( r1: ValueInterval, r2: ValueInterval, function: String): Throwable = { new UnsupportedOperationException(s\"Not supported pair: $r1, $r2 at $function()\") } def onceStrategyIdempotenceIsBrokenForBatchError[TreeType <: TreeNode[_]]( batchName: String, plan: TreeType, reOptimized: TreeType): Throwable = { new RuntimeException( s\"\"\" |Once strategy's idempotence is broken for batch $batchName |${sideBySide(plan.treeString, reOptimized.treeString).mkString(\"\\n\")} \"\"\".stripMargin) } def structuralIntegrityOfInputPlanIsBrokenInClassError(className: String): Throwable = { new RuntimeException(\"The structural integrity of the input plan is broken in \" + s\"$className.\") } def structuralIntegrityIsBrokenAfterApplyingRuleError( ruleName: String, batchName: String): Throwable = { new RuntimeException(s\"After applying rule $ruleName in batch $batchName, \" + \"the structural integrity of the plan is broken.\") } def ruleIdNotFoundForRuleError(ruleName: String): Throwable = { new NoSuchElementException(s\"Rule id not found for $ruleName\") } def cannotCreateArrayWithElementsExceedLimitError( numElements: Long, additionalErrorMessage: String): Throwable = { new RuntimeException( s\"\"\" |Cannot create array with $numElements |elements of data due to exceeding the limit |${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH} elements for ArrayData. |$additionalErrorMessage \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def indexOutOfBoundsOfArrayDataError(idx: Int): Throwable = { new SparkIndexOutOfBoundsException( errorClass = \"INDEX_OUT_OF_BOUNDS\", Array(toSQLValue(idx, IntegerType))) } def malformedRecordsDetectedInRecordParsingError(e: BadRecordException): Throwable = { new SparkException(\"Malformed records are detected in record parsing. \" + s\"Parse Mode: ${FailFastMode.name}. To process malformed records as null \" + \"result, try setting the option 'mode' as 'PERMISSIVE'.\", e) } def remoteOperationsUnsupportedError(): Throwable = { new RuntimeException(\"Remote operations not supported\") } def invalidKerberosConfigForHiveServer2Error(): Throwable = { new IOException( \"HiveServer2 Kerberos principal or keytab is not correctly configured\") } def parentSparkUIToAttachTabNotFoundError(): Throwable = { new SparkException(\"Parent SparkUI to attach this tab to not found!\") } def inferSchemaUnsupportedForHiveError(): Throwable = { new UnsupportedOperationException(\"inferSchema is not supported for hive data source.\") } def requestedPartitionsMismatchTablePartitionsError( table: CatalogTable, partition: Map[String, Option[String]]): Throwable = { new SparkException( s\"\"\" |Requested partitioning does not match the ${table.identifier.table} table: |Requested partitions: ${partition.keys.mkString(\",\")} |Table partitions: ${table.partitionColumnNames.mkString(\",\")} \"\"\".stripMargin) } def dynamicPartitionKeyNotAmongWrittenPartitionPathsError(key: String): Throwable = { new SparkException( s\"Dynamic partition key ${toSQLValue(key, StringType)} is not among written partition paths.\") } def cannotRemovePartitionDirError(partitionPath: Path): Throwable = { new RuntimeException(s\"Cannot remove partition directory '$partitionPath'\") } def cannotCreateStagingDirError(message: String, e: IOException): Throwable = { new RuntimeException(s\"Cannot create staging directory: $message\", e) } def serDeInterfaceNotFoundError(e: NoClassDefFoundError): Throwable = { new ClassNotFoundException(\"The SerDe interface removed since Hive 2.3(HIVE-15167).\" + \" Please migrate your custom SerDes to Hive 2.3. See HIVE-15167 for more details.\", e) } def convertHiveTableToCatalogTableError( e: SparkException, dbName: String, tableName: String): Throwable = { new SparkException(s\"${e.getMessage}, db: $dbName, table: $tableName\", e) } def cannotRecognizeHiveTypeError( e: ParseException, fieldType: String, fieldName: String): Throwable = { new SparkException( s\"Cannot recognize hive type string: $fieldType, column: $fieldName\", e) } def getTablesByTypeUnsupportedByHiveVersionError(): Throwable = { new UnsupportedOperationException(\"Hive 2.2 and lower versions don't support \" + \"getTablesByType. Please use Hive 2.3 or higher version.\") } def dropTableWithPurgeUnsupportedError(): Throwable = { new UnsupportedOperationException(\"DROP TABLE ... PURGE\") } def alterTableWithDropPartitionAndPurgeUnsupportedError(): Throwable = { new UnsupportedOperationException(\"ALTER TABLE ... DROP PARTITION ... PURGE\") } def invalidPartitionFilterError(): Throwable = { new UnsupportedOperationException( \"\"\"Partition filter cannot have both `\"` and `'` characters\"\"\") } def getPartitionMetadataByFilterError(e: InvocationTargetException): Throwable = { new RuntimeException( s\"\"\" |Caught Hive MetaException attempting to get partition metadata by filter |from Hive. You can set the Spark configuration setting |${SQLConf.HIVE_METASTORE_PARTITION_PRUNING_FALLBACK_ON_EXCEPTION.key} to true to work |around this problem, however this will result in degraded performance. Please |report a bug: https://issues.apache.org/jira/browse/SPARK \"\"\".stripMargin.replaceAll(\"\\n\", \" \"), e) } def unsupportedHiveMetastoreVersionError(version: String, key: String): Throwable = { new UnsupportedOperationException(s\"Unsupported Hive Metastore version ($version). \" + s\"Please set $key with a valid version.\") } def loadHiveClientCausesNoClassDefFoundError( cnf: NoClassDefFoundError, execJars: Seq[URL], key: String, e: InvocationTargetException): Throwable = { new ClassNotFoundException( s\"\"\" |$cnf when creating Hive client using classpath: ${execJars.mkString(\", \")}\\n |Please make sure that jars for your version of hive and hadoop are included in the |paths passed to $key. \"\"\".stripMargin.replaceAll(\"\\n\", \" \"), e) } def cannotFetchTablesOfDatabaseError(dbName: String, e: Exception): Throwable = { new SparkException(s\"Unable to fetch tables of db $dbName\", e) } def illegalLocationClauseForViewPartitionError(): Throwable = { new SparkException(\"LOCATION clause illegal for view partition\") } def renamePathAsExistsPathError(srcPath: Path, dstPath: Path): Throwable = { new SparkFileAlreadyExistsException(errorClass = \"FAILED_RENAME_PATH\", Array(srcPath.toString, dstPath.toString)) } def renameAsExistsPathError(dstPath: Path): Throwable = { new FileAlreadyExistsException(s\"Failed to rename as $dstPath already exists\") } def renameSrcPathNotFoundError(srcPath: Path): Throwable = { new SparkFileNotFoundException(errorClass = \"RENAME_SRC_PATH_NOT_FOUND\", Array(srcPath.toString)) } def failedRenameTempFileError(srcPath: Path, dstPath: Path): Throwable = { new IOException(s\"Failed to rename temp file $srcPath to $dstPath as rename returned false\") } def legacyMetadataPathExistsError(metadataPath: Path, legacyMetadataPath: Path): Throwable = { new SparkException( s\"\"\" |Error: we detected a possible problem with the location of your \"_spark_metadata\" |directory and you likely need to move it before restarting this query. | |Earlier version of Spark incorrectly escaped paths when writing out the |\"_spark_metadata\" directory for structured streaming. While this was corrected in |Spark 3.0, it appears that your query was started using an earlier version that |incorrectly handled the \"_spark_metadata\" path. | |Correct \"_spark_metadata\" Directory: $metadataPath |Incorrect \"_spark_metadata\" Directory: $legacyMetadataPath | |Please move the data from the incorrect directory to the correct one, delete the |incorrect directory, and then restart this query. If you believe you are receiving |this message in error, you can disable it with the SQL conf |${SQLConf.STREAMING_CHECKPOINT_ESCAPED_PATH_CHECK_ENABLED.key}. \"\"\".stripMargin) } def partitionColumnNotFoundInSchemaError(col: String, schema: StructType): Throwable = { new RuntimeException(s\"Partition column $col not found in schema $schema\") } def stateNotDefinedOrAlreadyRemovedError(): Throwable = { new NoSuchElementException(\"State is either not defined or has already been removed\") } def cannotSetTimeoutDurationError(): Throwable = { new UnsupportedOperationException( \"Cannot set timeout duration without enabling processing time timeout in \" + \"[map|flatMap]GroupsWithState\") } def cannotGetEventTimeWatermarkError(): Throwable = { new UnsupportedOperationException( \"Cannot get event time watermark timestamp without setting watermark before \" + \"[map|flatMap]GroupsWithState\") } def cannotSetTimeoutTimestampError(): Throwable = { new UnsupportedOperationException( \"Cannot set timeout timestamp without enabling event time timeout in \" + \"[map|flatMapGroupsWithState\") } def batchMetadataFileNotFoundError(batchMetadataFile: Path): Throwable = { new FileNotFoundException(s\"Unable to find batch $batchMetadataFile\") } def multiStreamingQueriesUsingPathConcurrentlyError( path: String, e: FileAlreadyExistsException): Throwable = { new ConcurrentModificationException( s\"Multiple streaming queries are concurrently using $path\", e) } def addFilesWithAbsolutePathUnsupportedError(commitProtocol: String): Throwable = { new UnsupportedOperationException( s\"$commitProtocol does not support adding files with an absolute path\") } def microBatchUnsupportedByDataSourceError(srcName: String): Throwable = { new UnsupportedOperationException( s\"Data source $srcName does not support microbatch processing.\") } def cannotExecuteStreamingRelationExecError(): Throwable = { new UnsupportedOperationException(\"StreamingRelationExec cannot be executed\") } def invalidStreamingOutputModeError(outputMode: Option[OutputMode]): Throwable = { new UnsupportedOperationException(s\"Invalid output mode: $outputMode\") } def catalogPluginClassNotFoundError(name: String): Throwable = { new CatalogNotFoundException( s\"Catalog '$name' plugin class not found: spark.sql.catalog.$name is not defined\") } def catalogPluginClassNotImplementedError(name: String, pluginClassName: String): Throwable = { new SparkException( s\"Plugin class for catalog '$name' does not implement CatalogPlugin: $pluginClassName\") } def catalogPluginClassNotFoundForCatalogError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException(s\"Cannot find catalog plugin class for catalog '$name': $pluginClassName\", e) } def catalogFailToFindPublicNoArgConstructorError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException( s\"Failed to find public no-arg constructor for catalog '$name': $pluginClassName)\", e) } def catalogFailToCallPublicNoArgConstructorError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException( s\"Failed to call public no-arg constructor for catalog '$name': $pluginClassName)\", e) } def cannotInstantiateAbstractCatalogPluginClassError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException(\"Cannot instantiate abstract catalog plugin class for \" + s\"catalog '$name': $pluginClassName\", e.getCause) } def failedToInstantiateConstructorForCatalogError( name: String, pluginClassName: String, e: Exception): Throwable = { new SparkException(\"Failed during instantiating constructor for catalog \" + s\"'$name': $pluginClassName\", e.getCause) } def noSuchElementExceptionError(): Throwable = { new NoSuchElementException } def noSuchElementExceptionError(key: String): Throwable = { new NoSuchElementException(key) } def cannotMutateReadOnlySQLConfError(): Throwable = { new UnsupportedOperationException(\"Cannot mutate ReadOnlySQLConf.\") } def cannotCloneOrCopyReadOnlySQLConfError(): Throwable = { new UnsupportedOperationException(\"Cannot clone/copy ReadOnlySQLConf.\") } def cannotGetSQLConfInSchedulerEventLoopThreadError(): Throwable = { new RuntimeException(\"Cannot get SQLConf inside scheduler event loop thread.\") } def unsupportedOperationExceptionError(): Throwable = { new UnsupportedOperationException } def nullLiteralsCannotBeCastedError(name: String): Throwable = { new UnsupportedOperationException(s\"null literals can't be casted to $name\") } def notUserDefinedTypeError(name: String, userClass: String): Throwable = { new SparkException(s\"$name is not an UserDefinedType. Please make sure registering \" + s\"an UserDefinedType for ${userClass}\") } def cannotLoadUserDefinedTypeError(name: String, userClass: String): Throwable = { new SparkException(s\"Can not load in UserDefinedType ${name} for user class ${userClass}.\") } def timeZoneIdNotSpecifiedForTimestampTypeError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_OPERATION\", messageParameters = Array( s\"${toSQLType(TimestampType)} must supply timeZoneId parameter \" + s\"while converting to the arrow timestamp type.\") ) } def notPublicClassError(name: String): Throwable = { new UnsupportedOperationException( s\"$name is not a public class. Only public classes are supported.\") } def primitiveTypesNotSupportedError(): Throwable = { new UnsupportedOperationException(\"Primitive types are not supported.\") } def fieldIndexOnRowWithoutSchemaError(): Throwable = { new UnsupportedOperationException(\"fieldIndex on a Row without schema is undefined.\") } def valueIsNullError(index: Int): Throwable = { new NullPointerException(s\"Value at index ${toSQLValue(index, IntegerType)} is null\") } def onlySupportDataSourcesProvidingFileFormatError(providingClass: String): Throwable = { new SparkException(s\"Only Data Sources providing FileFormat are supported: $providingClass\") } def failToSetOriginalPermissionBackError( permission: FsPermission, path: Path, e: Throwable): Throwable = { new SparkSecurityException(errorClass = \"RESET_PERMISSION_TO_ORIGINAL\", Array(permission.toString, path.toString, e.getMessage)) } def failToSetOriginalACLBackError(aclEntries: String, path: Path, e: Throwable): Throwable = { new SecurityException(s\"Failed to set original ACL $aclEntries back to \" + s\"the created path: $path. Exception: ${e.getMessage}\") } def multiFailuresInStageMaterializationError(error: Throwable): Throwable = { new SparkException(\"Multiple failures in stage materialization.\", error) } def unrecognizedCompressionSchemaTypeIDError(typeId: Int): Throwable = { new UnsupportedOperationException(s\"Unrecognized compression scheme type ID: $typeId\") } def getParentLoggerNotImplementedError(className: String): Throwable = { new SQLFeatureNotSupportedException(s\"$className.getParentLogger is not yet implemented.\") } def cannotCreateParquetConverterForTypeError(t: DecimalType, parquetType: String): Throwable = { new RuntimeException( s\"\"\" |Unable to create Parquet converter for ${t.typeName} |whose Parquet type is $parquetType without decimal metadata. Please read this |column/field as Spark BINARY type. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def cannotCreateParquetConverterForDecimalTypeError( t: DecimalType, parquetType: String): Throwable = { new RuntimeException( s\"\"\" |Unable to create Parquet converter for decimal type ${t.json} whose Parquet type is |$parquetType. Parquet DECIMAL type can only be backed by INT32, INT64, |FIXED_LEN_BYTE_ARRAY, or BINARY. \"\"\".stripMargin.replaceAll(\"\\n\", \" \")) } def cannotCreateParquetConverterForDataTypeError( t: DataType, parquetType: String): Throwable = { new RuntimeException(s\"Unable to create Parquet converter for data type ${t.json} \" + s\"whose Parquet type is $parquetType\") } def cannotAddMultiPartitionsOnNonatomicPartitionTableError(tableName: String): Throwable = { new UnsupportedOperationException( s\"Nonatomic partition table $tableName can not add multiple partitions.\") } def userSpecifiedSchemaUnsupportedByDataSourceError(provider: TableProvider): Throwable = { new UnsupportedOperationException( s\"${provider.getClass.getSimpleName} source does not support user-specified schema.\") } def cannotDropMultiPartitionsOnNonatomicPartitionTableError(tableName: String): Throwable = { new UnsupportedOperationException( s\"Nonatomic partition table $tableName can not drop multiple partitions.\") } def truncateMultiPartitionUnsupportedError(tableName: String): Throwable = { new UnsupportedOperationException( s\"The table $tableName does not support truncation of multiple partition.\") } def overwriteTableByUnsupportedExpressionError(table: Table): Throwable = { new SparkException(s\"Table does not support overwrite by expression: $table\") } def dynamicPartitionOverwriteUnsupportedByTableError(table: Table): Throwable = { new SparkException(s\"Table does not support dynamic partition overwrite: $table\") } def failedMergingSchemaError(schema: StructType, e: SparkException): Throwable = { new SparkException(s\"Failed merging schema:\\n${schema.treeString}\", e) } def cannotBroadcastTableOverMaxTableRowsError( maxBroadcastTableRows: Long, numRows: Long): Throwable = { new SparkException( s\"Cannot broadcast the table over $maxBroadcastTableRows rows: $numRows rows\") } def cannotBroadcastTableOverMaxTableBytesError( maxBroadcastTableBytes: Long, dataSize: Long): Throwable = { new SparkException(\"Cannot broadcast the table that is larger than\" + s\" ${maxBroadcastTableBytes >> 30}GB: ${dataSize >> 30} GB\") } def notEnoughMemoryToBuildAndBroadcastTableError(oe: OutOfMemoryError): Throwable = { new OutOfMemoryError(\"Not enough memory to build and broadcast the table to all \" + \"worker nodes. As a workaround, you can either disable broadcast by setting \" + s\"${SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key} to -1 or increase the spark \" + s\"driver memory by setting ${SparkLauncher.DRIVER_MEMORY} to a higher value.\") .initCause(oe.getCause) } def executeCodePathUnsupportedError(execName: String): Throwable = { new UnsupportedOperationException(s\"$execName does not support the execute() code path.\") } def cannotMergeClassWithOtherClassError(className: String, otherClass: String): Throwable = { new UnsupportedOperationException( s\"Cannot merge $className with $otherClass\") } def continuousProcessingUnsupportedByDataSourceError(sourceName: String): Throwable = { new UnsupportedOperationException( s\"Data source $sourceName does not support continuous processing.\") } def failedToReadDataError(failureReason: Throwable): Throwable = { new SparkException(\"Data read failed\", failureReason) } def failedToGenerateEpochMarkerError(failureReason: Throwable): Throwable = { new SparkException(\"Epoch marker generation failed\", failureReason) } def foreachWriterAbortedDueToTaskFailureError(): Throwable = { new SparkException(\"Foreach writer has been aborted due to a task failure\") } def integerOverflowError(message: String): Throwable = { new ArithmeticException(s\"Integer overflow. $message\") } def failedToReadDeltaFileError(fileToRead: Path, clazz: String, keySize: Int): Throwable = { new IOException( s\"Error reading delta file $fileToRead of $clazz: key size cannot be $keySize\") } def failedToReadSnapshotFileError(fileToRead: Path, clazz: String, message: String): Throwable = { new IOException(s\"Error reading snapshot file $fileToRead of $clazz: $message\") } def cannotPurgeAsBreakInternalStateError(): Throwable = { new UnsupportedOperationException(\"Cannot purge as it might break internal state.\") } def cleanUpSourceFilesUnsupportedError(): Throwable = { new UnsupportedOperationException(\"Clean up source files is not supported when\" + \" reading from the output directory of FileStreamSink.\") } def latestOffsetNotCalledError(): Throwable = { new UnsupportedOperationException( \"latestOffset(Offset, ReadLimit) should be called instead of this method\") } def legacyCheckpointDirectoryExistsError( checkpointPath: Path, legacyCheckpointDir: String): Throwable = { new SparkException( s\"\"\" |Error: we detected a possible problem with the location of your checkpoint and you |likely need to move it before restarting this query. | |Earlier version of Spark incorrectly escaped paths when writing out checkpoints for |structured streaming. While this was corrected in Spark 3.0, it appears that your |query was started using an earlier version that incorrectly handled the checkpoint |path. | |Correct Checkpoint Directory: $checkpointPath |Incorrect Checkpoint Directory: $legacyCheckpointDir | |Please move the data from the incorrect directory to the correct one, delete the |incorrect directory, and then restart this query. If you believe you are receiving |this message in error, you can disable it with the SQL conf |${SQLConf.STREAMING_CHECKPOINT_ESCAPED_PATH_CHECK_ENABLED.key}. \"\"\".stripMargin) } def subprocessExitedError( exitCode: Int, stderrBuffer: CircularBuffer, cause: Throwable): Throwable = { new SparkException(s\"Subprocess exited with status $exitCode. \" + s\"Error: ${stderrBuffer.toString}\", cause) } def outputDataTypeUnsupportedByNodeWithoutSerdeError( nodeName: String, dt: DataType): Throwable = { new SparkException(s\"$nodeName without serde does not support \" + s\"${dt.getClass.getSimpleName} as output data type\") } def invalidStartIndexError(numRows: Int, startIndex: Int): Throwable = { new ArrayIndexOutOfBoundsException( \"Invalid `startIndex` provided for generating iterator over the array. \" + s\"Total elements: $numRows, requested `startIndex`: $startIndex\") } def concurrentModificationOnExternalAppendOnlyUnsafeRowArrayError( className: String): Throwable = { new ConcurrentModificationException( s\"The backing $className has been modified since the creation of this Iterator\") } def doExecuteBroadcastNotImplementedError(nodeName: String): Throwable = { new UnsupportedOperationException(s\"$nodeName does not implement doExecuteBroadcast\") } def databaseNameConflictWithSystemPreservedDatabaseError(globalTempDB: String): Throwable = { new SparkException( s\"\"\" |$globalTempDB is a system preserved database, please rename your existing database |to resolve the name conflict, or set a different value for |${GLOBAL_TEMP_DATABASE.key}, and launch your Spark application again. \"\"\".stripMargin.split(\"\\n\").mkString(\" \")) } def commentOnTableUnsupportedError(): Throwable = { new SQLFeatureNotSupportedException(\"comment on table is not supported\") } def unsupportedUpdateColumnNullabilityError(): Throwable = { new SQLFeatureNotSupportedException(\"UpdateColumnNullability is not supported\") } def renameColumnUnsupportedForOlderMySQLError(): Throwable = { new SQLFeatureNotSupportedException( \"Rename column is only supported for MySQL version 8.0 and above.\") } def failedToExecuteQueryError(e: Throwable): QueryExecutionException = { val message = \"Hit an error when executing a query\" + (if (e.getMessage == null) \"\" else s\": ${e.getMessage}\") new QueryExecutionException(message, e) } def nestedFieldUnsupportedError(colName: String): Throwable = { new UnsupportedOperationException(s\"Nested field $colName is not supported.\") } def transformationsAndActionsNotInvokedByDriverError(): Throwable = { new SparkException( \"\"\" |Dataset transformations and actions can only be invoked by the driver, not inside of |other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() |* x) is invalid because the values transformation and count action cannot be |performed inside of the dataset1.map transformation. For more information, |see SPARK-28702. \"\"\".stripMargin.split(\"\\n\").mkString(\" \")) } def repeatedPivotsUnsupportedError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(s\"Repeated ${toSQLStmt(\"pivot\")}s.\")) } def pivotNotAfterGroupByUnsupportedError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array(s\"${toSQLStmt(\"pivot\")} not after a ${toSQLStmt(\"group by\")}.\")) } private val aesFuncName = toSQLId(\"aes_encrypt\") + \"/\" + toSQLId(\"aes_decrypt\") def invalidAesKeyLengthError(actualLength: Int): RuntimeException = { new SparkRuntimeException( errorClass = \"INVALID_PARAMETER_VALUE\", messageParameters = Array( \"key\", s\"the $aesFuncName function\", s\"expects a binary value with 16, 24 or 32 bytes, but got ${actualLength.toString} bytes.\")) } def aesModeUnsupportedError(mode: String, padding: String): RuntimeException = { new SparkRuntimeException( errorClass = \"UNSUPPORTED_FEATURE\", messageParameters = Array( s\"AES-$mode with the padding $padding by the $aesFuncName function.\")) } def aesCryptoError(detailMessage: String): RuntimeException = { new SparkRuntimeException( errorClass = \"INVALID_PARAMETER_VALUE\", messageParameters = Array( \"expr, key\", s\"the $aesFuncName function\", s\"Detail message: $detailMessage\")) } def hiveTableWithAnsiIntervalsError(tableName: String): Throwable = { new UnsupportedOperationException(s\"Hive table $tableName with ANSI intervals is not supported\") } def cannotConvertOrcTimestampToTimestampNTZError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_OPERATION\", messageParameters = Array( s\"Unable to convert ${toSQLType(TimestampType)} of Orc to \" + s\"data type ${toSQLType(TimestampNTZType)}.\")) } def cannotConvertOrcTimestampNTZToTimestampLTZError(): Throwable = { new SparkUnsupportedOperationException( errorClass = \"UNSUPPORTED_OPERATION\", messageParameters = Array( s\"Unable to convert ${toSQLType(TimestampNTZType)} of Orc to \" + s\"data type ${toSQLType(TimestampType)}.\")) } def writePartitionExceedConfigSizeWhenDynamicPartitionError( numWrittenParts: Int, maxDynamicPartitions: Int, maxDynamicPartitionsKey: String): Throwable = { new SparkException( s\"Number of dynamic partitions created is $numWrittenParts\" + s\", which is more than $maxDynamicPartitions\" + s\". To solve this try to set $maxDynamicPartitionsKey\" + s\" to at least $numWrittenParts.\") } def invalidNumberFormatError(input: UTF8String, format: String): Throwable = { new IllegalArgumentException( s\"The input string '$input' does not match the given number format: '$format'\") } def multipleBucketTransformsError(): Throwable = { new UnsupportedOperationException(\"Multiple bucket transforms are not supported.\") } def unsupportedCreateNamespaceCommentError(): Throwable = { new SQLFeatureNotSupportedException(\"Create namespace comment is not supported\") } def unsupportedRemoveNamespaceCommentError(): Throwable = { new SQLFeatureNotSupportedException(\"Remove namespace comment is not supported\") } def unsupportedDropNamespaceRestrictError(): Throwable = { new SQLFeatureNotSupportedException(\"Drop namespace restrict is not supported\") } def timestampAddOverflowError(micros: Long, amount: Int, unit: String): ArithmeticException = { new SparkArithmeticException( errorClass = \"DATETIME_OVERFLOW\", messageParameters = Array( s\"add ${toSQLValue(amount, IntegerType)} $unit to \" + s\"${toSQLValue(DateTimeUtils.microsToInstant(micros), TimestampType)}\")) } }",
            "## CLASS: org/apache/spark/rdd/RDD#\n abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) extends Serializable with Logging { if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) { // This is a warning instead of an exception in order to avoid breaking user programs that // might have defined nested RDDs without running jobs with them. logWarning(\"Spark does not support nested RDDs (see SPARK-5063)\") } private def sc: SparkContext = { if (_sc == null) { throw SparkCoreErrors.rddLacksSparkContextError() } _sc } /** Construct an RDD with just a one-to-one dependency on one parent  def this(@transient oneParent: RDD[_]) = this(oneParent.context, List(new OneToOneDependency(oneParent))) private[spark] def conf = sc.conf // ======================================================================= // Methods that should be implemented by subclasses of RDD // ======================================================================= /** * :: DeveloperApi :: * Implemented by subclasses to compute a given partition.  @DeveloperApi def compute(split: Partition, context: TaskContext): Iterator[T] /** * Implemented by subclasses to return the set of partitions in this RDD. This method will only * be called once, so it is safe to implement a time-consuming computation in it. * * The partitions in this array must satisfy the following property: * `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`  protected def getPartitions: Array[Partition] /** * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only * be called once, so it is safe to implement a time-consuming computation in it.  protected def getDependencies: Seq[Dependency[_]] = deps /** * Optionally overridden by subclasses to specify placement preferences.  protected def getPreferredLocations(split: Partition): Seq[String] = Nil /** Optionally overridden by subclasses to specify how they are partitioned.  @transient val partitioner: Option[Partitioner] = None // ======================================================================= // Methods and fields available on all RDDs // ======================================================================= /** The SparkContext that created this RDD.  def sparkContext: SparkContext = sc /** A unique ID for this RDD (within its SparkContext).  val id: Int = sc.newRddId() /** A friendly name for this RDD  @transient var name: String = _ /** Assign a name to this RDD  def setName(_name: String): this.type = { name = _name this } /** * Mark this RDD for persisting using the specified level. * * @param newLevel the target storage level * @param allowOverride whether to override any existing level with the new one  private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) { throw SparkCoreErrors.cannotChangeStorageLevelError() } // If this is the first time this RDD is marked for persisting, register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } /** * Set this RDD's storage level to persist its values across operations after the first time * it is computed. This can only be used to assign a new storage level if the RDD does not * have a storage level set yet. Local checkpointing is an exception.  def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { // This means the user previously called localCheckpoint(), which should have already // marked this RDD for persisting. Here we should override the old storage level with // one that is explicitly requested by the user (after adapting it to use disk). persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`).  def cache(): this.type = persist() /** * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk. * * @param blocking Whether to block until all blocks are deleted (default: false) * @return This RDD.  def unpersist(blocking: Boolean = false): this.type = { logInfo(s\"Removing RDD $id from persistence list\") sc.unpersistRDD(id, blocking) storageLevel = StorageLevel.NONE this } /** Get the RDD's current storage level, or StorageLevel.NONE if none is set.  def getStorageLevel: StorageLevel = storageLevel /** * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.). We do * not use `this` because RDDs are user-visible, so users might have added their own locking on * RDDs; sharing that could lead to a deadlock. * * One thread might hold the lock on many of these, for a chain of RDD dependencies; but * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no * chance of deadlock. * * Executors may reference the shared fields (though they should never mutate them, * that only happens on the driver).  private val stateLock = new Serializable {} // Our dependencies and partitions will be gotten by calling subclass's methods below, and will // be overwritten when we're checkpointed @volatile private var dependencies_ : Seq[Dependency[_]] = _ // When we overwrite the dependencies we keep a weak reference to the old dependencies // for user controlled cleanup. @volatile @transient private var legacyDependencies: WeakReference[Seq[Dependency[_]]] = _ @volatile @transient private var partitions_ : Array[Partition] = _ /** An Option holding our checkpoint RDD, if we are checkpointed  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD) /** * Get the list of dependencies of this RDD, taking into account whether the * RDD is checkpointed or not.  final def dependencies: Seq[Dependency[_]] = { checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse { if (dependencies_ == null) { stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } } } dependencies_ } } /** * Get the list of dependencies of this RDD ignoring checkpointing.  final private def internalDependencies: Option[Seq[Dependency[_]]] = { if (legacyDependencies != null) { legacyDependencies.get } else if (dependencies_ != null) { Some(dependencies_) } else { // This case should be infrequent. stateLock.synchronized { if (dependencies_ == null) { dependencies_ = getDependencies } Some(dependencies_) } } } /** * Get the array of partitions of this RDD, taking into account whether the * RDD is checkpointed or not.  final def partitions: Array[Partition] = { checkpointRDD.map(_.partitions).getOrElse { if (partitions_ == null) { stateLock.synchronized { if (partitions_ == null) { partitions_ = getPartitions partitions_.zipWithIndex.foreach { case (partition, index) => require(partition.index == index, s\"partitions($index).partition == ${partition.index}, but it should equal $index\") } } } } partitions_ } } /** * Returns the number of partitions of this RDD.  @Since(\"1.6.0\") final def getNumPartitions: Int = partitions.length /** * Get the preferred locations of a partition, taking into account whether the * RDD is checkpointed.  final def preferredLocations(split: Partition): Seq[String] = { checkpointRDD.map(_.getPreferredLocations(split)).getOrElse { getPreferredLocations(split) } } /** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but is available for implementers of custom * subclasses of RDD.  final def iterator(split: Partition, context: TaskContext): Iterator[T] = { if (storageLevel != StorageLevel.NONE) { getOrCompute(split, context) } else { computeOrReadCheckpoint(split, context) } } /** * Return the ancestors of the given RDD that are related to it only through a sequence of * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains * no ordering on the RDDs returned.  private[spark] def getNarrowAncestors: Seq[RDD[_]] = { val ancestors = new mutable.HashSet[RDD[_]] def visit(rdd: RDD[_]): Unit = { val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]]) val narrowParents = narrowDependencies.map(_.rdd) val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains) narrowParentsNotVisited.foreach { parent => ancestors.add(parent) visit(parent) } } visit(this) // In case there is a cycle, do not include the root itself ancestors.filterNot(_ == this).toSeq } /** * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] = { if (isCheckpointedAndMaterialized) { firstParent[T].iterator(split, context) } else { compute(split, context) } } /** * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached.  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = { val blockId = RDDBlockId(id, partition.index) var readCachedBlock = true // This method is called on executors, so we need call SparkEnv.get instead of sc.env. SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => { readCachedBlock = false computeOrReadCheckpoint(partition, context) }) match { // Block hit. case Left(blockResult) => if (readCachedBlock) { val existingMetrics = context.taskMetrics().inputMetrics existingMetrics.incBytesRead(blockResult.bytes) new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) { override def next(): T = { existingMetrics.incRecordsRead(1) delegate.next() } } } else { new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]]) } // Need to compute the block. case Right(iter) => new InterruptibleIterator(context, iter) } } /** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}. * * Note: Return statements are NOT allowed in the given body.  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body) // Transformations (return a new RDD) /** * Return a new RDD by applying a function to all elements of this RDD.  def map[U: ClassTag](f: T => U): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.map(cleanF)) } /** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results.  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (_, _, iter) => iter.flatMap(cleanF)) } /** * Return a new RDD containing only the elements that satisfy a predicate.  def filter(f: T => Boolean): RDD[T] = withScope { val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (_, _, iter) => iter.filter(cleanF), preservesPartitioning = true) } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { def removeDuplicatesInPartition(partition: Iterator[T]): Iterator[T] = { // Create an instance of external append only map which ignores values. val map = new ExternalAppendOnlyMap[T, Null, Null]( createCombiner = _ => null, mergeValue = (a, b) => a, mergeCombiners = (a, b) => a) map.insertAll(partition.map(_ -> null)) map.iterator.map(_._1) } partitioner match { case Some(_) if numPartitions == partitions.length => mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true) case _ => map(x => (x, null)).reduceByKey((x, _) => x, numPartitions).map(_._1) } } /** * Return a new RDD containing the distinct elements in this RDD.  def distinct(): RDD[T] = withScope { distinct(partitions.length) } /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle.  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope { coalesce(numPartitions, shuffle = true) } /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. If a larger number * of partitions is requested, it will stay at the current number of partitions. * * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * @note With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. The optional partition coalescer * passed in must be serializable.  def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null) : RDD[T] = withScope { require(numPartitions > 0, s\"Number of partitions ($numPartitions) must be positive.\") if (shuffle) { /** Distributes elements evenly across output partitions, starting from a random partition.  val distributePartition = (index: Int, items: Iterator[T]) => { var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions) items.map { t => // Note that the hash code of the key will just be the key itself. The HashPartitioner // will mod it with the number of total partitions. position = position + 1 (position, t) } } : Iterator[(Int, T)] // include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T]( mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true), new HashPartitioner(numPartitions)), numPartitions, partitionCoalescer).values } else { new CoalescedRDD(this, numPartitions, partitionCoalescer) } } /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]].  def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = { require(fraction >= 0, s\"Fraction must be nonnegative, but got ${fraction}\") withScope { require(fraction >= 0.0, \"Negative fraction value: \" + fraction) if (withReplacement) { new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) } else { new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) } } } /** * Randomly splits this RDD with the provided weights. * * @param weights weights for splits, will be normalized if they don't sum to 1 * @param seed random seed * * @return split RDDs in an array  def randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] = { require(weights.forall(_ >= 0), s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\") require(weights.sum > 0, s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\") withScope { val sum = weights.sum val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _) normalizedCumWeights.sliding(2).map { x => randomSampleWithRange(x(0), x(1), seed) }.toArray } } /** * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability * range. * @param lb lower bound to use for the Bernoulli sampler * @param ub upper bound to use for the Bernoulli sampler * @param seed the seed for the Random number generator * @return A random sub-sample of the RDD without replacement.  private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = { this.mapPartitionsWithIndex( { (index, partition) => val sampler = new BernoulliCellSampler[T](lb, ub) sampler.setSeed(seed + index) sampler.sample(partition) }, isOrderSensitive = true, preservesPartitioning = true) } /** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope { val numStDev = 10.0 require(num >= 0, \"Negative number of elements requested\") require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), \"Cannot support a sample size > Int.MaxValue - \" + s\"$numStDev * math.sqrt(Int.MaxValue)\") if (num == 0) { new Array[T](0) } else { val initialCount = this.count() if (initialCount == 0) { new Array[T](0) } else { val rand = new Random(seed) if (!withReplacement && num >= initialCount) { Utils.randomizeInPlace(this.collect(), rand) } else { val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length < num) { logWarning(s\"Needed to re-sample due to insufficient sample size. Repeat #$numIters\") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 } Utils.randomizeInPlace(samples, rand).take(num) } } } } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def union(other: RDD[T]): RDD[T] = withScope { sc.union(this, other) } /** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them).  def ++(other: RDD[T]): RDD[T] = withScope { this.union(other) } /** * Return this RDD sorted by the given key function.  def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length) (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope { this.keyBy[K](f) .sortByKey(ascending, numPartitions) .values } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally.  def intersection(other: RDD[T]): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null))) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. * * @note This method performs a shuffle internally. * * @param partitioner Partitioner to use for the resulting RDD  def intersection( other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner) .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty } .keys } /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * elements, even if the input RDDs did. Performs a hash partition across the cluster * * @note This method performs a shuffle internally. * * @param numPartitions How many partitions to use in the resulting RDD  def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope { intersection(other, new HashPartitioner(numPartitions)) } /** * Return an RDD created by coalescing all elements within each partition into an array.  def glom(): RDD[Array[T]] = withScope { new MapPartitionsRDD[Array[T], T](this, (_, _, iter) => Iterator(iter.toArray)) } /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`.  def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { new CartesianRDD(sc, this, other) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy[K](f, defaultPartitioner(this)) } /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K]( f: T => K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope { groupBy(f, new HashPartitioner(numPartitions)) } /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance.  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope { val cleanF = sc.clean(f) this.map(t => (cleanF(t), t)).groupByKey(p) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command)) } /** * Return an RDD created by piping elements to a forked external process.  def pipe(command: String, env: Map[String, String]): RDD[String] = withScope { // Similar to Runtime.exec(), if we are given a single string, split it into words // using a standard StringTokenizer (i.e. by spaces) pipe(PipedRDD.tokenize(command), env) } /** * Return an RDD created by piping elements to a forked external process. The resulting RDD * is computed by executing the given process once per partition. All elements * of each input partition are written to a process's stdin as lines of input separated * by a newline. The resulting partition consists of the process's stdout output, with * each line of stdout resulting in one element of the output partition. A process is invoked * even for empty partitions. * * The print behavior can be customized by providing two functions. * * @param command command to run in forked process. * @param env environment variables to set. * @param printPipeContext Before piping elements, this function is called as an opportunity * to pipe context data. Print line function (like out.println) will be * passed as printPipeContext's parameter. * @param printRDDElement Use this function to customize how to pipe elements. This function * will be called with each RDD element as the 1st parameter, and the * print line function (like out.println()) as the 2nd parameter. * An example of pipe the RDD data of groupBy() in a streaming way, * instead of constructing a huge String to concat all the elements: * {{{ * def printRDDElement(record:(String, Seq[String]), f:String=>Unit) = * for (e <- record._2) {f(e)} * }}} * @param separateWorkingDir Use separate working directories for each task. * @param bufferSize Buffer size for the stdin writer for the piped process. * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with * the piped process * @return the result RDD  def pipe( command: Seq[String], env: Map[String, String] = Map(), printPipeContext: (String => Unit) => Unit = null, printRDDElement: (T, String => Unit) => Unit = null, separateWorkingDir: Boolean = false, bufferSize: Int = 8192, encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope { new PipedRDD(this, command, env, if (printPipeContext ne null) sc.clean(printPipeContext) else null, if (printRDDElement ne null) sc.clean(printRDDElement) else null, separateWorkingDir, bufferSize, encoding) } /** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => cleanedF(iter), preservesPartitioning) } /** * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning. * It is a performance API to be used carefully only if we are sure that the RDD elements are * serializable and don't require closure cleaning. * * @param preservesPartitioning indicates whether the input function preserves the partitioner, * which should be `false` unless this is a pair RDD and the input * function doesn't modify the keys. * @param isOrderSensitive whether or not the function is order-sensitive. If it's order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndexInternal[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false, isOrderSensitive: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter), preservesPartitioning = preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * [performance] Spark's internal mapPartitions method that skips closure cleaning.  private[spark] def mapPartitionsInternal[U: ClassTag]( f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { new MapPartitionsRDD( this, (_: TaskContext, _: Int, iter: Iterator[T]) => f(iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.  def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning) } /** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. * * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order * sensitive, it may return totally different result when the input order * is changed. Mostly stateful functions are order-sensitive.  private[spark] def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) => Iterator[U], preservesPartitioning: Boolean, isOrderSensitive: Boolean): RDD[U] = withScope { val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter), preservesPartitioning, isOrderSensitive = isOrderSensitive) } /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD, * second element in each RDD, etc. Assumes that the two RDDs have the *same number of * partitions* and the *same number of elements in each partition* (e.g. one was made through * a map on the other).  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope { zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) => new Iterator[(T, U)] { def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match { case (true, true) => true case (false, false) => false case _ => throw SparkCoreErrors.canOnlyZipRDDsWithSamePartitionSizeError() } def next(): (T, U) = (thisIter.next(), otherIter.next()) } } } /** * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by * applying a function to the zipped partitions. Assumes that all the RDDs have the * *same number of partitions*, but does *not* require them to have the same number * of elements in each partition.  def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning) } def zipPartitions[B: ClassTag, V: ClassTag] (rdd2: RDD[B]) (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C]) (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning) } def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag] (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D]) (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope { zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f) } // Actions (launch a job to return a value to the user program) /** * Applies a function f to all elements of this RDD.  def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF)) } /** * Applies a function f to each partition of this RDD.  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope { val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) => cleanF(iter)) } /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory.  def collect(): Array[T] = withScope { val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray) Array.concat(results: _*) } /** * Return an iterator that contains all of the elements in this RDD. * * The iterator will consume as much memory as the largest partition in this RDD. * * @note This results in multiple Spark jobs, and if the input RDD is the result * of a wide transformation (e.g. join with different partitioners), to avoid * recomputing the input RDD should be cached first.  def toLocalIterator: Iterator[T] = withScope { def collectPartition(p: Int): Array[T] = { sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head } partitions.indices.iterator.flatMap(i => collectPartition(i)) } /** * Return an RDD that contains all matching values by applying `f`.  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope { val cleanF = sc.clean(f) filter(cleanF.isDefinedAt).map(cleanF) } /** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &lt;= us.  def subtract(other: RDD[T]): RDD[T] = withScope { subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope { subtract(other, new HashPartitioner(numPartitions)) } /** * Return an RDD with the elements from `this` that are not in `other`.  def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope { if (partitioner == Some(p)) { // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() { override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) } // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys } else { this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys } } /** * Reduces the elements of this RDD using the specified commutative and * associative binary operator.  def reduce(f: (T, T) => T): T = withScope { val cleanF = sc.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } var jobResult: Option[T] = None val mergeResult = (_: Int, taskResult: Option[T]) => { if (taskResult.isDefined) { jobResult = jobResult match { case Some(value) => Some(f(value, taskResult.get)) case None => taskResult } } } sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Reduces the elements of this RDD in a multi-level tree pattern. * * @param depth suggested depth of the tree (default: 2) * @see [[org.apache.spark.rdd.RDD#reduce]]  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") val cleanF = context.clean(f) val reducePartition: Iterator[T] => Option[T] = iter => { if (iter.hasNext) { Some(iter.reduceLeft(cleanF)) } else { None } } val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it))) val op: (Option[T], Option[T]) => Option[T] = (c, x) => { if (c.isDefined && x.isDefined) { Some(cleanF(c.get, x.get)) } else if (c.isDefined) { c } else if (x.isDefined) { x } else { None } } partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth) .getOrElse(throw SparkCoreErrors.emptyCollectionError()) } /** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral \"zero value\". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions  def fold(zeroValue: T)(op: (T, T) => T): T = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp) val mergeResult = (_: Int, taskResult: T) => jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult } /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral \"zero value\". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope { // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (_: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult } /** * Aggregates the elements of this RDD in a multi-level tree pattern. * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]]. * * @param depth suggested depth of the tree (default: 2)  def treeAggregate[U: ClassTag](zeroValue: U)( seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2): U = withScope { treeAggregate(zeroValue, seqOp, combOp, depth, finalAggregateOnExecutor = false) } /** * [[org.apache.spark.rdd.RDD#treeAggregate]] with a parameter to do the final * aggregation on the executor * * @param finalAggregateOnExecutor do final aggregation on executor  def treeAggregate[U: ClassTag]( zeroValue: U, seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int, finalAggregateOnExecutor: Boolean): U = withScope { require(depth >= 1, s\"Depth must be greater than or equal to 1 but got $depth.\") if (partitions.length == 0) { Utils.clone(zeroValue, context.env.closureSerializer.newInstance()) } else { val cleanSeqOp = context.clean(seqOp) val cleanCombOp = context.clean(combOp) val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it))) var numPartitions = partiallyAggregated.partitions.length val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2) // If creating an extra level doesn't help reduce // the wall-clock time, we stop tree aggregation. // Don't trigger TreeAggregation when it doesn't save wall-clock time while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) { numPartitions /= scale val curNumPartitions = numPartitions partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex { (i, iter) => iter.map((i % curNumPartitions, _)) }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values } if (finalAggregateOnExecutor && partiallyAggregated.partitions.length > 1) { // define a new partitioner that results in only 1 partition val constantPartitioner = new Partitioner { override def numPartitions: Int = 1 override def getPartition(key: Any): Int = 0 } // map the partially aggregated rdd into a key-value rdd // do the computation in the single executor with one partition // get the new RDD[U] partiallyAggregated = partiallyAggregated .map(v => (0.toByte, v)) .foldByKey(zeroValue, constantPartitioner)(cleanCombOp) .values } val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) partiallyAggregated.fold(copiedZeroValue)(cleanCombOp) } } /** * Return the number of elements in the RDD.  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum /** * Approximate version of count() that returns a potentially incomplete result * within a timeout, even if not all tasks have finished. * * The confidence is the probability that the error bounds of the result will * contain the true value. That is, if countApprox were called repeatedly * with confidence 0.9, we would expect 90% of the results to contain the * true count. The confidence must be in the range [0,1] or an exception will * be thrown. * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countApprox( timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") val countElements: (TaskContext, Iterator[T]) => Long = { (_, iter) => var result = 0L while (iter.hasNext) { result += 1L iter.next() } result } val evaluator = new CountEvaluator(partitions.length, confidence) sc.runApproximateJob(this, countElements, evaluator, timeout) } /** * Return the count of each unique value in this RDD as a local map of (value, count) pairs. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using * * {{{ * rdd.map(x => (x, 1L)).reduceByKey(_ + _) * }}} * * , which returns an RDD[T, Long] instead of a map.  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope { map(value => (value, null)).countByKey() } /** * Approximate version of countByValue(). * * @param timeout maximum time to wait for the job, in milliseconds * @param confidence the desired statistical confidence in the result * @return a potentially incomplete result, with error bounds  def countByValueApprox(timeout: Long, confidence: Double = 0.95) (implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]] = withScope { require(0.0 <= confidence && confidence <= 1.0, s\"confidence ($confidence) must be in [0,1]\") if (elementClassTag.runtimeClass.isArray) { throw SparkCoreErrors.countByValueApproxNotSupportArraysError() } val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (_, iter) => val map = new OpenHashMap[T, Long] iter.foreach { t => map.changeValue(t, 1L, _ + 1L) } map } val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence) sc.runApproximateJob(this, countPartition, evaluator, timeout) } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater * than `p`) would trigger sparse representation of registers, which may reduce the memory * consumption and increase accuracy when the cardinality is small. * * @param p The precision value for the normal set. * `p` must be a value between 4 and `sp` if `sp` is not zero (32 max). * @param sp The precision value for the sparse set, between 0 and 32. * If `sp` equals 0, the sparse representation is skipped.  def countApproxDistinct(p: Int, sp: Int): Long = withScope { require(p >= 4, s\"p ($p) must be >= 4\") require(sp <= 32, s\"sp ($sp) must be <= 32\") require(sp == 0 || p <= sp, s\"p ($p) cannot be greater than sp ($sp)\") val zeroCounter = new HyperLogLogPlus(p, sp) aggregate(zeroCounter)( (hll: HyperLogLogPlus, v: T) => { hll.offer(v) hll }, (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => { h1.addAll(h2) h1 }).cardinality() } /** * Return approximate number of distinct elements in the RDD. * * The algorithm used is based on streamlib's implementation of \"HyperLogLog in Practice: * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm\", available * <a href=\"https://doi.org/10.1145/2452376.2452456\">here</a>. * * @param relativeSD Relative accuracy. Smaller values create counters that require more space. * It must be greater than 0.000017.  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope { require(relativeSD > 0.000017, s\"accuracy ($relativeSD) must be greater than 0.000017\") val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt countApproxDistinct(if (p < 4) 4 else p, 0) } /** * Zips this RDD with its element indices. The ordering is first based on the partition index * and then the ordering of items within each partition. So the first item in the first * partition gets index 0, and the last item in the last partition receives the largest index. * * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type. * This method needs to trigger a spark job when this RDD contains more than one partitions. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The index assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithIndex(): RDD[(T, Long)] = withScope { new ZippedWithIndexRDD(this) } /** * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k, * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]]. * * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of * elements in a partition. The unique ID assigned to each element is therefore not guaranteed, * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.  def zipWithUniqueId(): RDD[(T, Long)] = withScope { val n = this.partitions.length.toLong this.mapPartitionsWithIndex { case (k, iter) => Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) => (item, i * n + k) } } } /** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`.  def take(num: Int): Array[T] = withScope { val scaleUpFactor = Math.max(conf.get(RDD_LIMIT_SCALE_UP_FACTOR), 2) if (num == 0) { new Array[T](0) } else { val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size < num && partsScanned < totalParts) { // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned > 0) { // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) { numPartsToTry = partsScanned * scaleUpFactor } else { // As left > 0, numPartsToTry is always >= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) } } val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size } buf.toArray } } /** * Return the first element in this RDD.  def first(): T = withScope { take(1) match { case Array(t) => t case _ => throw SparkCoreErrors.emptyCollectionError() } } /** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { takeOrdered(num)(ord.reverse) } /** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * {{{ * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * }}} * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope { if (num == 0) { Array.empty } else { val mapRDDs = mapPartitions { items => // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) } if (mapRDDs.partitions.length == 0) { Array.empty } else { mapRDDs.reduce { (queue1, queue2) => queue1 ++= queue2 queue1 }.toArray.sorted(ord) } } } /** * Returns the max of this RDD as defined by the implicit Ordering[T]. * @return the maximum element of the RDD *  def max()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.max) } /** * Returns the min of this RDD as defined by the implicit Ordering[T]. * @return the minimum element of the RDD *  def min()(implicit ord: Ordering[T]): T = withScope { this.reduce(ord.min) } /** * @note Due to complications in the internal implementation, this method will raise an * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`. * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.) * @return true if and only if the RDD contains no elements at all. Note that an RDD * may be empty even when it has at least 1 partition.  def isEmpty(): Boolean = withScope { partitions.length == 0 || take(1).length == 0 } /** * Save this RDD as a text file, using string representations of elements.  def saveAsTextFile(path: String): Unit = withScope { saveAsTextFile(path, null) } /** * Save this RDD as a compressed text file, using string representations of elements.  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope { this.mapPartitions { iter => val text = new Text() iter.map { x => require(x != null, \"text files do not allow null rows\") text.set(x.toString) (NullWritable.get(), text) } }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec) } /** * Save this RDD as a SequenceFile of serialized objects.  def saveAsObjectFile(path: String): Unit = withScope { this.mapPartitions(iter => iter.grouped(10).map(_.toArray)) .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x)))) .saveAsSequenceFile(path) } /** * Creates tuples of the elements in this RDD by applying `f`.  def keyBy[K](f: T => K): RDD[(K, T)] = withScope { val cleanedF = sc.clean(f) map(x => (cleanedF(x), x)) } /** A private method for tests, to look at the contents of each partition  private[spark] def collectPartitions(): Array[Array[T]] = withScope { sc.runJob(this, (iter: Iterator[T]) => iter.toArray) } /** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation.  def checkpoint(): Unit = RDDCheckpointData.synchronized { // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) { throw SparkCoreErrors.checkpointDirectoryHasNotBeenSetInSparkContextError() } else if (checkpointData.isEmpty) { checkpointData = Some(new ReliableRDDCheckpointData(this)) } } /** * Mark this RDD for local checkpointing using Spark's existing caching layer. * * This method is for users who wish to truncate RDD lineages while skipping the expensive * step of replicating the materialized data in a reliable distributed file system. This is * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX). * * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed * data is written to ephemeral local storage in the executors instead of to a reliable, * fault-tolerant storage. The effect is that if an executor fails during the computation, * the checkpointed data may no longer be accessible, causing an irrecoverable job failure. * * This is NOT safe to use with dynamic allocation, which removes executors along * with their cached blocks. If you must use both features, you are advised to set * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value. * * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used.  def localCheckpoint(): this.type = RDDCheckpointData.synchronized { if (conf.get(DYN_ALLOCATION_ENABLED) && conf.contains(DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT)) { logWarning(\"Local checkpointing is NOT safe to use with dynamic allocation, \" + \"which removes executors along with their cached blocks. If you must use both \" + \"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` \" + \"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to \" + \"at least 1 hour.\") } // Note: At this point we do not actually know whether the user will call persist() on // this RDD later, so we must explicitly call it here ourselves to ensure the cached // blocks are registered for cleanup later in the SparkContext. // // If, however, the user has already called persist() on this RDD, then we must adapt // the storage level he/she specified to one that is appropriate for local checkpointing // (i.e. uses disk) to guarantee correctness. if (storageLevel == StorageLevel.NONE) { persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL) } else { persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true) } // If this RDD is already checkpointed and materialized, its lineage is already truncated. // We must not override our `checkpointData` in this case because it is needed to recover // the checkpointed data. If it is overridden, next time materializing on this RDD will // cause error. if (isCheckpointedAndMaterialized) { logWarning(\"Not marking RDD for local checkpoint because it was already \" + \"checkpointed and materialized\") } else { // Lineage is not truncated yet, so just override any existing checkpoint data with ours checkpointData match { case Some(_: ReliableRDDCheckpointData[_]) => logWarning( \"RDD was already marked for reliable checkpointing: overriding with local checkpoint.\") case _ => } checkpointData = Some(new LocalRDDCheckpointData(this)) } this } /** * Return whether this RDD is checkpointed and materialized, either reliably or locally.  def isCheckpointed: Boolean = isCheckpointedAndMaterialized /** * Return whether this RDD is checkpointed and materialized, either reliably or locally. * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the * return value. Exposed for testing.  private[spark] def isCheckpointedAndMaterialized: Boolean = checkpointData.exists(_.isCheckpointed) /** * Return whether this RDD is marked for local checkpointing. * Exposed for testing.  private[rdd] def isLocallyCheckpointed: Boolean = { checkpointData match { case Some(_: LocalRDDCheckpointData[T]) => true case _ => false } } /** * Return whether this RDD is reliably checkpointed and materialized.  private[rdd] def isReliablyCheckpointed: Boolean = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true case _ => false } } /** * Gets the name of the directory to which this RDD was checkpointed. * This is not defined if the RDD is checkpointed locally.  def getCheckpointFile: Option[String] = { checkpointData match { case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir case _ => None } } /** * Removes an RDD's shuffles and it's non-persisted ancestors. * When running without a shuffle service, cleaning up shuffle files enables downscaling. * If you use the RDD after this call, you should checkpoint and materialize it first. * If you are uncertain of what you are doing, please do not use this feature. * Additional techniques for mitigating orphaned shuffle files: * * Tuning the driver GC to be more aggressive, so the regular context cleaner is triggered * * Setting an appropriate TTL for shuffle files to be auto cleaned  @DeveloperApi @Since(\"3.1.0\") def cleanShuffleDependencies(blocking: Boolean = false): Unit = { sc.cleaner.foreach { cleaner => /** * Clean the shuffles & all of its parents.  def cleanEagerly(dep: Dependency[_]): Unit = { dep match { case dependency: ShuffleDependency[_, _, _] => val shuffleId = dependency.shuffleId cleaner.doCleanupShuffle(shuffleId, blocking) case _ => // do nothing } val rdd = dep.rdd val rddDepsOpt = rdd.internalDependencies if (rdd.getStorageLevel == StorageLevel.NONE) { rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly)) } } internalDependencies.foreach(deps => deps.foreach(cleanEagerly)) } } /** * :: Experimental :: * Marks the current stage as a barrier stage, where Spark must launch all tasks together. * In case of a task failure, instead of only restarting the failed task, Spark will abort the * entire stage and re-launch all tasks for this stage. * The barrier execution mode feature is experimental and it only handles limited scenarios. * Please read the linked SPIP and design docs to understand the limitations and future plans. * @return an [[RDDBarrier]] instance that provides actions within a barrier stage * @see [[org.apache.spark.BarrierTaskContext]] * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24374\">SPIP: Barrier Execution Mode</a> * @see <a href=\"https://jira.apache.org/jira/browse/SPARK-24582\">Design Doc</a>  @Experimental @Since(\"2.4.0\") def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this)) /** * Specify a ResourceProfile to use when calculating this RDD. This is only supported on * certain cluster managers and currently requires dynamic allocation to be enabled. * It will result in new executors with the resources specified being acquired to * calculate the RDD.  @Experimental @Since(\"3.1.0\") def withResources(rp: ResourceProfile): this.type = { resourceProfile = Option(rp) sc.resourceProfileManager.addResourceProfile(resourceProfile.get) this } /** * Get the ResourceProfile specified with this RDD or null if it wasn't specified. * @return the user specified ResourceProfile or null (for Java compatibility) if * none was specified  @Experimental @Since(\"3.1.0\") def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null) // ======================================================================= // Other internal methods and fields // ======================================================================= private var storageLevel: StorageLevel = StorageLevel.NONE @transient private var resourceProfile: Option[ResourceProfile] = None /** User code that created this RDD (e.g. `textFile`, `parallelize`).  @transient private[spark] val creationSite = sc.getCallSite() /** * The scope associated with the operation that created this RDD. * * This is more flexible than the call site and can be defined hierarchically. For more * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the * user instantiates this RDD himself without using any Spark operations.  @transient private[spark] val scope: Option[RDDOperationScope] = { Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson) } private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse(\"\") private[spark] def elementClassTag: ClassTag[T] = classTag[T] private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default, // we stop as soon as we find the first such RDD, an optimization that allows us to write // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both // an RDD and its parent in every batch, in which case the parent may never be checkpointed // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847). private val checkpointAllMarkedAncestors = Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean) /** Returns the first parent RDD  protected[spark] def firstParent[U: ClassTag]: RDD[U] = { dependencies.head.rdd.asInstanceOf[RDD[U]] } /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T]  protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = { dependencies(j).rdd.asInstanceOf[RDD[U]] } /** The [[org.apache.spark.SparkContext]] that this RDD was created on.  def context: SparkContext = sc /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(cls: Class[T]): RDD[T] = { val classTag: ClassTag[T] = ClassTag.apply(cls) this.retag(classTag) } /** * Private API for changing an RDD's ClassTag. * Used for internal Java-Scala API compatibility.  private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = { this.mapPartitions(identity, preservesPartitioning = true)(classTag) } // Avoid handling doCheckpoint multiple times to prevent excessive recursion @transient private var doCheckpointCalled = false /** * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD * has completed (therefore the RDD has been materialized and potentially stored in memory). * doCheckpoint() is called recursively on the parent RDDs.  private[spark] def doCheckpoint(): Unit = { RDDOperationScope.withScope(sc, \"checkpoint\", allowNesting = false, ignoreParent = true) { if (!doCheckpointCalled) { doCheckpointCalled = true if (checkpointData.isDefined) { if (checkpointAllMarkedAncestors) { // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint // them in parallel. // Checkpoint parents first because our lineage will be truncated after we // checkpoint ourselves dependencies.foreach(_.rdd.doCheckpoint()) } checkpointData.get.checkpoint() } else { dependencies.foreach(_.rdd.doCheckpoint()) } } } } /** * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`) * created from the checkpoint file, and forget its old dependencies and partitions.  private[spark] def markCheckpointed(): Unit = stateLock.synchronized { legacyDependencies = new WeakReference(dependencies_) clearDependencies() partitions_ = null deps = null // Forget the constructor argument for dependencies too } /** * Clears the dependencies of this RDD. This method must ensure that all references * to the original parent RDDs are removed to enable the parent RDDs to be garbage * collected. Subclasses of RDD may override this method for implementing their own cleaning * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example.  protected def clearDependencies(): Unit = stateLock.synchronized { dependencies_ = null } /** A description of this RDD and its recursive dependencies for debugging.  def toDebugString: String = { // Get a debug description of an rdd without its children def debugSelf(rdd: RDD[_]): Seq[String] = { import Utils.bytesToString val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else \"\" val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info => \" CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format( info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize))) s\"$rdd [$persistence]\" +: storageInfo } // Apply a different rule to the last child def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = { val len = rdd.dependencies.length len match { case 0 => Seq.empty case 1 => val d = rdd.dependencies.head debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true) case _ => val frontDeps = rdd.dependencies.take(len - 1) val frontDepStrings = frontDeps.flatMap( d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]])) val lastDep = rdd.dependencies.last val lastDepStrings = debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true) frontDepStrings ++ lastDepStrings } } // The first RDD in the dependency stack has no parents, so no need for a +- def firstDebugString(rdd: RDD[_]): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val nextPrefix = (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset)) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix $desc\" } ++ debugChildren(rdd, nextPrefix) } def shuffleDebugString(rdd: RDD[_], prefix: String = \"\", isLastChild: Boolean): Seq[String] = { val partitionStr = \"(\" + rdd.partitions.length + \")\" val leftOffset = (partitionStr.length - 1) / 2 val thisPrefix = prefix.replaceAll(\"\\\\|\\\\s+$\", \"\") val nextPrefix = ( thisPrefix + (if (isLastChild) \" \" else \"| \") + (\" \" * leftOffset) + \"|\" + (\" \" * (partitionStr.length - leftOffset))) debugSelf(rdd).zipWithIndex.map{ case (desc: String, 0) => s\"$thisPrefix+-$partitionStr $desc\" case (desc: String, _) => s\"$nextPrefix$desc\" } ++ debugChildren(rdd, nextPrefix) } def debugString( rdd: RDD[_], prefix: String = \"\", isShuffle: Boolean = true, isLastChild: Boolean = false): Seq[String] = { if (isShuffle) { shuffleDebugString(rdd, prefix, isLastChild) } else { debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix) } } firstDebugString(this).mkString(\"\\n\") } override def toString: String = \"%s%s[%d] at %s\".format( Option(name).map(_ + \" \").getOrElse(\"\"), getClass.getSimpleName, id, getCreationSite) def toJavaRDD() : JavaRDD[T] = { new JavaRDD(this)(elementClassTag) } /** * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a * barrier stage. * * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a * [[ShuffledRDD]] indicates start of a new stage. * * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the * [[MapPartitionsRDD]] shall be marked as barrier.  private[spark] def isBarrier(): Boolean = isBarrier_ // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long // RDD chain. @transient protected lazy val isBarrier_ : Boolean = dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier()) private final lazy val _outputDeterministicLevel: DeterministicLevel.Value = getOutputDeterministicLevel /** * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]] * for the definition. * * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For * RDDs with parents, we will generate a deterministic level candidate per parent according to * the dependency. The deterministic level of the current RDD is the deterministic level * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to * provide custom logic of calculating output deterministic level.  // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs. // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for // different partitions. private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = { if (isReliablyCheckpointed) { DeterministicLevel.DETERMINATE } else { _outputDeterministicLevel } } @DeveloperApi protected def getOutputDeterministicLevel: DeterministicLevel.Value = { val deterministicLevelCandidates = dependencies.map { // The shuffle is not really happening, treat it like narrow dependency and assume the output // deterministic level of current RDD is same as parent. case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) => dep.rdd.outputDeterministicLevel case dep: ShuffleDependency[_, _, _] => if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) { // If map output was indeterminate, shuffle output will be indeterminate as well DeterministicLevel.INDETERMINATE } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) { // if aggregator specified (and so unique keys) and key ordering specified - then // consistent ordering. DeterministicLevel.DETERMINATE } else { // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and // the arrival order of these shuffle blocks are totally random. Even if the parent map // RDD is DETERMINATE, the reduce RDD is always UNORDERED. DeterministicLevel.UNORDERED } // For narrow dependency, assume the output deterministic level of current RDD is same as // parent. case dep => dep.rdd.outputDeterministicLevel } if (deterministicLevelCandidates.isEmpty) { // By default we assume the root RDD is determinate. DeterministicLevel.DETERMINATE } else { deterministicLevelCandidates.maxBy(_.id) } } } /** * Defines implicit functions that provide extra functionalities on RDDs of specific types. * * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`.  object RDD { private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS = \"spark.checkpoint.checkpointAllMarkedAncestors\" // The following implicit functions were in SparkContext before 1.3 and users had to // `import SparkContext._` to enable them. Now we move them here to make the compiler find // them automatically. However, we still keep the old functions in SparkContext for backward // compatibility and forward to the following functions directly. implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = { new PairRDDFunctions(rdd) } implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = { new AsyncRDDActions(rdd) } implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], keyWritableFactory: WritableFactory[K], valueWritableFactory: WritableFactory[V]) : SequenceFileRDDFunctions[K, V] = { implicit val keyConverter = keyWritableFactory.convert implicit val valueConverter = valueWritableFactory.convert new SequenceFileRDDFunctions(rdd, keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt)) } implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : OrderedRDDFunctions[K, V, (K, V)] = { new OrderedRDDFunctions[K, V, (K, V)](rdd) } implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = { new DoubleRDDFunctions(rdd) } implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T]) : DoubleRDDFunctions = { new DoubleRDDFunctions(rdd.map(x => num.toDouble(x))) } } /** * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels: * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun. * 2. UNORDERED: The RDD output is always the same data set but the order can be different * after a rerun. * 3. INDETERMINATE. The RDD output can be different after a rerun. * * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE.  private[spark] object DeterministicLevel extends Enumeration { val DETERMINATE, UNORDERED, INDETERMINATE = Value }",
            "## OBJECT: org/apache/spark/util/Utils.\n* Various utility methods used by Spark.  private[spark] object Utils extends Logging { val random = new Random() private val sparkUncaughtExceptionHandler = new SparkUncaughtExceptionHandler @volatile private var cachedLocalDir: String = \"\" /** * Define a default value for driver memory here since this value is referenced across the code * base and nearly all files already use Utils.scala  val DEFAULT_DRIVER_MEM_MB = JavaUtils.DEFAULT_DRIVER_MEM_MB.toInt val MAX_DIR_CREATION_ATTEMPTS: Int = 10 @volatile private var localRootDirs: Array[String] = null /** Scheme used for files that are locally available on worker nodes in the cluster.  val LOCAL_SCHEME = \"local\" private val weakStringInterner = Interners.newWeakInterner[String]() private val PATTERN_FOR_COMMAND_LINE_ARG = \"-D(.+?)=(.+)\".r /** Serialize an object using Java serialization  def serialize[T](o: T): Array[Byte] = { val bos = new ByteArrayOutputStream() val oos = new ObjectOutputStream(bos) oos.writeObject(o) oos.close() bos.toByteArray } /** Deserialize an object using Java serialization  def deserialize[T](bytes: Array[Byte]): T = { val bis = new ByteArrayInputStream(bytes) val ois = new ObjectInputStream(bis) ois.readObject.asInstanceOf[T] } /** Deserialize an object using Java serialization and the given ClassLoader  def deserialize[T](bytes: Array[Byte], loader: ClassLoader): T = { val bis = new ByteArrayInputStream(bytes) val ois = new ObjectInputStream(bis) { override def resolveClass(desc: ObjectStreamClass): Class[_] = { // scalastyle:off classforname Class.forName(desc.getName, false, loader) // scalastyle:on classforname } } ois.readObject.asInstanceOf[T] } /** Deserialize a Long value (used for [[org.apache.spark.api.python.PythonPartitioner]])  def deserializeLongValue(bytes: Array[Byte]) : Long = { // Note: we assume that we are given a Long value encoded in network (big-endian) byte order var result = bytes(7) & 0xFFL result = result + ((bytes(6) & 0xFFL) << 8) result = result + ((bytes(5) & 0xFFL) << 16) result = result + ((bytes(4) & 0xFFL) << 24) result = result + ((bytes(3) & 0xFFL) << 32) result = result + ((bytes(2) & 0xFFL) << 40) result = result + ((bytes(1) & 0xFFL) << 48) result + ((bytes(0) & 0xFFL) << 56) } /** Serialize via nested stream using specific serializer  def serializeViaNestedStream(os: OutputStream, ser: SerializerInstance)( f: SerializationStream => Unit): Unit = { val osWrapper = ser.serializeStream(new OutputStream { override def write(b: Int): Unit = os.write(b) override def write(b: Array[Byte], off: Int, len: Int): Unit = os.write(b, off, len) }) try { f(osWrapper) } finally { osWrapper.close() } } /** Deserialize via nested stream using specific serializer  def deserializeViaNestedStream(is: InputStream, ser: SerializerInstance)( f: DeserializationStream => Unit): Unit = { val isWrapper = ser.deserializeStream(new InputStream { override def read(): Int = is.read() override def read(b: Array[Byte], off: Int, len: Int): Int = is.read(b, off, len) }) try { f(isWrapper) } finally { isWrapper.close() } } /** String interning to reduce the memory usage.  def weakIntern(s: String): String = { weakStringInterner.intern(s) } /** * Get the ClassLoader which loaded Spark.  def getSparkClassLoader: ClassLoader = getClass.getClassLoader /** * Get the Context ClassLoader on this thread or, if not present, the ClassLoader that * loaded Spark. * * This should be used whenever passing a ClassLoader to Class.ForName or finding the currently * active loader when setting up ClassLoader delegation chains.  def getContextOrSparkClassLoader: ClassLoader = Option(Thread.currentThread().getContextClassLoader).getOrElse(getSparkClassLoader) /** Determines whether the provided class is loadable in the current thread.  def classIsLoadable(clazz: String): Boolean = { Try { classForName(clazz, initialize = false) }.isSuccess } // scalastyle:off classforname /** * Preferred alternative to Class.forName(className), as well as * Class.forName(className, initialize, loader) with current thread's ContextClassLoader.  def classForName[C]( className: String, initialize: Boolean = true, noSparkClassLoader: Boolean = false): Class[C] = { if (!noSparkClassLoader) { Class.forName(className, initialize, getContextOrSparkClassLoader).asInstanceOf[Class[C]] } else { Class.forName(className, initialize, Thread.currentThread().getContextClassLoader). asInstanceOf[Class[C]] } // scalastyle:on classforname } /** * Run a segment of code using a different context class loader in the current thread  def withContextClassLoader[T](ctxClassLoader: ClassLoader)(fn: => T): T = { val oldClassLoader = Thread.currentThread().getContextClassLoader() try { Thread.currentThread().setContextClassLoader(ctxClassLoader) fn } finally { Thread.currentThread().setContextClassLoader(oldClassLoader) } } /** * Primitive often used when writing [[java.nio.ByteBuffer]] to [[java.io.DataOutput]]  def writeByteBuffer(bb: ByteBuffer, out: DataOutput): Unit = { if (bb.hasArray) { out.write(bb.array(), bb.arrayOffset() + bb.position(), bb.remaining()) } else { val originalPosition = bb.position() val bbval = new Array[Byte](bb.remaining()) bb.get(bbval) out.write(bbval) bb.position(originalPosition) } } /** * Primitive often used when writing [[java.nio.ByteBuffer]] to [[java.io.OutputStream]]  def writeByteBuffer(bb: ByteBuffer, out: OutputStream): Unit = { if (bb.hasArray) { out.write(bb.array(), bb.arrayOffset() + bb.position(), bb.remaining()) } else { val originalPosition = bb.position() val bbval = new Array[Byte](bb.remaining()) bb.get(bbval) out.write(bbval) bb.position(originalPosition) } } /** * JDK equivalent of `chmod 700 file`. * * @param file the file whose permissions will be modified * @return true if the permissions were successfully changed, false otherwise.  def chmod700(file: File): Boolean = { file.setReadable(false, false) && file.setReadable(true, true) && file.setWritable(false, false) && file.setWritable(true, true) && file.setExecutable(false, false) && file.setExecutable(true, true) } /** * Create a directory given the abstract pathname * @return true, if the directory is successfully created; otherwise, return false.  def createDirectory(dir: File): Boolean = { try { // SPARK-35907: The check was required by File.mkdirs() because it could sporadically // fail silently. After switching to Files.createDirectories(), ideally, there should // no longer be silent fails. But the check is kept for the safety concern. We can // remove the check when we're sure that Files.createDirectories() would never fail silently. Files.createDirectories(dir.toPath) if ( !dir.exists() || !dir.isDirectory) { logError(s\"Failed to create directory \" + dir) } dir.isDirectory } catch { case e: Exception => logError(s\"Failed to create directory \" + dir, e) false } } /** * Create a directory inside the given parent directory. The directory is guaranteed to be * newly created, and is not marked for automatic deletion.  def createDirectory(root: String, namePrefix: String = \"spark\"): File = { var attempts = 0 val maxAttempts = MAX_DIR_CREATION_ATTEMPTS var dir: File = null while (dir == null) { attempts += 1 if (attempts > maxAttempts) { throw new IOException(\"Failed to create a temp directory (under \" + root + \") after \" + maxAttempts + \" attempts!\") } try { dir = new File(root, namePrefix + \"-\" + UUID.randomUUID.toString) // SPARK-35907: // This could throw more meaningful exception information if directory creation failed. Files.createDirectories(dir.toPath) } catch { case e @ (_ : IOException | _ : SecurityException) => logError(s\"Failed to create directory $dir\", e) dir = null } } dir.getCanonicalFile } /** * Create a temporary directory inside the given parent directory. The directory will be * automatically deleted when the VM shuts down.  def createTempDir( root: String = System.getProperty(\"java.io.tmpdir\"), namePrefix: String = \"spark\"): File = { val dir = createDirectory(root, namePrefix) ShutdownHookManager.registerShutdownDeleteDir(dir) dir } /** * Copy all data from an InputStream to an OutputStream. NIO way of file stream to file stream * copying is disabled by default unless explicitly set transferToEnabled as true, * the parameter transferToEnabled should be configured by spark.file.transferTo = [true|false].  def copyStream( in: InputStream, out: OutputStream, closeStreams: Boolean = false, transferToEnabled: Boolean = false): Long = { tryWithSafeFinally { (in, out) match { case (input: FileInputStream, output: FileOutputStream) if transferToEnabled => // When both streams are File stream, use transferTo to improve copy performance. val inChannel = input.getChannel val outChannel = output.getChannel val size = inChannel.size() copyFileStreamNIO(inChannel, outChannel, 0, size) size case (input, output) => var count = 0L val buf = new Array[Byte](8192) var n = 0 while (n != -1) { n = input.read(buf) if (n != -1) { output.write(buf, 0, n) count += n } } count } } { if (closeStreams) { try { in.close() } finally { out.close() } } } } /** * Copy the first `maxSize` bytes of data from the InputStream to an in-memory * buffer, primarily to check for corruption. * * This returns a new InputStream which contains the same data as the original input stream. * It may be entirely on in-memory buffer, or it may be a combination of in-memory data, and then * continue to read from the original stream. The only real use of this is if the original input * stream will potentially detect corruption while the data is being read (e.g. from compression). * This allows for an eager check of corruption in the first maxSize bytes of data. * * @return An InputStream which includes all data from the original stream (combining buffered * data and remaining data in the original stream)  def copyStreamUpTo(in: InputStream, maxSize: Long): InputStream = { var count = 0L val out = new ChunkedByteBufferOutputStream(64 * 1024, ByteBuffer.allocate) val fullyCopied = tryWithSafeFinally { val bufSize = Math.min(8192L, maxSize) val buf = new Array[Byte](bufSize.toInt) var n = 0 while (n != -1 && count < maxSize) { n = in.read(buf, 0, Math.min(maxSize - count, bufSize).toInt) if (n != -1) { out.write(buf, 0, n) count += n } } count < maxSize } { try { if (count < maxSize) { in.close() } } finally { out.close() } } if (fullyCopied) { out.toChunkedByteBuffer.toInputStream(dispose = true) } else { new SequenceInputStream( out.toChunkedByteBuffer.toInputStream(dispose = true), in) } } def copyFileStreamNIO( input: FileChannel, output: WritableByteChannel, startPosition: Long, bytesToCopy: Long): Unit = { val outputInitialState = output match { case outputFileChannel: FileChannel => Some((outputFileChannel.position(), outputFileChannel)) case _ => None } var count = 0L // In case transferTo method transferred less data than we have required. while (count < bytesToCopy) { count += input.transferTo(count + startPosition, bytesToCopy - count, output) } assert(count == bytesToCopy, s\"request to copy $bytesToCopy bytes, but actually copied $count bytes.\") // Check the position after transferTo loop to see if it is in the right position and // give user information if not. // Position will not be increased to the expected length after calling transferTo in // kernel version 2.6.32, this issue can be seen in // https://bugs.openjdk.java.net/browse/JDK-7052359 // This will lead to stream corruption issue when using sort-based shuffle (SPARK-3948). outputInitialState.foreach { case (initialPos, outputFileChannel) => val finalPos = outputFileChannel.position() val expectedPos = initialPos + bytesToCopy assert(finalPos == expectedPos, s\"\"\" |Current position $finalPos do not equal to expected position $expectedPos |after transferTo, please check your kernel version to see if it is 2.6.32, |this is a kernel bug which will lead to unexpected behavior when using transferTo. |You can set spark.file.transferTo = false to disable this NIO feature. \"\"\".stripMargin) } } /** * A file name may contain some invalid URI characters, such as \" \". This method will convert the * file name to a raw path accepted by `java.net.URI(String)`. * * Note: the file name must not contain \"/\" or \"\\\"  def encodeFileNameToURIRawPath(fileName: String): String = { require(!fileName.contains(\"/\") && !fileName.contains(\"\\\\\")) // `file` and `localhost` are not used. Just to prevent URI from parsing `fileName` as // scheme or host. The prefix \"/\" is required because URI doesn't accept a relative path. // We should remove it after we get the raw path. new URI(\"file\", null, \"localhost\", -1, \"/\" + fileName, null, null).getRawPath.substring(1) } /** * Get the file name from uri's raw path and decode it. If the raw path of uri ends with \"/\", * return the name before the last \"/\".  def decodeFileNameInURI(uri: URI): String = { val rawPath = uri.getRawPath val rawFileName = rawPath.split(\"/\").last new URI(\"file:///\" + rawFileName).getPath.substring(1) } /** * Download a file or directory to target directory. Supports fetching the file in a variety of * ways, including HTTP, Hadoop-compatible filesystems, and files on a standard filesystem, based * on the URL parameter. Fetching directories is only supported from Hadoop-compatible * filesystems. * * If `useCache` is true, first attempts to fetch the file to a local cache that's shared * across executors running the same application. `useCache` is used mainly for * the executors, and not in local mode. * * Throws SparkException if the target file already exists and has different contents than * the requested file. * * If `shouldUntar` is true, it untars the given url if it is a tar.gz or tgz into `targetDir`. * This is a legacy behavior, and users should better use `spark.archives` configuration or * `SparkContext.addArchive`  def fetchFile( url: String, targetDir: File, conf: SparkConf, hadoopConf: Configuration, timestamp: Long, useCache: Boolean, shouldUntar: Boolean = true): File = { val fileName = decodeFileNameInURI(new URI(url)) val targetFile = new File(targetDir, fileName) val fetchCacheEnabled = conf.getBoolean(\"spark.files.useFetchCache\", defaultValue = true) if (useCache && fetchCacheEnabled) { val cachedFileName = s\"${url.hashCode}${timestamp}_cache\" val lockFileName = s\"${url.hashCode}${timestamp}_lock\" // Set the cachedLocalDir for the first time and re-use it later if (cachedLocalDir.isEmpty) { this.synchronized { if (cachedLocalDir.isEmpty) { cachedLocalDir = getLocalDir(conf) } } } val localDir = new File(cachedLocalDir) val lockFile = new File(localDir, lockFileName) val lockFileChannel = new RandomAccessFile(lockFile, \"rw\").getChannel() // Only one executor entry. // The FileLock is only used to control synchronization for executors download file, // it's always safe regardless of lock type (mandatory or advisory). val lock = lockFileChannel.lock() val cachedFile = new File(localDir, cachedFileName) try { if (!cachedFile.exists()) { doFetchFile(url, localDir, cachedFileName, conf, hadoopConf) } } finally { lock.release() lockFileChannel.close() } copyFile( url, cachedFile, targetFile, conf.getBoolean(\"spark.files.overwrite\", false) ) } else { doFetchFile(url, targetDir, fileName, conf, hadoopConf) } if (shouldUntar) { // Decompress the file if it's a .tar or .tar.gz if (fileName.endsWith(\".tar.gz\") || fileName.endsWith(\".tgz\")) { logWarning( \"Untarring behavior will be deprecated at spark.files and \" + \"SparkContext.addFile. Consider using spark.archives or SparkContext.addArchive \" + \"instead.\") logInfo(\"Untarring \" + fileName) executeAndGetOutput(Seq(\"tar\", \"-xzf\", fileName), targetDir) } else if (fileName.endsWith(\".tar\")) { logWarning( \"Untarring behavior will be deprecated at spark.files and \" + \"SparkContext.addFile. Consider using spark.archives or SparkContext.addArchive \" + \"instead.\") logInfo(\"Untarring \" + fileName) executeAndGetOutput(Seq(\"tar\", \"-xf\", fileName), targetDir) } } // Make the file executable - That's necessary for scripts FileUtil.chmod(targetFile.getAbsolutePath, \"a+x\") // Windows does not grant read permission by default to non-admin users // Add read permission to owner explicitly if (isWindows) { FileUtil.chmod(targetFile.getAbsolutePath, \"u+r\") } targetFile } /** * Unpacks an archive file into the specified directory. It expects .jar, .zip, .tar.gz, .tgz * and .tar files. This behaves same as Hadoop's archive in distributed cache. This method is * basically copied from `org.apache.hadoop.yarn.util.FSDownload.unpack`.  def unpack(source: File, dest: File): Unit = { if (!source.exists()) { throw new FileNotFoundException(source.getAbsolutePath) } val lowerSrc = StringUtils.toLowerCase(source.getName) if (lowerSrc.endsWith(\".jar\")) { RunJar.unJar(source, dest, RunJar.MATCH_ANY) } else if (lowerSrc.endsWith(\".zip\")) { // TODO(SPARK-37677): should keep file permissions. Java implementation doesn't. FileUtil.unZip(source, dest) } else if (lowerSrc.endsWith(\".tar.gz\") || lowerSrc.endsWith(\".tgz\")) { FileUtil.unTar(source, dest) } else if (lowerSrc.endsWith(\".tar\")) { // TODO(SPARK-38632): should keep file permissions. Java implementation doesn't. unTarUsingJava(source, dest) } else { logWarning(s\"Cannot unpack $source, just copying it to $dest.\") copyRecursive(source, dest) } } /** * The method below was copied from `FileUtil.unTar` but uses Java-based implementation * to work around a security issue, see also SPARK-38631.  private def unTarUsingJava(source: File, dest: File): Unit = { if (!dest.mkdirs && !dest.isDirectory) { throw new IOException(s\"Mkdirs failed to create $dest\") } else { try { // Should not fail because all Hadoop 2.1+ (from HADOOP-9264) // have 'unTarUsingJava'. val mth = classOf[FileUtil].getDeclaredMethod( \"unTarUsingJava\", classOf[File], classOf[File], classOf[Boolean]) mth.setAccessible(true) mth.invoke(null, source, dest, java.lang.Boolean.FALSE) } catch { // Re-throw the original exception. case e: java.lang.reflect.InvocationTargetException if e.getCause != null => throw e.getCause } } } /** Records the duration of running `body`.  def timeTakenMs[T](body: => T): (T, Long) = { val startTime = System.nanoTime() val result = body val endTime = System.nanoTime() (result, math.max(NANOSECONDS.toMillis(endTime - startTime), 0)) } /** * Download `in` to `tempFile`, then move it to `destFile`. * * If `destFile` already exists: * - no-op if its contents equal those of `sourceFile`, * - throw an exception if `fileOverwrite` is false, * - attempt to overwrite it otherwise. * * @param url URL that `sourceFile` originated from, for logging purposes. * @param in InputStream to download. * @param destFile File path to move `tempFile` to. * @param fileOverwrite Whether to delete/overwrite an existing `destFile` that does not match * `sourceFile`  private def downloadFile( url: String, in: InputStream, destFile: File, fileOverwrite: Boolean): Unit = { val tempFile = File.createTempFile(\"fetchFileTemp\", null, new File(destFile.getParentFile.getAbsolutePath)) logInfo(s\"Fetching $url to $tempFile\") try { val out = new FileOutputStream(tempFile) Utils.copyStream(in, out, closeStreams = true) copyFile(url, tempFile, destFile, fileOverwrite, removeSourceFile = true) } finally { // Catch-all for the couple of cases where for some reason we didn't move `tempFile` to // `destFile`. if (tempFile.exists()) { tempFile.delete() } } } /** * Copy `sourceFile` to `destFile`. * * If `destFile` already exists: * - no-op if its contents equal those of `sourceFile`, * - throw an exception if `fileOverwrite` is false, * - attempt to overwrite it otherwise. * * @param url URL that `sourceFile` originated from, for logging purposes. * @param sourceFile File path to copy/move from. * @param destFile File path to copy/move to. * @param fileOverwrite Whether to delete/overwrite an existing `destFile` that does not match * `sourceFile` * @param removeSourceFile Whether to remove `sourceFile` after / as part of moving/copying it to * `destFile`.  private def copyFile( url: String, sourceFile: File, destFile: File, fileOverwrite: Boolean, removeSourceFile: Boolean = false): Unit = { if (destFile.exists) { if (!filesEqualRecursive(sourceFile, destFile)) { if (fileOverwrite) { logInfo( s\"File $destFile exists and does not match contents of $url, replacing it with $url\" ) if (!destFile.delete()) { throw new SparkException( \"Failed to delete %s while attempting to overwrite it with %s\".format( destFile.getAbsolutePath, sourceFile.getAbsolutePath ) ) } } else { throw new SparkException( s\"File $destFile exists and does not match contents of $url\") } } else { // Do nothing if the file contents are the same, i.e. this file has been copied // previously. logInfo( \"%s has been previously copied to %s\".format( sourceFile.getAbsolutePath, destFile.getAbsolutePath ) ) return } } // The file does not exist in the target directory. Copy or move it there. if (removeSourceFile) { Files.move(sourceFile.toPath, destFile.toPath) } else { logInfo(s\"Copying ${sourceFile.getAbsolutePath} to ${destFile.getAbsolutePath}\") copyRecursive(sourceFile, destFile) } } private def filesEqualRecursive(file1: File, file2: File): Boolean = { if (file1.isDirectory && file2.isDirectory) { val subfiles1 = file1.listFiles() val subfiles2 = file2.listFiles() if (subfiles1.size != subfiles2.size) { return false } subfiles1.sortBy(_.getName).zip(subfiles2.sortBy(_.getName)).forall { case (f1, f2) => filesEqualRecursive(f1, f2) } } else if (file1.isFile && file2.isFile) { GFiles.equal(file1, file2) } else { false } } private def copyRecursive(source: File, dest: File): Unit = { if (source.isDirectory) { if (!dest.mkdir()) { throw new IOException(s\"Failed to create directory ${dest.getPath}\") } val subfiles = source.listFiles() subfiles.foreach(f => copyRecursive(f, new File(dest, f.getName))) } else { Files.copy(source.toPath, dest.toPath) } } /** * Download a file or directory to target directory. Supports fetching the file in a variety of * ways, including HTTP, Hadoop-compatible filesystems, and files on a standard filesystem, based * on the URL parameter. Fetching directories is only supported from Hadoop-compatible * filesystems. * * Throws SparkException if the target file already exists and has different contents than * the requested file.  def doFetchFile( url: String, targetDir: File, filename: String, conf: SparkConf, hadoopConf: Configuration): File = { val targetFile = new File(targetDir, filename) val uri = new URI(url) val fileOverwrite = conf.getBoolean(\"spark.files.overwrite\", defaultValue = false) Option(uri.getScheme).getOrElse(\"file\") match { case \"spark\" => if (SparkEnv.get == null) { throw new IllegalStateException( \"Cannot retrieve files with 'spark' scheme without an active SparkEnv.\") } val source = SparkEnv.get.rpcEnv.openChannel(url) val is = Channels.newInputStream(source) downloadFile(url, is, targetFile, fileOverwrite) case \"http\" | \"https\" | \"ftp\" => val uc = new URL(url).openConnection() val timeoutMs = conf.getTimeAsSeconds(\"spark.files.fetchTimeout\", \"60s\").toInt * 1000 uc.setConnectTimeout(timeoutMs) uc.setReadTimeout(timeoutMs) uc.connect() val in = uc.getInputStream() downloadFile(url, in, targetFile, fileOverwrite) case \"file\" => // In the case of a local file, copy the local file to the target directory. // Note the difference between uri vs url. val sourceFile = if (uri.isAbsolute) new File(uri) else new File(uri.getPath) copyFile(url, sourceFile, targetFile, fileOverwrite) case _ => val fs = getHadoopFileSystem(uri, hadoopConf) val path = new Path(uri) fetchHcfsFile(path, targetDir, fs, conf, hadoopConf, fileOverwrite, filename = Some(filename)) } targetFile } /** * Fetch a file or directory from a Hadoop-compatible filesystem. * * Visible for testing  private[spark] def fetchHcfsFile( path: Path, targetDir: File, fs: FileSystem, conf: SparkConf, hadoopConf: Configuration, fileOverwrite: Boolean, filename: Option[String] = None): Unit = { if (!targetDir.exists() && !targetDir.mkdir()) { throw new IOException(s\"Failed to create directory ${targetDir.getPath}\") } val dest = new File(targetDir, filename.getOrElse(path.getName)) if (fs.isFile(path)) { val in = fs.open(path) try { downloadFile(path.toString, in, dest, fileOverwrite) } finally { in.close() } } else { fs.listStatus(path).foreach { fileStatus => fetchHcfsFile(fileStatus.getPath(), dest, fs, conf, hadoopConf, fileOverwrite) } } } /** * Validate that a given URI is actually a valid URL as well. * @param uri The URI to validate  @throws[MalformedURLException](\"when the URI is an invalid URL\") def validateURL(uri: URI): Unit = { Option(uri.getScheme).getOrElse(\"file\") match { case \"http\" | \"https\" | \"ftp\" => try { uri.toURL } catch { case e: MalformedURLException => val ex = new MalformedURLException(s\"URI (${uri.toString}) is not a valid URL.\") ex.initCause(e) throw ex } case _ => // will not be turned into a URL anyway } } /** * Get the path of a temporary directory. Spark's local directories can be configured through * multiple settings, which are used with the following precedence: * * - If called from inside of a YARN container, this will return a directory chosen by YARN. * - If the SPARK_LOCAL_DIRS environment variable is set, this will return a directory from it. * - Otherwise, if the spark.local.dir is set, this will return a directory from it. * - Otherwise, this will return java.io.tmpdir. * * Some of these configuration options might be lists of multiple paths, but this method will * always return a single directory. The return directory is chosen randomly from the array * of directories it gets from getOrCreateLocalRootDirs.  def getLocalDir(conf: SparkConf): String = { val localRootDirs = getOrCreateLocalRootDirs(conf) if (localRootDirs.isEmpty) { val configuredLocalDirs = getConfiguredLocalDirs(conf) throw new IOException( s\"Failed to get a temp directory under [${configuredLocalDirs.mkString(\",\")}].\") } else { localRootDirs(scala.util.Random.nextInt(localRootDirs.length)) } } private[spark] def isRunningInYarnContainer(conf: SparkConf): Boolean = { // These environment variables are set by YARN. conf.getenv(\"CONTAINER_ID\") != null } /** * Returns if the current codes are running in a Spark task, e.g., in executors.  def isInRunningSparkTask: Boolean = TaskContext.get() != null /** * Gets or creates the directories listed in spark.local.dir or SPARK_LOCAL_DIRS, * and returns only the directories that exist / could be created. * * If no directories could be created, this will return an empty list. * * This method will cache the local directories for the application when it's first invoked. * So calling it multiple times with a different configuration will always return the same * set of directories.  private[spark] def getOrCreateLocalRootDirs(conf: SparkConf): Array[String] = { if (localRootDirs == null) { this.synchronized { if (localRootDirs == null) { localRootDirs = getOrCreateLocalRootDirsImpl(conf) } } } localRootDirs } /** * Return the configured local directories where Spark can write files. This * method does not create any directories on its own, it only encapsulates the * logic of locating the local directories according to deployment mode.  def getConfiguredLocalDirs(conf: SparkConf): Array[String] = { val shuffleServiceEnabled = conf.get(config.SHUFFLE_SERVICE_ENABLED) if (isRunningInYarnContainer(conf)) { // If we are in yarn mode, systems can have different disk layouts so we must set it // to what Yarn on this system said was available. Note this assumes that Yarn has // created the directories already, and that they are secured so that only the // user has access to them. randomizeInPlace(getYarnLocalDirs(conf).split(\",\")) } else if (conf.getenv(\"SPARK_EXECUTOR_DIRS\") != null) { conf.getenv(\"SPARK_EXECUTOR_DIRS\").split(File.pathSeparator) } else if (conf.getenv(\"SPARK_LOCAL_DIRS\") != null) { conf.getenv(\"SPARK_LOCAL_DIRS\").split(\",\") } else if (conf.getenv(\"MESOS_SANDBOX\") != null && !shuffleServiceEnabled) { // Mesos already creates a directory per Mesos task. Spark should use that directory // instead so all temporary files are automatically cleaned up when the Mesos task ends. // Note that we don't want this if the shuffle service is enabled because we want to // continue to serve shuffle files after the executors that wrote them have already exited. Array(conf.getenv(\"MESOS_SANDBOX\")) } else { if (conf.getenv(\"MESOS_SANDBOX\") != null && shuffleServiceEnabled) { logInfo(\"MESOS_SANDBOX available but not using provided Mesos sandbox because \" + s\"${config.SHUFFLE_SERVICE_ENABLED.key} is enabled.\") } // In non-Yarn mode (or for the driver in yarn-client mode), we cannot trust the user // configuration to point to a secure directory. So create a subdirectory with restricted // permissions under each listed directory. conf.get(\"spark.local.dir\", System.getProperty(\"java.io.tmpdir\")).split(\",\") } } private def getOrCreateLocalRootDirsImpl(conf: SparkConf): Array[String] = { val configuredLocalDirs = getConfiguredLocalDirs(conf) val uris = configuredLocalDirs.filter { root => // Here, we guess if the given value is a URI at its best - check if scheme is set. Try(new URI(root).getScheme != null).getOrElse(false) } if (uris.nonEmpty) { logWarning( \"The configured local directories are not expected to be URIs; however, got suspicious \" + s\"values [${uris.mkString(\", \")}]. Please check your configured local directories.\") } configuredLocalDirs.flatMap { root => try { val rootDir = new File(root) if (rootDir.exists || rootDir.mkdirs()) { val dir = createTempDir(root) chmod700(dir) Some(dir.getAbsolutePath) } else { logError(s\"Failed to create dir in $root. Ignoring this directory.\") None } } catch { case e: IOException => logError(s\"Failed to create local root dir in $root. Ignoring this directory.\") None } } } /** Get the Yarn approved local directories.  private def getYarnLocalDirs(conf: SparkConf): String = { val localDirs = Option(conf.getenv(\"LOCAL_DIRS\")).getOrElse(\"\") if (localDirs.isEmpty) { throw new Exception(\"Yarn Local dirs can't be empty\") } localDirs } /** Used by unit tests. Do not call from other places.  private[spark] def clearLocalRootDirs(): Unit = { localRootDirs = null } /** * Shuffle the elements of a collection into a random order, returning the * result in a new collection. Unlike scala.util.Random.shuffle, this method * uses a local random number generator, avoiding inter-thread contention.  def randomize[T: ClassTag](seq: TraversableOnce[T]): Seq[T] = { randomizeInPlace(seq.toArray) } /** * Shuffle the elements of an array into a random order, modifying the * original array. Returns the original array.  def randomizeInPlace[T](arr: Array[T], rand: Random = new Random): Array[T] = { for (i <- (arr.length - 1) to 1 by -1) { val j = rand.nextInt(i + 1) val tmp = arr(j) arr(j) = arr(i) arr(i) = tmp } arr } /** * Get the local host's IP address in dotted-quad format (e.g. 1.2.3.4). * Note, this is typically not used from within core spark.  private lazy val localIpAddress: InetAddress = findLocalInetAddress() private def findLocalInetAddress(): InetAddress = { val defaultIpOverride = System.getenv(\"SPARK_LOCAL_IP\") if (defaultIpOverride != null) { InetAddress.getByName(defaultIpOverride) } else { val address = InetAddress.getLocalHost if (address.isLoopbackAddress) { // Address resolves to something like 127.0.1.1, which happens on Debian; try to find // a better address using the local network interfaces // getNetworkInterfaces returns ifs in reverse order compared to ifconfig output order // on unix-like system. On windows, it returns in index order. // It's more proper to pick ip address following system output order. val activeNetworkIFs = NetworkInterface.getNetworkInterfaces.asScala.toSeq val reOrderedNetworkIFs = if (isWindows) activeNetworkIFs else activeNetworkIFs.reverse for (ni <- reOrderedNetworkIFs) { val addresses = ni.getInetAddresses.asScala .filterNot(addr => addr.isLinkLocalAddress || addr.isLoopbackAddress).toSeq if (addresses.nonEmpty) { val addr = addresses.find(_.isInstanceOf[Inet4Address]).getOrElse(addresses.head) // because of Inet6Address.toHostName may add interface at the end if it knows about it val strippedAddress = InetAddress.getByAddress(addr.getAddress) // We've found an address that looks reasonable! logWarning(\"Your hostname, \" + InetAddress.getLocalHost.getHostName + \" resolves to\" + \" a loopback address: \" + address.getHostAddress + \"; using \" + strippedAddress.getHostAddress + \" instead (on interface \" + ni.getName + \")\") logWarning(\"Set SPARK_LOCAL_IP if you need to bind to another address\") return strippedAddress } } logWarning(\"Your hostname, \" + InetAddress.getLocalHost.getHostName + \" resolves to\" + \" a loopback address: \" + address.getHostAddress + \", but we couldn't find any\" + \" external IP address!\") logWarning(\"Set SPARK_LOCAL_IP if you need to bind to another address\") } address } } private var customHostname: Option[String] = sys.env.get(\"SPARK_LOCAL_HOSTNAME\") /** * Allow setting a custom host name because when we run on Mesos we need to use the same * hostname it reports to the master.  def setCustomHostname(hostname: String): Unit = { // DEBUG code Utils.checkHost(hostname) customHostname = Some(hostname) } /** * Get the local machine's FQDN.  def localCanonicalHostName(): String = { customHostname.getOrElse(localIpAddress.getCanonicalHostName) } /** * Get the local machine's hostname.  def localHostName(): String = { customHostname.getOrElse(localIpAddress.getHostAddress) } /** * Get the local machine's URI.  def localHostNameForURI(): String = { customHostname.getOrElse(InetAddresses.toUriString(localIpAddress)) } /** * Checks if the host contains only valid hostname/ip without port * NOTE: Incase of IPV6 ip it should be enclosed inside []  def checkHost(host: String): Unit = { if (host != null && host.split(\":\").length > 2) { assert(host.startsWith(\"[\") && host.endsWith(\"]\"), s\"Expected hostname or IPv6 IP enclosed in [] but got $host\") } else { assert(host != null && host.indexOf(':') == -1, s\"Expected hostname or IP but got $host\") } } def checkHostPort(hostPort: String): Unit = { if (hostPort != null && hostPort.split(\":\").length > 2) { assert(hostPort != null && hostPort.indexOf(\"]:\") != -1, s\"Expected host and port but got $hostPort\") } else { assert(hostPort != null && hostPort.indexOf(':') != -1, s\"Expected host and port but got $hostPort\") } } // Typically, this will be of order of number of nodes in cluster // If not, we should change it to LRUCache or something. private val hostPortParseResults = new ConcurrentHashMap[String, (String, Int)]() def parseHostPort(hostPort: String): (String, Int) = { // Check cache first. val cached = hostPortParseResults.get(hostPort) if (cached != null) { return cached } def setDefaultPortValue: (String, Int) = { val retval = (hostPort, 0) hostPortParseResults.put(hostPort, retval) retval } // checks if the hostport contains IPV6 ip and parses the host, port if (hostPort != null && hostPort.split(\":\").length > 2) { val index: Int = hostPort.lastIndexOf(\"]:\") if (-1 == index) { return setDefaultPortValue } val port = hostPort.substring(index + 2).trim() val retval = (hostPort.substring(0, index + 1).trim(), if (port.isEmpty) 0 else port.toInt) hostPortParseResults.putIfAbsent(hostPort, retval) } else { val index: Int = hostPort.lastIndexOf(':') if (-1 == index) { return setDefaultPortValue } val port = hostPort.substring(index + 1).trim() val retval = (hostPort.substring(0, index).trim(), if (port.isEmpty) 0 else port.toInt) hostPortParseResults.putIfAbsent(hostPort, retval) } hostPortParseResults.get(hostPort) } /** * Return the string to tell how long has passed in milliseconds. * @param startTimeNs - a timestamp in nanoseconds returned by `System.nanoTime`.  def getUsedTimeNs(startTimeNs: Long): String = { s\"${TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTimeNs)} ms\" } /** * Lists files recursively.  def recursiveList(f: File): Array[File] = { require(f.isDirectory) val result = f.listFiles.toBuffer val dirList = result.filter(_.isDirectory) while (dirList.nonEmpty) { val curDir = dirList.remove(0) val files = curDir.listFiles() result ++= files dirList ++= files.filter(_.isDirectory) } result.toArray } /** * Delete a file or directory and its contents recursively. * Don't follow directories if they are symlinks. * Throws an exception if deletion is unsuccessful.  def deleteRecursively(file: File): Unit = { if (file != null) { JavaUtils.deleteRecursively(file) ShutdownHookManager.removeShutdownDeleteDir(file) } } /** * Determines if a directory contains any files newer than cutoff seconds. * * @param dir must be the path to a directory, or IllegalArgumentException is thrown * @param cutoff measured in seconds. Returns true if there are any files or directories in the * given directory whose last modified time is later than this many seconds ago  def doesDirectoryContainAnyNewFiles(dir: File, cutoff: Long): Boolean = { if (!dir.isDirectory) { throw new IllegalArgumentException(s\"$dir is not a directory!\") } val filesAndDirs = dir.listFiles() val cutoffTimeInMillis = System.currentTimeMillis - (cutoff * 1000) filesAndDirs.exists(_.lastModified() > cutoffTimeInMillis) || filesAndDirs.filter(_.isDirectory).exists( subdir => doesDirectoryContainAnyNewFiles(subdir, cutoff) ) } /** * Convert a time parameter such as (50s, 100ms, or 250us) to milliseconds for internal use. If * no suffix is provided, the passed number is assumed to be in ms.  def timeStringAsMs(str: String): Long = { JavaUtils.timeStringAsMs(str) } /** * Convert a time parameter such as (50s, 100ms, or 250us) to seconds for internal use. If * no suffix is provided, the passed number is assumed to be in seconds.  def timeStringAsSeconds(str: String): Long = { JavaUtils.timeStringAsSec(str) } /** * Convert a passed byte string (e.g. 50b, 100k, or 250m) to bytes for internal use. * * If no suffix is provided, the passed number is assumed to be in bytes.  def byteStringAsBytes(str: String): Long = { JavaUtils.byteStringAsBytes(str) } /** * Convert a passed byte string (e.g. 50b, 100k, or 250m) to kibibytes for internal use. * * If no suffix is provided, the passed number is assumed to be in kibibytes.  def byteStringAsKb(str: String): Long = { JavaUtils.byteStringAsKb(str) } /** * Convert a passed byte string (e.g. 50b, 100k, or 250m) to mebibytes for internal use. * * If no suffix is provided, the passed number is assumed to be in mebibytes.  def byteStringAsMb(str: String): Long = { JavaUtils.byteStringAsMb(str) } /** * Convert a passed byte string (e.g. 50b, 100k, or 250m, 500g) to gibibytes for internal use. * * If no suffix is provided, the passed number is assumed to be in gibibytes.  def byteStringAsGb(str: String): Long = { JavaUtils.byteStringAsGb(str) } /** * Convert a Java memory parameter passed to -Xmx (such as 300m or 1g) to a number of mebibytes.  def memoryStringToMb(str: String): Int = { // Convert to bytes, rather than directly to MiB, because when no units are specified the unit // is assumed to be bytes (JavaUtils.byteStringAsBytes(str) / 1024 / 1024).toInt } /** * Convert a quantity in bytes to a human-readable string such as \"4.0 MiB\".  def bytesToString(size: Long): String = bytesToString(BigInt(size)) def bytesToString(size: BigInt): String = { val EiB = 1L << 60 val PiB = 1L << 50 val TiB = 1L << 40 val GiB = 1L << 30 val MiB = 1L << 20 val KiB = 1L << 10 if (size >= BigInt(1L << 11) * EiB) { // The number is too large, show it in scientific notation. BigDecimal(size, new MathContext(3, RoundingMode.HALF_UP)).toString() + \" B\" } else { val (value, unit) = { if (size >= 2 * EiB) { (BigDecimal(size) / EiB, \"EiB\") } else if (size >= 2 * PiB) { (BigDecimal(size) / PiB, \"PiB\") } else if (size >= 2 * TiB) { (BigDecimal(size) / TiB, \"TiB\") } else if (size >= 2 * GiB) { (BigDecimal(size) / GiB, \"GiB\") } else if (size >= 2 * MiB) { (BigDecimal(size) / MiB, \"MiB\") } else if (size >= 2 * KiB) { (BigDecimal(size) / KiB, \"KiB\") } else { (BigDecimal(size), \"B\") } } \"%.1f %s\".formatLocal(Locale.US, value, unit) } } /** * Returns a human-readable string representing a duration such as \"35ms\"  def msDurationToString(ms: Long): String = { val second = 1000 val minute = 60 * second val hour = 60 * minute val locale = Locale.US ms match { case t if t < second => \"%d ms\".formatLocal(locale, t) case t if t < minute => \"%.1f s\".formatLocal(locale, t.toFloat / second) case t if t < hour => \"%.1f m\".formatLocal(locale, t.toFloat / minute) case t => \"%.2f h\".formatLocal(locale, t.toFloat / hour) } } /** * Convert a quantity in megabytes to a human-readable string such as \"4.0 MiB\".  def megabytesToString(megabytes: Long): String = { bytesToString(megabytes * 1024L * 1024L) } /** * Execute a command and return the process running the command.  def executeCommand( command: Seq[String], workingDir: File = new File(\".\"), extraEnvironment: Map[String, String] = Map.empty, redirectStderr: Boolean = true): Process = { val builder = new ProcessBuilder(command: _*).directory(workingDir) val environment = builder.environment() for ((key, value) <- extraEnvironment) { environment.put(key, value) } val process = builder.start() if (redirectStderr) { val threadName = \"redirect stderr for command \" + command(0) def log(s: String): Unit = logInfo(s) processStreamByLine(threadName, process.getErrorStream, log) } process } /** * Execute a command and get its output, throwing an exception if it yields a code other than 0.  def executeAndGetOutput( command: Seq[String], workingDir: File = new File(\".\"), extraEnvironment: Map[String, String] = Map.empty, redirectStderr: Boolean = true): String = { val process = executeCommand(command, workingDir, extraEnvironment, redirectStderr) val output = new StringBuilder val threadName = \"read stdout for \" + command(0) def appendToOutput(s: String): Unit = output.append(s).append(\"\\n\") val stdoutThread = processStreamByLine(threadName, process.getInputStream, appendToOutput) val exitCode = process.waitFor() stdoutThread.join() // Wait for it to finish reading output if (exitCode != 0) { logError(s\"Process $command exited with code $exitCode: $output\") throw new SparkException(s\"Process $command exited with code $exitCode\") } output.toString } /** * Return and start a daemon thread that processes the content of the input stream line by line.  def processStreamByLine( threadName: String, inputStream: InputStream, processLine: String => Unit): Thread = { val t = new Thread(threadName) { override def run(): Unit = { for (line <- Source.fromInputStream(inputStream).getLines()) { processLine(line) } } } t.setDaemon(true) t.start() t } /** * Execute a block of code that evaluates to Unit, forwarding any uncaught exceptions to the * default UncaughtExceptionHandler * * NOTE: This method is to be called by the spark-started JVM process.  def tryOrExit(block: => Unit): Unit = { try { block } catch { case e: ControlThrowable => throw e case t: Throwable => sparkUncaughtExceptionHandler.uncaughtException(t) } } /** * Execute a block of code that evaluates to Unit, stop SparkContext if there is any uncaught * exception * * NOTE: This method is to be called by the driver-side components to avoid stopping the * user-started JVM process completely; in contrast, tryOrExit is to be called in the * spark-started JVM process .  def tryOrStopSparkContext(sc: SparkContext)(block: => Unit): Unit = { try { block } catch { case e: ControlThrowable => throw e case t: Throwable => val currentThreadName = Thread.currentThread().getName if (sc != null) { logError(s\"uncaught error in thread $currentThreadName, stopping SparkContext\", t) sc.stopInNewThread() } if (!NonFatal(t)) { logError(s\"throw uncaught fatal error in thread $currentThreadName\", t) throw t } } } /** * Execute a block of code that returns a value, re-throwing any non-fatal uncaught * exceptions as IOException. This is used when implementing Externalizable and Serializable's * read and write methods, since Java's serializer will not report non-IOExceptions properly; * see SPARK-4080 for more context.  def tryOrIOException[T](block: => T): T = { try { block } catch { case e: IOException => logError(\"Exception encountered\", e) throw e case NonFatal(e) => logError(\"Exception encountered\", e) throw new IOException(e) } } /** Executes the given block. Log non-fatal errors if any, and only throw fatal errors  def tryLogNonFatalError(block: => Unit): Unit = { try { block } catch { case NonFatal(t) => logError(s\"Uncaught exception in thread ${Thread.currentThread().getName}\", t) } } /** * Execute a block of code, then a finally block, but if exceptions happen in * the finally block, do not suppress the original exception. * * This is primarily an issue with `finally { out.close() }` blocks, where * close needs to be called to clean up `out`, but if an exception happened * in `out.write`, it's likely `out` may be corrupted and `out.close` will * fail as well. This would then suppress the original/likely more meaningful * exception from the original `out.write` call.  def tryWithSafeFinally[T](block: => T)(finallyBlock: => Unit): T = { var originalThrowable: Throwable = null try { block } catch { case t: Throwable => // Purposefully not using NonFatal, because even fatal exceptions // we don't want to have our finallyBlock suppress originalThrowable = t throw originalThrowable } finally { try { finallyBlock } catch { case t: Throwable if (originalThrowable != null && originalThrowable != t) => originalThrowable.addSuppressed(t) logWarning(s\"Suppressing exception in finally: ${t.getMessage}\", t) throw originalThrowable } } } /** * Execute a block of code and call the failure callbacks in the catch block. If exceptions occur * in either the catch or the finally block, they are appended to the list of suppressed * exceptions in original exception which is then rethrown. * * This is primarily an issue with `catch { abort() }` or `finally { out.close() }` blocks, * where the abort/close needs to be called to clean up `out`, but if an exception happened * in `out.write`, it's likely `out` may be corrupted and `abort` or `out.close` will * fail as well. This would then suppress the original/likely more meaningful * exception from the original `out.write` call.  def tryWithSafeFinallyAndFailureCallbacks[T](block: => T) (catchBlock: => Unit = (), finallyBlock: => Unit = ()): T = { var originalThrowable: Throwable = null try { block } catch { case cause: Throwable => // Purposefully not using NonFatal, because even fatal exceptions // we don't want to have our finallyBlock suppress originalThrowable = cause try { logError(\"Aborting task\", originalThrowable) if (TaskContext.get() != null) { TaskContext.get().markTaskFailed(originalThrowable) } catchBlock } catch { case t: Throwable => if (originalThrowable != t) { originalThrowable.addSuppressed(t) logWarning(s\"Suppressing exception in catch: ${t.getMessage}\", t) } } throw originalThrowable } finally { try { finallyBlock } catch { case t: Throwable if (originalThrowable != null && originalThrowable != t) => originalThrowable.addSuppressed(t) logWarning(s\"Suppressing exception in finally: ${t.getMessage}\", t) throw originalThrowable } } } // A regular expression to match classes of the internal Spark API's // that we want to skip when finding the call site of a method. private val SPARK_CORE_CLASS_REGEX = \"\"\"^org\\.apache\\.spark(\\.api\\.java)?(\\.util)?(\\.rdd)?(\\.broadcast)?\\.[A-Z]\"\"\".r private val SPARK_SQL_CLASS_REGEX = \"\"\"^org\\.apache\\.spark\\.sql.*\"\"\".r /** Default filtering function for finding call sites using `getCallSite`.  private def sparkInternalExclusionFunction(className: String): Boolean = { val SCALA_CORE_CLASS_PREFIX = \"scala\" val isSparkClass = SPARK_CORE_CLASS_REGEX.findFirstIn(className).isDefined || SPARK_SQL_CLASS_REGEX.findFirstIn(className).isDefined val isScalaClass = className.startsWith(SCALA_CORE_CLASS_PREFIX) // If the class is a Spark internal class or a Scala class, then exclude. isSparkClass || isScalaClass } /** * When called inside a class in the spark package, returns the name of the user code class * (outside the spark package) that called into Spark, as well as which Spark method they called. * This is used, for example, to tell users where in their code each RDD got created. * * @param skipClass Function that is used to exclude non-user-code classes.  def getCallSite(skipClass: String => Boolean = sparkInternalExclusionFunction): CallSite = { // Keep crawling up the stack trace until we find the first function not inside of the spark // package. We track the last (shallowest) contiguous Spark method. This might be an RDD // transformation, a SparkContext function (such as parallelize), or anything else that leads // to instantiation of an RDD. We also track the first (deepest) user method, file, and line. var lastSparkMethod = \"<unknown>\" var firstUserFile = \"<unknown>\" var firstUserLine = 0 var insideSpark = true val callStack = new ArrayBuffer[String]() :+ \"<unknown>\" Thread.currentThread.getStackTrace().foreach { ste: StackTraceElement => // When running under some profilers, the current stack trace might contain some bogus // frames. This is intended to ensure that we don't crash in these situations by // ignoring any frames that we can't examine. if (ste != null && ste.getMethodName != null && !ste.getMethodName.contains(\"getStackTrace\")) { if (insideSpark) { if (skipClass(ste.getClassName)) { lastSparkMethod = if (ste.getMethodName == \"<init>\") { // Spark method is a constructor; get its class name ste.getClassName.substring(ste.getClassName.lastIndexOf('.') + 1) } else { ste.getMethodName } callStack(0) = ste.toString // Put last Spark method on top of the stack trace. } else { if (ste.getFileName != null) { firstUserFile = ste.getFileName if (ste.getLineNumber >= 0) { firstUserLine = ste.getLineNumber } } callStack += ste.toString insideSpark = false } } else { callStack += ste.toString } } } val callStackDepth = System.getProperty(\"spark.callstack.depth\", \"20\").toInt val shortForm = if (firstUserFile == \"HiveSessionImpl.java\") { // To be more user friendly, show a nicer string for queries submitted from the JDBC // server. \"Spark JDBC Server Query\" } else { s\"$lastSparkMethod at $firstUserFile:$firstUserLine\" } val longForm = callStack.take(callStackDepth).mkString(\"\\n\") CallSite(shortForm, longForm) } private var compressedLogFileLengthCache: LoadingCache[String, java.lang.Long] = null private def getCompressedLogFileLengthCache( sparkConf: SparkConf): LoadingCache[String, java.lang.Long] = this.synchronized { if (compressedLogFileLengthCache == null) { val compressedLogFileLengthCacheSize = sparkConf.get( UNCOMPRESSED_LOG_FILE_LENGTH_CACHE_SIZE_CONF) compressedLogFileLengthCache = CacheBuilder.newBuilder() .maximumSize(compressedLogFileLengthCacheSize) .build[String, java.lang.Long](new CacheLoader[String, java.lang.Long]() { override def load(path: String): java.lang.Long = { Utils.getCompressedFileLength(new File(path)) } }) } compressedLogFileLengthCache } /** * Return the file length, if the file is compressed it returns the uncompressed file length. * It also caches the uncompressed file size to avoid repeated decompression. The cache size is * read from workerConf.  def getFileLength(file: File, workConf: SparkConf): Long = { if (file.getName.endsWith(\".gz\")) { getCompressedLogFileLengthCache(workConf).get(file.getAbsolutePath) } else { file.length } } /** Return uncompressed file length of a compressed file.  private def getCompressedFileLength(file: File): Long = { var gzInputStream: GZIPInputStream = null try { // Uncompress .gz file to determine file size. var fileSize = 0L gzInputStream = new GZIPInputStream(new FileInputStream(file)) val bufSize = 1024 val buf = new Array[Byte](bufSize) var numBytes = ByteStreams.read(gzInputStream, buf, 0, bufSize) while (numBytes > 0) { fileSize += numBytes numBytes = ByteStreams.read(gzInputStream, buf, 0, bufSize) } fileSize } catch { case e: Throwable => logError(s\"Cannot get file length of ${file}\", e) throw e } finally { if (gzInputStream != null) { gzInputStream.close() } } } /** Return a string containing part of a file from byte 'start' to 'end'.  def offsetBytes(path: String, length: Long, start: Long, end: Long): String = { val file = new File(path) val effectiveEnd = math.min(length, end) val effectiveStart = math.max(0, start) val buff = new Array[Byte]((effectiveEnd-effectiveStart).toInt) val stream = if (path.endsWith(\".gz\")) { new GZIPInputStream(new FileInputStream(file)) } else { new FileInputStream(file) } try { ByteStreams.skipFully(stream, effectiveStart) ByteStreams.readFully(stream, buff) } finally { stream.close() } Source.fromBytes(buff).mkString } /** * Return a string containing data across a set of files. The `startIndex` * and `endIndex` is based on the cumulative size of all the files take in * the given order. See figure below for more details.  def offsetBytes(files: Seq[File], fileLengths: Seq[Long], start: Long, end: Long): String = { assert(files.length == fileLengths.length) val startIndex = math.max(start, 0) val endIndex = math.min(end, fileLengths.sum) val fileToLength = files.zip(fileLengths).toMap logDebug(\"Log files: \\n\" + fileToLength.mkString(\"\\n\")) val stringBuffer = new StringBuffer((endIndex - startIndex).toInt) var sum = 0L files.zip(fileLengths).foreach { case (file, fileLength) => val startIndexOfFile = sum val endIndexOfFile = sum + fileToLength(file) logDebug(s\"Processing file $file, \" + s\"with start index = $startIndexOfFile, end index = $endIndex\") /* ____________ range 1: | | | case A | files: |==== file 1 ====|====== file 2 ======|===== file 3 =====| | case B . case C . case D | range 2: |___________.____________________.______________|  if (startIndex <= startIndexOfFile && endIndex >= endIndexOfFile) { // Case C: read the whole file stringBuffer.append(offsetBytes(file.getAbsolutePath, fileLength, 0, fileToLength(file))) } else if (startIndex > startIndexOfFile && startIndex < endIndexOfFile) { // Case A and B: read from [start of required range] to [end of file / end of range] val effectiveStartIndex = startIndex - startIndexOfFile val effectiveEndIndex = math.min(endIndex - startIndexOfFile, fileToLength(file)) stringBuffer.append(Utils.offsetBytes( file.getAbsolutePath, fileLength, effectiveStartIndex, effectiveEndIndex)) } else if (endIndex > startIndexOfFile && endIndex < endIndexOfFile) { // Case D: read from [start of file] to [end of require range] val effectiveStartIndex = math.max(startIndex - startIndexOfFile, 0) val effectiveEndIndex = endIndex - startIndexOfFile stringBuffer.append(Utils.offsetBytes( file.getAbsolutePath, fileLength, effectiveStartIndex, effectiveEndIndex)) } sum += fileToLength(file) logDebug(s\"After processing file $file, string built is ${stringBuffer.toString}\") } stringBuffer.toString } /** * Clone an object using a Spark serializer.  def clone[T: ClassTag](value: T, serializer: SerializerInstance): T = { serializer.deserialize[T](serializer.serialize(value)) } private def isSpace(c: Char): Boolean = { \" \\t\\r\\n\".indexOf(c) != -1 } /** * Split a string of potentially quoted arguments from the command line the way that a shell * would do it to determine arguments to a command. For example, if the string is 'a \"b c\" d', * then it would be parsed as three arguments: 'a', 'b c' and 'd'.  def splitCommandString(s: String): Seq[String] = { val buf = new ArrayBuffer[String] var inWord = false var inSingleQuote = false var inDoubleQuote = false val curWord = new StringBuilder def endWord(): Unit = { buf += curWord.toString curWord.clear() } var i = 0 while (i < s.length) { val nextChar = s.charAt(i) if (inDoubleQuote) { if (nextChar == '\"') { inDoubleQuote = false } else if (nextChar == '\\\\') { if (i < s.length - 1) { // Append the next character directly, because only \" and \\ may be escaped in // double quotes after the shell's own expansion curWord.append(s.charAt(i + 1)) i += 1 } } else { curWord.append(nextChar) } } else if (inSingleQuote) { if (nextChar == '\\'') { inSingleQuote = false } else { curWord.append(nextChar) } // Backslashes are not treated specially in single quotes } else if (nextChar == '\"') { inWord = true inDoubleQuote = true } else if (nextChar == '\\'') { inWord = true inSingleQuote = true } else if (!isSpace(nextChar)) { curWord.append(nextChar) inWord = true } else if (inWord && isSpace(nextChar)) { endWord() inWord = false } i += 1 } if (inWord || inDoubleQuote || inSingleQuote) { endWord() } buf.toSeq } /* Calculates 'x' modulo 'mod', takes to consideration sign of x, * i.e. if 'x' is negative, than 'x' % 'mod' is negative too * so function return (x % mod) + mod in that case.  def nonNegativeMod(x: Int, mod: Int): Int = { val rawMod = x % mod rawMod + (if (rawMod < 0) mod else 0) } // Handles idiosyncrasies with hash (add more as required) // This method should be kept in sync with // org.apache.spark.network.util.JavaUtils#nonNegativeHash(). def nonNegativeHash(obj: AnyRef): Int = { // Required ? if (obj eq null) return 0 val hash = obj.hashCode // math.abs fails for Int.MinValue val hashAbs = if (Int.MinValue != hash) math.abs(hash) else 0 // Nothing else to guard against ? hashAbs } /** * Returns the system properties map that is thread-safe to iterator over. It gets the * properties which have been set explicitly, as well as those for which only a default value * has been defined.  def getSystemProperties: Map[String, String] = { System.getProperties.stringPropertyNames().asScala .map(key => (key, System.getProperty(key))).toMap } /** * Method executed for repeating a task for side effects. * Unlike a for comprehension, it permits JVM JIT optimization  def times(numIters: Int)(f: => Unit): Unit = { var i = 0 while (i < numIters) { f i += 1 } } /** * Timing method based on iterations that permit JVM JIT optimization. * * @param numIters number of iterations * @param f function to be executed. If prepare is not None, the running time of each call to f * must be an order of magnitude longer than one nanosecond for accurate timing. * @param prepare function to be executed before each call to f. Its running time doesn't count. * @return the total time across all iterations (not counting preparation time) in nanoseconds.  def timeIt(numIters: Int)(f: => Unit, prepare: Option[() => Unit] = None): Long = { if (prepare.isEmpty) { val startNs = System.nanoTime() times(numIters)(f) System.nanoTime() - startNs } else { var i = 0 var sum = 0L while (i < numIters) { prepare.get.apply() val startNs = System.nanoTime() f sum += System.nanoTime() - startNs i += 1 } sum } } /** * Counts the number of elements of an iterator using a while loop rather than calling * [[scala.collection.Iterator#size]] because it uses a for loop, which is slightly slower * in the current version of Scala.  def getIteratorSize(iterator: Iterator[_]): Long = { var count = 0L while (iterator.hasNext) { count += 1L iterator.next() } count } /** * Generate a zipWithIndex iterator, avoid index value overflowing problem * in scala's zipWithIndex  def getIteratorZipWithIndex[T](iter: Iterator[T], startIndex: Long): Iterator[(T, Long)] = { new Iterator[(T, Long)] { require(startIndex >= 0, \"startIndex should be >= 0.\") var index: Long = startIndex - 1L def hasNext: Boolean = iter.hasNext def next(): (T, Long) = { index += 1L (iter.next(), index) } } } /** * Creates a symlink. * * @param src absolute path to the source * @param dst relative path for the destination  def symlink(src: File, dst: File): Unit = { if (!src.isAbsolute()) { throw new IOException(\"Source must be absolute\") } if (dst.isAbsolute()) { throw new IOException(\"Destination must be relative\") } Files.createSymbolicLink(dst.toPath, src.toPath) } /** Return the class name of the given object, removing all dollar signs  def getFormattedClassName(obj: AnyRef): String = { getSimpleName(obj.getClass).replace(\"$\", \"\") } /** * Return a Hadoop FileSystem with the scheme encoded in the given path.  def getHadoopFileSystem(path: URI, conf: Configuration): FileSystem = { FileSystem.get(path, conf) } /** * Return a Hadoop FileSystem with the scheme encoded in the given path.  def getHadoopFileSystem(path: String, conf: Configuration): FileSystem = { getHadoopFileSystem(new URI(path), conf) } /** * Whether the underlying operating system is Windows.  val isWindows = SystemUtils.IS_OS_WINDOWS /** * Whether the underlying operating system is Mac OS X.  val isMac = SystemUtils.IS_OS_MAC_OSX /** * Whether the underlying operating system is Mac OS X and processor is Apple Silicon.  val isMacOnAppleSilicon = SystemUtils.IS_OS_MAC_OSX && SystemUtils.OS_ARCH.equals(\"aarch64\") /** * Pattern for matching a Windows drive, which contains only a single alphabet character.  val windowsDrive = \"([a-zA-Z])\".r /** * Indicates whether Spark is currently running unit tests.  def isTesting: Boolean = { // Scala's `sys.env` creates a ton of garbage by constructing Scala immutable maps, so // we directly use the Java APIs instead. System.getenv(\"SPARK_TESTING\") != null || System.getProperty(IS_TESTING.key) != null } /** * Terminates a process waiting for at most the specified duration. * * @return the process exit value if it was successfully terminated, else None  def terminateProcess(process: Process, timeoutMs: Long): Option[Int] = { // Politely destroy first process.destroy() if (process.waitFor(timeoutMs, TimeUnit.MILLISECONDS)) { // Successful exit Option(process.exitValue()) } else { try { process.destroyForcibly() } catch { case NonFatal(e) => logWarning(\"Exception when attempting to kill process\", e) } // Wait, again, although this really should return almost immediately if (process.waitFor(timeoutMs, TimeUnit.MILLISECONDS)) { Option(process.exitValue()) } else { logWarning(\"Timed out waiting to forcibly kill process\") None } } } /** * Return the stderr of a process after waiting for the process to terminate. * If the process does not terminate within the specified timeout, return None.  def getStderr(process: Process, timeoutMs: Long): Option[String] = { val terminated = process.waitFor(timeoutMs, TimeUnit.MILLISECONDS) if (terminated) { Some(Source.fromInputStream(process.getErrorStream).getLines().mkString(\"\\n\")) } else { None } } /** * Execute the given block, logging and re-throwing any uncaught exception. * This is particularly useful for wrapping code that runs in a thread, to ensure * that exceptions are printed, and to avoid having to catch Throwable.  def logUncaughtExceptions[T](f: => T): T = { try { f } catch { case ct: ControlThrowable => throw ct case t: Throwable => logError(s\"Uncaught exception in thread ${Thread.currentThread().getName}\", t) throw t } } /** Executes the given block in a Try, logging any uncaught exceptions.  def tryLog[T](f: => T): Try[T] = { try { val res = f scala.util.Success(res) } catch { case ct: ControlThrowable => throw ct case t: Throwable => logError(s\"Uncaught exception in thread ${Thread.currentThread().getName}\", t) scala.util.Failure(t) } } /** Returns true if the given exception was fatal. See docs for scala.util.control.NonFatal.  def isFatalError(e: Throwable): Boolean = { e match { case NonFatal(_) | _: InterruptedException | _: NotImplementedError | _: ControlThrowable | _: LinkageError => false case _ => true } } /** * Return a well-formed URI for the file described by a user input string. * * If the supplied path does not contain a scheme, or is a relative path, it will be * converted into an absolute path with a file:// scheme.  def resolveURI(path: String): URI = { try { val uri = new URI(path) if (uri.getScheme() != null) { return uri } // make sure to handle if the path has a fragment (applies to yarn // distributed cache) if (uri.getFragment() != null) { val absoluteURI = new File(uri.getPath()).getAbsoluteFile().toURI() return new URI(absoluteURI.getScheme(), absoluteURI.getHost(), absoluteURI.getPath(), uri.getFragment()) } } catch { case e: URISyntaxException => } new File(path).getCanonicalFile().toURI() } /** Resolve a comma-separated list of paths.  def resolveURIs(paths: String): String = { if (paths == null || paths.trim.isEmpty) { \"\" } else { paths.split(\",\").filter(_.trim.nonEmpty).map { p => Utils.resolveURI(p) }.mkString(\",\") } } /** Check whether a path is an absolute URI.  def isAbsoluteURI(path: String): Boolean = { try { val uri = new URI(path: String) uri.isAbsolute } catch { case _: URISyntaxException => false } } /** Return all non-local paths from a comma-separated list of paths.  def nonLocalPaths(paths: String, testWindows: Boolean = false): Array[String] = { val windows = isWindows || testWindows if (paths == null || paths.trim.isEmpty) { Array.empty } else { paths.split(\",\").filter { p => val uri = resolveURI(p) Option(uri.getScheme).getOrElse(\"file\") match { case windowsDrive(d) if windows => false case \"local\" | \"file\" => false case _ => true } } } } /** * Load default Spark properties from the given file. If no file is provided, * use the common defaults file. This mutates state in the given SparkConf and * in this JVM's system properties if the config specified in the file is not * already set. Return the path of the properties file used.  def loadDefaultSparkProperties(conf: SparkConf, filePath: String = null): String = { val path = Option(filePath).getOrElse(getDefaultPropertiesFile()) Option(path).foreach { confFile => getPropertiesFromFile(confFile).filter { case (k, v) => k.startsWith(\"spark.\") }.foreach { case (k, v) => conf.setIfMissing(k, v) sys.props.getOrElseUpdate(k, v) } } path } /** * Updates Spark config with properties from a set of Properties. * Provided properties have the highest priority.  def updateSparkConfigFromProperties( conf: SparkConf, properties: Map[String, String]) : Unit = { properties.filter { case (k, v) => k.startsWith(\"spark.\") }.foreach { case (k, v) => conf.set(k, v) } } /** * Implements the same logic as JDK `java.lang.String#trim` by removing leading and trailing * non-printable characters less or equal to '\\u0020' (SPACE) but preserves natural line * delimiters according to [[java.util.Properties]] load method. The natural line delimiters are * removed by JDK during load. Therefore any remaining ones have been specifically provided and * escaped by the user, and must not be ignored * * @param str * @return the trimmed value of str  private[util] def trimExceptCRLF(str: String): String = { val nonSpaceOrNaturalLineDelimiter: Char => Boolean = { ch => ch > ' ' || ch == '\\r' || ch == '\\n' } val firstPos = str.indexWhere(nonSpaceOrNaturalLineDelimiter) val lastPos = str.lastIndexWhere(nonSpaceOrNaturalLineDelimiter) if (firstPos >= 0 && lastPos >= 0) { str.substring(firstPos, lastPos + 1) } else { \"\" } } /** Load properties present in the given file.  def getPropertiesFromFile(filename: String): Map[String, String] = { val file = new File(filename) require(file.exists(), s\"Properties file $file does not exist\") require(file.isFile(), s\"Properties file $file is not a normal file\") val inReader = new InputStreamReader(new FileInputStream(file), StandardCharsets.UTF_8) try { val properties = new Properties() properties.load(inReader) properties.stringPropertyNames().asScala .map { k => (k, trimExceptCRLF(properties.getProperty(k))) } .toMap } catch { case e: IOException => throw new SparkException(s\"Failed when loading Spark properties from $filename\", e) } finally { inReader.close() } } /** Return the path of the default Spark properties file.  def getDefaultPropertiesFile(env: Map[String, String] = sys.env): String = { env.get(\"SPARK_CONF_DIR\") .orElse(env.get(\"SPARK_HOME\").map { t => s\"$t${File.separator}conf\" }) .map { t => new File(s\"$t${File.separator}spark-defaults.conf\")} .filter(_.isFile) .map(_.getAbsolutePath) .orNull } /** * Return a nice string representation of the exception. It will call \"printStackTrace\" to * recursively generate the stack trace including the exception and its causes.  def exceptionString(e: Throwable): String = { if (e == null) { \"\" } else { // Use e.printStackTrace here because e.getStackTrace doesn't include the cause val stringWriter = new StringWriter() e.printStackTrace(new PrintWriter(stringWriter)) stringWriter.toString } } private implicit class Lock(lock: LockInfo) { def lockString: String = { lock match { case monitor: MonitorInfo => s\"Monitor(${lock.getClassName}@${lock.getIdentityHashCode}})\" case _ => s\"Lock(${lock.getClassName}@${lock.getIdentityHashCode}})\" } } } /** Return a thread dump of all threads' stacktraces. Used to capture dumps for the web UI  def getThreadDump(): Array[ThreadStackTrace] = { // We need to filter out null values here because dumpAllThreads() may return null array // elements for threads that are dead / don't exist. val threadInfos = ManagementFactory.getThreadMXBean.dumpAllThreads(true, true).filter(_ != null) threadInfos.sortWith { case (threadTrace1, threadTrace2) => val v1 = if (threadTrace1.getThreadName.contains(\"Executor task launch\")) 1 else 0 val v2 = if (threadTrace2.getThreadName.contains(\"Executor task launch\")) 1 else 0 if (v1 == v2) { val name1 = threadTrace1.getThreadName().toLowerCase(Locale.ROOT) val name2 = threadTrace2.getThreadName().toLowerCase(Locale.ROOT) val nameCmpRes = name1.compareTo(name2) if (nameCmpRes == 0) { threadTrace1.getThreadId < threadTrace2.getThreadId } else { nameCmpRes < 0 } } else { v1 > v2 } }.map(threadInfoToThreadStackTrace) } def getThreadDumpForThread(threadId: Long): Option[ThreadStackTrace] = { if (threadId <= 0) { None } else { // The Int.MaxValue here requests the entire untruncated stack trace of the thread: val threadInfo = Option(ManagementFactory.getThreadMXBean.getThreadInfo(threadId, Int.MaxValue)) threadInfo.map(threadInfoToThreadStackTrace) } } private def threadInfoToThreadStackTrace(threadInfo: ThreadInfo): ThreadStackTrace = { val monitors = threadInfo.getLockedMonitors.map(m => m.getLockedStackFrame -> m).toMap val stackTrace = StackTrace(threadInfo.getStackTrace.map { frame => monitors.get(frame) match { case Some(monitor) => monitor.getLockedStackFrame.toString + s\" => holding ${monitor.lockString}\" case None => frame.toString } }) // use a set to dedup re-entrant locks that are held at multiple places val heldLocks = (threadInfo.getLockedSynchronizers ++ threadInfo.getLockedMonitors).map(_.lockString).toSet ThreadStackTrace( threadId = threadInfo.getThreadId, threadName = threadInfo.getThreadName, threadState = threadInfo.getThreadState, stackTrace = stackTrace, blockedByThreadId = if (threadInfo.getLockOwnerId < 0) None else Some(threadInfo.getLockOwnerId), blockedByLock = Option(threadInfo.getLockInfo).map(_.lockString).getOrElse(\"\"), holdingLocks = heldLocks.toSeq) } /** * Convert all spark properties set in the given SparkConf to a sequence of java options.  def sparkJavaOpts(conf: SparkConf, filterKey: (String => Boolean) = _ => true): Seq[String] = { conf.getAll .filter { case (k, _) => filterKey(k) } .map { case (k, v) => s\"-D$k=$v\" } } /** * Maximum number of retries when binding to a port before giving up.  def portMaxRetries(conf: SparkConf): Int = { val maxRetries = conf.getOption(\"spark.port.maxRetries\").map(_.toInt) if (conf.contains(IS_TESTING)) { // Set a higher number of retries for tests... maxRetries.getOrElse(100) } else { maxRetries.getOrElse(16) } } /** * Returns the user port to try when trying to bind a service. Handles wrapping and skipping * privileged ports.  def userPort(base: Int, offset: Int): Int = { (base + offset - 1024) % (65536 - 1024) + 1024 } /** * Attempt to start a service on the given port, or fail after a number of attempts. * Each subsequent attempt uses 1 + the port used in the previous attempt (unless the port is 0). * * @param startPort The initial port to start the service on. * @param startService Function to start service on a given port. * This is expected to throw java.net.BindException on port collision. * @param conf A SparkConf used to get the maximum number of retries when binding to a port. * @param serviceName Name of the service. * @return (service: T, port: Int)  def startServiceOnPort[T]( startPort: Int, startService: Int => (T, Int), conf: SparkConf, serviceName: String = \"\"): (T, Int) = { require(startPort == 0 || (1024 <= startPort && startPort < 65536), \"startPort should be between 1024 and 65535 (inclusive), or 0 for a random free port.\") val serviceString = if (serviceName.isEmpty) \"\" else s\" '$serviceName'\" val maxRetries = portMaxRetries(conf) for (offset <- 0 to maxRetries) { // Do not increment port if startPort is 0, which is treated as a special port val tryPort = if (startPort == 0) { startPort } else { userPort(startPort, offset) } try { val (service, port) = startService(tryPort) logInfo(s\"Successfully started service$serviceString on port $port.\") return (service, port) } catch { case e: Exception if isBindCollision(e) => if (offset >= maxRetries) { val exceptionMessage = if (startPort == 0) { s\"${e.getMessage}: Service$serviceString failed after \" + s\"$maxRetries retries (on a random free port)! \" + s\"Consider explicitly setting the appropriate binding address for \" + s\"the service$serviceString (for example ${DRIVER_BIND_ADDRESS.key} \" + s\"for SparkDriver) to the correct binding address.\" } else { s\"${e.getMessage}: Service$serviceString failed after \" + s\"$maxRetries retries (starting from $startPort)! Consider explicitly setting \" + s\"the appropriate port for the service$serviceString (for example spark.ui.port \" + s\"for SparkUI) to an available port or increasing spark.port.maxRetries.\" } val exception = new BindException(exceptionMessage) // restore original stack trace exception.setStackTrace(e.getStackTrace) throw exception } if (startPort == 0) { // As startPort 0 is for a random free port, it is most possibly binding address is // not correct. logWarning(s\"Service$serviceString could not bind on a random free port. \" + \"You may check whether configuring an appropriate binding address.\") } else { logWarning(s\"Service$serviceString could not bind on port $tryPort. \" + s\"Attempting port ${tryPort + 1}.\") } } } // Should never happen throw new SparkException(s\"Failed to start service$serviceString on port $startPort\") } /** * Return whether the exception is caused by an address-port collision when binding.  def isBindCollision(exception: Throwable): Boolean = { exception match { case e: BindException => if (e.getMessage != null) { return true } isBindCollision(e.getCause) case e: MultiException => e.getThrowables.asScala.exists(isBindCollision) case e: NativeIoException => (e.getMessage != null && e.getMessage.startsWith(\"bind() failed: \")) || isBindCollision(e.getCause) case e: Exception => isBindCollision(e.getCause) case _ => false } } /** * configure a new log4j level  def setLogLevel(l: Level): Unit = { val ctx = LogManager.getContext(false).asInstanceOf[LoggerContext] val config = ctx.getConfiguration() val loggerConfig = config.getLoggerConfig(LogManager.ROOT_LOGGER_NAME) loggerConfig.setLevel(l) ctx.updateLoggers() // Setting threshold to null as rootLevel will define log level for spark-shell Logging.sparkShellThresholdLevel = null } /** * Return the current system LD_LIBRARY_PATH name  def libraryPathEnvName: String = { if (isWindows) { \"PATH\" } else if (isMac) { \"DYLD_LIBRARY_PATH\" } else { \"LD_LIBRARY_PATH\" } } /** * Return the prefix of a command that appends the given library paths to the * system-specific library path environment variable. On Unix, for instance, * this returns the string LD_LIBRARY_PATH=\"path1:path2:$LD_LIBRARY_PATH\".  def libraryPathEnvPrefix(libraryPaths: Seq[String]): String = { val libraryPathScriptVar = if (isWindows) { s\"%${libraryPathEnvName}%\" } else { \"$\" + libraryPathEnvName } val libraryPath = (libraryPaths :+ libraryPathScriptVar).mkString(\"\\\"\", File.pathSeparator, \"\\\"\") val ampersand = if (Utils.isWindows) { \" &\" } else { \"\" } s\"$libraryPathEnvName=$libraryPath$ampersand\" } /** * Return the value of a config either through the SparkConf or the Hadoop configuration. * We Check whether the key is set in the SparkConf before look at any Hadoop configuration. * If the key is set in SparkConf, no matter whether it is running on YARN or not, * gets the value from SparkConf. * Only when the key is not set in SparkConf and running on YARN, * gets the value from Hadoop configuration.  def getSparkOrYarnConfig(conf: SparkConf, key: String, default: String): String = { if (conf.contains(key)) { conf.get(key, default) } else if (conf.get(SparkLauncher.SPARK_MASTER, null) == \"yarn\") { new YarnConfiguration(SparkHadoopUtil.get.newConfiguration(conf)).get(key, default) } else { default } } /** * Return a pair of host and port extracted from the `sparkUrl`. * * A spark url (`spark://host:port`) is a special URI that its scheme is `spark` and only contains * host and port. * * @throws org.apache.spark.SparkException if sparkUrl is invalid.  @throws(classOf[SparkException]) def extractHostPortFromSparkUrl(sparkUrl: String): (String, Int) = { try { val uri = new java.net.URI(sparkUrl) val host = uri.getHost val port = uri.getPort if (uri.getScheme != \"spark\" || host == null || port < 0 || (uri.getPath != null && !uri.getPath.isEmpty) || // uri.getPath returns \"\" instead of null uri.getFragment != null || uri.getQuery != null || uri.getUserInfo != null) { throw new SparkException(\"Invalid master URL: \" + sparkUrl) } (host, port) } catch { case e: java.net.URISyntaxException => throw new SparkException(\"Invalid master URL: \" + sparkUrl, e) } } /** * Returns the current user name. This is the currently logged in user, unless that's been * overridden by the `SPARK_USER` environment variable.  def getCurrentUserName(): String = { Option(System.getenv(\"SPARK_USER\")) .getOrElse(UserGroupInformation.getCurrentUser().getShortUserName()) } val EMPTY_USER_GROUPS = Set.empty[String] // Returns the groups to which the current user belongs. def getCurrentUserGroups(sparkConf: SparkConf, username: String): Set[String] = { val groupProviderClassName = sparkConf.get(USER_GROUPS_MAPPING) if (groupProviderClassName != \"\") { try { val groupMappingServiceProvider = classForName(groupProviderClassName). getConstructor().newInstance(). asInstanceOf[org.apache.spark.security.GroupMappingServiceProvider] val currentUserGroups = groupMappingServiceProvider.getGroups(username) return currentUserGroups } catch { case e: Exception => logError(s\"Error getting groups for user=$username\", e) } } EMPTY_USER_GROUPS } /** * Split the comma delimited string of master URLs into a list. * For instance, \"spark://abc,def\" becomes [spark://abc, spark://def].  def parseStandaloneMasterUrls(masterUrls: String): Array[String] = { masterUrls.stripPrefix(\"spark://\").split(\",\").map(\"spark://\" + _) } /** An identifier that backup masters use in their responses.  val BACKUP_STANDALONE_MASTER_PREFIX = \"Current state is not alive\" /** Return true if the response message is sent from a backup Master on standby.  def responseFromBackup(msg: String): Boolean = { msg.startsWith(BACKUP_STANDALONE_MASTER_PREFIX) } /** * To avoid calling `Utils.getCallSite` for every single RDD we create in the body, * set a dummy call site that RDDs use instead. This is for performance optimization.  def withDummyCallSite[T](sc: SparkContext)(body: => T): T = { val oldShortCallSite = sc.getLocalProperty(CallSite.SHORT_FORM) val oldLongCallSite = sc.getLocalProperty(CallSite.LONG_FORM) try { sc.setLocalProperty(CallSite.SHORT_FORM, \"\") sc.setLocalProperty(CallSite.LONG_FORM, \"\") body } finally { // Restore the old ones here sc.setLocalProperty(CallSite.SHORT_FORM, oldShortCallSite) sc.setLocalProperty(CallSite.LONG_FORM, oldLongCallSite) } } /** * Return whether the specified file is a parent directory of the child file.  @tailrec def isInDirectory(parent: File, child: File): Boolean = { if (child == null || parent == null) { return false } if (!child.exists() || !parent.exists() || !parent.isDirectory()) { return false } if (parent.equals(child)) { return true } isInDirectory(parent, child.getParentFile) } /** * * @return whether it is local mode  def isLocalMaster(conf: SparkConf): Boolean = { val master = conf.get(\"spark.master\", \"\") master == \"local\" || master.startsWith(\"local[\") } /** * Push based shuffle can only be enabled when below conditions are met: * - the application is submitted to run in YARN mode * - external shuffle service enabled * - IO encryption disabled * - serializer(such as KryoSerializer) supports relocation of serialized objects  def isPushBasedShuffleEnabled(conf: SparkConf, isDriver: Boolean, checkSerializer: Boolean = true): Boolean = { val pushBasedShuffleEnabled = conf.get(PUSH_BASED_SHUFFLE_ENABLED) if (pushBasedShuffleEnabled) { val canDoPushBasedShuffle = { val isTesting = conf.get(IS_TESTING).getOrElse(false) val isShuffleServiceAndYarn = conf.get(SHUFFLE_SERVICE_ENABLED) && conf.get(SparkLauncher.SPARK_MASTER, null) == \"yarn\" lazy val serializerIsSupported = { if (checkSerializer) { Option(SparkEnv.get) .map(_.serializer) .filter(_ != null) .getOrElse(instantiateSerializerFromConf[Serializer](SERIALIZER, conf, isDriver)) .supportsRelocationOfSerializedObjects } else { // if no need to check Serializer, always set serializerIsSupported as true true } } // TODO: [SPARK-36744] needs to support IO encryption for push-based shuffle val ioEncryptionDisabled = !conf.get(IO_ENCRYPTION_ENABLED) (isShuffleServiceAndYarn || isTesting) && ioEncryptionDisabled && serializerIsSupported } if (!canDoPushBasedShuffle) { logWarning(\"Push-based shuffle can only be enabled when the application is submitted \" + \"to run in YARN mode, with external shuffle service enabled, IO encryption disabled, \" + \"and relocation of serialized objects supported.\") } canDoPushBasedShuffle } else { false } } // Create an instance of Serializer or ShuffleManager with the given name, // possibly initializing it with our conf def instantiateSerializerOrShuffleManager[T](className: String, conf: SparkConf, isDriver: Boolean): T = { val cls = Utils.classForName(className) // Look for a constructor taking a SparkConf and a boolean isDriver, then one taking just // SparkConf, then one taking no arguments try { cls.getConstructor(classOf[SparkConf], java.lang.Boolean.TYPE) .newInstance(conf, java.lang.Boolean.valueOf(isDriver)) .asInstanceOf[T] } catch { case _: NoSuchMethodException => try { cls.getConstructor(classOf[SparkConf]).newInstance(conf).asInstanceOf[T] } catch { case _: NoSuchMethodException => cls.getConstructor().newInstance().asInstanceOf[T] } } } // Create an instance of Serializer named by the given SparkConf property // if the property is not set, possibly initializing it with our conf def instantiateSerializerFromConf[T](propertyName: ConfigEntry[String], conf: SparkConf, isDriver: Boolean): T = { instantiateSerializerOrShuffleManager[T]( conf.get(propertyName), conf, isDriver) } /** * Return whether dynamic allocation is enabled in the given conf.  def isDynamicAllocationEnabled(conf: SparkConf): Boolean = { val dynamicAllocationEnabled = conf.get(DYN_ALLOCATION_ENABLED) dynamicAllocationEnabled && (!isLocalMaster(conf) || conf.get(DYN_ALLOCATION_TESTING)) } def isStreamingDynamicAllocationEnabled(conf: SparkConf): Boolean = { val streamingDynamicAllocationEnabled = conf.get(STREAMING_DYN_ALLOCATION_ENABLED) streamingDynamicAllocationEnabled && (!isLocalMaster(conf) || conf.get(STREAMING_DYN_ALLOCATION_TESTING)) } /** * Return the initial number of executors for dynamic allocation.  def getDynamicAllocationInitialExecutors(conf: SparkConf): Int = { if (conf.get(DYN_ALLOCATION_INITIAL_EXECUTORS) < conf.get(DYN_ALLOCATION_MIN_EXECUTORS)) { logWarning(s\"${DYN_ALLOCATION_INITIAL_EXECUTORS.key} less than \" + s\"${DYN_ALLOCATION_MIN_EXECUTORS.key} is invalid, ignoring its setting, \" + \"please update your configs.\") } if (conf.get(EXECUTOR_INSTANCES).getOrElse(0) < conf.get(DYN_ALLOCATION_MIN_EXECUTORS)) { logWarning(s\"${EXECUTOR_INSTANCES.key} less than \" + s\"${DYN_ALLOCATION_MIN_EXECUTORS.key} is invalid, ignoring its setting, \" + \"please update your configs.\") } val initialExecutors = Seq( conf.get(DYN_ALLOCATION_MIN_EXECUTORS), conf.get(DYN_ALLOCATION_INITIAL_EXECUTORS), conf.get(EXECUTOR_INSTANCES).getOrElse(0)).max logInfo(s\"Using initial executors = $initialExecutors, max of \" + s\"${DYN_ALLOCATION_INITIAL_EXECUTORS.key}, ${DYN_ALLOCATION_MIN_EXECUTORS.key} and \" + s\"${EXECUTOR_INSTANCES.key}\") initialExecutors } def tryWithResource[R <: Closeable, T](createResource: => R)(f: R => T): T = { val resource = createResource try f.apply(resource) finally resource.close() } /** * Returns a path of temporary file which is in the same directory with `path`.  def tempFileWith(path: File): File = { new File(path.getAbsolutePath + \".\" + UUID.randomUUID()) } /** * Returns the name of this JVM process. This is OS dependent but typically (OSX, Linux, Windows), * this is formatted as PID@hostname.  def getProcessName(): String = { ManagementFactory.getRuntimeMXBean().getName() } /** * Utility function that should be called early in `main()` for daemons to set up some common * diagnostic state.  def initDaemon(log: Logger): Unit = { log.info(s\"Started daemon with process name: ${Utils.getProcessName()}\") SignalUtils.registerLogger(log) } /** * Return the jar files pointed by the \"spark.jars\" property. Spark internally will distribute * these jars through file server. In the YARN mode, it will return an empty list, since YARN * has its own mechanism to distribute jars.  def getUserJars(conf: SparkConf): Seq[String] = { conf.get(JARS).filter(_.nonEmpty) } /** * Return the local jar files which will be added to REPL's classpath. These jar files are * specified by --jars (spark.jars) or --packages, remote jars will be downloaded to local by * SparkSubmit at first.  def getLocalUserJarsForShell(conf: SparkConf): Seq[String] = { val localJars = conf.getOption(\"spark.repl.local.jars\") localJars.map(_.split(\",\")).map(_.filter(_.nonEmpty)).toSeq.flatten } private[spark] val REDACTION_REPLACEMENT_TEXT = \"*********(redacted)\" /** * Redact the sensitive values in the given map. If a map key matches the redaction pattern then * its value is replaced with a dummy text.  def redact(conf: SparkConf, kvs: Seq[(String, String)]): Seq[(String, String)] = { val redactionPattern = conf.get(SECRET_REDACTION_PATTERN) redact(redactionPattern, kvs) } /** * Redact the sensitive values in the given map. If a map key matches the redaction pattern then * its value is replaced with a dummy text.  def redact[K, V](regex: Option[Regex], kvs: Seq[(K, V)]): Seq[(K, V)] = { regex match { case None => kvs case Some(r) => redact(r, kvs) } } /** * Redact the sensitive information in the given string.  def redact(regex: Option[Regex], text: String): String = { regex match { case None => text case Some(r) => if (text == null || text.isEmpty) { text } else { r.replaceAllIn(text, REDACTION_REPLACEMENT_TEXT) } } } private def redact[K, V](redactionPattern: Regex, kvs: Seq[(K, V)]): Seq[(K, V)] = { // If the sensitive information regex matches with either the key or the value, redact the value // While the original intent was to only redact the value if the key matched with the regex, // we've found that especially in verbose mode, the value of the property may contain sensitive // information like so: // \"sun.java.command\":\"org.apache.spark.deploy.SparkSubmit ... \\ // --conf spark.executorEnv.HADOOP_CREDSTORE_PASSWORD=secret_password ... // // And, in such cases, simply searching for the sensitive information regex in the key name is // not sufficient. The values themselves have to be searched as well and redacted if matched. // This does mean we may be accounting more false positives - for example, if the value of an // arbitrary property contained the term 'password', we may redact the value from the UI and // logs. In order to work around it, user would have to make the spark.redaction.regex property // more specific. kvs.map { case (key: String, value: String) => redactionPattern.findFirstIn(key) .orElse(redactionPattern.findFirstIn(value)) .map { _ => (key, REDACTION_REPLACEMENT_TEXT) } .getOrElse((key, value)) case (key, value: String) => redactionPattern.findFirstIn(value) .map { _ => (key, REDACTION_REPLACEMENT_TEXT) } .getOrElse((key, value)) case (key, value) => (key, value) }.asInstanceOf[Seq[(K, V)]] } /** * Looks up the redaction regex from within the key value pairs and uses it to redact the rest * of the key value pairs. No care is taken to make sure the redaction property itself is not * redacted. So theoretically, the property itself could be configured to redact its own value * when printing.  def redact(kvs: Map[String, String]): Seq[(String, String)] = { val redactionPattern = kvs.getOrElse( SECRET_REDACTION_PATTERN.key, SECRET_REDACTION_PATTERN.defaultValueString ).r redact(redactionPattern, kvs.toArray) } def redactCommandLineArgs(conf: SparkConf, commands: Seq[String]): Seq[String] = { val redactionPattern = conf.get(SECRET_REDACTION_PATTERN) commands.map { case PATTERN_FOR_COMMAND_LINE_ARG(key, value) => val (_, newValue) = redact(redactionPattern, Seq((key, value))).head s\"-D$key=$newValue\" case cmd => cmd } } def stringToSeq(str: String): Seq[String] = { str.split(\",\").map(_.trim()).filter(_.nonEmpty) } /** * Create instances of extension classes. * * The classes in the given list must: * - Be sub-classes of the given base class. * - Provide either a no-arg constructor, or a 1-arg constructor that takes a SparkConf. * * The constructors are allowed to throw \"UnsupportedOperationException\" if the extension does not * want to be registered; this allows the implementations to check the Spark configuration (or * other state) and decide they do not need to be added. A log message is printed in that case. * Other exceptions are bubbled up.  def loadExtensions[T <: AnyRef]( extClass: Class[T], classes: Seq[String], conf: SparkConf): Seq[T] = { classes.flatMap { name => try { val klass = classForName[T](name) require(extClass.isAssignableFrom(klass), s\"$name is not a subclass of ${extClass.getName()}.\") val ext = Try(klass.getConstructor(classOf[SparkConf])) match { case Success(ctor) => ctor.newInstance(conf) case Failure(_) => klass.getConstructor().newInstance() } Some(ext) } catch { case _: NoSuchMethodException => throw new SparkException( s\"$name did not have a zero-argument constructor or a\" + \" single-argument constructor that accepts SparkConf. Note: if the class is\" + \" defined inside of another Scala class, then its constructors may accept an\" + \" implicit parameter that references the enclosing class; in this case, you must\" + \" define the class as a top-level class in order to prevent this extra\" + \" parameter from breaking Spark's ability to find a valid constructor.\") case e: InvocationTargetException => e.getCause() match { case uoe: UnsupportedOperationException => logDebug(s\"Extension $name not being initialized.\", uoe) logInfo(s\"Extension $name not being initialized.\") None case null => throw e case cause => throw cause } } } } /** * Check the validity of the given Kubernetes master URL and return the resolved URL. Prefix * \"k8s://\" is appended to the resolved URL as the prefix is used by KubernetesClusterManager * in canCreate to determine if the KubernetesClusterManager should be used.  def checkAndGetK8sMasterUrl(rawMasterURL: String): String = { require(rawMasterURL.startsWith(\"k8s://\"), \"Kubernetes master URL must start with k8s://.\") val masterWithoutK8sPrefix = rawMasterURL.substring(\"k8s://\".length) // To handle master URLs, e.g., k8s://host:port. if (!masterWithoutK8sPrefix.contains(\"://\")) { val resolvedURL = s\"https://$masterWithoutK8sPrefix\" logInfo(\"No scheme specified for kubernetes master URL, so defaulting to https. Resolved \" + s\"URL is $resolvedURL.\") return s\"k8s://$resolvedURL\" } val masterScheme = new URI(masterWithoutK8sPrefix).getScheme val resolvedURL = Option(masterScheme).map(_.toLowerCase(Locale.ROOT)) match { case Some(\"https\") => masterWithoutK8sPrefix case Some(\"http\") => logWarning(\"Kubernetes master URL uses HTTP instead of HTTPS.\") masterWithoutK8sPrefix case _ => throw new IllegalArgumentException(\"Invalid Kubernetes master scheme: \" + masterScheme + \" found in URL: \" + masterWithoutK8sPrefix) } s\"k8s://$resolvedURL\" } /** * Replaces all the {{EXECUTOR_ID}} occurrences with the Executor Id * and {{APP_ID}} occurrences with the App Id.  def substituteAppNExecIds(opt: String, appId: String, execId: String): String = { opt.replace(\"{{APP_ID}}\", appId).replace(\"{{EXECUTOR_ID}}\", execId) } /** * Replaces all the {{APP_ID}} occurrences with the App Id.  def substituteAppId(opt: String, appId: String): String = { opt.replace(\"{{APP_ID}}\", appId) } def createSecret(conf: SparkConf): String = { val bits = conf.get(AUTH_SECRET_BIT_LENGTH) val rnd = new SecureRandom() val secretBytes = new Array[Byte](bits / JByte.SIZE) rnd.nextBytes(secretBytes) Hex.encodeHexString(secretBytes) } /** * Returns true if and only if the underlying class is a member class. * * Note: jdk8u throws a \"Malformed class name\" error if a given class is a deeply-nested * inner class (See SPARK-34607 for details). This issue has already been fixed in jdk9+, so * we can remove this helper method safely if we drop the support of jdk8u.  def isMemberClass(cls: Class[_]): Boolean = { try { cls.isMemberClass } catch { case _: InternalError => // We emulate jdk8u `Class.isMemberClass` below: // public boolean isMemberClass() { // return getSimpleBinaryName() != null && !isLocalOrAnonymousClass(); // } // `getSimpleBinaryName()` returns null if a given class is a top-level class, // so we replace it with `cls.getEnclosingClass != null`. The second condition checks // if a given class is not a local or an anonymous class, so we replace it with // `cls.getEnclosingMethod == null` because `cls.getEnclosingMethod()` return a value // only in either case (JVM Spec 4.8.6). // // Note: The newer jdk evaluates `!isLocalOrAnonymousClass()` first, // we reorder the conditions to follow it. cls.getEnclosingMethod == null && cls.getEnclosingClass != null } } /** * Safer than Class obj's getSimpleName which may throw Malformed class name error in scala. * This method mimics scalatest's getSimpleNameOfAnObjectsClass.  def getSimpleName(cls: Class[_]): String = { try { cls.getSimpleName } catch { // TODO: the value returned here isn't even quite right; it returns simple names // like UtilsSuite$MalformedClassObject$MalformedClass instead of MalformedClass // The exact value may not matter much as it's used in log statements case _: InternalError => stripDollars(stripPackages(cls.getName)) } } /** * Remove the packages from full qualified class name  private def stripPackages(fullyQualifiedName: String): String = { fullyQualifiedName.split(\"\\\\.\").takeRight(1)(0) } /** * Remove trailing dollar signs from qualified class name, * and return the trailing part after the last dollar sign in the middle  @scala.annotation.tailrec private def stripDollars(s: String): String = { val lastDollarIndex = s.lastIndexOf('$') if (lastDollarIndex < s.length - 1) { // The last char is not a dollar sign if (lastDollarIndex == -1 || !s.contains(\"$iw\")) { // The name does not have dollar sign or is not an interpreter // generated class, so we should return the full string s } else { // The class name is interpreter generated, // return the part after the last dollar sign // This is the same behavior as getClass.getSimpleName s.substring(lastDollarIndex + 1) } } else { // The last char is a dollar sign // Find last non-dollar char val lastNonDollarChar = s.reverse.find(_ != '$') lastNonDollarChar match { case None => s case Some(c) => val lastNonDollarIndex = s.lastIndexOf(c) if (lastNonDollarIndex == -1) { s } else { // Strip the trailing dollar signs // Invoke stripDollars again to get the simple name stripDollars(s.substring(0, lastNonDollarIndex + 1)) } } } } /** * Regular expression matching full width characters. * * Looked at all the 0x0000-0xFFFF characters (unicode) and showed them under Xshell. * Found all the full width characters, then get the regular expression.  private val fullWidthRegex = (\"\"\"[\"\"\" + // scalastyle:off nonascii \"\\u1100-\\u115F\" + \"\\u2E80-\\uA4CF\" + \"\\uAC00-\\uD7A3\" + \"\\uF900-\\uFAFF\" + \"\\uFE10-\\uFE19\" + \"\\uFE30-\\uFE6F\" + \"\\uFF00-\\uFF60\" + \"\\uFFE0-\\uFFE6\" + // scalastyle:on nonascii \"\"\"]\"\"\").r /** * Return the number of half widths in a given string. Note that a full width character * occupies two half widths. * * For a string consisting of 1 million characters, the execution of this method requires * about 50ms.  def stringHalfWidth(str: String): Int = { if (str == null) 0 else str.length + fullWidthRegex.findAllIn(str).size } def sanitizeDirName(str: String): String = { str.replaceAll(\"[ :/]\", \"-\").replaceAll(\"[.${}'\\\"]\", \"_\").toLowerCase(Locale.ROOT) } def isClientMode(conf: SparkConf): Boolean = { \"client\".equals(conf.get(SparkLauncher.DEPLOY_MODE, \"client\")) } /** Returns whether the URI is a \"local:\" URI.  def isLocalUri(uri: String): Boolean = { uri.startsWith(s\"$LOCAL_SCHEME:\") } /** Check whether the file of the path is splittable.  def isFileSplittable(path: Path, codecFactory: CompressionCodecFactory): Boolean = { val codec = codecFactory.getCodec(path) codec == null || codec.isInstanceOf[SplittableCompressionCodec] } /** Create a new properties object with the same values as `props`  def cloneProperties(props: Properties): Properties = { if (props == null) { return props } val resultProps = new Properties() props.forEach((k, v) => resultProps.put(k, v)) resultProps } /** * Convert a sequence of `Path`s to a metadata string. When the length of metadata string * exceeds `stopAppendingThreshold`, stop appending paths for saving memory.  def buildLocationMetadata(paths: Seq[Path], stopAppendingThreshold: Int): String = { val metadata = new StringBuilder(s\"(${paths.length} paths)[\") var index: Int = 0 while (index < paths.length && metadata.length < stopAppendingThreshold) { if (index > 0) { metadata.append(\", \") } metadata.append(paths(index).toString) index += 1 } if (paths.length > index) { if (index > 0) { metadata.append(\", \") } metadata.append(\"...\") } metadata.append(\"]\") metadata.toString } /** * Convert MEMORY_OFFHEAP_SIZE to MB Unit, return 0 if MEMORY_OFFHEAP_ENABLED is false.  def executorOffHeapMemorySizeAsMb(sparkConf: SparkConf): Int = { val sizeInMB = Utils.memoryStringToMb(sparkConf.get(MEMORY_OFFHEAP_SIZE).toString) checkOffHeapEnabled(sparkConf, sizeInMB).toInt } /** * return 0 if MEMORY_OFFHEAP_ENABLED is false.  def checkOffHeapEnabled(sparkConf: SparkConf, offHeapSize: Long): Long = { if (sparkConf.get(MEMORY_OFFHEAP_ENABLED)) { require(offHeapSize > 0, s\"${MEMORY_OFFHEAP_SIZE.key} must be > 0 when ${MEMORY_OFFHEAP_ENABLED.key} == true\") offHeapSize } else { 0 } } /** Returns a string message about delegation token generation failure  def createFailedToGetTokenMessage(serviceName: String, e: scala.Throwable): String = { val message = \"Failed to get token from service %s due to %s. \" + \"If %s is not used, set spark.security.credentials.%s.enabled to false.\" message.format(serviceName, e, serviceName, serviceName) } /** * Decompress a zip file into a local dir. File names are read from the zip file. Note, we skip * addressing the directory here. Also, we rely on the caller side to address any exceptions.  def unzipFilesFromFile(fs: FileSystem, dfsZipFile: Path, localDir: File): Seq[File] = { val files = new ArrayBuffer[File]() val in = new ZipInputStream(fs.open(dfsZipFile)) var out: OutputStream = null try { var entry = in.getNextEntry() while (entry != null) { if (!entry.isDirectory) { val fileName = localDir.toPath.resolve(entry.getName).getFileName.toString val outFile = new File(localDir, fileName) files += outFile out = new FileOutputStream(outFile) IOUtils.copy(in, out) out.close() in.closeEntry() } entry = in.getNextEntry() } in.close() // so that any error in closing does not get ignored logInfo(s\"Unzipped from $dfsZipFile\\n\\t${files.mkString(\"\\n\\t\")}\") } finally { // Close everything no matter what happened IOUtils.closeQuietly(in) IOUtils.closeQuietly(out) } files.toSeq } /** * Return the median number of a long array * * @param sizes * @param alreadySorted * @return  def median(sizes: Array[Long], alreadySorted: Boolean): Long = { val len = sizes.length val sortedSize = if (alreadySorted) sizes else sizes.sorted len match { case _ if (len % 2 == 0) => math.max((sortedSize(len / 2) + sortedSize(len / 2 - 1)) / 2, 1) case _ => math.max(sortedSize(len / 2), 1) } } } private[util] object CallerContext extends Logging { val callerContextSupported: Boolean = { SparkHadoopUtil.get.conf.getBoolean(\"hadoop.caller.context.enabled\", false) && { try { Utils.classForName(\"org.apache.hadoop.ipc.CallerContext\") Utils.classForName(\"org.apache.hadoop.ipc.CallerContext$Builder\") true } catch { case _: ClassNotFoundException => false case NonFatal(e) => logWarning(\"Fail to load the CallerContext class\", e) false } } } } /** * An utility class used to set up Spark caller contexts to HDFS and Yarn. The `context` will be * constructed by parameters passed in. * When Spark applications run on Yarn and HDFS, its caller contexts will be written into Yarn RM * audit log and hdfs-audit.log. That can help users to better diagnose and understand how * specific applications impacting parts of the Hadoop system and potential problems they may be * creating (e.g. overloading NN). As HDFS mentioned in HDFS-9184, for a given HDFS operation, it's * very helpful to track which upper level job issues it. * * @param from who sets up the caller context (TASK, CLIENT, APPMASTER) * * The parameters below are optional: * @param upstreamCallerContext caller context the upstream application passes in * @param appId id of the app this task belongs to * @param appAttemptId attempt id of the app this task belongs to * @param jobId id of the job this task belongs to * @param stageId id of the stage this task belongs to * @param stageAttemptId attempt id of the stage this task belongs to * @param taskId task id * @param taskAttemptNumber task attempt id  private[spark] class CallerContext( from: String, upstreamCallerContext: Option[String] = None, appId: Option[String] = None, appAttemptId: Option[String] = None, jobId: Option[Int] = None, stageId: Option[Int] = None, stageAttemptId: Option[Int] = None, taskId: Option[Long] = None, taskAttemptNumber: Option[Int] = None) extends Logging { private val context = prepareContext(\"SPARK_\" + from + appId.map(\"_\" + _).getOrElse(\"\") + appAttemptId.map(\"_\" + _).getOrElse(\"\") + jobId.map(\"_JId_\" + _).getOrElse(\"\") + stageId.map(\"_SId_\" + _).getOrElse(\"\") + stageAttemptId.map(\"_\" + _).getOrElse(\"\") + taskId.map(\"_TId_\" + _).getOrElse(\"\") + taskAttemptNumber.map(\"_\" + _).getOrElse(\"\") + upstreamCallerContext.map(\"_\" + _).getOrElse(\"\")) private def prepareContext(context: String): String = { // The default max size of Hadoop caller context is 128 lazy val len = SparkHadoopUtil.get.conf.getInt(\"hadoop.caller.context.max.size\", 128) if (context == null || context.length <= len) { context } else { val finalContext = context.substring(0, len) logWarning(s\"Truncated Spark caller context from $context to $finalContext\") finalContext } } /** * Set up the caller context [[context]] by invoking Hadoop CallerContext API of * [[org.apache.hadoop.ipc.CallerContext]], which was added in hadoop 2.8.  def setCurrentContext(): Unit = { if (CallerContext.callerContextSupported) { try { val callerContext = Utils.classForName(\"org.apache.hadoop.ipc.CallerContext\") val builder: Class[AnyRef] = Utils.classForName(\"org.apache.hadoop.ipc.CallerContext$Builder\") val builderInst = builder.getConstructor(classOf[String]).newInstance(context) val hdfsContext = builder.getMethod(\"build\").invoke(builderInst) callerContext.getMethod(\"setCurrent\", callerContext).invoke(null, hdfsContext) } catch { case NonFatal(e) => logWarning(\"Fail to set Spark caller context\", e) } } } } /** * A utility class to redirect the child process's stdout or stderr.  private[spark] class RedirectThread( in: InputStream, out: OutputStream, name: String, propagateEof: Boolean = false) extends Thread(name) { setDaemon(true) override def run(): Unit = { scala.util.control.Exception.ignoring(classOf[IOException]) { // FIXME: We copy the stream on the level of bytes to avoid encoding problems. Utils.tryWithSafeFinally { val buf = new Array[Byte](1024) var len = in.read(buf) while (len != -1) { out.write(buf, 0, len) out.flush() len = in.read(buf) } } { if (propagateEof) { out.close() } } } } } /** * An [[OutputStream]] that will store the last 10 kilobytes (by default) written to it * in a circular buffer. The current contents of the buffer can be accessed using * the toString method.  private[spark] class CircularBuffer(sizeInBytes: Int = 10240) extends java.io.OutputStream { private var pos: Int = 0 private var isBufferFull = false private val buffer = new Array[Byte](sizeInBytes) def write(input: Int): Unit = { buffer(pos) = input.toByte pos = (pos + 1) % buffer.length isBufferFull = isBufferFull || (pos == 0) } override def toString: String = { if (!isBufferFull) { return new String(buffer, 0, pos, StandardCharsets.UTF_8) } val nonCircularBuffer = new Array[Byte](sizeInBytes) System.arraycopy(buffer, pos, nonCircularBuffer, 0, buffer.length - pos) System.arraycopy(buffer, 0, nonCircularBuffer, buffer.length - pos, pos) new String(nonCircularBuffer, StandardCharsets.UTF_8) } }",
            "## METHOD: org/apache/spark/sql/catalyst/parser/AstBuilder#visitStringConstant().\n* main purpose is to prevent slight differences due to back to back conversions i.e.: * String -> Literal -> String.  protected def visitStringConstant( ctx: ConstantContext, legacyNullAsString: Boolean): String = withOrigin(ctx) { expression(ctx) match { case Literal(null, _) if !legacyNullAsString => null case l @ Literal(null, _) => l.toString case l: Literal => // TODO For v2 commands, we will cast the string back to its actual value, // which is a waste and can be improved in the future. Cast(l, StringType, Some(conf.sessionLocalTimeZone)).eval().toString case other => throw new IllegalArgumentException(s\"Only literals are allowed in the \" + s\"partition spec, but got ${other.sql}\") } } /** * Add ORDER BY/SORT BY/CLUSTER BY/DISTRIBUTE BY/LIMIT/WINDOWS clauses to the logical plan. These * clauses determine the shape (ordering/partitioning/rows) of the query result.  private def withQueryResultClauses( ctx: QueryOrganizationContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) { import ctx._ // Handle ORDER BY, SORT BY, DISTRIBUTE BY, and CLUSTER BY clause. val withOrder = if ( !order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) { // ORDER BY ... Sort(order.asScala.map(visitSortItem).toSeq, global = true, query) } else if (order.isEmpty && !sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) { // SORT BY ... Sort(sort.asScala.map(visitSortItem).toSeq, global = false, query) } else if (order.isEmpty && sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) { // DISTRIBUTE BY ... withRepartitionByExpression(ctx, expressionList(distributeBy), query) } else if (order.isEmpty && !sort.isEmpty && !distributeBy.isEmpty && clusterBy.isEmpty) { // SORT BY ... DISTRIBUTE BY ... Sort( sort.asScala.map(visitSortItem).toSeq, global = false, withRepartitionByExpression(ctx, expressionList(distributeBy), query)) } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && !clusterBy.isEmpty) { // CLUSTER BY ... val expressions = expressionList(clusterBy) Sort( expressions.map(SortOrder(_, Ascending)), global = false, withRepartitionByExpression(ctx, expressions, query)) } else if (order.isEmpty && sort.isEmpty && distributeBy.isEmpty && clusterBy.isEmpty) { // [EMPTY] query } else { throw QueryParsingErrors.combinationQueryResultClausesUnsupportedError(ctx) } // WINDOWS val withWindow = withOrder.optionalMap(windowClause)(withWindowClause) // LIMIT // - LIMIT ALL is the same as omitting the LIMIT clause withWindow.optional(limit) { Limit(typedVisit(limit), withWindow) } } /** * Create a clause for DISTRIBUTE BY.  protected def withRepartitionByExpression( ctx: QueryOrganizationContext, expressions: Seq[Expression], query: LogicalPlan): LogicalPlan = { throw QueryParsingErrors.distributeByUnsupportedError(ctx) } override def visitTransformQuerySpecification( ctx: TransformQuerySpecificationContext): LogicalPlan = withOrigin(ctx) { val from = OneRowRelation().optional(ctx.fromClause) { visitFromClause(ctx.fromClause) } withTransformQuerySpecification( ctx, ctx.transformClause, ctx.lateralView, ctx.whereClause, ctx.aggregationClause, ctx.havingClause, ctx.windowClause, from ) } override def visitRegularQuerySpecification( ctx: RegularQuerySpecificationContext): LogicalPlan = withOrigin(ctx) { val from = OneRowRelation().optional(ctx.fromClause) { visitFromClause(ctx.fromClause) } withSelectQuerySpecification( ctx, ctx.selectClause, ctx.lateralView, ctx.whereClause, ctx.aggregationClause, ctx.havingClause, ctx.windowClause, from ) } override def visitNamedExpressionSeq( ctx: NamedExpressionSeqContext): Seq[Expression] = { Option(ctx).toSeq .flatMap(_.namedExpression.asScala) .map(typedVisit[Expression]) } override def visitExpressionSeq(ctx: ExpressionSeqContext): Seq[Expression] = { Option(ctx).toSeq .flatMap(_.expression.asScala) .map(typedVisit[Expression]) } /** * Create a logical plan using a having clause.  private def withHavingClause( ctx: HavingClauseContext, plan: LogicalPlan): LogicalPlan = { // Note that we add a cast to non-predicate expressions. If the expression itself is // already boolean, the optimizer will get rid of the unnecessary cast. val predicate = expression(ctx.booleanExpression) match { case p: Predicate => p case e => Cast(e, BooleanType) } UnresolvedHaving(predicate, plan) } /** * Create a logical plan using a where clause.  private def withWhereClause(ctx: WhereClauseContext, plan: LogicalPlan): LogicalPlan = { Filter(expression(ctx.booleanExpression), plan) } /** * Add a hive-style transform (SELECT TRANSFORM/MAP/REDUCE) query specification to a logical plan.  private def withTransformQuerySpecification( ctx: ParserRuleContext, transformClause: TransformClauseContext, lateralView: java.util.List[LateralViewContext], whereClause: WhereClauseContext, aggregationClause: AggregationClauseContext, havingClause: HavingClauseContext, windowClause: WindowClauseContext, relation: LogicalPlan): LogicalPlan = withOrigin(ctx) { if (transformClause.setQuantifier != null) { throw QueryParsingErrors.transformNotSupportQuantifierError(transformClause.setQuantifier) } // Create the attributes. val (attributes, schemaLess) = if (transformClause.colTypeList != null) { // Typed return columns. (createSchema(transformClause.colTypeList).toAttributes, false) } else if (transformClause.identifierSeq != null) { // Untyped return columns. val attrs = visitIdentifierSeq(transformClause.identifierSeq).map { name => AttributeReference(name, StringType, nullable = true)() } (attrs, false) } else { (Seq(AttributeReference(\"key\", StringType)(), AttributeReference(\"value\", StringType)()), true) } val plan = visitCommonSelectQueryClausePlan( relation, visitExpressionSeq(transformClause.expressionSeq), lateralView, whereClause, aggregationClause, havingClause, windowClause, isDistinct = false) ScriptTransformation( string(transformClause.script), attributes, plan, withScriptIOSchema( ctx, transformClause.inRowFormat, transformClause.recordWriter, transformClause.outRowFormat, transformClause.recordReader, schemaLess ) ) } /** * Add a regular (SELECT) query specification to a logical plan. The query specification * is the core of the logical plan, this is where sourcing (FROM clause), projection (SELECT), * aggregation (GROUP BY ... HAVING ...) and filtering (WHERE) takes place. * * Note that query hints are ignored (both by the parser and the builder).  private def withSelectQuerySpecification( ctx: ParserRuleContext, selectClause: SelectClauseContext, lateralView: java.util.List[LateralViewContext], whereClause: WhereClauseContext, aggregationClause: AggregationClauseContext, havingClause: HavingClauseContext, windowClause: WindowClauseContext, relation: LogicalPlan): LogicalPlan = withOrigin(ctx) { val isDistinct = selectClause.setQuantifier() != null && selectClause.setQuantifier().DISTINCT() != null val plan = visitCommonSelectQueryClausePlan( relation, visitNamedExpressionSeq(selectClause.namedExpressionSeq), lateralView, whereClause, aggregationClause, havingClause, windowClause, isDistinct) // Hint selectClause.hints.asScala.foldRight(plan)(withHints) } def visitCommonSelectQueryClausePlan( relation: LogicalPlan, expressions: Seq[Expression], lateralView: java.util.List[LateralViewContext], whereClause: WhereClauseContext, aggregationClause: AggregationClauseContext, havingClause: HavingClauseContext, windowClause: WindowClauseContext, isDistinct: Boolean): LogicalPlan = { // Add lateral views. val withLateralView = lateralView.asScala.foldLeft(relation)(withGenerate) // Add where. val withFilter = withLateralView.optionalMap(whereClause)(withWhereClause) // Add aggregation or a project. val namedExpressions = expressions.map { case e: NamedExpression => e case e: Expression => UnresolvedAlias(e) } def createProject() = if (namedExpressions.nonEmpty) { Project(namedExpressions, withFilter) } else { withFilter } val withProject = if (aggregationClause == null && havingClause != null) { if (conf.getConf(SQLConf.LEGACY_HAVING_WITHOUT_GROUP_BY_AS_WHERE)) { // If the legacy conf is set, treat HAVING without GROUP BY as WHERE. val predicate = expression(havingClause.booleanExpression) match { case p: Predicate => p case e => Cast(e, BooleanType) } Filter(predicate, createProject()) } else { // According to SQL standard, HAVING without GROUP BY means global aggregate. withHavingClause(havingClause, Aggregate(Nil, namedExpressions, withFilter)) } } else if (aggregationClause != null) { val aggregate = withAggregationClause(aggregationClause, namedExpressions, withFilter) aggregate.optionalMap(havingClause)(withHavingClause) } else { // When hitting this branch, `having` must be null. createProject() } // Distinct val withDistinct = if (isDistinct) { Distinct(withProject) } else { withProject } // Window val withWindow = withDistinct.optionalMap(windowClause)(withWindowClause) withWindow } // Script Transform's input/output format. type ScriptIOFormat = (Seq[(String, String)], Option[String], Seq[(String, String)], Option[String]) protected def getRowFormatDelimited(ctx: RowFormatDelimitedContext): ScriptIOFormat = { // TODO we should use the visitRowFormatDelimited function here. However HiveScriptIOSchema // expects a seq of pairs in which the old parsers' token names are used as keys. // Transforming the result of visitRowFormatDelimited would be quite a bit messier than // retrieving the key value pairs ourselves. val entries = entry(\"TOK_TABLEROWFORMATFIELD\", ctx.fieldsTerminatedBy) ++ entry(\"TOK_TABLEROWFORMATCOLLITEMS\", ctx.collectionItemsTerminatedBy) ++ entry(\"TOK_TABLEROWFORMATMAPKEYS\", ctx.keysTerminatedBy) ++ entry(\"TOK_TABLEROWFORMATNULL\", ctx.nullDefinedAs) ++ Option(ctx.linesSeparatedBy).toSeq.map { token => val value = string(token) validate( value == \"\\n\", s\"LINES TERMINATED BY only supports newline '\\\\n' right now: $value\", ctx) \"TOK_TABLEROWFORMATLINES\" -> value } (entries, None, Seq.empty, None) } /** * Create a [[ScriptInputOutputSchema]].  protected def withScriptIOSchema( ctx: ParserRuleContext, inRowFormat: RowFormatContext, recordWriter: Token, outRowFormat: RowFormatContext, recordReader: Token, schemaLess: Boolean): ScriptInputOutputSchema = { def format(fmt: RowFormatContext): ScriptIOFormat = fmt match { case c: RowFormatDelimitedContext => getRowFormatDelimited(c) case c: RowFormatSerdeContext => throw QueryParsingErrors.transformWithSerdeUnsupportedError(ctx) // SPARK-32106: When there is no definition about format, we return empty result // to use a built-in default Serde in SparkScriptTransformationExec. case null => (Nil, None, Seq.empty, None) } val (inFormat, inSerdeClass, inSerdeProps, reader) = format(inRowFormat) val (outFormat, outSerdeClass, outSerdeProps, writer) = format(outRowFormat) ScriptInputOutputSchema( inFormat, outFormat, inSerdeClass, outSerdeClass, inSerdeProps, outSerdeProps, reader, writer, schemaLess) } /** * Create a logical plan for a given 'FROM' clause. Note that we support multiple (comma * separated) relations here, these get converted into a single plan by condition-less inner join.  override def visitFromClause(ctx: FromClauseContext): LogicalPlan = withOrigin(ctx) { val from = ctx.relation.asScala.foldLeft(null: LogicalPlan) { (left, relation) => val right = plan(relation.relationPrimary) val join = right.optionalMap(left) { (left, right) => if (relation.LATERAL != null) { if (!relation.relationPrimary.isInstanceOf[AliasedQueryContext]) { throw QueryParsingErrors.invalidLateralJoinRelationError(relation.relationPrimary) } LateralJoin(left, LateralSubquery(right), Inner, None) } else { Join(left, right, Inner, None, JoinHint.NONE) } } withJoinRelations(join, relation) } if (ctx.pivotClause() != null) { if (!ctx.lateralView.isEmpty) { throw QueryParsingErrors.lateralWithPivotInFromClauseNotAllowedError(ctx) } withPivot(ctx.pivotClause, from) } else { ctx.lateralView.asScala.foldLeft(from)(withGenerate) } } /** * Connect two queries by a Set operator. * * Supported Set operators are: * - UNION [ DISTINCT | ALL ] * - EXCEPT [ DISTINCT | ALL ] * - MINUS [ DISTINCT | ALL ] * - INTERSECT [DISTINCT | ALL]  override def visitSetOperation(ctx: SetOperationContext): LogicalPlan = withOrigin(ctx) { val left = plan(ctx.left) val right = plan(ctx.right) val all = Option(ctx.setQuantifier()).exists(_.ALL != null) ctx.operator.getType match { case SqlBaseParser.UNION if all => Union(left, right) case SqlBaseParser.UNION => Distinct(Union(left, right)) case SqlBaseParser.INTERSECT if all => Intersect(left, right, isAll = true) case SqlBaseParser.INTERSECT => Intersect(left, right, isAll = false) case SqlBaseParser.EXCEPT if all => Except(left, right, isAll = true) case SqlBaseParser.EXCEPT => Except(left, right, isAll = false) case SqlBaseParser.SETMINUS if all => Except(left, right, isAll = true) case SqlBaseParser.SETMINUS => Except(left, right, isAll = false) } } /** * Add a [[WithWindowDefinition]] operator to a logical plan.  private def withWindowClause( ctx: WindowClauseContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) { // Collect all window specifications defined in the WINDOW clause. val baseWindowTuples = ctx.namedWindow.asScala.map { wCtx => (wCtx.name.getText, typedVisit[WindowSpec](wCtx.windowSpec)) } baseWindowTuples.groupBy(_._1).foreach { kv => if (kv._2.size > 1) { throw QueryParsingErrors.repetitiveWindowDefinitionError(kv._1, ctx) } } val baseWindowMap = baseWindowTuples.toMap // Handle cases like // window w1 as (partition by p_mfgr order by p_name // range between 2 preceding and 2 following), // w2 as w1 val windowMapView = baseWindowMap.mapValues { case WindowSpecReference(name) => baseWindowMap.get(name) match { case Some(spec: WindowSpecDefinition) => spec case Some(ref) => throw QueryParsingErrors.invalidWindowReferenceError(name, ctx) case None => throw QueryParsingErrors.cannotResolveWindowReferenceError(name, ctx) } case spec: WindowSpecDefinition => spec } // Note that mapValues creates a view instead of materialized map. We force materialization by // mapping over identity. WithWindowDefinition(windowMapView.map(identity).toMap, query) } /** * Add an [[Aggregate]] to a logical plan.  private def withAggregationClause( ctx: AggregationClauseContext, selectExpressions: Seq[NamedExpression], query: LogicalPlan): LogicalPlan = withOrigin(ctx) { if (ctx.groupingExpressionsWithGroupingAnalytics.isEmpty) { val groupByExpressions = expressionList(ctx.groupingExpressions) if (ctx.GROUPING != null) { // GROUP BY ... GROUPING SETS (...) // `groupByExpressions` can be non-empty for Hive compatibility. It may add extra grouping // expressions that do not exist in GROUPING SETS (...), and the value is always null. // For example, `SELECT a, b, c FROM ... GROUP BY a, b, c GROUPING SETS (a, b)`, the output // of column `c` is always null. val groupingSets = ctx.groupingSet.asScala.map(_.expression.asScala.map(e => expression(e)).toSeq) Aggregate(Seq(GroupingSets(groupingSets.toSeq, groupByExpressions)), selectExpressions, query) } else { // GROUP BY .... (WITH CUBE | WITH ROLLUP)? val mappedGroupByExpressions = if (ctx.CUBE != null) { Seq(Cube(groupByExpressions.map(Seq(_)))) } else if (ctx.ROLLUP != null) { Seq(Rollup(groupByExpressions.map(Seq(_)))) } else { groupByExpressions } Aggregate(mappedGroupByExpressions, selectExpressions, query) } } else { val groupByExpressions = ctx.groupingExpressionsWithGroupingAnalytics.asScala .map(groupByExpr => { val groupingAnalytics = groupByExpr.groupingAnalytics if (groupingAnalytics != null) { visitGroupingAnalytics(groupingAnalytics) } else { expression(groupByExpr.expression) } }) Aggregate(groupByExpressions.toSeq, selectExpressions, query) } } override def visitGroupingAnalytics( groupingAnalytics: GroupingAnalyticsContext): BaseGroupingSets = { val groupingSets = groupingAnalytics.groupingSet.asScala .map(_.expression.asScala.map(e => expression(e)).toSeq) if (groupingAnalytics.CUBE != null) { // CUBE(A, B, (A, B), ()) is not supported. if (groupingSets.exists(_.isEmpty)) { throw QueryParsingErrors.invalidGroupingSetError(\"CUBE\", groupingAnalytics) } Cube(groupingSets.toSeq) } else if (groupingAnalytics.ROLLUP != null) { // ROLLUP(A, B, (A, B), ()) is not supported. if (groupingSets.exists(_.isEmpty)) { throw QueryParsingErrors.invalidGroupingSetError(\"ROLLUP\", groupingAnalytics) } Rollup(groupingSets.toSeq) } else { assert(groupingAnalytics.GROUPING != null && groupingAnalytics.SETS != null) val groupingSets = groupingAnalytics.groupingElement.asScala.flatMap { expr => val groupingAnalytics = expr.groupingAnalytics() if (groupingAnalytics != null) { visitGroupingAnalytics(groupingAnalytics).selectedGroupByExprs } else { Seq(expr.groupingSet().expression().asScala.map(e => expression(e)).toSeq) } } GroupingSets(groupingSets.toSeq) } } /** * Add [[UnresolvedHint]]s to a logical plan.  private def withHints( ctx: HintContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) { var plan = query ctx.hintStatements.asScala.reverse.foreach { stmt => plan = UnresolvedHint(stmt.hintName.getText, stmt.parameters.asScala.map(expression).toSeq, plan) } plan } /** * Add a [[Pivot]] to a logical plan.  private def withPivot( ctx: PivotClauseContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) { val aggregates = Option(ctx.aggregates).toSeq .flatMap(_.namedExpression.asScala) .map(typedVisit[Expression]) val pivotColumn = if (ctx.pivotColumn.identifiers.size == 1) { UnresolvedAttribute.quoted(ctx.pivotColumn.identifier.getText) } else { CreateStruct( ctx.pivotColumn.identifiers.asScala.map( identifier => UnresolvedAttribute.quoted(identifier.getText)).toSeq) } val pivotValues = ctx.pivotValues.asScala.map(visitPivotValue) Pivot(None, pivotColumn, pivotValues.toSeq, aggregates, query) } /** * Create a Pivot column value with or without an alias.  override def visitPivotValue(ctx: PivotValueContext): Expression = withOrigin(ctx) { val e = expression(ctx.expression) if (ctx.identifier != null) { Alias(e, ctx.identifier.getText)() } else { e } } /** * Add a [[Generate]] (Lateral View) to a logical plan.  private def withGenerate( query: LogicalPlan, ctx: LateralViewContext): LogicalPlan = withOrigin(ctx) { val expressions = expressionList(ctx.expression) Generate( UnresolvedGenerator(visitFunctionName(ctx.qualifiedName), expressions), unrequiredChildIndex = Nil, outer = ctx.OUTER != null, // scalastyle:off caselocale Some(ctx.tblName.getText.toLowerCase), // scalastyle:on caselocale ctx.colName.asScala.map(_.getText).map(UnresolvedAttribute.quoted).toSeq, query) } /** * Create a single relation referenced in a FROM clause. This method is used when a part of the * join condition is nested, for example: * {{{ * select * from t1 join (t2 cross join t3) on col1 = col2 * }}}  override def visitRelation(ctx: RelationContext): LogicalPlan = withOrigin(ctx) { withJoinRelations(plan(ctx.relationPrimary), ctx) } /** * Join one more [[LogicalPlan]]s to the current logical plan.  private def withJoinRelations(base: LogicalPlan, ctx: RelationContext): LogicalPlan = { ctx.joinRelation.asScala.foldLeft(base) { (left, join) => withOrigin(join) { val baseJoinType = join.joinType match { case null => Inner case jt if jt.CROSS != null => Cross case jt if jt.FULL != null => FullOuter case jt if jt.SEMI != null => LeftSemi case jt if jt.ANTI != null => LeftAnti case jt if jt.LEFT != null => LeftOuter case jt if jt.RIGHT != null => RightOuter case _ => Inner } if (join.LATERAL != null && !join.right.isInstanceOf[AliasedQueryContext]) { throw QueryParsingErrors.invalidLateralJoinRelationError(join.right) } // Resolve the join type and join condition val (joinType, condition) = Option(join.joinCriteria) match { case Some(c) if c.USING != null => if (join.LATERAL != null) { throw QueryParsingErrors.lateralJoinWithUsingJoinUnsupportedError(ctx) } (UsingJoin(baseJoinType, visitIdentifierList(c.identifierList)), None) case Some(c) if c.booleanExpression != null => (baseJoinType, Option(expression(c.booleanExpression))) case Some(c) => throw new IllegalStateException(s\"Unimplemented joinCriteria: $c\") case None if join.NATURAL != null => if (join.LATERAL != null) { throw QueryParsingErrors.lateralJoinWithNaturalJoinUnsupportedError(ctx) } if (baseJoinType == Cross) { throw QueryParsingErrors.naturalCrossJoinUnsupportedError(ctx) } (NaturalJoin(baseJoinType), None) case None => (baseJoinType, None) } if (join.LATERAL != null) { if (!Seq(Inner, Cross, LeftOuter).contains(joinType)) { throw QueryParsingErrors.unsupportedLateralJoinTypeError(ctx, joinType.sql) } LateralJoin(left, LateralSubquery(plan(join.right)), joinType, condition) } else { Join(left, plan(join.right), joinType, condition, JoinHint.NONE) } } } } /** * Add a [[Sample]] to a logical plan. * * This currently supports the following sampling methods: * - TABLESAMPLE(x ROWS): Sample the table down to the given number of rows. * - TABLESAMPLE(x PERCENT) [REPEATABLE (y)]: Sample the table down to the given percentage with * seed 'y'. Note that percentages are defined as a number between 0 and 100. * - TABLESAMPLE(BUCKET x OUT OF y) [REPEATABLE (z)]: Sample the table down to a 'x' divided by * 'y' fraction with seed 'z'.  private def withSample(ctx: SampleContext, query: LogicalPlan): LogicalPlan = withOrigin(ctx) { // Create a sampled plan if we need one. def sample(fraction: Double, seed: Long): Sample = { // The range of fraction accepted by Sample is [0, 1]. Because Hive's block sampling // function takes X PERCENT as the input and the range of X is [0, 100], we need to // adjust the fraction. val eps = RandomSampler.roundingEpsilon validate(fraction >= 0.0 - eps && fraction <= 1.0 + eps, s\"Sampling fraction ($fraction) must be on interval [0, 1]\", ctx) Sample(0.0, fraction, withReplacement = false, seed, query) } if (ctx.sampleMethod() == null) { throw QueryParsingErrors.emptyInputForTableSampleError(ctx) } val seed = if (ctx.seed != null) { ctx.seed.getText.toLong } else { (math.random * 1000).toLong } ctx.sampleMethod() match { case ctx: SampleByRowsContext => Limit(expression(ctx.expression), query) case ctx: SampleByPercentileContext => val fraction = ctx.percentage.getText.toDouble val sign = if (ctx.negativeSign == null) 1 else -1 sample(sign * fraction / 100.0d, seed) case ctx: SampleByBytesContext => val bytesStr = ctx.bytes.getText if (bytesStr.matches(\"[0-9]+[bBkKmMgG]\")) { throw QueryParsingErrors.tableSampleByBytesUnsupportedError(\"byteLengthLiteral\", ctx) } else { throw QueryParsingErrors.invalidByteLengthLiteralError(bytesStr, ctx) } case ctx: SampleByBucketContext if ctx.ON() != null => if (ctx.identifier != null) { throw QueryParsingErrors.tableSampleByBytesUnsupportedError( \"BUCKET x OUT OF y ON colname\", ctx) } else { throw QueryParsingErrors.tableSampleByBytesUnsupportedError( \"BUCKET x OUT OF y ON function\", ctx) } case ctx: SampleByBucketContext => sample(ctx.numerator.getText.toDouble / ctx.denominator.getText.toDouble, seed) } } /** * Create a logical plan for a sub-query.  override def visitSubquery(ctx: SubqueryContext): LogicalPlan = withOrigin(ctx) { plan(ctx.query) } /** * Create an un-aliased table reference. This is typically used for top-level table references, * for example: * {{{ * INSERT INTO db.tbl2 * TABLE db.tbl1 * }}}  override def visitTable(ctx: TableContext): LogicalPlan = withOrigin(ctx) { UnresolvedRelation(visitMultipartIdentifier(ctx.multipartIdentifier)) } /** * Create an aliased table reference. This is typically used in FROM clauses.  override def visitTableName(ctx: TableNameContext): LogicalPlan = withOrigin(ctx) { val tableId = visitMultipartIdentifier(ctx.multipartIdentifier) val relation = UnresolvedRelation(tableId) val table = mayApplyAliasPlan( ctx.tableAlias, relation.optionalMap(ctx.temporalClause)(withTimeTravel)) table.optionalMap(ctx.sample)(withSample) } private def withTimeTravel( ctx: TemporalClauseContext, plan: LogicalPlan): LogicalPlan = withOrigin(ctx) { val v = ctx.version val version = if (ctx.INTEGER_VALUE != null) { Some(v.getText) } else { Option(v).map(string) } val timestamp = Option(ctx.timestamp).map(expression) if (timestamp.exists(_.references.nonEmpty)) { throw QueryParsingErrors.invalidTimeTravelSpec( \"timestamp expression cannot refer to any columns\", ctx.timestamp) } if (timestamp.exists(e => SubqueryExpression.hasSubquery(e))) { throw QueryParsingErrors.invalidTimeTravelSpec( \"timestamp expression cannot contain subqueries\", ctx.timestamp) } RelationTimeTravel(plan, timestamp, version) } /** * Create a table-valued function call with arguments, e.g. range(1000)  override def visitTableValuedFunction(ctx: TableValuedFunctionContext) : LogicalPlan = withOrigin(ctx) { val func = ctx.functionTable val aliases = if (func.tableAlias.identifierList != null) { visitIdentifierList(func.tableAlias.identifierList) } else { Seq.empty } val name = getFunctionMultiparts(func.functionName) if (name.length > 1) { throw QueryParsingErrors.invalidTableValuedFunctionNameError(name, ctx) } val tvf = UnresolvedTableValuedFunction( name.asFunctionIdentifier, func.expression.asScala.map(expression).toSeq, aliases) tvf.optionalMap(func.tableAlias.strictIdentifier)(aliasPlan) } /** * Create an inline table (a virtual table in Hive parlance).  override def visitInlineTable(ctx: InlineTableContext): LogicalPlan = withOrigin(ctx) { // Get the backing expressions. val rows = ctx.expression.asScala.map { e => expression(e) match { // inline table comes in two styles: // style 1: values (1), (2), (3) -- multiple columns are supported // style 2: values 1, 2, 3 -- only a single column is supported here case struct: CreateNamedStruct => struct.valExprs // style 1 case child => Seq(child) // style 2 } } val aliases = if (ctx.tableAlias.identifierList != null) { visitIdentifierList(ctx.tableAlias.identifierList) } else { Seq.tabulate(rows.head.size)(i => s\"col${i + 1}\") } val table = UnresolvedInlineTable(aliases, rows.toSeq) table.optionalMap(ctx.tableAlias.strictIdentifier)(aliasPlan) } /** * Create an alias (SubqueryAlias) for a join relation. This is practically the same as * visitAliasedQuery and visitNamedExpression, ANTLR4 however requires us to use 3 different * hooks. We could add alias names for output columns, for example: * {{{ * SELECT a, b, c, d FROM (src1 s1 INNER JOIN src2 s2 ON s1.id = s2.id) dst(a, b, c, d) * }}}  override def visitAliasedRelation(ctx: AliasedRelationContext): LogicalPlan = withOrigin(ctx) { val relation = plan(ctx.relation).optionalMap(ctx.sample)(withSample) mayApplyAliasPlan(ctx.tableAlias, relation) } /** * Create an alias (SubqueryAlias) for a sub-query. This is practically the same as * visitAliasedRelation and visitNamedExpression, ANTLR4 however requires us to use 3 different * hooks. We could add alias names for output columns, for example: * {{{ * SELECT col1, col2 FROM testData AS t(col1, col2) * }}}  override def visitAliasedQuery(ctx: AliasedQueryContext): LogicalPlan = withOrigin(ctx) { val relation = plan(ctx.query).optionalMap(ctx.sample)(withSample) if (ctx.tableAlias.strictIdentifier == null) { // For un-aliased subqueries, use a default alias name that is not likely to conflict with // normal subquery names, so that parent operators can only access the columns in subquery by // unqualified names. Users can still use this special qualifier to access columns if they // know it, but that's not recommended. SubqueryAlias(\"__auto_generated_subquery_name\", relation) } else { mayApplyAliasPlan(ctx.tableAlias, relation) } } /** * Create an alias ([[SubqueryAlias]]) for a [[LogicalPlan]].  private def aliasPlan(alias: ParserRuleContext, plan: LogicalPlan): LogicalPlan = { SubqueryAlias(alias.getText, plan) } /** * If aliases specified in a FROM clause, create a subquery alias ([[SubqueryAlias]]) and * column aliases for a [[LogicalPlan]].  private def mayApplyAliasPlan(tableAlias: TableAliasContext, plan: LogicalPlan): LogicalPlan = { if (tableAlias.strictIdentifier != null) { val alias = tableAlias.strictIdentifier.getText if (tableAlias.identifierList != null) { val columnNames = visitIdentifierList(tableAlias.identifierList) SubqueryAlias(alias, UnresolvedSubqueryColumnAliases(columnNames, plan)) } else { SubqueryAlias(alias, plan) } } else { plan } } /** * Create a Sequence of Strings for a parenthesis enclosed alias list.  override def visitIdentifierList(ctx: IdentifierListContext): Seq[String] = withOrigin(ctx) { visitIdentifierSeq(ctx.identifierSeq) } /** * Create a Sequence of Strings for an identifier list.  override def visitIdentifierSeq(ctx: IdentifierSeqContext): Seq[String] = withOrigin(ctx) { ctx.ident.asScala.map(_.getText).toSeq } /* ******************************************************************************************** * Table Identifier parsing * ********************************************************************************************  /** * Create a [[TableIdentifier]] from a 'tableName' or 'databaseName'.'tableName' pattern.  override def visitTableIdentifier( ctx: TableIdentifierContext): TableIdentifier = withOrigin(ctx) { TableIdentifier(ctx.table.getText, Option(ctx.db).map(_.getText)) } /** * Create a [[FunctionIdentifier]] from a 'functionName' or 'databaseName'.'functionName' pattern.  override def visitFunctionIdentifier( ctx: FunctionIdentifierContext): FunctionIdentifier = withOrigin(ctx) { FunctionIdentifier(ctx.function.getText, Option(ctx.db).map(_.getText)) } /** * Create a multi-part identifier.  override def visitMultipartIdentifier(ctx: MultipartIdentifierContext): Seq[String] = withOrigin(ctx) { ctx.parts.asScala.map(_.getText).toSeq } /* ******************************************************************************************** * Expression parsing * ********************************************************************************************  /** * Create an expression from the given context. This method just passes the context on to the * visitor and only takes care of typing (We assume that the visitor returns an Expression here).  protected def expression(ctx: ParserRuleContext): Expression = typedVisit(ctx) /** * Create sequence of expressions from the given sequence of contexts.  private def expressionList(trees: java.util.List[ExpressionContext]): Seq[Expression] = { trees.asScala.map(expression).toSeq } /** * Create a star (i.e. all) expression; this selects all elements (in the specified object). * Both un-targeted (global) and targeted aliases are supported.  override def visitStar(ctx: StarContext): Expression = withOrigin(ctx) { UnresolvedStar(Option(ctx.qualifiedName()).map(_.identifier.asScala.map(_.getText).toSeq)) } /** * Create an aliased expression if an alias is specified. Both single and multi-aliases are * supported.  override def visitNamedExpression(ctx: NamedExpressionContext): Expression = withOrigin(ctx) { val e = expression(ctx.expression) if (ctx.name != null) { Alias(e, ctx.name.getText)() } else if (ctx.identifierList != null) { MultiAlias(e, visitIdentifierList(ctx.identifierList)) } else { e } } /** * Combine a number of boolean expressions into a balanced expression tree. These expressions are * either combined by a logical [[And]] or a logical [[Or]]. * * A balanced binary tree is created because regular left recursive trees cause considerable * performance degradations and can cause stack overflows.  override def visitLogicalBinary(ctx: LogicalBinaryContext): Expression = withOrigin(ctx) { val expressionType = ctx.operator.getType val expressionCombiner = expressionType match { case SqlBaseParser.AND => And.apply _ case SqlBaseParser.OR => Or.apply _ } // Collect all similar left hand contexts. val contexts = ArrayBuffer(ctx.right) var current = ctx.left def collectContexts: Boolean = current match { case lbc: LogicalBinaryContext if lbc.operator.getType == expressionType => contexts += lbc.right current = lbc.left true case _ => contexts += current false } while (collectContexts) { // No body - all updates take place in the collectContexts. } // Reverse the contexts to have them in the same sequence as in the SQL statement & turn them // into expressions. val expressions = contexts.reverseMap(expression) // Create a balanced tree. def reduceToExpressionTree(low: Int, high: Int): Expression = high - low match { case 0 => expressions(low) case 1 => expressionCombiner(expressions(low), expressions(high)) case x => val mid = low + x / 2 expressionCombiner( reduceToExpressionTree(low, mid), reduceToExpressionTree(mid + 1, high)) } reduceToExpressionTree(0, expressions.size - 1) } /** * Invert a boolean expression.  override def visitLogicalNot(ctx: LogicalNotContext): Expression = withOrigin(ctx) { Not(expression(ctx.booleanExpression())) } /** * Create a filtering correlated sub-query (EXISTS).  override def visitExists(ctx: ExistsContext): Expression = { Exists(plan(ctx.query)) } /** * Create a comparison expression. This compares two expressions. The following comparison * operators are supported: * - Equal: '=' or '==' * - Null-safe Equal: '<=>' * - Not Equal: '<>' or '!=' * - Less than: '<' * - Less then or Equal: '<=' * - Greater than: '>' * - Greater then or Equal: '>='  override def visitComparison(ctx: ComparisonContext): Expression = withOrigin(ctx) { val left = expression(ctx.left) val right = expression(ctx.right) val operator = ctx.comparisonOperator().getChild(0).asInstanceOf[TerminalNode] operator.getSymbol.getType match { case SqlBaseParser.EQ => EqualTo(left, right) case SqlBaseParser.NSEQ => EqualNullSafe(left, right) case SqlBaseParser.NEQ | SqlBaseParser.NEQJ => Not(EqualTo(left, right)) case SqlBaseParser.LT => LessThan(left, right) case SqlBaseParser.LTE => LessThanOrEqual(left, right) case SqlBaseParser.GT => GreaterThan(left, right) case SqlBaseParser.GTE => GreaterThanOrEqual(left, right) } } /** * Create a predicated expression. A predicated expression is a normal expression with a * predicate attached to it, for example: * {{{ * a + 1 IS NULL * }}}  override def visitPredicated(ctx: PredicatedContext): Expression = withOrigin(ctx) { val e = expression(ctx.valueExpression) if (ctx.predicate != null) { withPredicate(e, ctx.predicate) } else { e } } /** * Add a predicate to the given expression. Supported expressions are: * - (NOT) BETWEEN * - (NOT) IN * - (NOT) (LIKE | ILIKE) (ANY | SOME | ALL) * - (NOT) RLIKE * - IS (NOT) NULL. * - IS (NOT) (TRUE | FALSE | UNKNOWN) * - IS (NOT) DISTINCT FROM  private def withPredicate(e: Expression, ctx: PredicateContext): Expression = withOrigin(ctx) { // Invert a predicate if it has a valid NOT clause. def invertIfNotDefined(e: Expression): Expression = ctx.NOT match { case null => e case not => Not(e) } def getValueExpressions(e: Expression): Seq[Expression] = e match { case c: CreateNamedStruct => c.valExprs case other => Seq(other) } def lowerLikeArgsIfNeeded( expr: Expression, patterns: Seq[UTF8String]): (Expression, Seq[UTF8String]) = ctx.kind.getType match { // scalastyle:off caselocale case SqlBaseParser.ILIKE => (Lower(expr), patterns.map(_.toLowerCase)) // scalastyle:on caselocale case _ => (expr, patterns) } def getLike(expr: Expression, pattern: Expression): Expression = ctx.kind.getType match { case SqlBaseParser.ILIKE => new ILike(expr, pattern) case _ => new Like(expr, pattern) } // Create the predicate. ctx.kind.getType match { case SqlBaseParser.BETWEEN => // BETWEEN is translated to lower <= e && e <= upper invertIfNotDefined(And( GreaterThanOrEqual(e, expression(ctx.lower)), LessThanOrEqual(e, expression(ctx.upper)))) case SqlBaseParser.IN if ctx.query != null => invertIfNotDefined(InSubquery(getValueExpressions(e), ListQuery(plan(ctx.query)))) case SqlBaseParser.IN => invertIfNotDefined(In(e, ctx.expression.asScala.map(expression).toSeq)) case SqlBaseParser.LIKE | SqlBaseParser.ILIKE => Option(ctx.quantifier).map(_.getType) match { case Some(SqlBaseParser.ANY) | Some(SqlBaseParser.SOME) => validate(!ctx.expression.isEmpty, \"Expected something between '(' and ')'.\", ctx) val expressions = expressionList(ctx.expression) if (expressions.forall(_.foldable) && expressions.forall(_.dataType == StringType)) { // If there are many pattern expressions, will throw StackOverflowError. // So we use LikeAny or NotLikeAny instead. val patterns = expressions.map(_.eval(EmptyRow).asInstanceOf[UTF8String]) val (expr, pat) = lowerLikeArgsIfNeeded(e, patterns) ctx.NOT match { case null => LikeAny(expr, pat) case _ => NotLikeAny(expr, pat) } } else { ctx.expression.asScala.map(expression) .map(p => invertIfNotDefined(getLike(e, p))).toSeq.reduceLeft(Or) } case Some(SqlBaseParser.ALL) => validate(!ctx.expression.isEmpty, \"Expected something between '(' and ')'.\", ctx) val expressions = expressionList(ctx.expression) if (expressions.forall(_.foldable) && expressions.forall(_.dataType == StringType)) { // If there are many pattern expressions, will throw StackOverflowError. // So we use LikeAll or NotLikeAll instead. val patterns = expressions.map(_.eval(EmptyRow).asInstanceOf[UTF8String]) val (expr, pat) = lowerLikeArgsIfNeeded(e, patterns) ctx.NOT match { case null => LikeAll(expr, pat) case _ => NotLikeAll(expr, pat) } } else { ctx.expression.asScala.map(expression) .map(p => invertIfNotDefined(getLike(e, p))).toSeq.reduceLeft(And) } case _ => val escapeChar = Option(ctx.escapeChar).map(string).map { str => if (str.length != 1) { throw QueryParsingErrors.invalidEscapeStringError(ctx) } str.charAt(0) }.getOrElse('\\\\') val likeExpr = ctx.kind.getType match { case SqlBaseParser.ILIKE => ILike(e, expression(ctx.pattern), escapeChar) case _ => Like(e, expression(ctx.pattern), escapeChar) } invertIfNotDefined(likeExpr) } case SqlBaseParser.RLIKE => invertIfNotDefined(RLike(e, expression(ctx.pattern))) case SqlBaseParser.NULL if ctx.NOT != null => IsNotNull(e) case SqlBaseParser.NULL => IsNull(e) case SqlBaseParser.TRUE => ctx.NOT match { case null => EqualNullSafe(e, Literal(true)) case _ => Not(EqualNullSafe(e, Literal(true))) } case SqlBaseParser.FALSE => ctx.NOT match { case null => EqualNullSafe(e, Literal(false)) case _ => Not(EqualNullSafe(e, Literal(false))) } case SqlBaseParser.UNKNOWN => ctx.NOT match { case null => IsUnknown(e) case _ => IsNotUnknown(e) } case SqlBaseParser.DISTINCT if ctx.NOT != null => EqualNullSafe(e, expression(ctx.right)) case SqlBaseParser.DISTINCT => Not(EqualNullSafe(e, expression(ctx.right))) } } /** * Create a binary arithmetic expression. The following arithmetic operators are supported: * - Multiplication: '*' * - Division: '/' * - Hive Long Division: 'DIV' * - Modulo: '%' * - Addition: '+' * - Subtraction: '-' * - Binary AND: '&' * - Binary XOR * - Binary OR: '|'  override def visitArithmeticBinary(ctx: ArithmeticBinaryContext): Expression = withOrigin(ctx) { val left = expression(ctx.left) val right = expression(ctx.right) ctx.operator.getType match { case SqlBaseParser.ASTERISK => Multiply(left, right) case SqlBaseParser.SLASH => Divide(left, right) case SqlBaseParser.PERCENT => Remainder(left, right) case SqlBaseParser.DIV => IntegralDivide(left, right) case SqlBaseParser.PLUS => Add(left, right) case SqlBaseParser.MINUS => Subtract(left, right) case SqlBaseParser.CONCAT_PIPE => Concat(left :: right :: Nil) case SqlBaseParser.AMPERSAND => BitwiseAnd(left, right) case SqlBaseParser.HAT => BitwiseXor(left, right) case SqlBaseParser.PIPE => BitwiseOr(left, right) } } /** * Create a unary arithmetic expression. The following arithmetic operators are supported: * - Plus: '+' * - Minus: '-' * - Bitwise Not: '~'  override def visitArithmeticUnary(ctx: ArithmeticUnaryContext): Expression = withOrigin(ctx) { val value = expression(ctx.valueExpression) ctx.operator.getType match { case SqlBaseParser.PLUS => UnaryPositive(value) case SqlBaseParser.MINUS => UnaryMinus(value) case SqlBaseParser.TILDE => BitwiseNot(value) } } override def visitCurrentLike(ctx: CurrentLikeContext): Expression = withOrigin(ctx) { if (conf.enforceReservedKeywords) { ctx.name.getType match { case SqlBaseParser.CURRENT_DATE => CurrentDate() case SqlBaseParser.CURRENT_TIMESTAMP => CurrentTimestamp() case SqlBaseParser.CURRENT_USER => CurrentUser() } } else { // If the parser is not in ansi mode, we should return `UnresolvedAttribute`, in case there // are columns named `CURRENT_DATE` or `CURRENT_TIMESTAMP`. UnresolvedAttribute.quoted(ctx.name.getText) } } /** * Create a [[Cast]] expression.  override def visitCast(ctx: CastContext): Expression = withOrigin(ctx) { val rawDataType = typedVisit[DataType](ctx.dataType()) val dataType = CharVarcharUtils.replaceCharVarcharWithStringForCast(rawDataType) val cast = ctx.name.getType match { case SqlBaseParser.CAST => Cast(expression(ctx.expression), dataType) case SqlBaseParser.TRY_CAST => TryCast(expression(ctx.expression), dataType) } cast.setTagValue(Cast.USER_SPECIFIED_CAST, true) cast } /** * Create a [[CreateStruct]] expression.  override def visitStruct(ctx: StructContext): Expression = withOrigin(ctx) { CreateStruct.create(ctx.argument.asScala.map(expression).toSeq) } /** * Create a [[First]] expression.  override def visitFirst(ctx: FirstContext): Expression = withOrigin(ctx) { val ignoreNullsExpr = ctx.IGNORE != null First(expression(ctx.expression), ignoreNullsExpr).toAggregateExpression() } /** * Create a [[Last]] expression.  override def visitLast(ctx: LastContext): Expression = withOrigin(ctx) { val ignoreNullsExpr = ctx.IGNORE != null Last(expression(ctx.expression), ignoreNullsExpr).toAggregateExpression() } /** * Create a Position expression.  override def visitPosition(ctx: PositionContext): Expression = withOrigin(ctx) { new StringLocate(expression(ctx.substr), expression(ctx.str)) } /** * Create a Extract expression.  override def visitExtract(ctx: ExtractContext): Expression = withOrigin(ctx) { val arguments = Seq(Literal(ctx.field.getText), expression(ctx.source)) UnresolvedFunction(\"extract\", arguments, isDistinct = false) } /** * Create a Percentile expression.  override def visitPercentile(ctx: PercentileContext): Expression = withOrigin(ctx) { val percentage = expression(ctx.percentage) val sortOrder = visitSortItem(ctx.sortItem) val percentile = ctx.name.getType match { case SqlBaseParser.PERCENTILE_CONT => sortOrder.direction match { case Ascending => PercentileCont(sortOrder.child, percentage) case Descending => PercentileCont(sortOrder.child, percentage, true) } case SqlBaseParser.PERCENTILE_DISC => sortOrder.direction match { case Ascending => PercentileDisc(sortOrder.child, percentage) case Descending => PercentileDisc(sortOrder.child, percentage, true) } } val aggregateExpression = percentile.toAggregateExpression() ctx.windowSpec match { case spec: WindowRefContext => UnresolvedWindowExpression(aggregateExpression, visitWindowRef(spec)) case spec: WindowDefContext => WindowExpression(aggregateExpression, visitWindowDef(spec)) case _ => aggregateExpression } } /** * Create a Substring/Substr expression.  override def visitSubstring(ctx: SubstringContext): Expression = withOrigin(ctx) { if (ctx.len != null) { Substring(expression(ctx.str), expression(ctx.pos), expression(ctx.len)) } else { new Substring(expression(ctx.str), expression(ctx.pos)) } } /** * Create a Trim expression.  override def visitTrim(ctx: TrimContext): Expression = withOrigin(ctx) { val srcStr = expression(ctx.srcStr) val trimStr = Option(ctx.trimStr).map(expression) Option(ctx.trimOption).map(_.getType).getOrElse(SqlBaseParser.BOTH) match { case SqlBaseParser.BOTH => StringTrim(srcStr, trimStr) case SqlBaseParser.LEADING => StringTrimLeft(srcStr, trimStr) case SqlBaseParser.TRAILING => StringTrimRight(srcStr, trimStr) case other => throw QueryParsingErrors.trimOptionUnsupportedError(other, ctx) } } /** * Create a Overlay expression.  override def visitOverlay(ctx: OverlayContext): Expression = withOrigin(ctx) { val input = expression(ctx.input) val replace = expression(ctx.replace) val position = expression(ctx.position) val lengthOpt = Option(ctx.length).map(expression) lengthOpt match { case Some(length) => Overlay(input, replace, position, length) case None => new Overlay(input, replace, position) } } /** * Create a (windowed) Function expression.  override def visitFunctionCall(ctx: FunctionCallContext): Expression = withOrigin(ctx) { // Create the function call. val name = ctx.functionName.getText val isDistinct = Option(ctx.setQuantifier()).exists(_.DISTINCT != null) // Call `toSeq`, otherwise `ctx.argument.asScala.map(expression)` is `Buffer` in Scala 2.13 val arguments = ctx.argument.asScala.map(expression).toSeq match { case Seq(UnresolvedStar(None)) if name.toLowerCase(Locale.ROOT) == \"count\" && !isDistinct => // Transform COUNT(*) into COUNT(1). Seq(Literal(1)) case expressions => expressions } val filter = Option(ctx.where).map(expression(_)) val ignoreNulls = Option(ctx.nullsOption).map(_.getType == SqlBaseParser.IGNORE).getOrElse(false) val function = UnresolvedFunction( getFunctionMultiparts(ctx.functionName), arguments, isDistinct, filter, ignoreNulls) // Check if the function is evaluated in a windowed context. ctx.windowSpec match { case spec: WindowRefContext => UnresolvedWindowExpression(function, visitWindowRef(spec)) case spec: WindowDefContext => WindowExpression(function, visitWindowDef(spec)) case _ => function } } /** * Create a function database (optional) and name pair.  protected def visitFunctionName(ctx: QualifiedNameContext): FunctionIdentifier = { visitFunctionName(ctx, ctx.identifier().asScala.map(_.getText).toSeq) } /** * Create a function database (optional) and name pair.  private def visitFunctionName(ctx: ParserRuleContext, texts: Seq[String]): FunctionIdentifier = { texts match { case Seq(db, fn) => FunctionIdentifier(fn, Option(db)) case Seq(fn) => FunctionIdentifier(fn, None) case other => throw QueryParsingErrors.functionNameUnsupportedError(texts.mkString(\".\"), ctx) } } protected def getFunctionMultiparts(ctx: FunctionNameContext): Seq[String] = { if (ctx.qualifiedName != null) { ctx.qualifiedName().identifier().asScala.map(_.getText).toSeq } else { Seq(ctx.getText) } } /** * Create an [[LambdaFunction]].  override def visitLambda(ctx: LambdaContext): Expression = withOrigin(ctx) { val arguments = ctx.identifier().asScala.map { name => UnresolvedNamedLambdaVariable(UnresolvedAttribute.quoted(name.getText).nameParts) } val function = expression(ctx.expression).transformUp { case a: UnresolvedAttribute => UnresolvedNamedLambdaVariable(a.nameParts) } LambdaFunction(function, arguments.toSeq) } /** * Create a reference to a window frame, i.e. [[WindowSpecReference]].  override def visitWindowRef(ctx: WindowRefContext): WindowSpecReference = withOrigin(ctx) { WindowSpecReference(ctx.name.getText) } /** * Create a window definition, i.e. [[WindowSpecDefinition]].  override def visitWindowDef(ctx: WindowDefContext): WindowSpecDefinition = withOrigin(ctx) { // CLUSTER BY ... | PARTITION BY ... ORDER BY ... val partition = ctx.partition.asScala.map(expression) val order = ctx.sortItem.asScala.map(visitSortItem) // RANGE/ROWS BETWEEN ... val frameSpecOption = Option(ctx.windowFrame).map { frame => val frameType = frame.frameType.getType match { case SqlBaseParser.RANGE => RangeFrame case SqlBaseParser.ROWS => RowFrame } SpecifiedWindowFrame( frameType, visitFrameBound(frame.start), Option(frame.end).map(visitFrameBound).getOrElse(CurrentRow)) } WindowSpecDefinition( partition.toSeq, order.toSeq, frameSpecOption.getOrElse(UnspecifiedFrame)) } /** * Create or resolve a frame boundary expressions.  override def visitFrameBound(ctx: FrameBoundContext): Expression = withOrigin(ctx) { def value: Expression = { val e = expression(ctx.expression) validate(e.resolved && e.foldable, \"Frame bound value must be a literal.\", ctx) e } ctx.boundType.getType match { case SqlBaseParser.PRECEDING if ctx.UNBOUNDED != null => UnboundedPreceding case SqlBaseParser.PRECEDING => UnaryMinus(value) case SqlBaseParser.CURRENT => CurrentRow case SqlBaseParser.FOLLOWING if ctx.UNBOUNDED != null => UnboundedFollowing case SqlBaseParser.FOLLOWING => value } } /** * Create a [[CreateStruct]] expression.  override def visitRowConstructor(ctx: RowConstructorContext): Expression = withOrigin(ctx) { CreateStruct(ctx.namedExpression().asScala.map(expression).toSeq) } /** * Create a [[ScalarSubquery]] expression.  override def visitSubqueryExpression( ctx: SubqueryExpressionContext): Expression = withOrigin(ctx) { ScalarSubquery(plan(ctx.query)) } /** * Create a value based [[CaseWhen]] expression. This has the following SQL form: * {{{ * CASE [expression] * WHEN [value] THEN [expression] * ... * ELSE [expression] * END * }}}  override def visitSimpleCase(ctx: SimpleCaseContext): Expression = withOrigin(ctx) { val e = expression(ctx.value) val branches = ctx.whenClause.asScala.map { wCtx => (EqualTo(e, expression(wCtx.condition)), expression(wCtx.result)) } CaseWhen(branches.toSeq, Option(ctx.elseExpression).map(expression)) } /** * Create a condition based [[CaseWhen]] expression. This has the following SQL syntax: * {{{ * CASE * WHEN [predicate] THEN [expression] * ... * ELSE [expression] * END * }}} * * @param ctx the parse tree *  override def visitSearchedCase(ctx: SearchedCaseContext): Expression = withOrigin(ctx) { val branches = ctx.whenClause.asScala.map { wCtx => (expression(wCtx.condition), expression(wCtx.result)) } CaseWhen(branches.toSeq, Option(ctx.elseExpression).map(expression)) } /** * Currently only regex in expressions of SELECT statements are supported; in other * places, e.g., where `(a)?+.+` = 2, regex are not meaningful.  private def canApplyRegex(ctx: ParserRuleContext): Boolean = withOrigin(ctx) { var parent = ctx.getParent while (parent != null) { if (parent.isInstanceOf[NamedExpressionContext]) return true parent = parent.getParent } return false } /** * Returns whether the pattern is a regex expression (instead of a normal * string). Normal string is a string with all alphabets/digits and \"_\".  private def isRegex(pattern: String): Boolean = { pattern.exists(p => !Character.isLetterOrDigit(p) && p != '_') } /** * Create a dereference expression. The return type depends on the type of the parent. * If the parent is an [[UnresolvedAttribute]], it can be a [[UnresolvedAttribute]] or * a [[UnresolvedRegex]] for regex quoted in ``; if the parent is some other expression, * it can be [[UnresolvedExtractValue]].  override def visitDereference(ctx: DereferenceContext): Expression = withOrigin(ctx) { val attr = ctx.fieldName.getText expression(ctx.base) match { case unresolved_attr @ UnresolvedAttribute(nameParts) => ctx.fieldName.getStart.getText match { case escapedIdentifier(columnNameRegex) if conf.supportQuotedRegexColumnName && isRegex(columnNameRegex) && canApplyRegex(ctx) => UnresolvedRegex(columnNameRegex, Some(unresolved_attr.name), conf.caseSensitiveAnalysis) case _ => UnresolvedAttribute(nameParts :+ attr) } case e => UnresolvedExtractValue(e, Literal(attr)) } } /** * Create an [[UnresolvedAttribute]] expression or a [[UnresolvedRegex]] if it is a regex * quoted in ``  override def visitColumnReference(ctx: ColumnReferenceContext): Expression = withOrigin(ctx) { ctx.getStart.getText match { case escapedIdentifier(columnNameRegex) if conf.supportQuotedRegexColumnName && isRegex(columnNameRegex) && canApplyRegex(ctx) => UnresolvedRegex(columnNameRegex, None, conf.caseSensitiveAnalysis) case _ => UnresolvedAttribute.quoted(ctx.getText) } } /** * Create an [[UnresolvedExtractValue]] expression, this is used for subscript access to an array.  override def visitSubscript(ctx: SubscriptContext): Expression = withOrigin(ctx) { UnresolvedExtractValue(expression(ctx.value), expression(ctx.index)) } /** * Create an expression for an expression between parentheses. This is need because the ANTLR * visitor cannot automatically convert the nested context into an expression.  override def visitParenthesizedExpression( ctx: ParenthesizedExpressionContext): Expression = withOrigin(ctx) { expression(ctx.expression) } /** * Create a [[SortOrder]] expression.  override def visitSortItem(ctx: SortItemContext): SortOrder = withOrigin(ctx) { val direction = if (ctx.DESC != null) { Descending } else { Ascending } val nullOrdering = if (ctx.FIRST != null) { NullsFirst } else if (ctx.LAST != null) { NullsLast } else { direction.defaultNullOrdering } SortOrder(expression(ctx.expression), direction, nullOrdering, Seq.empty) } /** * Create a typed Literal expression. A typed literal has the following SQL syntax: * {{{ * [TYPE] '[VALUE]' * }}} * Currently Date, Timestamp, Interval and Binary typed literals are supported.  override def visitTypeConstructor(ctx: TypeConstructorContext): Literal = withOrigin(ctx) { val value = string(ctx.STRING) val valueType = ctx.identifier.getText.toUpperCase(Locale.ROOT) def toLiteral[T](f: UTF8String => Option[T], t: DataType): Literal = { f(UTF8String.fromString(value)).map(Literal(_, t)).getOrElse { throw QueryParsingErrors.cannotParseValueTypeError(valueType, value, ctx) } } def constructTimestampLTZLiteral(value: String): Literal = { val zoneId = getZoneId(conf.sessionLocalTimeZone) val specialTs = convertSpecialTimestamp(value, zoneId).map(Literal(_, TimestampType)) specialTs.getOrElse(toLiteral(stringToTimestamp(_, zoneId), TimestampType)) } try { valueType match { case \"DATE\" => val zoneId = getZoneId(conf.sessionLocalTimeZone) val specialDate = convertSpecialDate(value, zoneId).map(Literal(_, DateType)) specialDate.getOrElse(toLiteral(stringToDate, DateType)) case \"TIMESTAMP_NTZ\" if isTesting => convertSpecialTimestampNTZ(value, getZoneId(conf.sessionLocalTimeZone)) .map(Literal(_, TimestampNTZType)) .getOrElse(toLiteral(stringToTimestampWithoutTimeZone, TimestampNTZType)) case \"TIMESTAMP_LTZ\" if isTesting => constructTimestampLTZLiteral(value) case \"TIMESTAMP\" => SQLConf.get.timestampType match { case TimestampNTZType => convertSpecialTimestampNTZ(value, getZoneId(conf.sessionLocalTimeZone)) .map(Literal(_, TimestampNTZType)) .getOrElse { val containsTimeZonePart = DateTimeUtils.parseTimestampString(UTF8String.fromString(value))._2.isDefined // If the input string contains time zone part, return a timestamp with local time // zone literal. if (containsTimeZonePart) { constructTimestampLTZLiteral(value) } else { toLiteral(stringToTimestampWithoutTimeZone, TimestampNTZType) } } case TimestampType => constructTimestampLTZLiteral(value) } case \"INTERVAL\" => val interval = try { IntervalUtils.stringToInterval(UTF8String.fromString(value)) } catch { case e: IllegalArgumentException => val ex = QueryParsingErrors.cannotParseIntervalValueError(value, ctx) ex.setStackTrace(e.getStackTrace) throw ex } if (!conf.legacyIntervalEnabled) { val units = value .split(\"\\\\s\") .map(_.toLowerCase(Locale.ROOT).stripSuffix(\"s\")) .filter(s => s != \"interval\" && s.matches(\"[a-z]+\")) constructMultiUnitsIntervalLiteral(ctx, interval, units) } else { Literal(interval, CalendarIntervalType) } case \"X\" => val padding = if (value.length % 2 != 0) \"0\" else \"\" try { Literal(Hex.decodeHex(padding + value)) } catch { case _: DecoderException => throw new IllegalArgumentException( s\"contains illegal character for hexBinary: $padding$value\"); } case other => throw QueryParsingErrors.literalValueTypeUnsupportedError(other, ctx) } } catch { case e: IllegalArgumentException => throw QueryParsingErrors.parsingValueTypeError(e, valueType, ctx) } } /** * Create a NULL literal expression.  override def visitNullLiteral(ctx: NullLiteralContext): Literal = withOrigin(ctx) { Literal(null) } /** * Create a Boolean literal expression.  override def visitBooleanLiteral(ctx: BooleanLiteralContext): Literal = withOrigin(ctx) { if (ctx.getText.toBoolean) { Literal.TrueLiteral } else { Literal.FalseLiteral } } /** * Create an integral literal expression. The code selects the most narrow integral type * possible, either a BigDecimal, a Long or an Integer is returned.  override def visitIntegerLiteral(ctx: IntegerLiteralContext): Literal = withOrigin(ctx) { BigDecimal(ctx.getText) match { case v if v.isValidInt => Literal(v.intValue) case v if v.isValidLong => Literal(v.longValue) case v => Literal(v.underlying()) } } /** * Create a decimal literal for a regular decimal number.  override def visitDecimalLiteral(ctx: DecimalLiteralContext): Literal = withOrigin(ctx) { Literal(BigDecimal(ctx.getText).underlying()) } /** * Create a decimal literal for a regular decimal number or a scientific decimal number.  override def visitLegacyDecimalLiteral( ctx: LegacyDecimalLiteralContext): Literal = withOrigin(ctx) { Literal(BigDecimal(ctx.getText).underlying()) } /** * Create a double literal for number with an exponent, e.g. 1E-30  override def visitExponentLiteral(ctx: ExponentLiteralContext): Literal = { numericLiteral(ctx, ctx.getText, /* exponent values don't have a suffix  Double.MinValue, Double.MaxValue, DoubleType.simpleString)(_.toDouble) } /** Create a numeric literal expression.  private def numericLiteral( ctx: NumberContext, rawStrippedQualifier: String, minValue: BigDecimal, maxValue: BigDecimal, typeName: String)(converter: String => Any): Literal = withOrigin(ctx) { try { val rawBigDecimal = BigDecimal(rawStrippedQualifier) if (rawBigDecimal < minValue || rawBigDecimal > maxValue) { throw QueryParsingErrors.invalidNumericLiteralRangeError( rawStrippedQualifier, minValue, maxValue, typeName, ctx) } Literal(converter(rawStrippedQualifier)) } catch { case e: NumberFormatException => throw new ParseException(e.getMessage, ctx) } } /** * Create a Byte Literal expression.  override def visitTinyIntLiteral(ctx: TinyIntLiteralContext): Literal = { val rawStrippedQualifier = ctx.getText.substring(0, ctx.getText.length - 1) numericLiteral(ctx, rawStrippedQualifier, Byte.MinValue, Byte.MaxValue, ByteType.simpleString)(_.toByte) } /** * Create a Short Literal expression.  override def visitSmallIntLiteral(ctx: SmallIntLiteralContext): Literal = { val rawStrippedQualifier = ctx.getText.substring(0, ctx.getText.length - 1) numericLiteral(ctx, rawStrippedQualifier, Short.MinValue, Short.MaxValue, ShortType.simpleString)(_.toShort) } /** * Create a Long Literal expression.  override def visitBigIntLiteral(ctx: BigIntLiteralContext): Literal = { val rawStrippedQualifier = ctx.getText.substring(0, ctx.getText.length - 1) numericLiteral(ctx, rawStrippedQualifier, Long.MinValue, Long.MaxValue, LongType.simpleString)(_.toLong) } /** * Create a Float Literal expression.  override def visitFloatLiteral(ctx: FloatLiteralContext): Literal = { val rawStrippedQualifier = ctx.getText.substring(0, ctx.getText.length - 1) numericLiteral(ctx, rawStrippedQualifier, Float.MinValue, Float.MaxValue, FloatType.simpleString)(_.toFloat) } /** * Create a Double Literal expression.  override def visitDoubleLiteral(ctx: DoubleLiteralContext): Literal = { val rawStrippedQualifier = ctx.getText.substring(0, ctx.getText.length - 1) numericLiteral(ctx, rawStrippedQualifier, Double.MinValue, Double.MaxValue, DoubleType.simpleString)(_.toDouble) } /** * Create a BigDecimal Literal expression.  override def visitBigDecimalLiteral(ctx: BigDecimalLiteralContext): Literal = { val raw = ctx.getText.substring(0, ctx.getText.length - 2) try { Literal(BigDecimal(raw).underlying()) } catch { case e: AnalysisException => throw new ParseException(e.message, ctx) } } /** * Create a String literal expression.  override def visitStringLiteral(ctx: StringLiteralContext): Literal = withOrigin(ctx) { Literal(createString(ctx)) } /** * Create a String from a string literal context. This supports multiple consecutive string * literals, these are concatenated, for example this expression \"'hello' 'world'\" will be * converted into \"helloworld\". * * Special characters can be escaped by using Hive/C-style escaping.  private def createString(ctx: StringLiteralContext): String = { if (conf.escapedStringLiterals) { ctx.STRING().asScala.map(stringWithoutUnescape).mkString } else { ctx.STRING().asScala.map(string).mkString } } /** * Create an [[UnresolvedRelation]] from a multi-part identifier context.  private def createUnresolvedRelation( ctx: MultipartIdentifierContext): UnresolvedRelation = withOrigin(ctx) { UnresolvedRelation(visitMultipartIdentifier(ctx)) } /** * Create an [[UnresolvedTable]] from a multi-part identifier context.  private def createUnresolvedTable( ctx: MultipartIdentifierContext, commandName: String, relationTypeMismatchHint: Option[String] = None): UnresolvedTable = withOrigin(ctx) { UnresolvedTable(visitMultipartIdentifier(ctx), commandName, relationTypeMismatchHint) } /** * Create an [[UnresolvedView]] from a multi-part identifier context.  private def createUnresolvedView( ctx: MultipartIdentifierContext, commandName: String, allowTemp: Boolean = true, relationTypeMismatchHint: Option[String] = None): UnresolvedView = withOrigin(ctx) { UnresolvedView(visitMultipartIdentifier(ctx), commandName, allowTemp, relationTypeMismatchHint) } /** * Create an [[UnresolvedTableOrView]] from a multi-part identifier context.  private def createUnresolvedTableOrView( ctx: MultipartIdentifierContext, commandName: String, allowTempView: Boolean = true): UnresolvedTableOrView = withOrigin(ctx) { UnresolvedTableOrView(visitMultipartIdentifier(ctx), commandName, allowTempView) } /** * Construct an [[Literal]] from [[CalendarInterval]] and * units represented as a [[Seq]] of [[String]].  private def constructMultiUnitsIntervalLiteral( ctx: ParserRuleContext, calendarInterval: CalendarInterval, units: Seq[String]): Literal = { val yearMonthFields = Set.empty[Byte] val dayTimeFields = Set.empty[Byte] for (unit <- units) { if (YearMonthIntervalType.stringToField.contains(unit)) { yearMonthFields += YearMonthIntervalType.stringToField(unit) } else if (DayTimeIntervalType.stringToField.contains(unit)) { dayTimeFields += DayTimeIntervalType.stringToField(unit) } else if (unit == \"week\") { dayTimeFields += DayTimeIntervalType.DAY } else { assert(unit == \"millisecond\" || unit == \"microsecond\") dayTimeFields += DayTimeIntervalType.SECOND } } if (yearMonthFields.nonEmpty) { if (dayTimeFields.nonEmpty) { val literalStr = source(ctx) throw QueryParsingErrors.mixedIntervalUnitsError(literalStr, ctx) } Literal( calendarInterval.months, YearMonthIntervalType(yearMonthFields.min, yearMonthFields.max) ) } else { Literal( IntervalUtils.getDuration(calendarInterval, TimeUnit.MICROSECONDS), DayTimeIntervalType(dayTimeFields.min, dayTimeFields.max)) } } /** * Create a [[CalendarInterval]] or ANSI interval literal expression. * Two syntaxes are supported: * - multiple unit value pairs, for instance: interval 2 months 2 days. * - from-to unit, for instance: interval '1-2' year to month.  override def visitInterval(ctx: IntervalContext): Literal = withOrigin(ctx) { val calendarInterval = parseIntervalLiteral(ctx) if (ctx.errorCapturingUnitToUnitInterval != null && !conf.legacyIntervalEnabled) { // Check the `to` unit to distinguish year-month and day-time intervals because // `CalendarInterval` doesn't have enough info. For instance, new CalendarInterval(0, 0, 0) // can be derived from INTERVAL '0-0' YEAR TO MONTH as well as from // INTERVAL '0 00:00:00' DAY TO SECOND. val fromUnit = ctx.errorCapturingUnitToUnitInterval.body.from.getText.toLowerCase(Locale.ROOT) val toUnit = ctx.errorCapturingUnitToUnitInterval.body.to.getText.toLowerCase(Locale.ROOT) if (toUnit == \"month\") { assert(calendarInterval.days == 0 && calendarInterval.microseconds == 0) val start = YearMonthIntervalType.stringToField(fromUnit) Literal(calendarInterval.months, YearMonthIntervalType(start, YearMonthIntervalType.MONTH)) } else { assert(calendarInterval.months == 0) val micros = IntervalUtils.getDuration(calendarInterval, TimeUnit.MICROSECONDS) val start = DayTimeIntervalType.stringToField(fromUnit) val end = DayTimeIntervalType.stringToField(toUnit) Literal(micros, DayTimeIntervalType(start, end)) } } else if (ctx.errorCapturingMultiUnitsInterval != null && !conf.legacyIntervalEnabled) { val units = ctx.errorCapturingMultiUnitsInterval.body.unit.asScala.map( _.getText.toLowerCase(Locale.ROOT).stripSuffix(\"s\")).toSeq constructMultiUnitsIntervalLiteral(ctx, calendarInterval, units) } else { Literal(calendarInterval, CalendarIntervalType) } } /** * Create a [[CalendarInterval]] object  protected def parseIntervalLiteral(ctx: IntervalContext): CalendarInterval = withOrigin(ctx) { if (ctx.errorCapturingMultiUnitsInterval != null) { val innerCtx = ctx.errorCapturingMultiUnitsInterval if (innerCtx.unitToUnitInterval != null) { throw QueryParsingErrors.moreThanOneFromToUnitInIntervalLiteralError( innerCtx.unitToUnitInterval) } visitMultiUnitsInterval(innerCtx.multiUnitsInterval) } else if (ctx.errorCapturingUnitToUnitInterval != null) { val innerCtx = ctx.errorCapturingUnitToUnitInterval if (innerCtx.error1 != null || innerCtx.error2 != null) { val errorCtx = if (innerCtx.error1 != null) innerCtx.error1 else innerCtx.error2 throw QueryParsingErrors.moreThanOneFromToUnitInIntervalLiteralError(errorCtx) } visitUnitToUnitInterval(innerCtx.body) } else { throw QueryParsingErrors.invalidIntervalLiteralError(ctx) } } /** * Creates a [[CalendarInterval]] with multiple unit value pairs, e.g. 1 YEAR 2 DAYS.  override def visitMultiUnitsInterval(ctx: MultiUnitsIntervalContext): CalendarInterval = { withOrigin(ctx) { val units = ctx.unit.asScala val values = ctx.intervalValue().asScala try { assert(units.length == values.length) val kvs = units.indices.map { i => val u = units(i).getText val v = if (values(i).STRING() != null) { val value = string(values(i).STRING()) // SPARK-32840: For invalid cases, e.g. INTERVAL '1 day 2' hour, // INTERVAL 'interval 1' day, we need to check ahead before they are concatenated with // units and become valid ones, e.g. '1 day 2 hour'. // Ideally, we only ensure the value parts don't contain any units here. if (value.exists(Character.isLetter)) { throw QueryParsingErrors.invalidIntervalFormError(value, ctx) } if (values(i).MINUS() == null) { value } else if (value.startsWith(\"-\")) { value.replaceFirst(\"-\", \"\") } else { s\"-$value\" } } else { values(i).getText } UTF8String.fromString(\" \" + v + \" \" + u) } IntervalUtils.stringToInterval(UTF8String.concat(kvs: _*)) } catch { case i: IllegalArgumentException => val e = new ParseException(i.getMessage, ctx) e.setStackTrace(i.getStackTrace) throw e } } } /** * Creates a [[CalendarInterval]] with from-to unit, e.g. '2-1' YEAR TO MONTH.  override def visitUnitToUnitInterval(ctx: UnitToUnitIntervalContext): CalendarInterval = { withOrigin(ctx) { val value = Option(ctx.intervalValue.STRING).map(string).map { interval => if (ctx.intervalValue().MINUS() == null) { interval } else if (interval.startsWith(\"-\")) { interval.replaceFirst(\"-\", \"\") } else { s\"-$interval\" } }.getOrElse { throw QueryParsingErrors.invalidFromToUnitValueError(ctx.intervalValue) } try { val from = ctx.from.getText.toLowerCase(Locale.ROOT) val to = ctx.to.getText.toLowerCase(Locale.ROOT) (from, to) match { case (\"year\", \"month\") => IntervalUtils.fromYearMonthString(value) case (\"day\", \"hour\") | (\"day\", \"minute\") | (\"day\", \"second\") | (\"hour\", \"minute\") | (\"hour\", \"second\") | (\"minute\", \"second\") => IntervalUtils.fromDayTimeString(value, DayTimeIntervalType.stringToField(from), DayTimeIntervalType.stringToField(to)) case _ => throw QueryParsingErrors.fromToIntervalUnsupportedError(from, to, ctx) } } catch { // Handle Exceptions thrown by CalendarInterval case e: IllegalArgumentException => val pe = new ParseException(e.getMessage, ctx) pe.setStackTrace(e.getStackTrace) throw pe } } } /* ******************************************************************************************** * DataType parsing * ********************************************************************************************  /** * Resolve/create a primitive type.  override def visitPrimitiveDataType(ctx: PrimitiveDataTypeContext): DataType = withOrigin(ctx) { val dataType = ctx.identifier.getText.toLowerCase(Locale.ROOT) (dataType, ctx.INTEGER_VALUE().asScala.toList) match { case (\"boolean\", Nil) => BooleanType case (\"tinyint\" | \"byte\", Nil) => ByteType case (\"smallint\" | \"short\", Nil) => ShortType case (\"int\" | \"integer\", Nil) => IntegerType case (\"bigint\" | \"long\", Nil) => LongType case (\"float\" | \"real\", Nil) => FloatType case (\"double\", Nil) => DoubleType case (\"date\", Nil) => DateType case (\"timestamp\", Nil) => SQLConf.get.timestampType // SPARK-38813: Remove TimestampNTZ type support in Spark 3.3 with minimal code changes. case (\"timestamp_ntz\", Nil) if isTesting => TimestampNTZType case (\"timestamp_ltz\", Nil) if isTesting => TimestampType case (\"string\", Nil) => StringType case (\"character\" | \"char\", length :: Nil) => CharType(length.getText.toInt) case (\"varchar\", length :: Nil) => VarcharType(length.getText.toInt) case (\"binary\", Nil) => BinaryType case (\"decimal\" | \"dec\" | \"numeric\", Nil) => DecimalType.USER_DEFAULT case (\"decimal\" | \"dec\" | \"numeric\", precision :: Nil) => DecimalType(precision.getText.toInt, 0) case (\"decimal\" | \"dec\" | \"numeric\", precision :: scale :: Nil) => DecimalType(precision.getText.toInt, scale.getText.toInt) case (\"void\", Nil) => NullType case (\"interval\", Nil) => CalendarIntervalType case (dt @ (\"character\" | \"char\" | \"varchar\"), Nil) => throw QueryParsingErrors.charTypeMissingLengthError(dt, ctx) case (dt, params) => val dtStr = if (params.nonEmpty) s\"$dt(${params.mkString(\",\")})\" else dt throw QueryParsingErrors.dataTypeUnsupportedError(dtStr, ctx) } } override def visitYearMonthIntervalDataType(ctx: YearMonthIntervalDataTypeContext): DataType = { val startStr = ctx.from.getText.toLowerCase(Locale.ROOT) val start = YearMonthIntervalType.stringToField(startStr) if (ctx.to != null) { val endStr = ctx.to.getText.toLowerCase(Locale.ROOT) val end = YearMonthIntervalType.stringToField(endStr) if (end <= start) { throw QueryParsingErrors.fromToIntervalUnsupportedError(startStr, endStr, ctx) } YearMonthIntervalType(start, end) } else { YearMonthIntervalType(start) } } override def visitDayTimeIntervalDataType(ctx: DayTimeIntervalDataTypeContext): DataType = { val startStr = ctx.from.getText.toLowerCase(Locale.ROOT) val start = DayTimeIntervalType.stringToField(startStr) if (ctx.to != null ) { val endStr = ctx.to.getText.toLowerCase(Locale.ROOT) val end = DayTimeIntervalType.stringToField(endStr) if (end <= start) { throw QueryParsingErrors.fromToIntervalUnsupportedError(startStr, endStr, ctx) } DayTimeIntervalType(start, end) } else { DayTimeIntervalType(start) } } /** * Create a complex DataType. Arrays, Maps and Structures are supported.  override def visitComplexDataType(ctx: ComplexDataTypeContext): DataType = withOrigin(ctx) { ctx.complex.getType match { case SqlBaseParser.ARRAY => ArrayType(typedVisit(ctx.dataType(0))) case SqlBaseParser.MAP => MapType(typedVisit(ctx.dataType(0)), typedVisit(ctx.dataType(1))) case SqlBaseParser.STRUCT => StructType(Option(ctx.complexColTypeList).toSeq.flatMap(visitComplexColTypeList)) } } /** * Create top level table schema.  protected def createSchema(ctx: ColTypeListContext): StructType = { StructType(Option(ctx).toSeq.flatMap(visitColTypeList)) } /** * Create a [[StructType]] from a number of column definitions.  override def visitColTypeList(ctx: ColTypeListContext): Seq[StructField] = withOrigin(ctx) { ctx.colType().asScala.map(visitColType).toSeq } /** * Create a top level [[StructField]] from a column definition.  override def visitColType(ctx: ColTypeContext): StructField = withOrigin(ctx) { import ctx._ val builder = new MetadataBuilder // Add comment to metadata Option(commentSpec()).map(visitCommentSpec).foreach { builder.putString(\"comment\", _) } StructField( name = colName.getText, dataType = typedVisit[DataType](ctx.dataType), nullable = NULL == null, metadata = builder.build()) } /** * Create a [[StructType]] from a sequence of [[StructField]]s.  protected def createStructType(ctx: ComplexColTypeListContext): StructType = { StructType(Option(ctx).toSeq.flatMap(visitComplexColTypeList)) } /** * Create a [[StructType]] from a number of column definitions.  override def visitComplexColTypeList( ctx: ComplexColTypeListContext): Seq[StructField] = withOrigin(ctx) { ctx.complexColType().asScala.map(visitComplexColType).toSeq } /** * Create a [[StructField]] from a column definition.  override def visitComplexColType(ctx: ComplexColTypeContext): StructField = withOrigin(ctx) { import ctx._ val structField = StructField( name = identifier.getText, dataType = typedVisit(dataType()), nullable = NULL == null) Option(commentSpec).map(visitCommentSpec).map(structField.withComment).getOrElse(structField) } /** * Create a location string.  override def visitLocationSpec(ctx: LocationSpecContext): String = withOrigin(ctx) { string(ctx.STRING) } /** * Create an optional location string.  protected def visitLocationSpecList(ctx: java.util.List[LocationSpecContext]): Option[String] = { ctx.asScala.headOption.map(visitLocationSpec) } /** * Create a comment string.  override def visitCommentSpec(ctx: CommentSpecContext): String = withOrigin(ctx) { string(ctx.STRING) } /** * Create an optional comment string.  protected def visitCommentSpecList(ctx: java.util.List[CommentSpecContext]): Option[String] = { ctx.asScala.headOption.map(visitCommentSpec) } /** * Create a [[BucketSpec]].  override def visitBucketSpec(ctx: BucketSpecContext): BucketSpec = withOrigin(ctx) { BucketSpec( ctx.INTEGER_VALUE.getText.toInt, visitIdentifierList(ctx.identifierList), Option(ctx.orderedIdentifierList) .toSeq .flatMap(_.orderedIdentifier.asScala) .map { orderedIdCtx => Option(orderedIdCtx.ordering).map(_.getText).foreach { dir => if (dir.toLowerCase(Locale.ROOT) != \"asc\") { operationNotAllowed(s\"Column ordering must be ASC, was '$dir'\", ctx) } } orderedIdCtx.ident.getText }) } /** * Convert a property list into a key-value map. * This should be called through [[visitPropertyKeyValues]] or [[visitPropertyKeys]].  override def visitPropertyList( ctx: PropertyListContext): Map[String, String] = withOrigin(ctx) { val properties = ctx.property.asScala.map { property => val key = visitPropertyKey(property.key) val value = visitPropertyValue(property.value) key -> value } // Check for duplicate property names. checkDuplicateKeys(properties.toSeq, ctx) properties.toMap } /** * Parse a key-value map from a [[PropertyListContext]], assuming all values are specified.  def visitPropertyKeyValues(ctx: PropertyListContext): Map[String, String] = { val props = visitPropertyList(ctx) val badKeys = props.collect { case (key, null) => key } if (badKeys.nonEmpty) { operationNotAllowed( s\"Values must be specified for key(s): ${badKeys.mkString(\"[\", \",\", \"]\")}\", ctx) } props } /** * Parse a list of keys from a [[PropertyListContext]], assuming no values are specified.  def visitPropertyKeys(ctx: PropertyListContext): Seq[String] = { val props = visitPropertyList(ctx) val badKeys = props.filter { case (_, v) => v != null }.keys if (badKeys.nonEmpty) { operationNotAllowed( s\"Values should not be specified for key(s): ${badKeys.mkString(\"[\", \",\", \"]\")}\", ctx) } props.keys.toSeq } /** * A property key can either be String or a collection of dot separated elements. This * function extracts the property key based on whether its a string literal or a property * identifier.  override def visitPropertyKey(key: PropertyKeyContext): String = { if (key.STRING != null) { string(key.STRING) } else { key.getText } } /** * A property value can be String, Integer, Boolean or Decimal. This function extracts * the property value based on whether its a string, integer, boolean or decimal literal.  override def visitPropertyValue(value: PropertyValueContext): String = { if (value == null) { null } else if (value.STRING != null) { string(value.STRING) } else if (value.booleanValue != null) { value.getText.toLowerCase(Locale.ROOT) } else { value.getText } } /** * Type to keep track of a table header: (identifier, isTemporary, ifNotExists, isExternal).  type TableHeader = (Seq[String], Boolean, Boolean, Boolean) /** * Type to keep track of table clauses: * - partition transforms * - partition columns * - bucketSpec * - properties * - options * - location * - comment * - serde * * Note: Partition transforms are based on existing table schema definition. It can be simple * column names, or functions like `year(date_col)`. Partition columns are column names with data * types like `i INT`, which should be appended to the existing table schema.  type TableClauses = ( Seq[Transform], Seq[StructField], Option[BucketSpec], Map[String, String], Map[String, String], Option[String], Option[String], Option[SerdeInfo]) /** * Validate a create table statement and return the [[TableIdentifier]].  override def visitCreateTableHeader( ctx: CreateTableHeaderContext): TableHeader = withOrigin(ctx) { val temporary = ctx.TEMPORARY != null val ifNotExists = ctx.EXISTS != null if (temporary && ifNotExists) { operationNotAllowed(\"CREATE TEMPORARY TABLE ... IF NOT EXISTS\", ctx) } val multipartIdentifier = ctx.multipartIdentifier.parts.asScala.map(_.getText).toSeq (multipartIdentifier, temporary, ifNotExists, ctx.EXTERNAL != null) } /** * Parse a qualified name to a multipart name.  override def visitQualifiedName(ctx: QualifiedNameContext): Seq[String] = withOrigin(ctx) { ctx.identifier.asScala.map(_.getText).toSeq } /** * Parse a list of transforms or columns.  override def visitPartitionFieldList( ctx: PartitionFieldListContext): (Seq[Transform], Seq[StructField]) = withOrigin(ctx) { val (transforms, columns) = ctx.fields.asScala.map { case transform: PartitionTransformContext => (Some(visitPartitionTransform(transform)), None) case field: PartitionColumnContext => (None, Some(visitColType(field.colType))) }.unzip (transforms.flatten.toSeq, columns.flatten.toSeq) } override def visitPartitionTransform( ctx: PartitionTransformContext): Transform = withOrigin(ctx) { def getFieldReference( ctx: ApplyTransformContext, arg: V2Expression): FieldReference = { lazy val name: String = ctx.identifier.getText arg match { case ref: FieldReference => ref case nonRef => throw QueryParsingErrors.partitionTransformNotExpectedError(name, nonRef.describe, ctx) } } def getSingleFieldReference( ctx: ApplyTransformContext, arguments: Seq[V2Expression]): FieldReference = { lazy val name: String = ctx.identifier.getText if (arguments.size > 1) { throw QueryParsingErrors.tooManyArgumentsForTransformError(name, ctx) } else if (arguments.isEmpty) { throw new IllegalStateException(s\"Not enough arguments for transform $name\") } else { getFieldReference(ctx, arguments.head) } } ctx.transform match { case identityCtx: IdentityTransformContext => IdentityTransform(FieldReference(typedVisit[Seq[String]](identityCtx.qualifiedName))) case applyCtx: ApplyTransformContext => val arguments = applyCtx.argument.asScala.map(visitTransformArgument).toSeq applyCtx.identifier.getText match { case \"bucket\" => val numBuckets: Int = arguments.head match { case LiteralValue(shortValue, ShortType) => shortValue.asInstanceOf[Short].toInt case LiteralValue(intValue, IntegerType) => intValue.asInstanceOf[Int] case LiteralValue(longValue, LongType) => longValue.asInstanceOf[Long].toInt case lit => throw QueryParsingErrors.invalidBucketsNumberError(lit.describe, applyCtx) } val fields = arguments.tail.map(arg => getFieldReference(applyCtx, arg)) BucketTransform(LiteralValue(numBuckets, IntegerType), fields) case \"years\" => YearsTransform(getSingleFieldReference(applyCtx, arguments)) case \"months\" => MonthsTransform(getSingleFieldReference(applyCtx, arguments)) case \"days\" => DaysTransform(getSingleFieldReference(applyCtx, arguments)) case \"hours\" => HoursTransform(getSingleFieldReference(applyCtx, arguments)) case name => ApplyTransform(name, arguments) } } } /** * Parse an argument to a transform. An argument may be a field reference (qualified name) or * a value literal.  override def visitTransformArgument(ctx: TransformArgumentContext): V2Expression = { withOrigin(ctx) { val reference = Option(ctx.qualifiedName) .map(typedVisit[Seq[String]]) .map(FieldReference(_)) val literal = Option(ctx.constant) .map(typedVisit[Literal]) .map(lit => LiteralValue(lit.value, lit.dataType)) reference.orElse(literal) .getOrElse(throw new IllegalStateException(\"Invalid transform argument\")) } } private def cleanNamespaceProperties( properties: Map[String, String], ctx: ParserRuleContext): Map[String, String] = withOrigin(ctx) { import SupportsNamespaces._ val legacyOn = conf.getConf(SQLConf.LEGACY_PROPERTY_NON_RESERVED) properties.filter { case (PROP_LOCATION, _) if !legacyOn => throw QueryParsingErrors.cannotCleanReservedNamespacePropertyError( PROP_LOCATION, ctx, \"please use the LOCATION clause to specify it\") case (PROP_LOCATION, _) => false case (PROP_OWNER, _) if !legacyOn => throw QueryParsingErrors.cannotCleanReservedNamespacePropertyError( PROP_OWNER, ctx, \"it will be set to the current user\") case (PROP_OWNER, _) => false case _ => true } } /** * Create a [[CreateNamespace]] command. * * For example: * {{{ * CREATE NAMESPACE [IF NOT EXISTS] ns1.ns2.ns3 * create_namespace_clauses; * * create_namespace_clauses (order insensitive): * [COMMENT namespace_comment] * [LOCATION path] * [WITH PROPERTIES (key1=val1, key2=val2, ...)] * }}}  override def visitCreateNamespace(ctx: CreateNamespaceContext): LogicalPlan = withOrigin(ctx) { import SupportsNamespaces._ checkDuplicateClauses(ctx.commentSpec(), \"COMMENT\", ctx) checkDuplicateClauses(ctx.locationSpec, \"LOCATION\", ctx) checkDuplicateClauses(ctx.PROPERTIES, \"WITH PROPERTIES\", ctx) checkDuplicateClauses(ctx.DBPROPERTIES, \"WITH DBPROPERTIES\", ctx) if (!ctx.PROPERTIES.isEmpty && !ctx.DBPROPERTIES.isEmpty) { throw QueryParsingErrors.propertiesAndDbPropertiesBothSpecifiedError(ctx) } var properties = ctx.propertyList.asScala.headOption .map(visitPropertyKeyValues) .getOrElse(Map.empty) properties = cleanNamespaceProperties(properties, ctx) visitCommentSpecList(ctx.commentSpec()).foreach { properties += PROP_COMMENT -> _ } visitLocationSpecList(ctx.locationSpec()).foreach { properties += PROP_LOCATION -> _ } CreateNamespace( UnresolvedDBObjectName( visitMultipartIdentifier(ctx.multipartIdentifier), isNamespace = true), ctx.EXISTS != null, properties) } /** * Create a [[DropNamespace]] command. * * For example: * {{{ * DROP (DATABASE|SCHEMA|NAMESPACE) [IF EXISTS] ns1.ns2 [RESTRICT|CASCADE]; * }}}  override def visitDropNamespace(ctx: DropNamespaceContext): LogicalPlan = withOrigin(ctx) { DropNamespace( UnresolvedNamespace(visitMultipartIdentifier(ctx.multipartIdentifier)), ctx.EXISTS != null, ctx.CASCADE != null) } /** * Create an [[SetNamespaceProperties]] logical plan. * * For example: * {{{ * ALTER (DATABASE|SCHEMA|NAMESPACE) database * SET (DBPROPERTIES|PROPERTIES) (property_name=property_value, ...); * }}}  override def visitSetNamespaceProperties(ctx: SetNamespacePropertiesContext): LogicalPlan = { withOrigin(ctx) { val properties = cleanNamespaceProperties(visitPropertyKeyValues(ctx.propertyList), ctx) SetNamespaceProperties( UnresolvedNamespace(visitMultipartIdentifier(ctx.multipartIdentifier)), properties) } } /** * Create an [[SetNamespaceLocation]] logical plan. * * For example: * {{{ * ALTER (DATABASE|SCHEMA|NAMESPACE) namespace SET LOCATION path; * }}}  override def visitSetNamespaceLocation(ctx: SetNamespaceLocationContext): LogicalPlan = { withOrigin(ctx) { SetNamespaceLocation( UnresolvedNamespace(visitMultipartIdentifier(ctx.multipartIdentifier)), visitLocationSpec(ctx.locationSpec)) } } /** * Create a [[ShowNamespaces]] command.  override def visitShowNamespaces(ctx: ShowNamespacesContext): LogicalPlan = withOrigin(ctx) { val multiPart = Option(ctx.multipartIdentifier).map(visitMultipartIdentifier) ShowNamespaces( UnresolvedNamespace(multiPart.getOrElse(Seq.empty[String])), Option(ctx.pattern).map(string)) } /** * Create a [[DescribeNamespace]]. * * For example: * {{{ * DESCRIBE (DATABASE|SCHEMA|NAMESPACE) [EXTENDED] database; * }}}  override def visitDescribeNamespace(ctx: DescribeNamespaceContext): LogicalPlan = withOrigin(ctx) { DescribeNamespace( UnresolvedNamespace(visitMultipartIdentifier(ctx.multipartIdentifier())), ctx.EXTENDED != null) } def cleanTableProperties( ctx: ParserRuleContext, properties: Map[String, String]): Map[String, String] = { import TableCatalog._ val legacyOn = conf.getConf(SQLConf.LEGACY_PROPERTY_NON_RESERVED) properties.filter { case (PROP_PROVIDER, _) if !legacyOn => throw QueryParsingErrors.cannotCleanReservedTablePropertyError( PROP_PROVIDER, ctx, \"please use the USING clause to specify it\") case (PROP_PROVIDER, _) => false case (PROP_LOCATION, _) if !legacyOn => throw QueryParsingErrors.cannotCleanReservedTablePropertyError( PROP_LOCATION, ctx, \"please use the LOCATION clause to specify it\") case (PROP_LOCATION, _) => false case (PROP_OWNER, _) if !legacyOn => throw QueryParsingErrors.cannotCleanReservedTablePropertyError( PROP_OWNER, ctx, \"it will be set to the current user\") case (PROP_OWNER, _) => false case (PROP_EXTERNAL, _) if !legacyOn => throw QueryParsingErrors.cannotCleanReservedTablePropertyError( PROP_EXTERNAL, ctx, \"please use CREATE EXTERNAL TABLE\") case (PROP_EXTERNAL, _) => false // It's safe to set whatever table comment, so we don't make it a reserved table property. case (PROP_COMMENT, _) => true case (k, _) => val isReserved = CatalogV2Util.TABLE_RESERVED_PROPERTIES.contains(k) if (!legacyOn && isReserved) { throw QueryParsingErrors.cannotCleanReservedTablePropertyError( k, ctx, \"please remove it from the TBLPROPERTIES list.\") } !isReserved } } def cleanTableOptions( ctx: ParserRuleContext, options: Map[String, String], location: Option[String]): (Map[String, String], Option[String]) = { var path = location val filtered = cleanTableProperties(ctx, options).filter { case (k, v) if k.equalsIgnoreCase(\"path\") && path.nonEmpty => throw QueryParsingErrors.duplicatedTablePathsFoundError(path.get, v, ctx) case (k, v) if k.equalsIgnoreCase(\"path\") => path = Some(v) false case _ => true } (filtered, path) } /** * Create a [[SerdeInfo]] for creating tables. * * Format: STORED AS (name | INPUTFORMAT input_format OUTPUTFORMAT output_format)  override def visitCreateFileFormat(ctx: CreateFileFormatContext): SerdeInfo = withOrigin(ctx) { (ctx.fileFormat, ctx.storageHandler) match { // Expected format: INPUTFORMAT input_format OUTPUTFORMAT output_format case (c: TableFileFormatContext, null) => SerdeInfo(formatClasses = Some(FormatClasses(string(c.inFmt), string(c.outFmt)))) // Expected format: SEQUENCEFILE | TEXTFILE | RCFILE | ORC | PARQUET | AVRO case (c: GenericFileFormatContext, null) => SerdeInfo(storedAs = Some(c.identifier.getText)) case (null, storageHandler) => operationNotAllowed(\"STORED BY\", ctx) case _ => throw QueryParsingErrors.storedAsAndStoredByBothSpecifiedError(ctx) } } /** * Create a [[SerdeInfo]] used for creating tables. * * Example format: * {{{ * SERDE serde_name [WITH SERDEPROPERTIES (k1=v1, k2=v2, ...)] * }}} * * OR * * {{{ * DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] * [COLLECTION ITEMS TERMINATED BY char] * [MAP KEYS TERMINATED BY char] * [LINES TERMINATED BY char] * [NULL DEFINED AS char] * }}}  def visitRowFormat(ctx: RowFormatContext): SerdeInfo = withOrigin(ctx) { ctx match { case serde: RowFormatSerdeContext => visitRowFormatSerde(serde) case delimited: RowFormatDelimitedContext => visitRowFormatDelimited(delimited) } } /** * Create SERDE row format name and properties pair.  override def visitRowFormatSerde(ctx: RowFormatSerdeContext): SerdeInfo = withOrigin(ctx) { import ctx._ SerdeInfo( serde = Some(string(name)), serdeProperties = Option(propertyList).map(visitPropertyKeyValues).getOrElse(Map.empty)) } /** * Create a delimited row format properties object.  override def visitRowFormatDelimited( ctx: RowFormatDelimitedContext): SerdeInfo = withOrigin(ctx) { // Collect the entries if any. def entry(key: String, value: Token): Seq[(String, String)] = { Option(value).toSeq.map(x => key -> string(x)) } // TODO we need proper support for the NULL format. val entries = entry(\"field.delim\", ctx.fieldsTerminatedBy) ++ entry(\"serialization.format\", ctx.fieldsTerminatedBy) ++ entry(\"escape.delim\", ctx.escapedBy) ++ // The following typo is inherited from Hive... entry(\"colelction.delim\", ctx.collectionItemsTerminatedBy) ++ entry(\"mapkey.delim\", ctx.keysTerminatedBy) ++ Option(ctx.linesSeparatedBy).toSeq.map { token => val value = string(token) validate( value == \"\\n\", s\"LINES TERMINATED BY only supports newline '\\\\n' right now: $value\", ctx) \"line.delim\" -> value } SerdeInfo(serdeProperties = entries.toMap) } /** * Throw a [[ParseException]] if the user specified incompatible SerDes through ROW FORMAT * and STORED AS. * * The following are allowed. Anything else is not: * ROW FORMAT SERDE ... STORED AS [SEQUENCEFILE | RCFILE | TEXTFILE] * ROW FORMAT DELIMITED ... STORED AS TEXTFILE * ROW FORMAT ... STORED AS INPUTFORMAT ... OUTPUTFORMAT ...  protected def validateRowFormatFileFormat( rowFormatCtx: RowFormatContext, createFileFormatCtx: CreateFileFormatContext, parentCtx: ParserRuleContext): Unit = { if (rowFormatCtx == null || createFileFormatCtx == null) { return } (rowFormatCtx, createFileFormatCtx.fileFormat) match { case (_, ffTable: TableFileFormatContext) => // OK case (rfSerde: RowFormatSerdeContext, ffGeneric: GenericFileFormatContext) => ffGeneric.identifier.getText.toLowerCase(Locale.ROOT) match { case (\"sequencefile\" | \"textfile\" | \"rcfile\") => // OK case fmt => operationNotAllowed( s\"ROW FORMAT SERDE is incompatible with format '$fmt', which also specifies a serde\", parentCtx) } case (rfDelimited: RowFormatDelimitedContext, ffGeneric: GenericFileFormatContext) => ffGeneric.identifier.getText.toLowerCase(Locale.ROOT) match { case \"textfile\" => // OK case fmt => operationNotAllowed( s\"ROW FORMAT DELIMITED is only compatible with 'textfile', not '$fmt'\", parentCtx) } case _ => // should never happen def str(ctx: ParserRuleContext): String = { (0 until ctx.getChildCount).map { i => ctx.getChild(i).getText }.mkString(\" \") } operationNotAllowed( s\"Unexpected combination of ${str(rowFormatCtx)} and ${str(createFileFormatCtx)}\", parentCtx) } } protected def validateRowFormatFileFormat( rowFormatCtx: Seq[RowFormatContext], createFileFormatCtx: Seq[CreateFileFormatContext], parentCtx: ParserRuleContext): Unit = { if (rowFormatCtx.size == 1 && createFileFormatCtx.size == 1) { validateRowFormatFileFormat(rowFormatCtx.head, createFileFormatCtx.head, parentCtx) } } override def visitCreateTableClauses(ctx: CreateTableClausesContext): TableClauses = { checkDuplicateClauses(ctx.TBLPROPERTIES, \"TBLPROPERTIES\", ctx) checkDuplicateClauses(ctx.OPTIONS, \"OPTIONS\", ctx) checkDuplicateClauses(ctx.PARTITIONED, \"PARTITIONED BY\", ctx) checkDuplicateClauses(ctx.createFileFormat, \"STORED AS/BY\", ctx) checkDuplicateClauses(ctx.rowFormat, \"ROW FORMAT\", ctx) checkDuplicateClauses(ctx.commentSpec(), \"COMMENT\", ctx) checkDuplicateClauses(ctx.bucketSpec(), \"CLUSTERED BY\", ctx) checkDuplicateClauses(ctx.locationSpec, \"LOCATION\", ctx) if (ctx.skewSpec.size > 0) { operationNotAllowed(\"CREATE TABLE ... SKEWED BY\", ctx) } val (partTransforms, partCols) = Option(ctx.partitioning).map(visitPartitionFieldList).getOrElse((Nil, Nil)) val bucketSpec = ctx.bucketSpec().asScala.headOption.map(visitBucketSpec) val properties = Option(ctx.tableProps).map(visitPropertyKeyValues).getOrElse(Map.empty) val cleanedProperties = cleanTableProperties(ctx, properties) val options = Option(ctx.options).map(visitPropertyKeyValues).getOrElse(Map.empty) val location = visitLocationSpecList(ctx.locationSpec()) val (cleanedOptions, newLocation) = cleanTableOptions(ctx, options, location) val comment = visitCommentSpecList(ctx.commentSpec()) val serdeInfo = getSerdeInfo(ctx.rowFormat.asScala.toSeq, ctx.createFileFormat.asScala.toSeq, ctx) (partTransforms, partCols, bucketSpec, cleanedProperties, cleanedOptions, newLocation, comment, serdeInfo) } protected def getSerdeInfo( rowFormatCtx: Seq[RowFormatContext], createFileFormatCtx: Seq[CreateFileFormatContext], ctx: ParserRuleContext): Option[SerdeInfo] = { validateRowFormatFileFormat(rowFormatCtx, createFileFormatCtx, ctx) val rowFormatSerdeInfo = rowFormatCtx.map(visitRowFormat) val fileFormatSerdeInfo = createFileFormatCtx.map(visitCreateFileFormat) (fileFormatSerdeInfo ++ rowFormatSerdeInfo).reduceLeftOption((l, r) => l.merge(r)) } private def partitionExpressions( partTransforms: Seq[Transform], partCols: Seq[StructField], ctx: ParserRuleContext): Seq[Transform] = { if (partTransforms.nonEmpty) { if (partCols.nonEmpty) { val references = partTransforms.map(_.describe()).mkString(\", \") val columns = partCols .map(field => s\"${field.name} ${field.dataType.simpleString}\") .mkString(\", \") operationNotAllowed( s\"\"\"PARTITION BY: Cannot mix partition expressions and partition columns: |Expressions: $references |Columns: $columns\"\"\".stripMargin, ctx) } partTransforms } else { // columns were added to create the schema. convert to column references partCols.map { column => IdentityTransform(FieldReference(Seq(column.name))) } } } /** * Create a table, returning a [[CreateTable]] or [[CreateTableAsSelect]] logical plan. * * Expected format: * {{{ * CREATE [TEMPORARY] TABLE [IF NOT EXISTS] [db_name.]table_name * [USING table_provider] * create_table_clauses * [[AS] select_statement]; * * create_table_clauses (order insensitive): * [PARTITIONED BY (partition_fields)] * [OPTIONS table_property_list] * [ROW FORMAT row_format] * [STORED AS file_format] * [CLUSTERED BY (col_name, col_name, ...) * [SORTED BY (col_name [ASC|DESC], ...)] * INTO num_buckets BUCKETS * ] * [LOCATION path] * [COMMENT table_comment] * [TBLPROPERTIES (property_name=property_value, ...)] * * partition_fields: * col_name, transform(col_name), transform(constant, col_name), ... | * col_name data_type [NOT NULL] [COMMENT col_comment], ... * }}}  override def visitCreateTable(ctx: CreateTableContext): LogicalPlan = withOrigin(ctx) { val (table, temp, ifNotExists, external) = visitCreateTableHeader(ctx.createTableHeader) val columns = Option(ctx.colTypeList()).map(visitColTypeList).getOrElse(Nil) val provider = Option(ctx.tableProvider).map(_.multipartIdentifier.getText) val (partTransforms, partCols, bucketSpec, properties, options, location, comment, serdeInfo) = visitCreateTableClauses(ctx.createTableClauses()) if (provider.isDefined && serdeInfo.isDefined) { operationNotAllowed(s\"CREATE TABLE ... USING ... ${serdeInfo.get.describe}\", ctx) } if (temp) { val asSelect = if (ctx.query == null) \"\" else \" AS ...\" operationNotAllowed( s\"CREATE TEMPORARY TABLE ...$asSelect, use CREATE TEMPORARY VIEW instead\", ctx) } val partitioning = partitionExpressions(partTransforms, partCols, ctx) ++ bucketSpec.map(_.asTransform) val tableSpec = TableSpec(properties, provider, options, location, comment, serdeInfo, external) Option(ctx.query).map(plan) match { case Some(_) if columns.nonEmpty => operationNotAllowed( \"Schema may not be specified in a Create Table As Select (CTAS) statement\", ctx) case Some(_) if partCols.nonEmpty => // non-reference partition columns are not allowed because schema can't be specified operationNotAllowed( \"Partition column types may not be specified in Create Table As Select (CTAS)\", ctx) case Some(query) => CreateTableAsSelect( UnresolvedDBObjectName(table, isNamespace = false), partitioning, query, tableSpec, Map.empty, ifNotExists) case _ => // Note: table schema includes both the table columns list and the partition columns // with data type. val schema = StructType(columns ++ partCols) CreateTable( UnresolvedDBObjectName(table, isNamespace = false), schema, partitioning, tableSpec, ignoreIfExists = ifNotExists) } } /** * Replace a table, returning a [[ReplaceTable]] or [[ReplaceTableAsSelect]] * logical plan. * * Expected format: * {{{ * [CREATE OR] REPLACE TABLE [db_name.]table_name * [USING table_provider] * replace_table_clauses * [[AS] select_statement]; * * replace_table_clauses (order insensitive): * [OPTIONS table_property_list] * [PARTITIONED BY (partition_fields)] * [CLUSTERED BY (col_name, col_name, ...) * [SORTED BY (col_name [ASC|DESC], ...)] * INTO num_buckets BUCKETS * ] * [LOCATION path] * [COMMENT table_comment] * [TBLPROPERTIES (property_name=property_value, ...)] * * partition_fields: * col_name, transform(col_name), transform(constant, col_name), ... | * col_name data_type [NOT NULL] [COMMENT col_comment], ... * }}}  override def visitReplaceTable(ctx: ReplaceTableContext): LogicalPlan = withOrigin(ctx) { val table = visitMultipartIdentifier(ctx.replaceTableHeader.multipartIdentifier()) val orCreate = ctx.replaceTableHeader().CREATE() != null val (partTransforms, partCols, bucketSpec, properties, options, location, comment, serdeInfo) = visitCreateTableClauses(ctx.createTableClauses()) val columns = Option(ctx.colTypeList()).map(visitColTypeList).getOrElse(Nil) val provider = Option(ctx.tableProvider).map(_.multipartIdentifier.getText) if (provider.isDefined && serdeInfo.isDefined) { operationNotAllowed(s\"CREATE TABLE ... USING ... ${serdeInfo.get.describe}\", ctx) } val partitioning = partitionExpressions(partTransforms, partCols, ctx) ++ bucketSpec.map(_.asTransform) val tableSpec = TableSpec(properties, provider, options, location, comment, serdeInfo, false) Option(ctx.query).map(plan) match { case Some(_) if columns.nonEmpty => operationNotAllowed( \"Schema may not be specified in a Replace Table As Select (RTAS) statement\", ctx) case Some(_) if partCols.nonEmpty => // non-reference partition columns are not allowed because schema can't be specified operationNotAllowed( \"Partition column types may not be specified in Replace Table As Select (RTAS)\", ctx) case Some(query) => ReplaceTableAsSelect( UnresolvedDBObjectName(table, isNamespace = false), partitioning, query, tableSpec, writeOptions = Map.empty, orCreate = orCreate) case _ => // Note: table schema includes both the table columns list and the partition columns // with data type. val schema = StructType(columns ++ partCols) ReplaceTable( UnresolvedDBObjectName(table, isNamespace = false), schema, partitioning, tableSpec, orCreate = orCreate) } } /** * Create a [[DropTable]] command.  override def visitDropTable(ctx: DropTableContext): LogicalPlan = withOrigin(ctx) { // DROP TABLE works with either a table or a temporary view. DropTable( createUnresolvedTableOrView(ctx.multipartIdentifier(), \"DROP TABLE\"), ctx.EXISTS != null, ctx.PURGE != null) } /** * Create a [[DropView]] command.  override def visitDropView(ctx: DropViewContext): AnyRef = withOrigin(ctx) { DropView( createUnresolvedView( ctx.multipartIdentifier(), commandName = \"DROP VIEW\", allowTemp = true, relationTypeMismatchHint = Some(\"Please use DROP TABLE instead.\")), ctx.EXISTS != null) } /** * Create a [[SetCatalogAndNamespace]] command.  override def visitUse(ctx: UseContext): LogicalPlan = withOrigin(ctx) { val nameParts = visitMultipartIdentifier(ctx.multipartIdentifier) SetCatalogAndNamespace(UnresolvedDBObjectName(nameParts, isNamespace = true)) } /** * Create a [[ShowTables]] command.  override def visitShowTables(ctx: ShowTablesContext): LogicalPlan = withOrigin(ctx) { val multiPart = Option(ctx.multipartIdentifier).map(visitMultipartIdentifier) ShowTables( UnresolvedNamespace(multiPart.getOrElse(Seq.empty[String])), Option(ctx.pattern).map(string)) } /** * Create a [[ShowTableExtended]] command.  override def visitShowTableExtended( ctx: ShowTableExtendedContext): LogicalPlan = withOrigin(ctx) { val multiPart = Option(ctx.multipartIdentifier).map(visitMultipartIdentifier) val partitionKeys = Option(ctx.partitionSpec).map { specCtx => UnresolvedPartitionSpec(visitNonOptionalPartitionSpec(specCtx), None) } ShowTableExtended( UnresolvedNamespace(multiPart.getOrElse(Seq.empty[String])), string(ctx.pattern), partitionKeys) } /** * Create a [[ShowViews]] command.  override def visitShowViews(ctx: ShowViewsContext): LogicalPlan = withOrigin(ctx) { val multiPart = Option(ctx.multipartIdentifier).map(visitMultipartIdentifier) ShowViews( UnresolvedNamespace(multiPart.getOrElse(Seq.empty[String])), Option(ctx.pattern).map(string)) } override def visitColPosition(ctx: ColPositionContext): ColumnPosition = { ctx.position.getType match { case SqlBaseParser.FIRST => ColumnPosition.first() case SqlBaseParser.AFTER => ColumnPosition.after(ctx.afterCol.getText) } } /** * Parse new column info from ADD COLUMN into a QualifiedColType.  override def visitQualifiedColTypeWithPosition( ctx: QualifiedColTypeWithPositionContext): QualifiedColType = withOrigin(ctx) { val name = typedVisit[Seq[String]](ctx.name) QualifiedColType( path = if (name.length > 1) Some(UnresolvedFieldName(name.init)) else None, colName = name.last, dataType = typedVisit[DataType](ctx.dataType), nullable = ctx.NULL == null, comment = Option(ctx.commentSpec()).map(visitCommentSpec), position = Option(ctx.colPosition).map( pos => UnresolvedFieldPosition(typedVisit[ColumnPosition](pos)))) } /** * Parse a [[AlterTableAddColumns]] command. * * For example: * {{{ * ALTER TABLE table1 * ADD COLUMNS (col_name data_type [COMMENT col_comment], ...); * }}}  override def visitAddTableColumns(ctx: AddTableColumnsContext): LogicalPlan = withOrigin(ctx) { val colToken = if (ctx.COLUMN() != null) \"COLUMN\" else \"COLUMNS\" AddColumns( createUnresolvedTable(ctx.multipartIdentifier, s\"ALTER TABLE ... ADD $colToken\"), ctx.columns.qualifiedColTypeWithPosition.asScala.map(typedVisit[QualifiedColType]).toSeq ) } /** * Parse a [[AlterTableRenameColumn]] command. * * For example: * {{{ * ALTER TABLE table1 RENAME COLUMN a.b.c TO x * }}}  override def visitRenameTableColumn( ctx: RenameTableColumnContext): LogicalPlan = withOrigin(ctx) { RenameColumn( createUnresolvedTable(ctx.table, \"ALTER TABLE ... RENAME COLUMN\"), UnresolvedFieldName(typedVisit[Seq[String]](ctx.from)), ctx.to.getText) } /** * Parse a [[AlterTableAlterColumn]] command to alter a column's property. * * For example: * {{{ * ALTER TABLE table1 ALTER COLUMN a.b.c TYPE bigint * ALTER TABLE table1 ALTER COLUMN a.b.c SET NOT NULL * ALTER TABLE table1 ALTER COLUMN a.b.c DROP NOT NULL * ALTER TABLE table1 ALTER COLUMN a.b.c COMMENT 'new comment' * ALTER TABLE table1 ALTER COLUMN a.b.c FIRST * ALTER TABLE table1 ALTER COLUMN a.b.c AFTER x * }}}  override def visitAlterTableAlterColumn( ctx: AlterTableAlterColumnContext): LogicalPlan = withOrigin(ctx) { val action = ctx.alterColumnAction val verb = if (ctx.CHANGE != null) \"CHANGE\" else \"ALTER\" if (action == null) { operationNotAllowed( s\"ALTER TABLE table $verb COLUMN requires a TYPE, a SET/DROP, a COMMENT, or a FIRST/AFTER\", ctx) } val dataType = if (action.dataType != null) { Some(typedVisit[DataType](action.dataType)) } else { None } val nullable = if (action.setOrDrop != null) { action.setOrDrop.getType match { case SqlBaseParser.SET => Some(false) case SqlBaseParser.DROP => Some(true) } } else { None } val comment = if (action.commentSpec != null) { Some(visitCommentSpec(action.commentSpec())) } else { None } val position = if (action.colPosition != null) { Some(UnresolvedFieldPosition(typedVisit[ColumnPosition](action.colPosition))) } else { None } assert(Seq(dataType, nullable, comment, position).count(_.nonEmpty) == 1) AlterColumn( createUnresolvedTable(ctx.table, s\"ALTER TABLE ... $verb COLUMN\"), UnresolvedFieldName(typedVisit[Seq[String]](ctx.column)), dataType = dataType, nullable = nullable, comment = comment, position = position) } /** * Parse a [[AlterTableAlterColumn]] command. This is Hive SQL syntax. * * For example: * {{{ * ALTER TABLE table [PARTITION partition_spec] * CHANGE [COLUMN] column_old_name column_new_name column_dataType [COMMENT column_comment] * [FIRST | AFTER column_name]; * }}}  override def visitHiveChangeColumn(ctx: HiveChangeColumnContext): LogicalPlan = withOrigin(ctx) { if (ctx.partitionSpec != null) { operationNotAllowed(\"ALTER TABLE table PARTITION partition_spec CHANGE COLUMN\", ctx) } val columnNameParts = typedVisit[Seq[String]](ctx.colName) if (!conf.resolver(columnNameParts.last, ctx.colType().colName.getText)) { throw QueryParsingErrors.operationInHiveStyleCommandUnsupportedError(\"Renaming column\", \"ALTER COLUMN\", ctx, Some(\"please run RENAME COLUMN instead\")) } if (ctx.colType.NULL != null) { throw QueryParsingErrors.operationInHiveStyleCommandUnsupportedError( \"NOT NULL\", \"ALTER COLUMN\", ctx, Some(\"please run ALTER COLUMN ... SET/DROP NOT NULL instead\")) } AlterColumn( createUnresolvedTable(ctx.table, s\"ALTER TABLE ... CHANGE COLUMN\"), UnresolvedFieldName(columnNameParts), dataType = Option(ctx.colType().dataType()).map(typedVisit[DataType]), nullable = None, comment = Option(ctx.colType().commentSpec()).map(visitCommentSpec), position = Option(ctx.colPosition).map( pos => UnresolvedFieldPosition(typedVisit[ColumnPosition](pos)))) } override def visitHiveReplaceColumns( ctx: HiveReplaceColumnsContext): LogicalPlan = withOrigin(ctx) { if (ctx.partitionSpec != null) { operationNotAllowed(\"ALTER TABLE table PARTITION partition_spec REPLACE COLUMNS\", ctx) } ReplaceColumns( createUnresolvedTable(ctx.multipartIdentifier, \"ALTER TABLE ... REPLACE COLUMNS\"), ctx.columns.qualifiedColTypeWithPosition.asScala.map { colType => if (colType.NULL != null) { throw QueryParsingErrors.operationInHiveStyleCommandUnsupportedError( \"NOT NULL\", \"REPLACE COLUMNS\", ctx) } if (colType.colPosition != null) { throw QueryParsingErrors.operationInHiveStyleCommandUnsupportedError( \"Column position\", \"REPLACE COLUMNS\", ctx) } val col = typedVisit[QualifiedColType](colType) if (col.path.isDefined) { throw QueryParsingErrors.operationInHiveStyleCommandUnsupportedError( \"Replacing with a nested column\", \"REPLACE COLUMNS\", ctx) } col }.toSeq ) } /** * Parse a [[AlterTableDropColumns]] command. * * For example: * {{{ * ALTER TABLE table1 DROP COLUMN a.b.c * ALTER TABLE table1 DROP COLUMNS a.b.c, x, y * }}}  override def visitDropTableColumns( ctx: DropTableColumnsContext): LogicalPlan = withOrigin(ctx) { val ifExists = ctx.EXISTS() != null val columnsToDrop = ctx.columns.multipartIdentifier.asScala.map(typedVisit[Seq[String]]) DropColumns( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... DROP COLUMNS\"), columnsToDrop.map(UnresolvedFieldName(_)).toSeq, ifExists) } /** * Parse [[SetViewProperties]] or [[SetTableProperties]] commands. * * For example: * {{{ * ALTER TABLE table SET TBLPROPERTIES ('table_property' = 'property_value'); * ALTER VIEW view SET TBLPROPERTIES ('table_property' = 'property_value'); * }}}  override def visitSetTableProperties( ctx: SetTablePropertiesContext): LogicalPlan = withOrigin(ctx) { val properties = visitPropertyKeyValues(ctx.propertyList) val cleanedTableProperties = cleanTableProperties(ctx, properties) if (ctx.VIEW != null) { SetViewProperties( createUnresolvedView( ctx.multipartIdentifier, commandName = \"ALTER VIEW ... SET TBLPROPERTIES\", allowTemp = false, relationTypeMismatchHint = alterViewTypeMismatchHint), cleanedTableProperties) } else { SetTableProperties( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... SET TBLPROPERTIES\", alterTableTypeMismatchHint), cleanedTableProperties) } } /** * Parse [[UnsetViewProperties]] or [[UnsetTableProperties]] commands. * * For example: * {{{ * ALTER TABLE table UNSET TBLPROPERTIES [IF EXISTS] ('comment', 'key'); * ALTER VIEW view UNSET TBLPROPERTIES [IF EXISTS] ('comment', 'key'); * }}}  override def visitUnsetTableProperties( ctx: UnsetTablePropertiesContext): LogicalPlan = withOrigin(ctx) { val properties = visitPropertyKeys(ctx.propertyList) val cleanedProperties = cleanTableProperties(ctx, properties.map(_ -> \"\").toMap).keys.toSeq val ifExists = ctx.EXISTS != null if (ctx.VIEW != null) { UnsetViewProperties( createUnresolvedView( ctx.multipartIdentifier, commandName = \"ALTER VIEW ... UNSET TBLPROPERTIES\", allowTemp = false, relationTypeMismatchHint = alterViewTypeMismatchHint), cleanedProperties, ifExists) } else { UnsetTableProperties( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... UNSET TBLPROPERTIES\", alterTableTypeMismatchHint), cleanedProperties, ifExists) } } /** * Create an [[SetTableLocation]] command. * * For example: * {{{ * ALTER TABLE table_name [PARTITION partition_spec] SET LOCATION \"loc\"; * }}}  override def visitSetTableLocation(ctx: SetTableLocationContext): LogicalPlan = withOrigin(ctx) { SetTableLocation( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... SET LOCATION ...\", alterTableTypeMismatchHint), Option(ctx.partitionSpec).map(visitNonOptionalPartitionSpec), visitLocationSpec(ctx.locationSpec)) } /** * Create a [[DescribeColumn]] or [[DescribeRelation]] commands.  override def visitDescribeRelation(ctx: DescribeRelationContext): LogicalPlan = withOrigin(ctx) { val isExtended = ctx.EXTENDED != null || ctx.FORMATTED != null val relation = createUnresolvedTableOrView( ctx.multipartIdentifier(), \"DESCRIBE TABLE\") if (ctx.describeColName != null) { if (ctx.partitionSpec != null) { throw QueryParsingErrors.descColumnForPartitionUnsupportedError(ctx) } else { DescribeColumn( relation, UnresolvedAttribute(ctx.describeColName.nameParts.asScala.map(_.getText).toSeq), isExtended) } } else { val partitionSpec = if (ctx.partitionSpec != null) { // According to the syntax, visitPartitionSpec returns `Map[String, Option[String]]`. visitPartitionSpec(ctx.partitionSpec).map { case (key, Some(value)) => key -> value case (key, _) => throw QueryParsingErrors.incompletePartitionSpecificationError(key, ctx) } } else { Map.empty[String, String] } DescribeRelation(relation, partitionSpec, isExtended) } } /** * Create an [[AnalyzeTable]], or an [[AnalyzeColumn]]. * Example SQL for analyzing a table or a set of partitions : * {{{ * ANALYZE TABLE multi_part_name [PARTITION (partcol1[=val1], partcol2[=val2], ...)] * COMPUTE STATISTICS [NOSCAN]; * }}} * * Example SQL for analyzing columns : * {{{ * ANALYZE TABLE multi_part_name COMPUTE STATISTICS FOR COLUMNS column1, column2; * }}} * * Example SQL for analyzing all columns of a table: * {{{ * ANALYZE TABLE multi_part_name COMPUTE STATISTICS FOR ALL COLUMNS; * }}}  override def visitAnalyze(ctx: AnalyzeContext): LogicalPlan = withOrigin(ctx) { def checkPartitionSpec(): Unit = { if (ctx.partitionSpec != null) { logWarning(\"Partition specification is ignored when collecting column statistics: \" + ctx.partitionSpec.getText) } } if (ctx.identifier != null && ctx.identifier.getText.toLowerCase(Locale.ROOT) != \"noscan\") { throw QueryParsingErrors.computeStatisticsNotExpectedError(ctx.identifier()) } if (ctx.ALL() != null) { checkPartitionSpec() AnalyzeColumn( createUnresolvedTableOrView( ctx.multipartIdentifier(), \"ANALYZE TABLE ... FOR ALL COLUMNS\"), None, allColumns = true) } else if (ctx.identifierSeq() == null) { val partitionSpec = if (ctx.partitionSpec != null) { visitPartitionSpec(ctx.partitionSpec) } else { Map.empty[String, Option[String]] } AnalyzeTable( createUnresolvedTableOrView( ctx.multipartIdentifier(), \"ANALYZE TABLE\", allowTempView = false), partitionSpec, noScan = ctx.identifier != null) } else { checkPartitionSpec() AnalyzeColumn( createUnresolvedTableOrView( ctx.multipartIdentifier(), \"ANALYZE TABLE ... FOR COLUMNS ...\"), Option(visitIdentifierSeq(ctx.identifierSeq())), allColumns = false) } } /** * Create an [[AnalyzeTables]]. * Example SQL for analyzing all tables in default database: * {{{ * ANALYZE TABLES IN default COMPUTE STATISTICS; * }}}  override def visitAnalyzeTables(ctx: AnalyzeTablesContext): LogicalPlan = withOrigin(ctx) { if (ctx.identifier != null && ctx.identifier.getText.toLowerCase(Locale.ROOT) != \"noscan\") { throw QueryParsingErrors.computeStatisticsNotExpectedError(ctx.identifier()) } val multiPart = Option(ctx.multipartIdentifier).map(visitMultipartIdentifier) AnalyzeTables( UnresolvedNamespace(multiPart.getOrElse(Seq.empty[String])), noScan = ctx.identifier != null) } /** * Create a [[RepairTable]]. * * For example: * {{{ * MSCK REPAIR TABLE multi_part_name [{ADD|DROP|SYNC} PARTITIONS] * }}}  override def visitRepairTable(ctx: RepairTableContext): LogicalPlan = withOrigin(ctx) { val (enableAddPartitions, enableDropPartitions, option) = if (ctx.SYNC() != null) { (true, true, \" ... SYNC PARTITIONS\") } else if (ctx.DROP() != null) { (false, true, \" ... DROP PARTITIONS\") } else if (ctx.ADD() != null) { (true, false, \" ... ADD PARTITIONS\") } else { (true, false, \"\") } RepairTable( createUnresolvedTable(ctx.multipartIdentifier, s\"MSCK REPAIR TABLE$option\"), enableAddPartitions, enableDropPartitions) } /** * Create a [[LoadData]]. * * For example: * {{{ * LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE multi_part_name * [PARTITION (partcol1=val1, partcol2=val2 ...)] * }}}  override def visitLoadData(ctx: LoadDataContext): LogicalPlan = withOrigin(ctx) { LoadData( child = createUnresolvedTable(ctx.multipartIdentifier, \"LOAD DATA\"), path = string(ctx.path), isLocal = ctx.LOCAL != null, isOverwrite = ctx.OVERWRITE != null, partition = Option(ctx.partitionSpec).map(visitNonOptionalPartitionSpec) ) } /** * Creates a [[ShowCreateTable]]  override def visitShowCreateTable(ctx: ShowCreateTableContext): LogicalPlan = withOrigin(ctx) { ShowCreateTable( createUnresolvedTableOrView( ctx.multipartIdentifier(), \"SHOW CREATE TABLE\", allowTempView = false), ctx.SERDE != null) } /** * Create a [[CacheTable]] or [[CacheTableAsSelect]]. * * For example: * {{{ * CACHE [LAZY] TABLE multi_part_name * [OPTIONS tablePropertyList] [[AS] query] * }}}  override def visitCacheTable(ctx: CacheTableContext): LogicalPlan = withOrigin(ctx) { import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._ val query = Option(ctx.query).map(plan) val relation = createUnresolvedRelation(ctx.multipartIdentifier) val tableName = relation.multipartIdentifier if (query.isDefined && tableName.length > 1) { val catalogAndNamespace = tableName.init throw QueryParsingErrors.addCatalogInCacheTableAsSelectNotAllowedError( catalogAndNamespace.quoted, ctx) } val options = Option(ctx.options).map(visitPropertyKeyValues).getOrElse(Map.empty) val isLazy = ctx.LAZY != null if (query.isDefined) { CacheTableAsSelect(tableName.head, query.get, source(ctx.query()), isLazy, options) } else { CacheTable(relation, tableName, isLazy, options) } } /** * Create an [[UncacheTable]] logical plan.  override def visitUncacheTable(ctx: UncacheTableContext): LogicalPlan = withOrigin(ctx) { UncacheTable( createUnresolvedRelation(ctx.multipartIdentifier), ctx.EXISTS != null) } /** * Create a [[TruncateTable]] command. * * For example: * {{{ * TRUNCATE TABLE multi_part_name [PARTITION (partcol1=val1, partcol2=val2 ...)] * }}}  override def visitTruncateTable(ctx: TruncateTableContext): LogicalPlan = withOrigin(ctx) { val table = createUnresolvedTable(ctx.multipartIdentifier, \"TRUNCATE TABLE\") Option(ctx.partitionSpec).map { spec => TruncatePartition(table, UnresolvedPartitionSpec(visitNonOptionalPartitionSpec(spec))) }.getOrElse(TruncateTable(table)) } /** * A command for users to list the partition names of a table. If partition spec is specified, * partitions that match the spec are returned. Otherwise an empty result set is returned. * * This function creates a [[ShowPartitionsStatement]] logical plan * * The syntax of using this command in SQL is: * {{{ * SHOW PARTITIONS multi_part_name [partition_spec]; * }}}  override def visitShowPartitions(ctx: ShowPartitionsContext): LogicalPlan = withOrigin(ctx) { val partitionKeys = Option(ctx.partitionSpec).map { specCtx => UnresolvedPartitionSpec(visitNonOptionalPartitionSpec(specCtx), None) } ShowPartitions( createUnresolvedTable(ctx.multipartIdentifier(), \"SHOW PARTITIONS\"), partitionKeys) } /** * Create a [[RefreshTable]]. * * For example: * {{{ * REFRESH TABLE multi_part_name * }}}  override def visitRefreshTable(ctx: RefreshTableContext): LogicalPlan = withOrigin(ctx) { RefreshTable( createUnresolvedTableOrView( ctx.multipartIdentifier(), \"REFRESH TABLE\")) } /** * A command for users to list the column names for a table. * This function creates a [[ShowColumns]] logical plan. * * The syntax of using this command in SQL is: * {{{ * SHOW COLUMNS (FROM | IN) tableName=multipartIdentifier * ((FROM | IN) namespace=multipartIdentifier)? * }}}  override def visitShowColumns(ctx: ShowColumnsContext): LogicalPlan = withOrigin(ctx) { val table = createUnresolvedTableOrView(ctx.table, \"SHOW COLUMNS\") val namespace = Option(ctx.ns).map(visitMultipartIdentifier) // Use namespace only if table name doesn't specify it. If namespace is already specified // in the table name, it's checked against the given namespace after table/view is resolved. val tableWithNamespace = if (namespace.isDefined && table.multipartIdentifier.length == 1) { CurrentOrigin.withOrigin(table.origin) { table.copy(multipartIdentifier = namespace.get ++ table.multipartIdentifier) } } else { table } ShowColumns(tableWithNamespace, namespace) } /** * Create an [[RecoverPartitions]] * * For example: * {{{ * ALTER TABLE multi_part_name RECOVER PARTITIONS; * }}}  override def visitRecoverPartitions( ctx: RecoverPartitionsContext): LogicalPlan = withOrigin(ctx) { RecoverPartitions( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... RECOVER PARTITIONS\", alterTableTypeMismatchHint)) } /** * Create an [[AddPartitions]]. * * For example: * {{{ * ALTER TABLE multi_part_name ADD [IF NOT EXISTS] PARTITION spec [LOCATION 'loc1'] * ALTER VIEW multi_part_name ADD [IF NOT EXISTS] PARTITION spec * }}} * * ALTER VIEW ... ADD PARTITION ... is not supported because the concept of partitioning * is associated with physical tables  override def visitAddTablePartition( ctx: AddTablePartitionContext): LogicalPlan = withOrigin(ctx) { if (ctx.VIEW != null) { operationNotAllowed(\"ALTER VIEW ... ADD PARTITION\", ctx) } // Create partition spec to location mapping. val specsAndLocs = ctx.partitionSpecLocation.asScala.map { splCtx => val spec = visitNonOptionalPartitionSpec(splCtx.partitionSpec) val location = Option(splCtx.locationSpec).map(visitLocationSpec) UnresolvedPartitionSpec(spec, location) } AddPartitions( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... ADD PARTITION ...\", alterTableTypeMismatchHint), specsAndLocs.toSeq, ctx.EXISTS != null) } /** * Create an [[RenamePartitions]] * * For example: * {{{ * ALTER TABLE multi_part_name PARTITION spec1 RENAME TO PARTITION spec2; * }}}  override def visitRenameTablePartition( ctx: RenameTablePartitionContext): LogicalPlan = withOrigin(ctx) { RenamePartitions( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... RENAME TO PARTITION\", alterTableTypeMismatchHint), UnresolvedPartitionSpec(visitNonOptionalPartitionSpec(ctx.from)), UnresolvedPartitionSpec(visitNonOptionalPartitionSpec(ctx.to))) } /** * Create an [[DropPartitions]] * * For example: * {{{ * ALTER TABLE multi_part_name DROP [IF EXISTS] PARTITION spec1[, PARTITION spec2, ...] * [PURGE]; * ALTER VIEW view DROP [IF EXISTS] PARTITION spec1[, PARTITION spec2, ...]; * }}} * * ALTER VIEW ... DROP PARTITION ... is not supported because the concept of partitioning * is associated with physical tables  override def visitDropTablePartitions( ctx: DropTablePartitionsContext): LogicalPlan = withOrigin(ctx) { if (ctx.VIEW != null) { operationNotAllowed(\"ALTER VIEW ... DROP PARTITION\", ctx) } val partSpecs = ctx.partitionSpec.asScala.map(visitNonOptionalPartitionSpec) .map(spec => UnresolvedPartitionSpec(spec)) DropPartitions( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... DROP PARTITION ...\", alterTableTypeMismatchHint), partSpecs.toSeq, ifExists = ctx.EXISTS != null, purge = ctx.PURGE != null) } /** * Create an [[SetTableSerDeProperties]] * * For example: * {{{ * ALTER TABLE multi_part_name [PARTITION spec] SET SERDE serde_name * [WITH SERDEPROPERTIES props]; * ALTER TABLE multi_part_name [PARTITION spec] SET SERDEPROPERTIES serde_properties; * }}}  override def visitSetTableSerDe(ctx: SetTableSerDeContext): LogicalPlan = withOrigin(ctx) { SetTableSerDeProperties( createUnresolvedTable( ctx.multipartIdentifier, \"ALTER TABLE ... SET [SERDE|SERDEPROPERTIES]\", alterTableTypeMismatchHint), Option(ctx.STRING).map(string), Option(ctx.propertyList).map(visitPropertyKeyValues), // TODO a partition spec is allowed to have optional values. This is currently violated. Option(ctx.partitionSpec).map(visitNonOptionalPartitionSpec)) } /** * Alter the query of a view. This creates a [[AlterViewAs]] * * For example: * {{{ * ALTER VIEW multi_part_name AS SELECT ...; * }}}  override def visitAlterViewQuery(ctx: AlterViewQueryContext): LogicalPlan = withOrigin(ctx) { AlterViewAs( createUnresolvedView(ctx.multipartIdentifier, \"ALTER VIEW ... AS\"), originalText = source(ctx.query), query = plan(ctx.query)) } /** * Create a [[RenameTable]] command. * * For example: * {{{ * ALTER TABLE multi_part_name1 RENAME TO multi_part_name2; * ALTER VIEW multi_part_name1 RENAME TO multi_part_name2; * }}}  override def visitRenameTable(ctx: RenameTableContext): LogicalPlan = withOrigin(ctx) { val isView = ctx.VIEW != null val relationStr = if (isView) \"VIEW\" else \"TABLE\" RenameTable( createUnresolvedTableOrView(ctx.from, s\"ALTER $relationStr ... RENAME TO\"), visitMultipartIdentifier(ctx.to), isView) } /** * A command for users to list the properties for a table. If propertyKey is specified, the value * for the propertyKey is returned. If propertyKey is not specified, all the keys and their * corresponding values are returned. * The syntax of using this command in SQL is: * {{{ * SHOW TBLPROPERTIES multi_part_name[('propertyKey')]; * }}}  override def visitShowTblProperties( ctx: ShowTblPropertiesContext): LogicalPlan = withOrigin(ctx) { ShowTableProperties( createUnresolvedTableOrView(ctx.table, \"SHOW TBLPROPERTIES\"), Option(ctx.key).map(visitPropertyKey)) } /** * Create a plan for a DESCRIBE FUNCTION statement.  override def visitDescribeFunction(ctx: DescribeFunctionContext): LogicalPlan = withOrigin(ctx) { import ctx._ val functionName = if (describeFuncName.STRING() != null) { Seq(string(describeFuncName.STRING())) } else if (describeFuncName.qualifiedName() != null) { visitQualifiedName(describeFuncName.qualifiedName) } else { Seq(describeFuncName.getText) } DescribeFunction( UnresolvedFunc( functionName, \"DESCRIBE FUNCTION\", requirePersistent = false, funcTypeMismatchHint = None), EXTENDED != null) } /** * Create a plan for a SHOW FUNCTIONS command.  override def visitShowFunctions(ctx: ShowFunctionsContext): LogicalPlan = withOrigin(ctx) { val (userScope, systemScope) = Option(ctx.identifier) .map(_.getText.toLowerCase(Locale.ROOT)) match { case None | Some(\"all\") => (true, true) case Some(\"system\") => (false, true) case Some(\"user\") => (true, false) case Some(x) => throw QueryParsingErrors.showFunctionsUnsupportedError(x, ctx.identifier()) } val ns = Option(ctx.ns).map(visitMultipartIdentifier) val legacy = Option(ctx.legacy).map(visitMultipartIdentifier) val nsPlan = if (ns.isDefined) { if (legacy.isDefined) { throw QueryParsingErrors.showFunctionsInvalidPatternError(ctx.legacy.getText, ctx.legacy) } UnresolvedNamespace(ns.get) } else if (legacy.isDefined) { UnresolvedNamespace(legacy.get.dropRight(1)) } else { UnresolvedNamespace(Nil) } val pattern = Option(ctx.pattern).map(string).orElse(legacy.map(_.last)) ShowFunctions(nsPlan, userScope, systemScope, pattern) } override def visitRefreshFunction(ctx: RefreshFunctionContext): LogicalPlan = withOrigin(ctx) { val functionIdentifier = visitMultipartIdentifier(ctx.multipartIdentifier) RefreshFunction(UnresolvedFunc( functionIdentifier, \"REFRESH FUNCTION\", requirePersistent = true, funcTypeMismatchHint = None)) } override def visitCommentNamespace(ctx: CommentNamespaceContext): LogicalPlan = withOrigin(ctx) { val comment = ctx.comment.getType match { case SqlBaseParser.NULL => \"\" case _ => string(ctx.STRING) } val nameParts = visitMultipartIdentifier(ctx.multipartIdentifier) CommentOnNamespace(UnresolvedNamespace(nameParts), comment) } override def visitCommentTable(ctx: CommentTableContext): LogicalPlan = withOrigin(ctx) { val comment = ctx.comment.getType match { case SqlBaseParser.NULL => \"\" case _ => string(ctx.STRING) } CommentOnTable(createUnresolvedTable(ctx.multipartIdentifier, \"COMMENT ON TABLE\"), comment) } /** * Create an index, returning a [[CreateIndex]] logical plan. * For example: * {{{ * CREATE INDEX index_name ON [TABLE] table_name [USING index_type] (column_index_property_list) * [OPTIONS indexPropertyList] * column_index_property_list: column_name [OPTIONS(indexPropertyList)] [ , . . . ] * indexPropertyList: index_property_name [= index_property_value] [ , . . . ] * }}}  override def visitCreateIndex(ctx: CreateIndexContext): LogicalPlan = withOrigin(ctx) { val (indexName, indexType) = if (ctx.identifier.size() == 1) { (ctx.identifier(0).getText, \"\") } else { (ctx.identifier(0).getText, ctx.identifier(1).getText) } val columns = ctx.columns.multipartIdentifierProperty.asScala .map(_.multipartIdentifier).map(typedVisit[Seq[String]]).toSeq val columnsProperties = ctx.columns.multipartIdentifierProperty.asScala .map(x => (Option(x.options).map(visitPropertyKeyValues).getOrElse(Map.empty))).toSeq val options = Option(ctx.options).map(visitPropertyKeyValues).getOrElse(Map.empty) CreateIndex( createUnresolvedTable(ctx.multipartIdentifier(), \"CREATE INDEX\"), indexName, indexType, ctx.EXISTS != null, columns.map(UnresolvedFieldName(_)).zip(columnsProperties), options) } /** * Drop an index, returning a [[DropIndex]] logical plan. * For example: * {{{ * DROP INDEX [IF EXISTS] index_name ON [TABLE] table_name * }}}  override def visitDropIndex(ctx: DropIndexContext): LogicalPlan = withOrigin(ctx) { val indexName = ctx.identifier.getText DropIndex( createUnresolvedTable(ctx.multipartIdentifier(), \"DROP INDEX\"), indexName, ctx.EXISTS != null) } private def alterViewTypeMismatchHint: Option[String] = Some(\"Please use ALTER TABLE instead.\") private def alterTableTypeMismatchHint: Option[String] = Some(\"Please use ALTER VIEW instead.\") /** * Create a TimestampAdd expression.  override def visitTimestampadd(ctx: TimestampaddContext): Expression = withOrigin(ctx) { TimestampAdd(ctx.unit.getText, expression(ctx.unitsAmount), expression(ctx.timestamp)) } /** * Create a TimestampDiff expression.  override def visitTimestampdiff(ctx: TimestampdiffContext): Expression = withOrigin(ctx) { TimestampDiff(ctx.unit.getText, expression(ctx.startTimestamp), expression(ctx.endTimestamp)) } }"
          ],
          "tokens": 1125,
          "more_info": "Returned basic answer with 3 sentences, needed to be asked for full solution in .md file"
        },
        "without_mcp": {
          "answer": "# Logging and Error Reporting Architecture (Spark 3.3.x)\n\nLast updated: 2025-12-27 13:29\n\n## Overview\nApache Spark uses a layered approach for logging and error reporting that separates API usage from the concrete logging backend and provides both runtime logs and structured event data for post-mortem analysis.\n\n- Logging facade: SLF4J\n- Default backend: Log4j 2 (configurable)\n- Code-level API: org.apache.spark.internal.Logging trait\n- Runtime log streams: driver and executor logs (stdout/stderr + loggers)\n- Structured events: SparkListenerBus  event logs  Spark History Server\n- Error taxonomy: SparkException hierarchy with domain-specific exceptions (e.g., AnalysisException); many errors tagged with error classes and SQLSTATE-like info in SQL modules\n\n## Key Components\n\n1. SLF4J (Facade)\n   - Spark code references org.slf4j.Logger via the internal Logging trait, avoiding compile-time coupling to a specific backend.\n\n2. Log4j 2 (Default Backend)\n   - Configured via conf/log4j2.properties.template (copy/rename to log4j2.properties to activate custom settings).\n   - Supports appenders (console, rolling file, socket, etc.), layouts, and per-logger levels.\n\n3. Spark \"Logging\" Trait\n   - org.apache.spark.internal.Logging mixed into many classes (drivers, executors, SQL components, etc.).\n   - Provides lazily evaluated log methods (logInfo, logWarn, logError, etc.) and consistent logger naming.\n\n4. Driver and Executor Log Streams\n   - Driver: logs emitted by the applications main process.\n   - Executors: logs for task execution; surfaced by the cluster manager (YARN, Kubernetes, Standalone) via log aggregation.\n   - Both include JVM stdout/stderr and Log4j 2 logger outputs.\n\n5. Spark Listener Bus and Event Logs\n   - The SparkListenerBus publishes structured events (job start/end, stage, task metrics, SQL query execution events, etc.).\n   - When spark.eventLog.enabled=true, these are serialized to storage (spark.eventLog.dir) for offline analysis.\n   - The Spark History Server reads event logs to reconstruct UIs and timelines for completed applications.\n\n6. Exception and Error Reporting\n   - Core exception: org.apache.spark.SparkException; subsystem-specific exceptions include AnalysisException, ParseException, TaskNotSerializableException, FileAlreadyExistsException (in SQL/DataSource paths), etc.\n   - SQL and some core modules annotate errors with error classes and parameters to produce consistent, actionable messages.\n   - Failures propagate through the scheduler (DAGScheduler/TaskScheduler) with retries and backoff for transient issues.\n\n## Data Flow\n\n1) Application code calls logX methods via the Logging trait  delegated to SLF4J  implemented by Log4j 2.\n2) Logs are emitted in driver and executor JVMs per process; appenders handle output (console, files, rolling, remote).\n3) Structured runtime events are emitted on the listener bus and optionally persisted as event logs.\n4) History Server consumes event logs to present post-mortem UIs and timelines; raw logs remain available via cluster manager UIs.\n\n## Configuration and Tuning\n\n- Backend and Levels\n  - Place conf/log4j2.properties in the classpath (copy from conf/log4j2.properties.template) to customize levels and appenders.\n  - Common categories:\n    - org.apache.spark (application logs)\n    - org.apache.hadoop (I/O and filesystem)\n    - org.eclipse.jetty / org.sparkproject.jetty (web UI)\n  - Use WARN/ERROR in production to reduce noise; DEBUG/TRACE for diagnostics.\n\n- Rolling and Retention\n  - Configure Log4j 2 RollingFileAppender for both driver and executor logs to prevent disk exhaustion.\n  - For cluster managers, leverage built-in log aggregation and retention policies (e.g., YARN log aggregation, Kubernetes sidecar or container logging).\n\n- Event Logging\n  - spark.eventLog.enabled=true to enable event persistence.\n  - spark.eventLog.dir to a durable, shared storage (HDFS/S3/ADLS/GCS) for History Server.\n  - Optionally enable compression: spark.eventLog.compress=true.\n\n- Executor/Driver Specific\n  - Executor log size/rotation can be controlled by Log4j 2 file appenders in executor classpath.\n  - Driver logs depend on the deployment mode (client vs. cluster) and cluster manager.\n\n## Error Handling Semantics\n\n- Scheduler-Level\n  - Task failures trigger retries up to spark.task.maxFailures; stage retries managed by DAGScheduler.\n  - Speculative execution may mitigate stragglers but not logic errors.\n\n- Exception Propagation\n  - Fatal driver errors typically abort the application; executor errors are reported back to the driver with failure reasons.\n  - The drivers uncaught exception handler logs stack traces via the logging backend before shutdown.\n\n- Structured Errors (SQL)\n  - Many SQL errors include an error class ID and message parameters to aid triage and documentation lookup.\n\n## Observability and Integrations\n\n- Metrics (complementary to logs)\n  - Sparks metrics system can emit to sinks (CSV, JMX, Graphite, Prometheus via JMX exporter), aiding correlation with logs.\n\n- External Shipping\n  - Use Log4j 2 SocketAppender/HTTP appenders, or cluster-native log collectors (e.g., Kubernetes logging drivers, YARN log aggregation) to forward logs to ELK/OpenSearch, Splunk, or cloud-native logging stacks.\n\n## Operational Guidance\n\n- For Production\n  - Enable event logging and deploy a Spark History Server pointed to spark.eventLog.dir.\n  - Keep org.apache.spark at INFO or WARN; elevate to DEBUG temporarily during incident analysis.\n  - Configure rolling policies and retention; verify executor-classpath logging config is applied on all nodes.\n\n- During Incidents\n  - Collect: driver log, relevant executor logs, and the applications event log.\n  - Correlate timestamps between logs and event timeline (stages/tasks) in the History Server.\n  - Look for error class IDs (SQL) and root-cause exceptions; check retries/backoffs to distinguish transient vs. permanent failures.\n\n## File Locations in This Repository\n\n- Logging configuration template: conf/log4j2.properties.template\n- Source references to logging API: widespread imports of org.apache.spark.internal.Logging across core and SQL modules.\n\n## Glossary\n\n- Driver: the process running the Spark applications main function and DAG scheduling.\n- Executor: worker JVMs that run tasks.\n- Event Log: structured JSON-like event stream written by the driver for offline analysis.\n- History Server: service that renders UIs from event logs for completed applications.\n\n## References\n\n- Apache Spark Documentation: Monitoring and Instrumentation\n- Log4j 2 Documentation\n- SLF4J Documentation\n",
          "answer_time": 92,
          "tokens": 1417
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 4,
          "score_b": 3,
          "reasoning": "Answer A provides a more complete and well-structured overview of both logging and error reporting architecture. It includes specific architectural components like EventLoggingListener, event bus architecture, and shows clear error flow diagrams. Answer B has good technical detail but appears incomplete (cuts off mid-sentence), has unnecessary metadata about generation time, and focuses more on code inspection methodology rather than delivering a comprehensive architectural overview."
        },
        "mcp_vs_no_mcp": {
          "winner": "without_mcp",
          "score_a": 3,
          "score_b": 5,
          "reasoning": "Answer B provides a more comprehensive and accurate logging architecture description. While Answer A focuses primarily on error reporting with limited logging details, Answer B covers the complete logging stack (SLF4J facade, Log4j2 backend, Spark Logging trait) with proper architectural layering. Answer B also includes practical configuration guidance, data flow explanation, and production considerations that Answer A lacks. Answer B's structure is more professional with clear sections and actionable information."
        }
      }
    },
    {
      "id": "Q010",
      "question": "What are all classes that class TaskSetManager calls?",
      "category": "related_entities",
      "ground_truth_contexts": [
        " Neighbors (sorted by combined metric importance) of Node: org/apache/spark/scheduler/TaskSetManager#, with name: tasksetmanager kind: CLASS located in: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala:\n1.Node_id: org/apache/spark/scheduler/TaskInfo#, name: taskinfo, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, relations: CALL\n2.Node_id: org/apache/spark/scheduler/ShuffleMapTask#, name: shufflemaptask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, relations: CALL\n3.Node_id: org/apache/spark/scheduler/PendingTasksByLocality#, name: pendingtasksbylocality, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL\n4.Node_id: org/apache/spark/scheduler/TaskSetExcludelist#, name: tasksetexcludelist, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala, relations: CALL\n5.Node_id: org/apache/spark/util/collection/MedianHeap#, name: medianheap, kind: CLASS, uri: core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, relations: CALL\n6.Node_id: org/apache/spark/scheduler/BarrierPendingLaunchTask#, name: barrierpendinglaunchtask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL\n"
      ],
      "key_entities": [
        "org/apache/spark/scheduler/TaskInfo#",
        "org/apache/spark/scheduler/ShuffleMapTask#",
        "org/apache/spark/scheduler/PendingTasksByLocality#",
        "org/apache/spark/scheduler/TaskSetExcludelist#",
        "org/apache/spark/util/collection/MedianHeap#",
        "org/apache/spark/scheduler/BarrierPendingLaunchTask#"
      ],
      "claude_stats": {
        "answer": "Based on the code analysis, the `TaskSetManager` class calls the following classes:\n\n1. **TaskInfo** - located in `core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala`\n2. **ShuffleMapTask** - located in `core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala`\n3. **PendingTasksByLocality** - located in `core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala` (same file as TaskSetManager)\n4. **TaskSetExcludelist** - located in `core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala`\n5. **MedianHeap** - located in `core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala`\n6. **BarrierPendingLaunchTask** - located in `core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala` (same file as TaskSetManager)\n\nThese classes are sorted by their combined metric importance.",
        "prompt_time": 0.347,
        "answer_time": 14,
        "context_tokens": 457,
        "tokens": 304,
        "used_context": [
          "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/scheduler/TaskSetManager#, with name: tasksetmanager kind: CLASS located in: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala:\n1.Node_id: org/apache/spark/scheduler/TaskInfo#, name: taskinfo, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, relations: CALL\n2.Node_id: org/apache/spark/scheduler/ShuffleMapTask#, name: shufflemaptask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, relations: CALL\n3.Node_id: org/apache/spark/scheduler/PendingTasksByLocality#, name: pendingtasksbylocality, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL\n4.Node_id: org/apache/spark/scheduler/TaskSetExcludelist#, name: tasksetexcludelist, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala, relations: CALL\n5.Node_id: org/apache/spark/util/collection/MedianHeap#, name: medianheap, kind: CLASS, uri: core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, relations: CALL\n6.Node_id: org/apache/spark/scheduler/BarrierPendingLaunchTask#, name: barrierpendinglaunchtask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### Full Solution: Classes called by org.apache.spark.scheduler.TaskSetManager\n\nGenerated: 2025-12-27 11:19 (local time)\n\nThis document provides the complete answer to the question: \"What are all classes that class TaskSetManager calls?\" based on the repositorys code graph analysis of Spark 3.3.0 within this project.\n\nSource of truth: Repository code graph (CALL relations) as queried during this session.\n\n---\n\n### Result: Called classes\n\nTaskSetManager calls the following classes:\n\n1) org.apache.spark.scheduler.TaskInfo  \n   - File: core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala\n\n2) org.apache.spark.scheduler.ShuffleMapTask  \n   - File: core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala\n\n3) org.apache.spark.scheduler.PendingTasksByLocality  \n   - File: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala\n\n4) org.apache.spark.scheduler.TaskSetExcludelist  \n   - File: core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala\n\n5) org.apache.spark.util.collection.MedianHeap  \n   - File: core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala\n\n6) org.apache.spark.scheduler.BarrierPendingLaunchTask  \n   - File: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala\n\n---\n\n### Notes and context\n\n- The list reflects direct CALL relations from TaskSetManager to other classes in the project, as returned by the code graph tool used in this environment. \n- Some of the listed classes (e.g., PendingTasksByLocality, BarrierPendingLaunchTask) are defined in the same source file as TaskSetManager but are separate classes referenced/called by it.\n- This document intentionally focuses on class-level calls (not methods, fields, or inheritance relationships).\n\n---\n\n### How this was derived\n\n- We queried the repositorys static code graph for CALL relations originating from org.apache.spark.scheduler.TaskSetManager.  \n- The tool enumerated each class that TaskSetManager invokes, along with their source file locations.  \n- The six classes above were returned by the graph and constitute the complete set for this query within the current project snapshot.\n\n---\n\n### Optional next steps\n\n",
          "prompt_time": 0.272,
          "answer_time": 60,
          "context_tokens": 347,
          "used_context": [
            "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/scheduler/TaskSetManager#, with name: tasksetmanager kind: CLASS located in: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala:\n1.Node_id: org/apache/spark/scheduler/TaskInfo#, name: taskinfo, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, relations: CALL\n2.Node_id: org/apache/spark/scheduler/ShuffleMapTask#, name: shufflemaptask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, relations: CALL\n3.Node_id: org/apache/spark/scheduler/PendingTasksByLocality#, name: pendingtasksbylocality, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL\n4.Node_id: org/apache/spark/scheduler/TaskSetExcludelist#, name: tasksetexcludelist, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetExcludeList.scala, relations: CALL\n5.Node_id: org/apache/spark/util/collection/MedianHeap#, name: medianheap, kind: CLASS, uri: core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, relations: CALL\n6.Node_id: org/apache/spark/scheduler/BarrierPendingLaunchTask#, name: barrierpendinglaunchtask, kind: CLASS, uri: core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, relations: CALL"
          ],
          "tokens": 531,
          "more_info": "Returned basic list with parameters, needed to be asked for .md file"
        },
        "without_mcp": {
          "answer": "# Classes called/used by org.apache.spark.scheduler.TaskSetManager\n\nThis list enumerates classes, traits, case classes, and singleton objects that TaskSetManager directly constructs, references, extends, or calls methods on within Spark 3.3.0 core source (core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala).\n\nNotes:\n- Includes inner classes defined in the same file that TaskSetManager uses.\n- Includes Scala/Java standard library classes where they are explicitly instantiated or referenced.\n- Includes Spark classes whose methods are invoked via fields (e.g., sched, sched.dagScheduler), even if not explicitly imported.\n\nSpark Scheduler and execution-related:\n- org.apache.spark.scheduler.TaskSchedulerImpl (constructor parameter `sched`; calls methods such as newTaskId, getRacksForHosts, getExecutorsAliveOnHost, getRackForHost, CPUS_PER_TASK, backend.killTask, dagScheduler..., isExecutorAlive, hasExecutorsAliveOnHost, hasHostAliveOnRack, mapOutputTracker)\n- org.apache.spark.scheduler.Schedulable (trait implemented)\n- org.apache.spark.internal.Logging (trait mixed in)\n- org.apache.spark.scheduler.DAGScheduler (accessed via `sched.dagScheduler`: taskStarted, taskGettingResult, taskEnded, taskSetFailed, speculativeTaskSubmitted)\n- org.apache.spark.scheduler.SchedulerBackend (accessed via `sched.backend`: killTask)\n- org.apache.spark.MapOutputTracker (accessed via `sched.mapOutputTracker.getEpoch`)\n- org.apache.spark.scheduler.TaskSet (managed task set)\n- org.apache.spark.scheduler.Task (elements of `taskSet.tasks`; serialized, named, etc.)\n- org.apache.spark.scheduler.ShuffleMapTask (type-checked via isInstanceOf)\n- org.apache.spark.scheduler.TaskInfo (constructed)\n- org.apache.spark.scheduler.TaskDescription (constructed)\n- org.apache.spark.scheduler.TaskLocality (enum/ADT constants and types)\n- org.apache.spark.scheduler.TaskLocation.ExecutorCacheTaskLocation (pattern matched)\n- org.apache.spark.scheduler.TaskLocation.HDFSCacheTaskLocation (pattern matched)\n- org.apache.spark.scheduler.Pool (parent pool reference type)\n\nSpark health/exclusion and speculation:\n- org.apache.spark.scheduler.HealthTracker (optional; methods like updateExcludedForFetchFailure)\n- org.apache.spark.scheduler.TaskSetExcludelist (constructed via helper; updates on failures)\n\nSpark resources and environment:\n- org.apache.spark.SparkEnv (to obtain serializer)\n- org.apache.spark.serializer.SerializerInstance (via SparkEnv.closureSerializer.newInstance)\n- org.apache.spark.resource.ResourceInformation (used in resource assignment maps)\n- org.apache.spark.resource.ResourceProfile (obtained from ResourceProfileManager via SparkContext)\n\nSpark errors, states, reasons, and failures:\n- org.apache.spark.TaskState.TaskState (task state type)\n- org.apache.spark.errors.SparkCoreErrors (failToSerializeTaskError)\n- org.apache.spark.scheduler.FetchFailed (matched)\n- org.apache.spark.scheduler.ExceptionFailure (matched; uses accums, metricPeaks, className, description, exception)\n- org.apache.spark.scheduler.TaskKilled (matched; uses accums, metricPeaks)\n- org.apache.spark.scheduler.ExecutorLostFailure (matched)\n- org.apache.spark.scheduler.TaskFailedReason (matched; includes TaskResultLost and others)\n- org.apache.spark.TaskOutputFileAlreadyExistException (matched via classOf)\n- org.apache.spark.scheduler.ExecutorExited (matched)\n- org.apache.spark.scheduler.ExecutorKilled (matched)\n- org.apache.spark.scheduler.ExecutorDecommission (matched)\n- org.apache.spark.scheduler.ExecutorProcessLost (matched)\n\nSpark utilities and accumulators/metrics:\n- org.apache.spark.util.Utils (timeTakenMs, logging helpers)\n- org.apache.spark.util.Clock (constructor parameter; used for timing)\n- org.apache.spark.util.SystemClock (default clock)\n- org.apache.spark.util.collection.MedianHeap (constructed for successful task durations)\n- org.apache.spark.util.AccumulatorV2 (handled from results/failures)\n- org.apache.spark.util.LongAccumulator (cast/read for result size tracking)\n\nSpark internal configuration entries used:\n- org.apache.spark.internal.config (namespace via wildcard import)\n  - SPECULATION_ENABLED, SPECULATION_QUANTILE, SPECULATION_MULTIPLIER,\n    SPECULATION_TASK_DURATION_THRESHOLD\n  - EXECUTOR_DECOMMISSION_KILL_INTERVAL\n  - LOCALITY_WAIT_PROCESS, LOCALITY_WAIT_NODE, LOCALITY_WAIT_RACK\n\nStandard library and Java classes:\n- scala.collection.mutable.HashMap\n- scala.collection.mutable.HashSet\n- scala.collection.mutable.ArrayBuffer\n- scala.collection.immutable.Map\n- java.util.concurrent.ConcurrentLinkedQueue\n- java.util.concurrent.TimeUnit\n- java.nio.ByteBuffer\n- scala.util.control.NonFatal\n- java.io.NotSerializableException\n\nInner classes/case classes within TaskSetManager.scala used by TaskSetManager:\n- org.apache.spark.scheduler.TaskSetManager.PendingTasksByLocality (constructed)\n- org.apache.spark.scheduler.TaskSetManager.BarrierPendingLaunchTask (constructed and stored)\n\nThis list was compiled by examining calls, constructions (new), pattern matches, and member method invocations appearing in TaskSetManager.scala of Spark 3.3.0.",
          "answer_time": 350,
          "tokens": 1049,
          "more_info": "Answer is to precise, include Java, Scala libraries"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 4,
          "score_b": 3,
          "reasoning": "Both answers provide identical correct information, but Answer A is more concise and user-friendly while still being complete. Answer B is overly verbose with unnecessary metadata, timestamps, and lengthy explanations that don't add value to answering the core question. Answer A gets straight to the point while offering helpful follow-up engagement."
        },
        "mcp_vs_no_mcp": {
          "winner": "without_mcp",
          "score_a": 2,
          "score_b": 4,
          "reasoning": "Answer B is significantly more comprehensive and complete, listing dozens of classes across multiple categories (scheduler, health/exclusion, resources, errors, utilities) with detailed explanations of how TaskSetManager interacts with each. Answer A only identifies 6 classes without context. While Answer B's formatting is cut off at the end, it demonstrates much deeper code analysis and provides organized categorization that better addresses the user's question about 'all classes' that TaskSetManager calls."
        }
      }
    },
    {
      "id": "Q011",
      "question": "What are all related entities of method merge?",
      "category": "related_entities",
      "ground_truth_contexts": [
        "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/sql/execution/aggregate/ScalaUDAF#merge()., with name: merge. kind: METHOD located in: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala: 1.Node_id: org/apache/spark/sql/expressions/UserDefinedAggregateFunction#merge()., name: merge, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala, relations: CALL 2.Node_id: org/apache/spark/sql/execution/aggregate/ScalaUDAF#, name: scalaudaf, kind: CLASS, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala, relations: DECLARATION_BY"
      ],
      "key_entities": [
        "org/apache/spark/sql/expressions/UserDefinedAggregateFunction#merge()",
        "org/apache/spark/sql/execution/aggregate/ScalaUDAF#"
      ],
      "claude_stats": {
        "answer": "Based on the search results, here are all the entities related to the **method `merge`** (from `DifferentiableLossAggregator`):\n\n**Entities called by `merge`:**\n1. **`blas` (OBJECT)** - from `org/apache/spark/ml/linalg/BLAS`\n2. **`getblas()` (METHOD)** - from `org/apache/spark/ml/linalg/BLAS`\n\n**Parent/Container:**\n3. **`DifferentiableLossAggregator` (TRAIT)** - the trait that declares the `merge` method\n\n**Type-related:**\n4. **`Agg` (TYPE_PARAMETER)** - the type parameter used as the return type of `merge`\n\n**Entities that call `merge`:**\n5. **`calculate()` (METHOD)** - from `RDDLossFunction`, which calls this `merge` method\n\n**Summary:** The `merge` method is part of the `DifferentiableLossAggregator` trait. It calls BLAS operations (`blas` object and `getblas()` method) for linear algebra computations, returns a type based on the `Agg` type parameter, and is invoked by the `calculate()` method in `RDDLossFunction`.\n\n",
        "prompt_time": 0.427,
        "answer_time": 17,
        "context_tokens": 498,
        "tokens": 361,
        "used_context": [
          "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#merge()., with name: merge kind: METHOD located in: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala:\n1.Node_id: org/apache/spark/ml/linalg/BLAS., name: blas, kind: OBJECT, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n2.Node_id: org/apache/spark/ml/linalg/BLAS.getBLAS()., name: getblas, kind: METHOD, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n3.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#, name: differentiablelossaggregator, kind: TRAIT, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: DECLARATION_BY\n4.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#[Agg], name: agg, kind: TYPE_PARAMETER, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: RETURN_TYPE\n5.Node_id: org/apache/spark/ml/optim/loss/RDDLossFunction#calculate()., name: calculate, kind: METHOD, uri: mllib/src/main/scala/org/apache/spark/ml/optim/loss/RDDLossFunction.scala, relations: CALL_BY"
        ],
        "more_info": "Chose wrong merge method"
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "### Related entities of method `merge`\n\nTimestamp: 2025-12-27 11:24 local time\n\nBased on the repositorys code graph, the method `merge` refers to:\n\n- org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#merge()\n- File: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala\n\nAll related entities and their relation types are:\n\n- CALL  org/apache/spark/ml/linalg/BLAS (object)\n  - File: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala\n- CALL  org/apache/spark/ml/linalg/BLAS.getBLAS() (method)\n  - File: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala\n- DECLARATION_BY  org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator (trait)\n  - File: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala\n- RETURN_TYPE  Type parameter `[Agg]` of `DifferentiableLossAggregator`\n  - File: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala\n- CALL_BY  org/apache/spark/ml/optim/loss/RDDLossFunction#calculate() (method)\n  - File: mllib/src/main/scala/org/apache/spark/ml/optim/loss/RDDLossFunction.scala\n\n",
          "prompt_time": 0.577,
          "answer_time": 100,
          "context_tokens": 152,
          "used_context": [
            "astbuilder - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala - Metric value: 4408.00 dataset - CLASS - sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala - Metric value: 3683.00 analyzer - CLASS - sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala - Metric value: 3483.00 sparkcontext - CLASS - core/src/main/scala/org/apache/spark/SparkContext.scala - Metric value: 2534.00 dagscheduler - CLASS - core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala - Metric value: 2448.00"
          ],
          "tokens": 308,
          "more_info": "After asking for full answer returned better list in .md file with all the information returned by Server MCP"
        },
        "without_mcp": {
          "answer": "No answer",
          "answer_time": 700,
          "tokens": 2,
          "more_info": "Timeout on powershell script, no answer"
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 4,
          "score_b": 3,
          "reasoning": "Answer A provides clearer structure with numbered lists and bold formatting, offers helpful context about what each class does, and includes an engaging follow-up question. Answer B has excessive documentation formatting that makes it less readable for a simple question, though it does provide organized information. Both answers contain the same accurate data, but A is more user-friendly and conversational."
        },
        "mcp_vs_no_mcp": {
          "winner": "claude",
          "score_a": 5,
          "score_b": 1,
          "reasoning": "Answer A completely addresses the question by providing exactly what was asked - the 5 classes with most lines of code, including specific line counts, file locations, and helpful context about what each class does. It's well-structured with clear formatting and logical flow. Answer B provides no information at all, failing to address the question entirely."
        }
      }
    },
    {
      "id": "Q012",
      "question": "What are all related entities of class objecthashaggregateexe?",
      "category": "related_entities",
      "ground_truth_contexts": [
        "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#, with name: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec# kind: CLASS located in: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala:\n1.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics., name: sqlmetrics, kind: OBJECT, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n2.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createMetric()., name: createmetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n3.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createTimingMetric()., name: createtimingmetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n4.Node_id: org/apache/spark/sql/execution/SparkPlan#sparkContext()., name: sparkcontext, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, relations: CALL\n5.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createSizeMetric()., name: createsizemetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n6.Node_id: org/apache/spark/sql/catalyst/plans/QueryPlan#output()., name: output, kind: METHOD, uri: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala, relations: CALL\n7.Node_id: org/apache/spark/sql/execution/aggregate/BaseAggregateExec#, name: baseaggregateexec, kind: TRAIT, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala, relations: EXTEND\n8.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#toString()., name: tostring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n9.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#doExecute()., name: doexecute, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n10.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#verboseString()., name: verbosestring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n11.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#withNewChildInternal()., name: withnewchildinternal, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION, RETURN_TYPE_BY\n12.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#simpleString()., name: simplestring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n13.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#copy()., name: copy, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n14.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#`<init>`()., name: <init>, kind: CONSTRUCTOR, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION"
      ],
      "key_entities": [
        "org/apache/spark/sql/execution/metric/SQLMetrics.",
        "org/apache/spark/sql/execution/metric/SQLMetrics.createMetric().",
        "org/apache/spark/sql/execution/metric/SQLMetrics.createTimingMetric().",
        "org/apache/spark/sql/execution/SparkPlan#sparkContext().",
        "org/apache/spark/sql/execution/metric/SQLMetrics.createSizeMetric().",
        "org/apache/spark/sql/catalyst/plans/QueryPlan#output().",
        "org/apache/spark/sql/execution/aggregate/BaseAggregateExec#",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#toString().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#doExecute().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#verboseString().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#withNewChildInternal().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#simpleString().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#copy().",
        "org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#`<init>`()."
      ],
      "claude_stats": {
        "answer": "Based on the analysis of the `ObjectHashAggregateExec` class, here are all its related entities:\n\n## External Dependencies (Called/Extended)\n\n1. **SQLMetrics** (OBJECT) - Used for metrics creation\n2. **createMetric()** (METHOD) - Called from SQLMetrics for metric creation\n3. **createTimingMetric()** (METHOD) - Called from SQLMetrics for timing metrics\n4. **sparkContext()** (METHOD) - Called from SparkPlan\n5. **createSizeMetric()** (METHOD) - Called from SQLMetrics for size metrics\n6. **output()** (METHOD) - Called from QueryPlan\n7. **BaseAggregateExec** (TRAIT) - Extended by ObjectHashAggregateExec\n\n## Internal Methods (Declared in ObjectHashAggregateExec)\n\n8. **toString()** - String representation method\n9. **doExecute()** - Execution method\n10. **verboseString()** - Verbose string representation\n11. **withNewChildInternal()** - Method for creating new instance with different child (also used as return type)\n12. **simpleString()** - Simple string representation\n13. **copy()** - Copy method\n14. **<init>()** - Constructor\n\nThe class primarily interacts with Spark's SQL execution metrics system (SQLMetrics) and extends the BaseAggregateExec trait, implementing various methods for execution and string representation.\n\n",
        "prompt_time": 0.163,
        "answer_time": 18,
        "context_tokens": 1183,
        "tokens": 360,
        "used_context": [
          "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#, with name: objecthashaggregateexec kind: CLASS located in: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala:\n1.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics., name: sqlmetrics, kind: OBJECT, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n2.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createMetric()., name: createmetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n3.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createTimingMetric()., name: createtimingmetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n4.Node_id: org/apache/spark/sql/execution/SparkPlan#sparkContext()., name: sparkcontext, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, relations: CALL\n5.Node_id: org/apache/spark/sql/execution/metric/SQLMetrics.createSizeMetric()., name: createsizemetric, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, relations: CALL\n6.Node_id: org/apache/spark/sql/catalyst/plans/QueryPlan#output()., name: output, kind: METHOD, uri: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala, relations: CALL\n7.Node_id: org/apache/spark/sql/execution/aggregate/BaseAggregateExec#, name: baseaggregateexec, kind: TRAIT, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala, relations: EXTEND\n8.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#toString()., name: tostring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n9.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#doExecute()., name: doexecute, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n10.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#verboseString()., name: verbosestring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n11.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#withNewChildInternal()., name: withnewchildinternal, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION, RETURN_TYPE_BY\n12.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#simpleString()., name: simplestring, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n13.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#copy()., name: copy, kind: METHOD, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION\n14.Node_id: org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec#`<init>`()., name: <init>, kind: CONSTRUCTOR, uri: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, relations: DECLARATION"
        ],
        "more_info": ""
      },
      "junie_stats": {
        "with_mcp": {
          "answer": "No answer",
          "prompt_time": 0.143,
          "answer_time": 240,
          "context_tokens": 353,
          "used_context": [
            "Neighbors (sorted by combined metric importance) of Node: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#merge()., with name: merge kind: METHOD located in: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala:\n1.Node_id: org/apache/spark/ml/linalg/BLAS., name: blas, kind: OBJECT, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n2.Node_id: org/apache/spark/ml/linalg/BLAS.getBLAS()., name: getblas, kind: METHOD, uri: mllib-local/src/main/scala/org/apache/spark/ml/linalg/BLAS.scala, relations: CALL\n3.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#, name: differentiablelossaggregator, kind: TRAIT, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: DECLARATION_BY\n4.Node_id: org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator#[Agg], name: agg, kind: TYPE_PARAMETER, uri: mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/DifferentiableLossAggregator.scala, relations: RETURN_TYPE\n5.Node_id: org/apache/spark/ml/optim/loss/RDDLossFunction#calculate()., name: calculate, kind: METHOD, uri: mllib/src/main/scala/org/apache/spark/ml/optim/loss/RDDLossFunction.scala, relations: CALL_BY\n"
          ],
          "tokens": 0,
          "more_info": "Infinite loop in which it queried mcp and then tried to access backend file that stores questions for test (for which access was denied), so no answer"
        },
        "without_mcp": {
          "answer": "# Related entities of the class: ObjectHashAggregateExec\n\nThis document enumerates the primary code entities related to the physical operator ObjectHashAggregateExec in the Apache Spark 3.3.0 codebase. File paths are relative to the repository root.\n\n\n## Declaration (primary subject)\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala\n  - case class ObjectHashAggregateExec(...)\n  - Extends: BaseAggregateExec\n  - Purpose: Hash-based aggregate operator that supports TypedImperativeAggregate functions with arbitrary JVM object states; can fall back to sort-based aggregation.\n\n\n## Direct supertype and foundational traits/classes\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/BaseAggregateExec.scala\n  - trait BaseAggregateExec extends UnaryExecNode with AliasAwareOutputPartitioning\n  - Provides common machinery for aggregate exec nodes: inputAttributes ordering, aggregate buffer handling, distribution, and output partitioning contracts.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala\n  - SparkPlan: base of execution operators; ObjectHashAggregateExec is a SparkPlan node via BaseAggregateExec  UnaryExecNode.\n\n\n## Immediate collaborators used by ObjectHashAggregateExec\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala\n  - class ObjectAggregationIterator\n  - Used inside ObjectHashAggregateExec.doExecute to perform the core aggregation over partitions and to handle fallback to sort-based aggregation.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala\n  - class ObjectAggregationMap\n  - Hash map backing used by ObjectAggregationIterator; stores grouping keys and safe-row aggregation buffers compatible with JVM object states.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala\n  - Abstract iterator discussed in comments; documents relation with ObjectAggregationIterator and sort-based fallback path.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala\n  - ObjectHashAggregateExec is constructed here from logical aggregates; utility chooses appropriate physical aggregate (hash/object-hash/sort) based on expressions and configuration.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/ReplaceHashWithSortAgg.scala\n  - Rule that may replace HashAggregateExec or ObjectHashAggregateExec with SortAggregateExec under certain preconditions (e.g., when child output is already sorted or for memory considerations).\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/RemoveRedundantProjects.scala\n  - Mentions BaseAggregateExec: keeps necessary Project nodes for correct column ordering required by aggregate execs (including ObjectHashAggregateExec) in Final or PartialMerge modes.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala\n  - References ObjectHashAggregateExec in streaming execution; handles stateful aggregates and tagging/planning nuances for streaming queries.\n\n\n## Expressions and types commonly used with ObjectHashAggregateExec\n- sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala\n  - TypedImperativeAggregate\n  - Note: ObjectHashAggregateExec supports TypedImperativeAggregate functions (e.g., percentile_approx, typed_count) by holding JVM object states in safe buffers.\n\n- sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/\n  - Expression, NamedExpression, Attribute, AttributeSeq, AggregateExpression, MutableProjection\n  - These are imported and used directly in ObjectHashAggregateExec for schema/row handling and codegen-free projection creation.\n\n\n## Configuration keys controlling ObjectHashAggregateExec\n- sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala\n  - OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD (\"spark.sql.objectHashAggregate.sortBased.fallbackThreshold\")\n    - Integer threshold of hash map entry count to trigger fallback to sort-based aggregation.\n  - USE_OBJECT_HASH_AGG (\"spark.sql.execution.useObjectHashAggregateExec\")\n    - Boolean flag to enable/disable ObjectHashAggregateExec.\n\n\n## Metrics and instrumentation\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala\n  - SQLMetrics used: numOutputRows, aggTime, spillSize, numTasksFallBacked\n  - Exposed via longMetric and tracked in doExecute.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala\n  - Helpers to create timing/size/count metrics consumed by ObjectHashAggregateExec.\n\n\n## Planner and rule interactions\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala\n  - Selects ObjectHashAggregateExec when query involves TypedImperativeAggregate or when configuration makes it favorable.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/ReplaceHashWithSortAgg.scala\n  - May convert ObjectHashAggregateExec to SortAggregateExec based on ordering and partial/final aggregation structure.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/bucketing/DisableUnnecessaryBucketedScan.scala\n  - Interacts with BaseAggregateExec (indirectly includes ObjectHashAggregateExec) regarding distribution/partitioning assumptions.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala\n  - Mentions BaseAggregateExec to propagate empty relations; applies to ObjectHashAggregateExec when grouping keys are non-empty and children are empty.\n\n\n## Test suites and benchmarks referencing ObjectHashAggregateExec\n- sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala\n  - Ensures ObjectHashAggregateExec appears in plans when TypedImperativeAggregate functions are used; includes helper assertions and plan introspection.\n\n- sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/ObjectHashAggregateSuite.scala\n  - Comprehensive suite for ObjectHashAggregateExec behavior, enabling/disabling via SQLConf and checking fallback behavior.\n\n- sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveUDAFSuite.scala\n  - Captures cases where Hive UDAFs trigger ObjectHashAggregateExec.\n\n- sql/core/src/test/scala/org/apache/spark/sql/execution/LogicalPlanTagInSparkPlanSuite.scala\n  - Verifies logical plan tags on ObjectHashAggregateExec for final aggregation detection.\n\n- sql/core/src/test/scala/org/apache/spark/sql/execution/PlannerSuite.scala\n  - Asserts that planner can generate ObjectHashAggregateExec under suitable conditions.\n\n- sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingAggregationSuite.scala\n  - Discusses behavior where ObjectHashAggregateExec may fall back to sort-based in streaming aggregations.\n\n- Benchmarks:\n  - sql/hive/src/test/scala/org/apache/spark/sql/execution/benchmark/ObjectHashAggregateExecBenchmark.scala\n  - sql/hive/benchmarks/ObjectHashAggregateExecBenchmark-results.txt\n  - sql/hive/benchmarks/ObjectHashAggregateExecBenchmark-jdk11-results.txt\n  - sql/hive/benchmarks/ObjectHashAggregateExecBenchmark-jdk17-results.txt\n\n\n## Representative creation and usage sites\n- Construction:\n  - sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala\n    - ObjectHashAggregateExec(...) is instantiated from logical aggregates when appropriate.\n\n- Execution:\n  - sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala#doExecute\n    - Creates ObjectAggregationIterator with child.execute() iterator, grouping/aggregate expressions, attributes, and fallback threshold.\n\n- Streaming specialization:\n  - sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala\n    - Pattern-matches ObjectHashAggregateExec when a.isStreaming.\n\n\n## Related/alternative physical aggregate operators\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala\n  - The standard unsafe-row hash aggregate; may be chosen instead of ObjectHashAggregateExec when no TypedImperativeAggregate requiring object state is present.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala\n  - Sort-based aggregate; target of fallback and of the ReplaceHashWithSortAgg rule.\n\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/MergingSessionsExec.scala\n  - Another BaseAggregateExec implementation (session window merges), illustrating the family of aggregate exec nodes.\n\n\n## Quick index of key files and lines\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala\n  - Declaration and doExecute: ~lines 60145\n- sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala\n  - OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD: ~20952106\n  - USE_OBJECT_HASH_AGG: ~21082114\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala\n  - Core iterator used by ObjectHashAggregateExec\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala\n  - Hash map for aggregation state\n- sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala\n  - Chooses and constructs ObjectHashAggregateExec\n- sql/core/src/main/scala/org/apache/spark/sql/execution/ReplaceHashWithSortAgg.scala\n  - Replacement logic for ObjectHashAggregateExec/HashAggregateExec  SortAggregateExec\n- sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala\n  - Pattern match for streaming ObjectHashAggregateExec\n- sql/core/src/test/scala and sql/hive/src/test/scala\n  - Multiple suites asserting presence/behavior of ObjectHashAggregateExec\n\n\n## Selection rationale\nThe entities listed above either define ObjectHashAggregateExec, construct it from logical plans, provide the iterator/map implementations it relies on at runtime, configure its behavior via SQLConf, instrument its execution via SQLMetrics, or test/benchmark it directly. Together, they form the ecosystem necessary to understand and evolve ObjectHashAggregateExec in Spark 3.3.0.\n",
          "answer_time": 104,
          "tokens": 2159
        }
      },
      "comparisons": {
        "claude_vs_junie_mcp": {
          "winner": "claude",
          "score_a": 4,
          "score_b": 1,
          "reasoning": "Answer A provides a comprehensive, well-structured response that directly addresses the question about related entities of ObjectHashAggregateExec. It categorizes entities clearly into external dependencies and internal methods, includes specific details like method names and their purposes, and offers additional help. Answer B provides no information at all, making it completely unhelpful."
        },
        "mcp_vs_no_mcp": {
          "winner": "without_mcp",
          "score_a": 3,
          "score_b": 5,
          "reasoning": "Answer B is significantly more comprehensive and well-structured. While Answer A focuses mainly on methods and basic dependencies, Answer B provides a complete architectural view including file paths, superclasses, collaborators, expressions, and configuration keys. Answer B also explains the purpose and context of each entity, making it much more useful for understanding the complete ecosystem around ObjectHashAggregateExec. Answer A is accurate but limited in scope."
        }
      }
    }
    }
  ]
}